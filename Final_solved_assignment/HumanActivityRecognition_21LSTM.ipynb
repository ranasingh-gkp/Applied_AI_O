{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom __future__ import print_function\\nimport numpy as np\\n\\nfrom hyperopt import Trials, STATUS_OK, tpe\\nfrom keras.datasets import mnist\\nfrom keras.layers.core import Dense, Dropout, Activation\\nfrom keras.models import Sequential\\nfrom keras.utils import np_utils\\n\\nfrom hyperas import optim\\nfrom hyperas.distributions import choice, uniform\\n\\n\\ndef create_model(X_train, y_train, X_test, y_test):\\n    \\n    epochs = 8\\n    batch_size = 32\\n    timesteps = x_train.shape[1]\\n    input_dim = len(x_train[0][0])\\n    n_classes = 6\\n    \\n    model = Sequential()\\n    \\n    model.add(LSTM({{choice([64,32, 16])}}, return_sequences = True, input_shape = (timesteps, input_dim)))\\n    model.add(Dropout({{uniform(0, 1)}}))\\n    \\n    model.add(LSTM({{choice([32, 16])}}))\\n    model.add(Dropout({{uniform(0, 1)}}))\\n    \\n    model.add(Dense(n_classes, activation='sigmoid'))\\n    \\n    print(model.summary())\\n    \\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='rmsprop')\\n    \\n    result = model.fit(X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=2, validation_split=0.01)\\n    \\n    validation_acc = np.amax(result.history['val_acc']) \\n    \\n    print('Best validation acc of epoch:', validation_acc)\\n    \\n    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://maxpumperla.com/hyperas/\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "\n",
    "def create_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    epochs = 8\n",
    "    batch_size = 32\n",
    "    timesteps = x_train.shape[1]\n",
    "    input_dim = len(x_train[0][0])\n",
    "    n_classes = 6\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM({{choice([64,32, 16])}}, return_sequences = True, input_shape = (timesteps, input_dim)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(LSTM({{choice([32, 16])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
    "    \n",
    "    result = model.fit(X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=2, validation_split=0.01)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_run, best_model = optim.minimize(model=create_model, data=load_data(), algo=tpe.suggest, max_evals=4, trials=Trials())\\nX_train, y_train, X_test, y_test = load_data()\\n\\nscore = best_model.evaluate(X_test, y_test)\\n\\nprint(\\'---------------------\\')\\nprint(\\'|      Accuracy      |\\')\\nprint(\\'---------------------\\')\\nacc = np.round((score[1]*100), 2)\\nprint(str(acc)+\"%\\n\")\\n    \\nprint(\\'----------------------------------\\')\\nprint(\\'|      Best Hyper-Parameters      |\\')\\nprint(\\'----------------------------------\\')\\nprint(best_run)\\nprint(\"\\n\\n\")\\n\\ntrue_labels = [np.argmax(i)+1 for i in y_test]\\npredicted_probs = best_model.predict(X_test)\\npredicted_labels = [np.argmax(i)+1 for i in predicted_probs]\\nprint_confusionMatrix(true_labels, predicted_labels)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''best_run, best_model = optim.minimize(model=create_model, data=load_data(), algo=tpe.suggest, max_evals=4, trials=Trials())\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "score = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "print('---------------------')\n",
    "print('|      Accuracy      |')\n",
    "print('---------------------')\n",
    "acc = np.round((score[1]*100), 2)\n",
    "print(str(acc)+\"%\\n\")\n",
    "    \n",
    "print('----------------------------------')\n",
    "print('|      Best Hyper-Parameters      |')\n",
    "print('----------------------------------')\n",
    "print(best_run)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "true_labels = [np.argmax(i)+1 for i in y_test]\n",
    "predicted_probs = best_model.predict(X_test)\n",
    "predicted_labels = [np.argmax(i)+1 for i in predicted_probs]\n",
    "print_confusionMatrix(true_labels, predicted_labels)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1: 1 LSTM with 32 hidden unit , rmsprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0626 22:24:19.299101  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0626 22:24:19.305118  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0626 22:24:19.308441  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0626 22:24:19.699645  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0626 22:24:19.712710  6868 deprecation.py:506] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 22:24:23.850056  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0626 22:24:23.889933  6868 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 22:24:25.472001  6868 deprecation.py:323] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 77s 10ms/step - loss: 1.3207 - acc: 0.4329 - val_loss: 1.1474 - val_acc: 0.4706\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.9877 - acc: 0.5702 - val_loss: 1.0286 - val_acc: 0.5046\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 74s 10ms/step - loss: 0.7947 - acc: 0.6477 - val_loss: 0.7681 - val_acc: 0.6074\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 0.6952 - acc: 0.6578 - val_loss: 0.7221 - val_acc: 0.6060\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 0.6491 - acc: 0.6802 - val_loss: 0.7290 - val_acc: 0.6169\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.6374 - acc: 0.6857 - val_loss: 1.2811 - val_acc: 0.5877\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.6218 - acc: 0.7231 - val_loss: 0.6598 - val_acc: 0.7194\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.5557 - acc: 0.7578 - val_loss: 0.7062 - val_acc: 0.7333\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.5000 - acc: 0.7933 - val_loss: 0.6462 - val_acc: 0.7618\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.4565 - acc: 0.8069 - val_loss: 0.5785 - val_acc: 0.7737\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.4371 - acc: 0.8171 - val_loss: 0.5561 - val_acc: 0.7788\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.3866 - acc: 0.8437 - val_loss: 0.5607 - val_acc: 0.8273\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.3693 - acc: 0.8819 - val_loss: 0.5133 - val_acc: 0.8687\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 0.3033 - acc: 0.9106 - val_loss: 0.5015 - val_acc: 0.8799\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 0.2572 - acc: 0.9226 - val_loss: 0.4646 - val_acc: 0.8823\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2539 - acc: 0.9232 - val_loss: 0.5301 - val_acc: 0.8826\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2158 - acc: 0.9331 - val_loss: 0.5799 - val_acc: 0.8612\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2564 - acc: 0.9244 - val_loss: 0.5184 - val_acc: 0.8680\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2175 - acc: 0.9319 - val_loss: 0.5122 - val_acc: 0.8870\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2374 - acc: 0.9321 - val_loss: 0.5969 - val_acc: 0.8711\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2009 - acc: 0.9392 - val_loss: 0.6558 - val_acc: 0.8714\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2106 - acc: 0.9387 - val_loss: 0.5078 - val_acc: 0.8782\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.1870 - acc: 0.9411 - val_loss: 0.4839 - val_acc: 0.8812\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.1881 - acc: 0.9399 - val_loss: 0.6952 - val_acc: 0.8721\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.2000 - acc: 0.9391 - val_loss: 0.5929 - val_acc: 0.8856\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.1904 - acc: 0.9412 - val_loss: 0.5378 - val_acc: 0.8823\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2367 - acc: 0.9358 - val_loss: 0.5179 - val_acc: 0.8697\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.1854 - acc: 0.9468 - val_loss: 0.5344 - val_acc: 0.8931\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.2251 - acc: 0.9402 - val_loss: 0.4379 - val_acc: 0.8918\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 61s 8ms/step - loss: 0.1702 - acc: 0.9457 - val_loss: 0.4320 - val_acc: 0.8938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c74c006a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.431993\n",
      "Test Accuracy: 89.379030%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 1 layer architecture we got 89.34% accuracy and a loss of 0.43\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2: 1 LSTM with 64 hodden unit , adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model2 = Sequential()\n",
    "# Configuring the parameters\n",
    "model2.add(LSTM(64, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model2.add(Dense(n_classes, activation='sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 1.4055 - acc: 0.3928 - val_loss: 1.2250 - val_acc: 0.4326\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 1.2155 - acc: 0.4698 - val_loss: 1.2551 - val_acc: 0.5151\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 1.0704 - acc: 0.5320 - val_loss: 1.0345 - val_acc: 0.4930\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 75s 10ms/step - loss: 1.0918 - acc: 0.5222 - val_loss: 1.2427 - val_acc: 0.5134\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 74s 10ms/step - loss: 1.1104 - acc: 0.5144 - val_loss: 0.8929 - val_acc: 0.5870\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 73s 10ms/step - loss: 0.8779 - acc: 0.6009 - val_loss: 0.8901 - val_acc: 0.5657\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.8381 - acc: 0.6034 - val_loss: 0.9905 - val_acc: 0.5704\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.8075 - acc: 0.6260 - val_loss: 0.9050 - val_acc: 0.5585\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.8471 - acc: 0.6119 - val_loss: 0.8664 - val_acc: 0.5864\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.7705 - acc: 0.6383 - val_loss: 0.7785 - val_acc: 0.6115\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.7098 - acc: 0.6536 - val_loss: 0.7785 - val_acc: 0.5959\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.6937 - acc: 0.6585 - val_loss: 0.8031 - val_acc: 0.6084\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.8204 - acc: 0.6266 - val_loss: 0.7719 - val_acc: 0.6006\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 1.1724 - acc: 0.4165 - val_loss: 0.9000 - val_acc: 0.5765\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.9995 - acc: 0.4483 - val_loss: 0.9984 - val_acc: 0.4279\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 79s 11ms/step - loss: 0.9568 - acc: 0.5365 - val_loss: 0.9455 - val_acc: 0.6325\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 77s 10ms/step - loss: 0.7482 - acc: 0.6825 - val_loss: 0.6589 - val_acc: 0.7078\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.7057 - acc: 0.7252 - val_loss: 0.7243 - val_acc: 0.7068\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 78s 11ms/step - loss: 0.6257 - acc: 0.7709 - val_loss: 0.5719 - val_acc: 0.7954\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 76s 10ms/step - loss: 0.8550 - acc: 0.6759 - val_loss: 0.9165 - val_acc: 0.6522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c752728470>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model2.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.916506\n",
      "Test Accuracy: 65.218867%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model2.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is seen that when i decrease opochs value from 30 to 20, accuracy decrease drastically.\n",
    "\n",
    "loss also increase significantly in 1 LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model3: 1 LSTM with 64 hodden unit , rmsprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model3 = Sequential()\n",
    "# Configuring the parameters\n",
    "model3.add(LSTM(64, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model3.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model3.add(Dense(n_classes, activation='sigmoid'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 76s 10ms/step - loss: 1.2690 - acc: 0.4430 - val_loss: 1.0997 - val_acc: 0.5324\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.9841 - acc: 0.5680 - val_loss: 0.8458 - val_acc: 0.6491\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.8499 - acc: 0.6334 - val_loss: 1.0590 - val_acc: 0.5127\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 72s 10ms/step - loss: 0.6973 - acc: 0.7001 - val_loss: 0.7259 - val_acc: 0.7038\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 75s 10ms/step - loss: 0.5915 - acc: 0.7455 - val_loss: 0.5523 - val_acc: 0.7608\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.4976 - acc: 0.8048 - val_loss: 0.5066 - val_acc: 0.8001\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.3891 - acc: 0.8637 - val_loss: 0.4102 - val_acc: 0.8578\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.2971 - acc: 0.9040 - val_loss: 0.8256 - val_acc: 0.7893\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.2370 - acc: 0.9204 - val_loss: 0.4101 - val_acc: 0.8697\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2206 - acc: 0.9301 - val_loss: 0.5412 - val_acc: 0.8744\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2309 - acc: 0.9233 - val_loss: 0.6137 - val_acc: 0.8578\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1880 - acc: 0.9354 - val_loss: 0.5275 - val_acc: 0.8795\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.2023 - acc: 0.9335 - val_loss: 0.4805 - val_acc: 0.8826\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1785 - acc: 0.9372 - val_loss: 0.3575 - val_acc: 0.8819\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1892 - acc: 0.9378 - val_loss: 0.6877 - val_acc: 0.8541\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1690 - acc: 0.9433 - val_loss: 0.4625 - val_acc: 0.8884\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1721 - acc: 0.9400 - val_loss: 0.6151 - val_acc: 0.8711\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1677 - acc: 0.9396 - val_loss: 0.3984 - val_acc: 0.8945\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 0.1567 - acc: 0.9444 - val_loss: 0.7661 - val_acc: 0.8548\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1481 - acc: 0.9464 - val_loss: 0.4669 - val_acc: 0.8982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c75a136f60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model3.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.466954\n",
      "Test Accuracy: 89.820156%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model3.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model3.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp optimizer is suitable for this problem. accuracy again reach to 89%\n",
    "\n",
    "loss also decrease from 0.9 to 0.46\n",
    "\n",
    "let check with 2 LSTM network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model4: 2 LSTM with 32 hidden unit , adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 13,894\n",
      "Trainable params: 13,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model4 = Sequential()\n",
    "# Configuring the parameters\n",
    "model4.add(LSTM(32,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Configuring the parameters\n",
    "model4.add(LSTM(32))\n",
    "model4.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model4.add(Dense(n_classes, activation='sigmoid'))\n",
    "print(model4.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 1.2312 - acc: 0.4869 - val_loss: 0.9302 - val_acc: 0.6210\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.8336 - acc: 0.6080 - val_loss: 0.7768 - val_acc: 0.6206\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 5732s 780ms/step - loss: 0.7795 - acc: 0.6174 - val_loss: 0.7749 - val_acc: 0.6430\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.8037 - acc: 0.6049 - val_loss: 0.7544 - val_acc: 0.5969\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 1588s 216ms/step - loss: 0.7207 - acc: 0.6138 - val_loss: 0.6889 - val_acc: 0.6125\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.7499 - acc: 0.5962 - val_loss: 0.7753 - val_acc: 0.5419\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.7406 - acc: 0.5967 - val_loss: 0.7400 - val_acc: 0.6111\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 144s 20ms/step - loss: 0.7154 - acc: 0.6372 - val_loss: 0.7954 - val_acc: 0.5948\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 169s 23ms/step - loss: 0.6815 - acc: 0.6446 - val_loss: 0.7485 - val_acc: 0.6162\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 159s 22ms/step - loss: 0.6546 - acc: 0.6613 - val_loss: 0.9102 - val_acc: 0.5823\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 174s 24ms/step - loss: 0.7418 - acc: 0.6239 - val_loss: 0.9104 - val_acc: 0.4601\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 197s 27ms/step - loss: 0.7262 - acc: 0.6288 - val_loss: 0.7161 - val_acc: 0.6162\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 157s 21ms/step - loss: 0.6820 - acc: 0.6522 - val_loss: 0.7361 - val_acc: 0.6152\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 158s 22ms/step - loss: 0.6417 - acc: 0.6636 - val_loss: 0.7225 - val_acc: 0.6223\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.6037 - acc: 0.6904 - val_loss: 0.7110 - val_acc: 0.6301\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.5680 - acc: 0.7130 - val_loss: 0.6425 - val_acc: 0.6603\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 121s 17ms/step - loss: 0.5061 - acc: 0.7669 - val_loss: 0.5578 - val_acc: 0.7482\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 148s 20ms/step - loss: 0.3826 - acc: 0.8464 - val_loss: 0.6141 - val_acc: 0.8079\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 190s 26ms/step - loss: 0.3548 - acc: 0.8902 - val_loss: 0.4817 - val_acc: 0.8697\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.3011 - acc: 0.9100 - val_loss: 0.4616 - val_acc: 0.8626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c75d23aef0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model4.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.461573\n",
      "Test Accuracy: 86.257211%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model4.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model4.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model5: 2 LSTM with 64 hidden unit , adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 13,894\n",
      "Trainable params: 13,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model5 = Sequential()\n",
    "# Configuring the parameters\n",
    "model5.add(LSTM(64,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "# Configuring the parameters\n",
    "model5.add(LSTM(32))\n",
    "model5.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model5.add(Dense(n_classes, activation='sigmoid'))\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 1.4015 - acc: 0.4169 - val_loss: 1.2836 - val_acc: 0.4564\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 1.2390 - acc: 0.4407 - val_loss: 1.2479 - val_acc: 0.4164\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 1.2551 - acc: 0.4348 - val_loss: 1.4182 - val_acc: 0.3488\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.1706 - acc: 0.4937 - val_loss: 1.0925 - val_acc: 0.5643\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.9595 - acc: 0.5817 - val_loss: 1.3757 - val_acc: 0.4147\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 154s 21ms/step - loss: 1.0993 - acc: 0.5267 - val_loss: 0.9664 - val_acc: 0.5711\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 146s 20ms/step - loss: 0.9192 - acc: 0.6043 - val_loss: 1.1931 - val_acc: 0.4676\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 1.1554 - acc: 0.5014 - val_loss: 1.2935 - val_acc: 0.4113\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.9674 - acc: 0.5543 - val_loss: 0.8394 - val_acc: 0.5911\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 1.1149 - acc: 0.5064 - val_loss: 1.0591 - val_acc: 0.4662\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.9692 - acc: 0.5234 - val_loss: 0.9272 - val_acc: 0.5253\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 1.3288 - acc: 0.4253 - val_loss: 1.2216 - val_acc: 0.4201\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 203s 28ms/step - loss: 1.1431 - acc: 0.4916 - val_loss: 0.9540 - val_acc: 0.5684\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 172s 23ms/step - loss: 0.8517 - acc: 0.5885 - val_loss: 0.8382 - val_acc: 0.6037\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.9328 - acc: 0.5692 - val_loss: 0.8958 - val_acc: 0.5755\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.8119 - acc: 0.6144 - val_loss: 0.7843 - val_acc: 0.6084\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 116s 16ms/step - loss: 0.7306 - acc: 0.6371 - val_loss: 0.7516 - val_acc: 0.6138\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.6908 - acc: 0.6525 - val_loss: 0.7487 - val_acc: 0.6216\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.6674 - acc: 0.6508 - val_loss: 0.7072 - val_acc: 0.6315\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 144s 20ms/step - loss: 0.6570 - acc: 0.6555 - val_loss: 0.7070 - val_acc: 0.6254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c762bd1ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model5.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.707013\n",
      "Test Accuracy: 62.538174%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model5.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model5.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 103,352\n",
      "Trainable params: 103,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "adam = keras.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (2,) but got array with shape (6,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5b8cb481a781>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (2,) but got array with shape (6,)"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train, epochs=20, batch_size=16,validation_data=(X_test, Y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model6: 2 LSTM with 64 hidden unit , rmsprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 13,894\n",
      "Trainable params: 13,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model6 = Sequential()\n",
    "# Configuring the parameters\n",
    "model6.add(LSTM(64,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "# Configuring the parameters\n",
    "model6.add(LSTM(32))\n",
    "model6.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model6.add(Dense(n_classes, activation='sigmoid'))\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 1.2233 - acc: 0.4961 - val_loss: 0.9743 - val_acc: 0.5986\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.8135 - acc: 0.6517 - val_loss: 0.7748 - val_acc: 0.7197\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.6675 - acc: 0.7274 - val_loss: 0.6315 - val_acc: 0.7913\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.4925 - acc: 0.8443 - val_loss: 0.5876 - val_acc: 0.8005\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.3329 - acc: 0.9048 - val_loss: 0.4312 - val_acc: 0.8663\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.2665 - acc: 0.9253 - val_loss: 0.3789 - val_acc: 0.8873\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.2311 - acc: 0.9271 - val_loss: 0.4710 - val_acc: 0.8690\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 214s 29ms/step - loss: 0.1946 - acc: 0.9334 - val_loss: 0.3532 - val_acc: 0.8972\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 173s 24ms/step - loss: 0.2094 - acc: 0.9380 - val_loss: 0.3589 - val_acc: 0.9033\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1785 - acc: 0.9415 - val_loss: 0.2869 - val_acc: 0.9070\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 144s 20ms/step - loss: 0.1875 - acc: 0.9418 - val_loss: 0.3843 - val_acc: 0.9016\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 156s 21ms/step - loss: 0.1964 - acc: 0.9399 - val_loss: 0.3523 - val_acc: 0.8860\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 231s 31ms/step - loss: 0.1772 - acc: 0.9382 - val_loss: 0.4540 - val_acc: 0.8839\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 156s 21ms/step - loss: 0.1703 - acc: 0.9455 - val_loss: 0.3913 - val_acc: 0.8928\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 143s 19ms/step - loss: 0.1718 - acc: 0.9446 - val_loss: 0.3111 - val_acc: 0.9152\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 142s 19ms/step - loss: 0.1520 - acc: 0.9490 - val_loss: 0.3391 - val_acc: 0.9128\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 144s 20ms/step - loss: 0.1565 - acc: 0.9472 - val_loss: 0.4047 - val_acc: 0.9060\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1499 - acc: 0.9470 - val_loss: 0.3015 - val_acc: 0.8992\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1385 - acc: 0.9498 - val_loss: 0.3873 - val_acc: 0.9057\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 145s 20ms/step - loss: 0.1550 - acc: 0.9510 - val_loss: 0.4510 - val_acc: 0.9063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7526f1550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model6.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.451065\n",
      "Test Accuracy: 90.634544%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model6.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score: %f\" % (scores[0]))\n",
    "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_test, axis=1)])\n",
    "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model6.predict(X_test), axis=1)])\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = ['laying','sitting','standing','walking','walking_downstairs','walking_upstairs']\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(Y_true, Y_predictions), index=class_names, columns=class_names )\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")\n",
    "\n",
    "# Setting tick labels for heatmap\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=90, ha='right', fontsize=14)\n",
    "plt.ylabel('True label',size=18)\n",
    "plt.xlabel('Predicted label',size=18)\n",
    "plt.title(\"Confusion Matrix\\n\",size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "    \n",
    "    test accuracy reach upto 90.0 % with 2 LSTM Layer and rmsprop optimizer.\n",
    "    \n",
    "    loss is 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - CNN are useful to get best features and realtions between sequnce data using convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "np.random.seed(36)\n",
    "rn.seed(36)\n",
    "tf.set_random_seed(36)\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(36)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gives train and validation data \n",
    "def data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Data directory\n",
    "    DATADIR = 'UCI_HAR_Dataset'\n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "        ]\n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Utility function to load the load\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "            signals_data.append( _read_csv(filename).as_matrix()) \n",
    "\n",
    "        # Transpose is used to change the dimensionality of the output,\n",
    "        # aggregating the signals by combination of sample/timestep.\n",
    "        # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        return pd.get_dummies(y).as_matrix()\n",
    "    \n",
    "    X_train, X_val = load_signals('train'), load_signals('test')\n",
    "    Y_train, Y_val = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, Y_train, X_val,  Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Scling data\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    def __init__(self):\n",
    "        self.scale = None\n",
    "\n",
    "    def transform(self, X):\n",
    "        temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "        temp_X1 = self.scale.transform(temp_X1)\n",
    "        return temp_X1.reshape(X.shape)\n",
    "\n",
    "    def fit(self, X):\n",
    "        # remove overlaping\n",
    "        remove = int(X.shape[1] / 2)\n",
    "        temp_X = X[:, -remove:, :]\n",
    "        # flatten data\n",
    "        temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
    "        scale = StandardScaler()\n",
    "        scale.fit(temp_X)\n",
    "        self.scale = scale\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scale = scaling_tseries_data()\n",
    "Scale.fit(X_train)\n",
    "X_train_sc = Scale.transform(X_train)\n",
    "X_val_sc = Scale.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of scaled X train (7352, 128, 9)\n",
      "Shape of scaled X test (2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of scaled X train',X_train_sc.shape)\n",
    "print('Shape of scaled X test',X_val_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 00:13:24.345396 18200 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 103,556\n",
      "Trainable params: 103,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 00:14:21.040678 18200 deprecation.py:323] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 11s 2ms/step - loss: 0.4758 - acc: 0.8126 - val_loss: 0.3243 - val_acc: 0.8812\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1452 - acc: 0.9382 - val_loss: 0.2754 - val_acc: 0.9050\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.1084 - acc: 0.9513 - val_loss: 0.2803 - val_acc: 0.9080\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.1042 - acc: 0.9546 - val_loss: 0.2464 - val_acc: 0.9226\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 7s 956us/step - loss: 0.1042 - acc: 0.9580 - val_loss: 0.2154 - val_acc: 0.9287\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 7s 963us/step - loss: 0.0900 - acc: 0.9607 - val_loss: 0.2656 - val_acc: 0.9148\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 7s 966us/step - loss: 0.0768 - acc: 0.9655 - val_loss: 0.2896 - val_acc: 0.9182\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 7s 969us/step - loss: 0.0847 - acc: 0.9652 - val_loss: 0.2754 - val_acc: 0.9209\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 7s 972us/step - loss: 0.0801 - acc: 0.9659 - val_loss: 0.2628 - val_acc: 0.9179\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 7s 998us/step - loss: 0.0679 - acc: 0.9697 - val_loss: 0.4709 - val_acc: 0.8989\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0802 - acc: 0.9637 - val_loss: 0.3035 - val_acc: 0.9084\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0798 - acc: 0.9693 - val_loss: 0.3813 - val_acc: 0.9009\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0687 - acc: 0.9702 - val_loss: 0.3235 - val_acc: 0.9237\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 7s 957us/step - loss: 0.0646 - acc: 0.9717 - val_loss: 0.3353 - val_acc: 0.9213\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 7s 989us/step - loss: 0.0555 - acc: 0.9747 - val_loss: 0.3702 - val_acc: 0.9162\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0604 - acc: 0.9751 - val_loss: 0.3671 - val_acc: 0.9226\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0484 - acc: 0.9777 - val_loss: 0.3847 - val_acc: 0.9165\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 7s 953us/step - loss: 0.0532 - acc: 0.9759 - val_loss: 0.4573 - val_acc: 0.9108\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0453 - acc: 0.9791 - val_loss: 0.4245 - val_acc: 0.9050\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0521 - acc: 0.9782 - val_loss: 0.5172 - val_acc: 0.9002\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0617 - acc: 0.9776 - val_loss: 0.5056 - val_acc: 0.9114\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.0511 - acc: 0.9786 - val_loss: 0.4103 - val_acc: 0.9189\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 7s 983us/step - loss: 0.0457 - acc: 0.9810 - val_loss: 0.4204 - val_acc: 0.9223\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 7s 978us/step - loss: 0.0391 - acc: 0.9804 - val_loss: 0.5341 - val_acc: 0.9128\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 7s 940us/step - loss: 0.0386 - acc: 0.9825 - val_loss: 0.5837 - val_acc: 0.9131\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 7s 947us/step - loss: 0.0956 - acc: 0.9776 - val_loss: 0.4244 - val_acc: 0.9247\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 7s 941us/step - loss: 0.0813 - acc: 0.9769 - val_loss: 0.5321 - val_acc: 0.9155\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 7s 935us/step - loss: 0.0513 - acc: 0.9820 - val_loss: 0.4871 - val_acc: 0.9158\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 7s 992us/step - loss: 0.0456 - acc: 0.9825 - val_loss: 0.6689 - val_acc: 0.9013\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 7s 916us/step - loss: 0.0483 - acc: 0.9846 - val_loss: 0.5577 - val_acc: 0.9169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201924a7128>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sc,Y_train, epochs=30, batch_size=16,validation_data=(X_val_sc, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is giving some good score in train as well as test but it is overfitting so much. i will try some regularization in below models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2,l1\n",
    "import keras\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 00:18:05.307078 18200 nn_ops.py:4224] Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 124, 16)           1552      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 124, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 62, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                31776     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 34,422\n",
      "Trainable params: 34,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',\n",
    "                 kernel_regularizer=l2(0.1),input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='relu',kernel_regularizer=l2(0.06),kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.65))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001)\n",
    "def step_decay(epoch):\n",
    "    return float(0.001 * math.pow(0.6, math.floor((1+epoch)/10)))\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 6s 879us/step - loss: 4.0377 - acc: 0.8041 - val_loss: 1.4630 - val_acc: 0.8826\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 6s 798us/step - loss: 0.7059 - acc: 0.9204 - val_loss: 0.6384 - val_acc: 0.8778\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 6s 803us/step - loss: 0.3666 - acc: 0.9263 - val_loss: 0.4930 - val_acc: 0.8833\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 6s 786us/step - loss: 0.2947 - acc: 0.9319 - val_loss: 0.4684 - val_acc: 0.8833\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 6s 784us/step - loss: 0.2707 - acc: 0.9331 - val_loss: 0.4442 - val_acc: 0.8548\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 6s 762us/step - loss: 0.2517 - acc: 0.9357 - val_loss: 0.4358 - val_acc: 0.8806\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 6s 750us/step - loss: 0.2363 - acc: 0.9365 - val_loss: 0.4217 - val_acc: 0.9002\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 6s 771us/step - loss: 0.2227 - acc: 0.9410 - val_loss: 0.4488 - val_acc: 0.8755\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 6s 757us/step - loss: 0.2156 - acc: 0.9402 - val_loss: 0.3881 - val_acc: 0.8907\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 6s 839us/step - loss: 0.2171 - acc: 0.9421 - val_loss: 0.3362 - val_acc: 0.9053\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 6s 772us/step - loss: 0.2062 - acc: 0.9436 - val_loss: 0.3603 - val_acc: 0.8958\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 6s 758us/step - loss: 0.1990 - acc: 0.9434 - val_loss: 0.4409 - val_acc: 0.8677\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 6s 757us/step - loss: 0.1952 - acc: 0.9442 - val_loss: 0.4216 - val_acc: 0.8578\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 6s 820us/step - loss: 0.1993 - acc: 0.9448 - val_loss: 0.3608 - val_acc: 0.9002\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 6s 766us/step - loss: 0.1934 - acc: 0.9442 - val_loss: 0.3534 - val_acc: 0.8928\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 6s 813us/step - loss: 0.2052 - acc: 0.9407 - val_loss: 0.4095 - val_acc: 0.8914\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 6s 753us/step - loss: 0.1921 - acc: 0.9461 - val_loss: 0.3495 - val_acc: 0.9016\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 6s 796us/step - loss: 0.1828 - acc: 0.9483 - val_loss: 0.3776 - val_acc: 0.8945\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 6s 761us/step - loss: 0.1901 - acc: 0.9437 - val_loss: 0.3898 - val_acc: 0.8935\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 7s 924us/step - loss: 0.1868 - acc: 0.9455 - val_loss: 0.3381 - val_acc: 0.9030\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 6s 868us/step - loss: 0.1912 - acc: 0.9438 - val_loss: 0.3981 - val_acc: 0.8853\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 6s 757us/step - loss: 0.1960 - acc: 0.9456 - val_loss: 0.3111 - val_acc: 0.9186\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 6s 755us/step - loss: 0.1911 - acc: 0.9452 - val_loss: 0.3140 - val_acc: 0.9233\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 6s 783us/step - loss: 0.1746 - acc: 0.9505 - val_loss: 0.3166 - val_acc: 0.9158\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 6s 819us/step - loss: 0.1727 - acc: 0.9498 - val_loss: 0.3292 - val_acc: 0.9101\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 6s 805us/step - loss: 0.1772 - acc: 0.9470 - val_loss: 0.3532 - val_acc: 0.8924\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 6s 783us/step - loss: 0.1715 - acc: 0.9486 - val_loss: 0.3498 - val_acc: 0.8839\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 6s 755us/step - loss: 0.1780 - acc: 0.9467 - val_loss: 0.3783 - val_acc: 0.9128\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 6s 781us/step - loss: 0.1813 - acc: 0.9459 - val_loss: 0.3888 - val_acc: 0.8687\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 6s 772us/step - loss: 0.1730 - acc: 0.9509 - val_loss: 0.3534 - val_acc: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20194010a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sc,Y_train, epochs=30, batch_size=16,validation_data=(X_val_sc, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still val accuracy is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide and Conquer-Based 1D CNN Human Activity Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:  https://www.mdpi.com/1424-8220/18/4/1055/pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifying data as 2 class dynamic vs static \n",
    "##data preparation\n",
    "def data_scaled_2class():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Data directory\n",
    "    DATADIR = 'UCI_HAR_Dataset'\n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "        ]\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        def __init__(self):\n",
    "            self.scale = None\n",
    "\n",
    "        def transform(self, X):\n",
    "            temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "            temp_X1 = self.scale.transform(temp_X1)\n",
    "            return temp_X1.reshape(X.shape)\n",
    "\n",
    "        def fit(self, X):\n",
    "            # remove overlaping\n",
    "            remove = int(X.shape[1] / 2)\n",
    "            temp_X = X[:, -remove:, :]\n",
    "            # flatten data\n",
    "            temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
    "            scale = StandardScaler()\n",
    "            scale.fit(temp_X)\n",
    "            ##saving for furter usage\n",
    "            ## will use in predicton pipeline\n",
    "            pickle.dump(scale,open('Scale_2class.p','wb'))\n",
    "            self.scale = scale\n",
    "            return self\n",
    "        \n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Utility function to load the load\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "            signals_data.append( _read_csv(filename).as_matrix()) \n",
    "\n",
    "        # Transpose is used to change the dimensionality of the output,\n",
    "        # aggregating the signals by combination of sample/timestep.\n",
    "        # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        y[y<=3] = 0\n",
    "        y[y>3] = 1\n",
    "        return pd.get_dummies(y).as_matrix()\n",
    "    \n",
    "    X_train_2c, X_val_2c = load_signals('train'), load_signals('test')\n",
    "    Y_train_2c, Y_val_2c = load_y('train'), load_y('test')\n",
    "    ###Scling data\n",
    "    Scale = scaling_tseries_data()\n",
    "    Scale.fit(X_train_2c)\n",
    "    X_train_2c = Scale.transform(X_train_2c)\n",
    "    X_val_2c = Scale.transform(X_val_2c)\n",
    "    return X_train_2c, Y_train_2c, X_val_2c,  Y_val_2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:80: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 2)\n",
      "(2947, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train_2c, Y_train_2c, X_val_2c,  Y_val_2c = data_scaled_2class()\n",
    "\n",
    "print(Y_train_2c.shape)\n",
    "print(Y_val_2c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 12:34:26.365523 18200 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 103,352\n",
      "Trainable params: 103,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model for classifying data into Static and Dynamic activities\n",
    "K.clear_session()\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 11s 1ms/step - loss: 0.0572 - acc: 0.9766 - val_loss: 0.0245 - val_acc: 0.9902\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0119 - val_acc: 0.9932\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 2.6472e-04 - acc: 1.0000 - val_loss: 0.0226 - val_acc: 0.9925\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0490 - val_acc: 0.9868\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0031 - acc: 0.9995 - val_loss: 0.0784 - val_acc: 0.9895\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0073 - acc: 0.9986 - val_loss: 0.0531 - val_acc: 0.9878\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0958 - val_acc: 0.9878\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 7.8703e-04 - acc: 0.9999 - val_loss: 0.1410 - val_acc: 0.9807\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.2632e-04 - acc: 0.9999 - val_loss: 0.0441 - val_acc: 0.9902\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 7.1497e-07 - acc: 1.0000 - val_loss: 0.0447 - val_acc: 0.9898\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 2.1635e-06 - acc: 1.0000 - val_loss: 0.0427 - val_acc: 0.9905\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 8.0645e-07 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9902\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0111 - val_acc: 0.9990\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.2359e-06 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9990\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 3.1452e-06 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.7195e-07 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.6198e-07 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 2.7493e-07 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 4.4344e-07 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.4193e-07 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2019aaf2630>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(X_train_2c,Y_train_2c, epochs=20, batch_size=16,validation_data=(X_val_2c, Y_val_2c), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 1.0 test_accuracy 0.9989820156090939\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = model.evaluate(X_val_2c,Y_val_2c,verbose=0)\n",
    "_,acc_train = model.evaluate(X_train_2c,Y_train_2c,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving model\n",
    "model.save('final_model_2class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifying data into dynammic or static correctly with very hig accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificaton of Static activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data preparation\n",
    "def data_scaled_static():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Data directory\n",
    "    DATADIR = 'UCI_HAR_Dataset'\n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "        ]\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        def __init__(self):\n",
    "            self.scale = None\n",
    "\n",
    "        def transform(self, X):\n",
    "            temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "            temp_X1 = self.scale.transform(temp_X1)\n",
    "            return temp_X1.reshape(X.shape)\n",
    "\n",
    "        def fit(self, X):\n",
    "            # remove overlaping\n",
    "            remove = int(X.shape[1] / 2)\n",
    "            temp_X = X[:, -remove:, :]\n",
    "            # flatten data\n",
    "            temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
    "            scale = StandardScaler()\n",
    "            scale.fit(temp_X)\n",
    "            #for furter use at prediction pipeline\n",
    "            pickle.dump(scale,open('Scale_static.p','wb'))\n",
    "            self.scale = scale\n",
    "            return self\n",
    "        \n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Utility function to load the load\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "            signals_data.append( _read_csv(filename).as_matrix()) \n",
    "\n",
    "        # Transpose is used to change the dimensionality of the output,\n",
    "        # aggregating the signals by combination of sample/timestep.\n",
    "        # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        y_subset = y>3\n",
    "        y = y[y_subset]\n",
    "        return pd.get_dummies(y).as_matrix(),y_subset\n",
    "    \n",
    "    Y_train_s,y_train_sub = load_y('train')\n",
    "    Y_val_s,y_test_sub = load_y('test')\n",
    "    X_train_s, X_val_s = load_signals('train'), load_signals('test')\n",
    "    X_train_s = X_train_s[y_train_sub]\n",
    "    X_val_s = X_val_s[y_test_sub]\n",
    "    \n",
    "    ###Scling data\n",
    "    Scale = scaling_tseries_data()\n",
    "    Scale.fit(X_train_s)\n",
    "    X_train_s = Scale.transform(X_train_s)\n",
    "    X_val_s = Scale.transform(X_val_s)\n",
    "\n",
    "    return X_train_s, Y_train_s, X_val_s,  Y_val_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:78: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape of train data (4067, 128, 9) Y shape (4067, 3)\n",
      "X Shape of val data (1560, 128, 9) Y shape (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_s, Y_train_s, X_val_s,  Y_val_s = data_scaled_static()\n",
    "print('X Shape of train data',X_train_s.shape, 'Y shape', Y_train_s.shape)\n",
    "print('X Shape of val data',X_val_s.shape,'Y shape',Y_val_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 17:10:50.753993 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 17:10:50.755985 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 17:10:51.004517 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 17:10:51.537074 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0729 17:10:51.599426 31700 deprecation.py:506] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0729 17:10:51.603416 31700 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0729 17:10:51.693181 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 122, 64)           4096      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                38430     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 48,795\n",
      "Trainable params: 48,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:11:04.472161 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 17:11:04.489114 31700 deprecation_wrapper.py:119] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0729 17:11:05.101507 31700 deprecation.py:323] From C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/20\n",
      "4067/4067 [==============================] - 9s 2ms/step - loss: 0.4261 - acc: 0.8729 - val_loss: 0.2451 - val_acc: 0.8910\n",
      "Epoch 2/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2119 - acc: 0.9208 - val_loss: 0.2941 - val_acc: 0.8929\n",
      "Epoch 3/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2772 - acc: 0.9235 - val_loss: 0.3355 - val_acc: 0.9006\n",
      "Epoch 4/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2035 - acc: 0.9307 - val_loss: 0.2676 - val_acc: 0.9032\n",
      "Epoch 5/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1578 - acc: 0.9390 - val_loss: 0.2669 - val_acc: 0.8962\n",
      "Epoch 6/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1340 - acc: 0.9530 - val_loss: 0.2308 - val_acc: 0.9346\n",
      "Epoch 7/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1198 - acc: 0.9548 - val_loss: 0.2768 - val_acc: 0.8968\n",
      "Epoch 8/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1593 - acc: 0.9462 - val_loss: 0.2104 - val_acc: 0.9359\n",
      "Epoch 9/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1573 - acc: 0.9508 - val_loss: 0.2404 - val_acc: 0.9199\n",
      "Epoch 10/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1444 - acc: 0.9498 - val_loss: 0.2086 - val_acc: 0.9359\n",
      "Epoch 11/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1668 - acc: 0.9479 - val_loss: 0.3156 - val_acc: 0.9192\n",
      "Epoch 12/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1960 - acc: 0.9489 - val_loss: 0.2231 - val_acc: 0.9167\n",
      "Epoch 13/20\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1855 - acc: 0.9516 - val_loss: 0.2829 - val_acc: 0.8968\n",
      "Epoch 14/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2269 - acc: 0.9493 - val_loss: 0.2603 - val_acc: 0.9141\n",
      "Epoch 15/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1751 - acc: 0.9589 - val_loss: 0.2102 - val_acc: 0.9301\n",
      "Epoch 16/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1856 - acc: 0.9538 - val_loss: 0.2590 - val_acc: 0.9231\n",
      "Epoch 17/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1648 - acc: 0.9562 - val_loss: 0.2351 - val_acc: 0.9288\n",
      "Epoch 18/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1458 - acc: 0.9646 - val_loss: 0.2136 - val_acc: 0.9282\n",
      "Epoch 19/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1341 - acc: 0.9666 - val_loss: 0.2724 - val_acc: 0.9199\n",
      "Epoch 20/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1405 - acc: 0.9671 - val_loss: 0.2491 - val_acc: 0.9199\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "adam = keras.optimizers.Adam(lr=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(X_train_s,Y_train_s, epochs=20, batch_size=32,validation_data=(X_val_s, Y_val_s), verbose=1)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(X_train_s, Y_train_s, X_val_s, Y_val_s):\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session(graph=tf.get_default_graph())\n",
    "    K.set_session(sess)\n",
    "    # Initiliazing the sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters={{choice([28,32,42])}}, kernel_size={{choice([3,5,7])}},activation='relu',kernel_initializer='he_uniform',\n",
    "                 kernel_regularizer=l2({{uniform(0,3)}}),input_shape=(128,9)))\n",
    "    \n",
    "    model.add(Conv1D(filters={{choice([16,24,32])}}, kernel_size={{choice([3,5,7])}}, \n",
    "                     activation='relu',kernel_regularizer=l2({{uniform(0,2)}}),kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout({{uniform(0.45,0.7)}}))\n",
    "    model.add(MaxPooling1D(pool_size={{choice([2,3,5])}}))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([16,32,64])}}, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "    adam = keras.optimizers.Adam(lr={{uniform(0.00065,0.004)}})\n",
    "    rmsprop = keras.optimizers.RMSprop(lr={{uniform(0.00065,0.004)}})\n",
    "   \n",
    "    choiceval = {{choice(['adam', 'rmsprop'])}}\n",
    "    \n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    else:\n",
    "        optim = rmsprop\n",
    "    \n",
    "    print(model.summary())\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "    \n",
    "    result = model.fit(X_train_s, Y_train_s,\n",
    "              batch_size={{choice([16,32,64])}},\n",
    "              nb_epoch={{choice([25,30,35])}},\n",
    "              verbose=2,\n",
    "              validation_data=(X_val_s, Y_val_s))\n",
    "                       \n",
    "    score, acc = model.evaluate(X_val_s, Y_val_s, verbose=0)\n",
    "    score1, acc1 = model.evaluate(X_train_s, Y_train_s, verbose=0)\n",
    "    print('Train accuracy',acc1,'Test accuracy:', acc)\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    K.clear_session()\n",
    "    return {'loss': -acc, 'status': STATUS_OK,'train_acc':acc1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:78: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random as rn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random as rn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.base import BaseEstimator, TransformerMixin\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2, l1\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import LearningRateScheduler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random as rn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.convolutional import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.base import BaseEstimator, TransformerMixin\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.base import BaseEstimator, TransformerMixin\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.utils import eval_hyperopt_space\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'filters': hp.choice('filters', [28,32,42]),\n",
      "        'kernel_size': hp.choice('kernel_size', [3,5,7]),\n",
      "        'l2': hp.uniform('l2', 0,3),\n",
      "        'filters_1': hp.choice('filters_1', [16,24,32]),\n",
      "        'kernel_size_1': hp.choice('kernel_size_1', [3,5,7]),\n",
      "        'l2_1': hp.uniform('l2_1', 0,2),\n",
      "        'Dropout': hp.uniform('Dropout', 0.45,0.7),\n",
      "        'pool_size': hp.choice('pool_size', [2,3,5]),\n",
      "        'Dense': hp.choice('Dense', [16,32,64]),\n",
      "        'lr': hp.uniform('lr', 0.00065,0.004),\n",
      "        'lr_1': hp.uniform('lr_1', 0.00065,0.004),\n",
      "        'choiceval': hp.choice('choiceval', ['adam', 'rmsprop']),\n",
      "        'Dense_1': hp.choice('Dense_1', [16,32,64]),\n",
      "        'nb_epoch': hp.choice('nb_epoch', [25,30,35]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: Obtain the dataset from multiple files.\n",
      "   4: Returns: X_train, X_test, y_train, y_test\n",
      "   5: \"\"\"\n",
      "   6: # Data directory\n",
      "   7: DATADIR = 'UCI_HAR_Dataset'\n",
      "   8: # Raw data signals\n",
      "   9: # Signals are from Accelerometer and Gyroscope\n",
      "  10: # The signals are in x,y,z directions\n",
      "  11: # Sensor signals are filtered to have only body acceleration\n",
      "  12: # excluding the acceleration due to gravity\n",
      "  13: # Triaxial acceleration from the accelerometer is total acceleration\n",
      "  14: SIGNALS = [\n",
      "  15:     \"body_acc_x\",\n",
      "  16:     \"body_acc_y\",\n",
      "  17:     \"body_acc_z\",\n",
      "  18:     \"body_gyro_x\",\n",
      "  19:     \"body_gyro_y\",\n",
      "  20:     \"body_gyro_z\",\n",
      "  21:     \"total_acc_x\",\n",
      "  22:     \"total_acc_y\",\n",
      "  23:     \"total_acc_z\"\n",
      "  24:     ]\n",
      "  25: from sklearn.base import BaseEstimator, TransformerMixin\n",
      "  26: class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
      "  27:     from sklearn.preprocessing import StandardScaler\n",
      "  28:     def __init__(self):\n",
      "  29:         self.scale = None\n",
      "  30: \n",
      "  31:     def transform(self, X):\n",
      "  32:         temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
      "  33:         temp_X1 = self.scale.transform(temp_X1)\n",
      "  34:         return temp_X1.reshape(X.shape)\n",
      "  35: \n",
      "  36:     def fit(self, X):\n",
      "  37:         # remove overlaping\n",
      "  38:         remove = int(X.shape[1] / 2)\n",
      "  39:         temp_X = X[:, -remove:, :]\n",
      "  40:         # flatten data\n",
      "  41:         temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
      "  42:         scale = StandardScaler()\n",
      "  43:         scale.fit(temp_X)\n",
      "  44:         #for furter use at prediction pipeline\n",
      "  45:         pickle.dump(scale,open('Scale_static.p','wb'))\n",
      "  46:         self.scale = scale\n",
      "  47:         return self\n",
      "  48:     \n",
      "  49: # Utility function to read the data from csv file\n",
      "  50: def _read_csv(filename):\n",
      "  51:     return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
      "  52: \n",
      "  53: # Utility function to load the load\n",
      "  54: def load_signals(subset):\n",
      "  55:     signals_data = []\n",
      "  56: \n",
      "  57:     for signal in SIGNALS:\n",
      "  58:         filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
      "  59:         signals_data.append( _read_csv(filename).as_matrix()) \n",
      "  60: \n",
      "  61:     # Transpose is used to change the dimensionality of the output,\n",
      "  62:     # aggregating the signals by combination of sample/timestep.\n",
      "  63:     # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
      "  64:     return np.transpose(signals_data, (1, 2, 0))\n",
      "  65: \n",
      "  66: def load_y(subset):\n",
      "  67:     \"\"\"\n",
      "  68:     The objective that we are trying to predict is a integer, from 1 to 6,\n",
      "  69:     that represents a human activity. We return a binary representation of \n",
      "  70:     every sample objective as a 6 bits vector using One Hot Encoding\n",
      "  71:     (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
      "  72:     \"\"\"\n",
      "  73:     filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
      "  74:     y = _read_csv(filename)[0]\n",
      "  75:     y_subset = y>3\n",
      "  76:     y = y[y_subset]\n",
      "  77:     return pd.get_dummies(y).as_matrix(),y_subset\n",
      "  78: \n",
      "  79: Y_train_s,y_train_sub = load_y('train')\n",
      "  80: Y_val_s,y_test_sub = load_y('test')\n",
      "  81: X_train_s, X_val_s = load_signals('train'), load_signals('test')\n",
      "  82: X_train_s = X_train_s[y_train_sub]\n",
      "  83: X_val_s = X_val_s[y_test_sub]\n",
      "  84: \n",
      "  85: ###Scling data\n",
      "  86: Scale = scaling_tseries_data()\n",
      "  87: Scale.fit(X_train_s)\n",
      "  88: X_train_s = Scale.transform(X_train_s)\n",
      "  89: X_val_s = Scale.transform(X_val_s)\n",
      "  90: \n",
      "  91: \n",
      "  92: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  93: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     np.random.seed(0)\n",
      "   4:     tf.set_random_seed(0)\n",
      "   5:     sess = tf.Session(graph=tf.get_default_graph())\n",
      "   6:     K.set_session(sess)\n",
      "   7:     # Initiliazing the sequential model\n",
      "   8:     model = Sequential()\n",
      "   9:     \n",
      "  10:     model.add(Conv1D(filters=space['filters'], kernel_size=space['kernel_size'],activation='relu',kernel_initializer='he_uniform',\n",
      "  11:                  kernel_regularizer=l2(space['l2']),input_shape=(128,9)))\n",
      "  12:     \n",
      "  13:     model.add(Conv1D(filters=space['filters_1'], kernel_size=space['kernel_size_1'], \n",
      "  14:                      activation='relu',kernel_regularizer=l2(space['l2_1']),kernel_initializer='he_uniform'))\n",
      "  15:     model.add(Dropout(space['Dropout']))\n",
      "  16:     model.add(MaxPooling1D(pool_size=space['pool_size']))\n",
      "  17:     model.add(Flatten())\n",
      "  18:     model.add(Dense(space['Dense'], activation='relu'))\n",
      "  19:     model.add(Dense(3, activation='softmax'))\n",
      "  20:         \n",
      "  21:     adam = keras.optimizers.Adam(lr=space['lr'])\n",
      "  22:     rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'])\n",
      "  23:    \n",
      "  24:     choiceval = space['choiceval']\n",
      "  25:     \n",
      "  26:     if choiceval == 'adam':\n",
      "  27:         optim = adam\n",
      "  28:     else:\n",
      "  29:         optim = rmsprop\n",
      "  30:     \n",
      "  31:     print(model.summary())\n",
      "  32:         \n",
      "  33:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
      "  34:     \n",
      "  35:     result = model.fit(X_train_s, Y_train_s,\n",
      "  36:               batch_size=space['Dense_1'],\n",
      "  37:               nb_epoch=space['nb_epoch'],\n",
      "  38:               verbose=2,\n",
      "  39:               validation_data=(X_val_s, Y_val_s))\n",
      "  40:                        \n",
      "  41:     score, acc = model.evaluate(X_val_s, Y_val_s, verbose=0)\n",
      "  42:     score1, acc1 = model.evaluate(X_train_s, Y_train_s, verbose=0)\n",
      "  43:     print('Train accuracy',acc1,'Test accuracy:', acc)\n",
      "  44:     print('-------------------------------------------------------------------------------------')\n",
      "  45:     K.clear_session()\n",
      "  46:     return {'loss': -acc, 'status': STATUS_OK,'train_acc':acc1}\n",
      "  47: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:569: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  return pd.get_dummies(y).as_matrix(),y_subset\n",
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:551: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  signals_data.append( _read_csv(filename).as_matrix())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 0/120 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:16:57.232643 31700 nn_ops.py:4224] Large dropout rate: 0.649177 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           5152                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1920)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                122944                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  0%|                                                                            | 0/120 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 5s - loss: 43.0197 - acc: 0.8375 - val_loss: 5.0265 - val_acc: 0.7686                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 4s - loss: 1.3852 - acc: 0.8699 - val_loss: 0.5577 - val_acc: 0.8224                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 4s - loss: 0.3976 - acc: 0.8776 - val_loss: 0.4837 - val_acc: 0.8244                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 4s - loss: 0.3494 - acc: 0.8891 - val_loss: 0.4099 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 4s - loss: 0.3465 - acc: 0.8807 - val_loss: 0.4582 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 4s - loss: 0.3299 - acc: 0.8894 - val_loss: 0.5766 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 4s - loss: 0.3207 - acc: 0.8847 - val_loss: 0.3599 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.3307 - acc: 0.8820 - val_loss: 0.4180 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 4s - loss: 0.3048 - acc: 0.8955 - val_loss: 0.3325 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 4s - loss: 0.3101 - acc: 0.8923 - val_loss: 0.3411 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 0.3101 - acc: 0.8891 - val_loss: 0.5703 - val_acc: 0.6929                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.3047 - acc: 0.8921 - val_loss: 0.4375 - val_acc: 0.8128                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 0.3194 - acc: 0.8849 - val_loss: 0.5725 - val_acc: 0.8032                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 0.2997 - acc: 0.8938 - val_loss: 0.5899 - val_acc: 0.7071                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 4s - loss: 0.2974 - acc: 0.8943 - val_loss: 0.3327 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 0.3119 - acc: 0.8948 - val_loss: 0.3607 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 0.2960 - acc: 0.8938 - val_loss: 0.4224 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 4s - loss: 0.3091 - acc: 0.8943 - val_loss: 0.4865 - val_acc: 0.7827                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.3055 - acc: 0.8879 - val_loss: 0.3764 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.2953 - acc: 0.8989 - val_loss: 0.4365 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 4s - loss: 0.2868 - acc: 0.8940 - val_loss: 0.4622 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 4s - loss: 0.3023 - acc: 0.8908 - val_loss: 0.5237 - val_acc: 0.8000                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 4s - loss: 0.2920 - acc: 0.8933 - val_loss: 0.3378 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 4s - loss: 0.2886 - acc: 0.8938 - val_loss: 0.3924 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 4s - loss: 0.2899 - acc: 0.8935 - val_loss: 0.3217 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 4s - loss: 0.2967 - acc: 0.8960 - val_loss: 0.3465 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 5s - loss: 0.2915 - acc: 0.8999 - val_loss: 0.3595 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 5s - loss: 0.2940 - acc: 0.8955 - val_loss: 0.3597 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 5s - loss: 0.2886 - acc: 0.8982 - val_loss: 0.3877 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 5s - loss: 0.2898 - acc: 0.8960 - val_loss: 0.3469 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 5s - loss: 0.2909 - acc: 0.8938 - val_loss: 0.3322 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 0.2844 - acc: 0.9019 - val_loss: 0.3498 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 0.2933 - acc: 0.8994 - val_loss: 0.3916 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 5s - loss: 0.3088 - acc: 0.8896 - val_loss: 0.3273 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 5s - loss: 0.2785 - acc: 0.8997 - val_loss: 0.3815 - val_acc: 0.8481                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8342758790263093                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8480769230769231                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "  1%|▍                                              | 1/120 [02:31<5:01:05, 151.81s/it, best loss: -0.8480769230769231]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:19:28.953362 31700 nn_ops.py:4224] Large dropout rate: 0.601446 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           2040                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,531                                                                                                   \n",
      "Trainable params: 65,531                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  1%|▍                                              | 1/120 [02:32<5:01:05, 151.81s/it, best loss: -0.8480769230769231]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 109.6863 - acc: 0.8104 - val_loss: 27.5473 - val_acc: 0.8647                                             \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 9.9715 - acc: 0.8943 - val_loss: 2.0637 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.8451 - acc: 0.8916 - val_loss: 0.5401 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.4214 - acc: 0.8724 - val_loss: 0.5129 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3881 - acc: 0.8800 - val_loss: 0.4764 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3693 - acc: 0.8916 - val_loss: 0.5903 - val_acc: 0.8288                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.4250 - acc: 0.8675 - val_loss: 0.5323 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3723 - acc: 0.8871 - val_loss: 0.4784 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3516 - acc: 0.8898 - val_loss: 0.4687 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3407 - acc: 0.8908 - val_loss: 0.4590 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3790 - acc: 0.8830 - val_loss: 0.4581 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3399 - acc: 0.8876 - val_loss: 0.4389 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3349 - acc: 0.8960 - val_loss: 0.4396 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3382 - acc: 0.8825 - val_loss: 0.4543 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3462 - acc: 0.8891 - val_loss: 0.4228 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3484 - acc: 0.8869 - val_loss: 0.4391 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3592 - acc: 0.8884 - val_loss: 0.4289 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3580 - acc: 0.8842 - val_loss: 0.4568 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.3477 - acc: 0.8869 - val_loss: 0.4514 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3376 - acc: 0.8884 - val_loss: 0.4332 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.3715 - acc: 0.8854 - val_loss: 0.4847 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.3329 - acc: 0.8906 - val_loss: 0.4259 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3502 - acc: 0.8901 - val_loss: 0.4686 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.3185 - acc: 0.8943 - val_loss: 0.4339 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3334 - acc: 0.8879 - val_loss: 0.4164 - val_acc: 0.8724                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8883698057536268                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8724358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                20512                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 24,083                                                                                                   \n",
      "Trainable params: 24,083                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  2%|▊                                              | 2/120 [03:49<4:14:12, 129.26s/it, best loss: -0.8724358974358974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 25.4823 - acc: 0.8589 - val_loss: 13.2134 - val_acc: 0.8859                                              \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 7.7711 - acc: 0.9053 - val_loss: 4.0805 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 2.3372 - acc: 0.9127 - val_loss: 1.3796 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.7678 - acc: 0.9174 - val_loss: 0.5929 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.4014 - acc: 0.9019 - val_loss: 0.4172 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.2939 - acc: 0.9149 - val_loss: 0.4566 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 2s - loss: 0.2951 - acc: 0.9066 - val_loss: 0.4068 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.2834 - acc: 0.9058 - val_loss: 0.4252 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.2957 - acc: 0.9174 - val_loss: 0.3557 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.2702 - acc: 0.9112 - val_loss: 0.3439 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.2840 - acc: 0.9088 - val_loss: 0.3918 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.2594 - acc: 0.9169 - val_loss: 0.3469 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2712 - acc: 0.9186 - val_loss: 0.3386 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2690 - acc: 0.9120 - val_loss: 0.3656 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2495 - acc: 0.9260 - val_loss: 0.3234 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2458 - acc: 0.9194 - val_loss: 0.3582 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2577 - acc: 0.9132 - val_loss: 0.3284 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2684 - acc: 0.9127 - val_loss: 0.3127 - val_acc: 0.9013                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.2511 - acc: 0.9176 - val_loss: 0.3374 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2363 - acc: 0.9228 - val_loss: 0.3247 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2618 - acc: 0.9174 - val_loss: 0.3714 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2780 - acc: 0.9139 - val_loss: 0.3067 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.2386 - acc: 0.9260 - val_loss: 0.3196 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2539 - acc: 0.9211 - val_loss: 0.3644 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.2878 - acc: 0.9122 - val_loss: 0.3536 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2362 - acc: 0.9262 - val_loss: 0.3068 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2206 - acc: 0.9321 - val_loss: 0.3641 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2537 - acc: 0.9157 - val_loss: 0.3225 - val_acc: 0.9058                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2638 - acc: 0.9179 - val_loss: 0.3107 - val_acc: 0.9122                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2436 - acc: 0.9253 - val_loss: 0.2923 - val_acc: 0.9115                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2290 - acc: 0.9299 - val_loss: 0.3174 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2255 - acc: 0.9262 - val_loss: 0.2858 - val_acc: 0.9122                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.2417 - acc: 0.9282 - val_loss: 0.3345 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2526 - acc: 0.9176 - val_loss: 0.3177 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.2439 - acc: 0.9277 - val_loss: 0.2978 - val_acc: 0.9006                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9365625768379641                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9006410256410257                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "  2%|█▏                                             | 3/120 [05:27<3:54:08, 120.07s/it, best loss: -0.9006410256410257]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:22:24.158199 31700 nn_ops.py:4224] Large dropout rate: 0.54313 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           2328                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  2%|█▏                                             | 3/120 [05:27<3:54:08, 120.07s/it, best loss: -0.9006410256410257]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 48.6761 - acc: 0.8208 - val_loss: 36.4390 - val_acc: 0.8769                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 27.6790 - acc: 0.9058 - val_loss: 19.9079 - val_acc: 0.8609                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 14.3495 - acc: 0.9120 - val_loss: 9.7341 - val_acc: 0.8571                                               \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 6.6449 - acc: 0.9206 - val_loss: 4.3153 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 2.7299 - acc: 0.9105 - val_loss: 1.6968 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 1.0360 - acc: 0.9110 - val_loss: 0.7380 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.4834 - acc: 0.9090 - val_loss: 0.4973 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3487 - acc: 0.9112 - val_loss: 0.4933 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3156 - acc: 0.9127 - val_loss: 0.4125 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2978 - acc: 0.9066 - val_loss: 0.3577 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2773 - acc: 0.9083 - val_loss: 0.3541 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2727 - acc: 0.9105 - val_loss: 0.3721 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2648 - acc: 0.9107 - val_loss: 0.3509 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2559 - acc: 0.9152 - val_loss: 0.4069 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2559 - acc: 0.9142 - val_loss: 0.4433 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2511 - acc: 0.9120 - val_loss: 0.3352 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2503 - acc: 0.9171 - val_loss: 0.3670 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2519 - acc: 0.9144 - val_loss: 0.3452 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2458 - acc: 0.9139 - val_loss: 0.3105 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2405 - acc: 0.9206 - val_loss: 0.3204 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2364 - acc: 0.9186 - val_loss: 0.3269 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2327 - acc: 0.9176 - val_loss: 0.3236 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2400 - acc: 0.9110 - val_loss: 0.3035 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2344 - acc: 0.9184 - val_loss: 0.3055 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2334 - acc: 0.9122 - val_loss: 0.3084 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2323 - acc: 0.9164 - val_loss: 0.3305 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2278 - acc: 0.9201 - val_loss: 0.2772 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2283 - acc: 0.9235 - val_loss: 0.3108 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2268 - acc: 0.9277 - val_loss: 0.2817 - val_acc: 0.9135                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2253 - acc: 0.9250 - val_loss: 0.2875 - val_acc: 0.9115                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9178755839685272                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9115384615384615                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "  3%|█▌                                             | 4/120 [06:53<3:32:38, 109.99s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:23:50.607777 31700 nn_ops.py:4224] Large dropout rate: 0.541898 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           4720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 624)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                20000                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 26,751                                                                                                   \n",
      "Trainable params: 26,751                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  3%|█▌                                             | 4/120 [06:54<3:32:38, 109.99s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 5s - loss: 17.8030 - acc: 0.8626 - val_loss: 0.6377 - val_acc: 0.8532                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 0.4010 - acc: 0.8810 - val_loss: 0.4033 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 5s - loss: 0.3270 - acc: 0.8844 - val_loss: 0.4515 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 5s - loss: 0.3053 - acc: 0.8953 - val_loss: 0.3437 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 5s - loss: 0.2984 - acc: 0.8935 - val_loss: 0.3630 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 5s - loss: 0.3025 - acc: 0.8896 - val_loss: 0.3932 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 5s - loss: 0.2923 - acc: 0.8921 - val_loss: 0.3435 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.2907 - acc: 0.8911 - val_loss: 0.3507 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.2949 - acc: 0.8985 - val_loss: 0.3770 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 5s - loss: 0.2890 - acc: 0.8943 - val_loss: 0.3976 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.2796 - acc: 0.9002 - val_loss: 0.4811 - val_acc: 0.7115                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 5s - loss: 0.2928 - acc: 0.8879 - val_loss: 0.3368 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 5s - loss: 0.2894 - acc: 0.8898 - val_loss: 0.3515 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 5s - loss: 0.2795 - acc: 0.8953 - val_loss: 0.3993 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 5s - loss: 0.2883 - acc: 0.8970 - val_loss: 0.3347 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 5s - loss: 0.2767 - acc: 0.8955 - val_loss: 0.3321 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 5s - loss: 0.2768 - acc: 0.8943 - val_loss: 0.3152 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 5s - loss: 0.2813 - acc: 0.8975 - val_loss: 0.3727 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 5s - loss: 0.2813 - acc: 0.8972 - val_loss: 0.3645 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 5s - loss: 0.2740 - acc: 0.8945 - val_loss: 0.5590 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 5s - loss: 0.2662 - acc: 0.8982 - val_loss: 0.3414 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 5s - loss: 0.2809 - acc: 0.8980 - val_loss: 0.3602 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 5s - loss: 0.2813 - acc: 0.8894 - val_loss: 0.3263 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 4s - loss: 0.2768 - acc: 0.8962 - val_loss: 0.3412 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.2741 - acc: 0.8997 - val_loss: 0.3222 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 5s - loss: 0.2792 - acc: 0.9007 - val_loss: 0.3425 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 5s - loss: 0.2704 - acc: 0.9051 - val_loss: 0.3406 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.2734 - acc: 0.9039 - val_loss: 0.3724 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.2817 - acc: 0.9034 - val_loss: 0.3411 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.2835 - acc: 0.9036 - val_loss: 0.3111 - val_acc: 0.8840                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9181214654536514                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8839743589743589                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           7080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 936)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                29984                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 39,095                                                                                                   \n",
      "Trainable params: 39,095                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  4%|█▉                                             | 5/120 [09:15<3:48:47, 119.37s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 5s - loss: 42.4149 - acc: 0.8372 - val_loss: 0.6089 - val_acc: 0.7994                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 5s - loss: 0.4258 - acc: 0.8640 - val_loss: 0.4936 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 5s - loss: 0.3771 - acc: 0.8729 - val_loss: 0.5783 - val_acc: 0.7974                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 5s - loss: 0.3497 - acc: 0.8839 - val_loss: 0.3857 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 5s - loss: 0.3568 - acc: 0.8810 - val_loss: 0.4221 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 5s - loss: 0.3484 - acc: 0.8795 - val_loss: 0.4239 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 5s - loss: 0.3401 - acc: 0.8835 - val_loss: 0.3829 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 5s - loss: 0.3590 - acc: 0.8835 - val_loss: 0.4059 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 6s - loss: 0.3570 - acc: 0.8849 - val_loss: 0.3751 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 6s - loss: 0.3589 - acc: 0.8780 - val_loss: 0.5003 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 5s - loss: 0.3432 - acc: 0.8869 - val_loss: 0.5274 - val_acc: 0.6981                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 6s - loss: 0.3481 - acc: 0.8773 - val_loss: 0.3986 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 6s - loss: 0.3410 - acc: 0.8803 - val_loss: 0.4222 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 6s - loss: 0.3530 - acc: 0.8822 - val_loss: 0.4183 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 6s - loss: 0.3440 - acc: 0.8864 - val_loss: 0.3599 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 6s - loss: 0.3428 - acc: 0.8871 - val_loss: 0.3770 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 5s - loss: 0.3426 - acc: 0.8835 - val_loss: 0.5000 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 5s - loss: 0.3536 - acc: 0.8822 - val_loss: 0.3946 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 5s - loss: 0.3508 - acc: 0.8864 - val_loss: 0.4223 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 5s - loss: 0.3446 - acc: 0.8869 - val_loss: 0.5772 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 5s - loss: 0.3354 - acc: 0.8862 - val_loss: 0.3423 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 5s - loss: 0.3438 - acc: 0.8876 - val_loss: 0.4020 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 5s - loss: 0.3511 - acc: 0.8820 - val_loss: 0.3981 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 5s - loss: 0.3367 - acc: 0.8881 - val_loss: 0.3479 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 5s - loss: 0.3577 - acc: 0.8798 - val_loss: 0.4209 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 6s - loss: 0.3376 - acc: 0.8943 - val_loss: 0.4888 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 8s - loss: 0.3471 - acc: 0.8901 - val_loss: 0.4119 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 6s - loss: 0.3371 - acc: 0.8906 - val_loss: 0.4464 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 7s - loss: 0.3386 - acc: 0.8933 - val_loss: 0.4049 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 7s - loss: 0.3389 - acc: 0.8874 - val_loss: 0.3493 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 7s - loss: 0.3502 - acc: 0.8896 - val_loss: 0.3676 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 6s - loss: 0.3329 - acc: 0.8881 - val_loss: 0.3480 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 7s - loss: 0.3296 - acc: 0.8930 - val_loss: 0.5664 - val_acc: 0.7891                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 6s - loss: 0.3355 - acc: 0.8898 - val_loss: 0.4310 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 7s - loss: 0.3380 - acc: 0.8876 - val_loss: 0.6392 - val_acc: 0.6949                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.682075239734448                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.6948717948717948                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 32)           4512                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1248)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                79936                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 86,435                                                                                                   \n",
      "Trainable params: 86,435                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  5%|██▎                                            | 6/120 [12:40<4:35:31, 145.01s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 7s - loss: 6.9787 - acc: 0.8203 - val_loss: 0.7538 - val_acc: 0.7526                                                \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 6s - loss: 0.4785 - acc: 0.8562 - val_loss: 0.5292 - val_acc: 0.8224                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 6s - loss: 0.4474 - acc: 0.8662 - val_loss: 0.5782 - val_acc: 0.8026                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 5s - loss: 0.4595 - acc: 0.8680 - val_loss: 0.4229 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 6s - loss: 0.5014 - acc: 0.8522 - val_loss: 0.5067 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 6s - loss: 0.9363 - acc: 0.8212 - val_loss: 3.6236 - val_acc: 0.7538                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 6s - loss: 4.1047 - acc: 0.7258 - val_loss: 5.6554 - val_acc: 0.6564                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 5s - loss: 5.7112 - acc: 0.6452 - val_loss: 5.6892 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 5s - loss: 8.7303 - acc: 0.4608 - val_loss: 11.0691 - val_acc: 0.3147                                               \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 6s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 6s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 6s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 5s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 4s - loss: 11.0456 - acc: 0.3162 - val_loss: 11.0692 - val_acc: 0.3147                                              \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.31620358986968283                                                                                                    \n",
      "Test accuracy:                                                                                                         \n",
      "0.31474358974358974                                                                                                    \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 42)           1176                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           4064                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1984)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                31760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 37,051                                                                                                   \n",
      "Trainable params: 37,051                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  6%|██▋                                            | 7/120 [15:37<4:51:18, 154.68s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 19.2036 - acc: 0.8353 - val_loss: 1.0626 - val_acc: 0.7949                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 0.4920 - acc: 0.8645 - val_loss: 0.4499 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.3680 - acc: 0.8771 - val_loss: 0.4198 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.3597 - acc: 0.8807 - val_loss: 0.3896 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3443 - acc: 0.8854 - val_loss: 0.4145 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3400 - acc: 0.8812 - val_loss: 0.4463 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3182 - acc: 0.8859 - val_loss: 0.3547 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3211 - acc: 0.8835 - val_loss: 0.4435 - val_acc: 0.8224                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3164 - acc: 0.8921 - val_loss: 0.3661 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 4s - loss: 0.3221 - acc: 0.8876 - val_loss: 0.3783 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3131 - acc: 0.8918 - val_loss: 0.7093 - val_acc: 0.6718                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3186 - acc: 0.8913 - val_loss: 0.4831 - val_acc: 0.7955                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3234 - acc: 0.8849 - val_loss: 0.4063 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3133 - acc: 0.8908 - val_loss: 0.5390 - val_acc: 0.7186                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3240 - acc: 0.8933 - val_loss: 0.3518 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3247 - acc: 0.8903 - val_loss: 0.3613 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3153 - acc: 0.8844 - val_loss: 0.3409 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3088 - acc: 0.8950 - val_loss: 0.5507 - val_acc: 0.7218                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.3163 - acc: 0.8876 - val_loss: 0.3373 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3015 - acc: 0.8960 - val_loss: 0.3536 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.3086 - acc: 0.8908 - val_loss: 0.3463 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2977 - acc: 0.8972 - val_loss: 0.4084 - val_acc: 0.8301                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3127 - acc: 0.8894 - val_loss: 0.3407 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.3143 - acc: 0.8898 - val_loss: 0.4689 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3054 - acc: 0.8921 - val_loss: 0.3576 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9053356282271945                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           2256                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                31264                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 34,403                                                                                                   \n",
      "Trainable params: 34,403                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  7%|███▏                                           | 8/120 [17:04<4:11:04, 134.51s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 126.9816 - acc: 0.8163 - val_loss: 71.3691 - val_acc: 0.8718                                             \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 45.1798 - acc: 0.8948 - val_loss: 26.1050 - val_acc: 0.8731                                              \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 16.3642 - acc: 0.8987 - val_loss: 9.3048 - val_acc: 0.8667                                               \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 5.6712 - acc: 0.8945 - val_loss: 3.1859 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 1.9171 - acc: 0.8957 - val_loss: 1.1929 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.7583 - acc: 0.8957 - val_loss: 0.6920 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.4548 - acc: 0.8896 - val_loss: 0.5052 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.4091 - acc: 0.8773 - val_loss: 0.5180 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3583 - acc: 0.8948 - val_loss: 0.4529 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3582 - acc: 0.8918 - val_loss: 0.4494 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3474 - acc: 0.8925 - val_loss: 0.4668 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3549 - acc: 0.8842 - val_loss: 0.4117 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3310 - acc: 0.8965 - val_loss: 0.4575 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3183 - acc: 0.9009 - val_loss: 0.5020 - val_acc: 0.7494                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3279 - acc: 0.8911 - val_loss: 0.4209 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3176 - acc: 0.8945 - val_loss: 0.4135 - val_acc: 0.8423                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.3132 - acc: 0.8940 - val_loss: 0.3956 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3106 - acc: 0.8960 - val_loss: 0.3729 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3049 - acc: 0.9012 - val_loss: 0.4273 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2986 - acc: 0.9004 - val_loss: 0.3975 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2999 - acc: 0.8989 - val_loss: 0.4066 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2982 - acc: 0.9004 - val_loss: 0.3808 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3071 - acc: 0.8921 - val_loss: 0.3700 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2998 - acc: 0.8960 - val_loss: 0.3959 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3053 - acc: 0.8967 - val_loss: 0.3800 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2840 - acc: 0.9026 - val_loss: 0.3850 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2803 - acc: 0.9044 - val_loss: 0.3839 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2933 - acc: 0.8989 - val_loss: 0.3499 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2778 - acc: 0.9041 - val_loss: 0.3680 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2868 - acc: 0.8982 - val_loss: 0.3458 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2709 - acc: 0.9002 - val_loss: 0.3445 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.3073 - acc: 0.8989 - val_loss: 0.3525 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.2772 - acc: 0.9063 - val_loss: 0.3453 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2776 - acc: 0.9016 - val_loss: 0.3394 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.2802 - acc: 0.9024 - val_loss: 0.3405 - val_acc: 0.8846                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8979591836734694                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8846153846153846                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           5064                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 576)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                18464                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 25,559                                                                                                   \n",
      "Trainable params: 25,559                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  8%|███▌                                           | 9/120 [18:43<3:48:54, 123.74s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 5s - loss: 25.7404 - acc: 0.7920 - val_loss: 0.6911 - val_acc: 0.8051                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 5s - loss: 0.4768 - acc: 0.8567 - val_loss: 0.6349 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 4s - loss: 0.4611 - acc: 0.8645 - val_loss: 0.5875 - val_acc: 0.7994                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 4s - loss: 0.4534 - acc: 0.8689 - val_loss: 0.5115 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 4s - loss: 0.4360 - acc: 0.8692 - val_loss: 0.4768 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 4s - loss: 0.4248 - acc: 0.8667 - val_loss: 0.5577 - val_acc: 0.7897                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 4s - loss: 0.4151 - acc: 0.8712 - val_loss: 0.4837 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.4169 - acc: 0.8773 - val_loss: 0.5245 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 4s - loss: 0.4147 - acc: 0.8731 - val_loss: 0.4853 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 4s - loss: 0.4253 - acc: 0.8721 - val_loss: 0.4841 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 0.4192 - acc: 0.8773 - val_loss: 0.7769 - val_acc: 0.6558                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.4148 - acc: 0.8709 - val_loss: 0.5924 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 0.4148 - acc: 0.8724 - val_loss: 0.5892 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 0.4064 - acc: 0.8741 - val_loss: 0.7698 - val_acc: 0.6647                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 4s - loss: 0.4063 - acc: 0.8805 - val_loss: 0.5200 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 0.4229 - acc: 0.8761 - val_loss: 0.4683 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 0.4008 - acc: 0.8714 - val_loss: 0.5500 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 4s - loss: 0.3930 - acc: 0.8820 - val_loss: 0.4734 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.4040 - acc: 0.8803 - val_loss: 0.5043 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.3965 - acc: 0.8748 - val_loss: 0.6455 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 4s - loss: 0.4184 - acc: 0.8756 - val_loss: 0.4771 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 4s - loss: 0.4190 - acc: 0.8748 - val_loss: 0.7701 - val_acc: 0.6051                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 4s - loss: 0.4077 - acc: 0.8739 - val_loss: 0.4680 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 5s - loss: 0.4031 - acc: 0.8766 - val_loss: 0.5400 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 7s - loss: 0.4053 - acc: 0.8778 - val_loss: 0.5987 - val_acc: 0.7147                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 5s - loss: 0.4016 - acc: 0.8734 - val_loss: 0.4988 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 4s - loss: 0.4141 - acc: 0.8776 - val_loss: 0.6631 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 4s - loss: 0.4049 - acc: 0.8854 - val_loss: 0.9067 - val_acc: 0.6500                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 4s - loss: 0.4115 - acc: 0.8751 - val_loss: 0.5393 - val_acc: 0.7865                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 4s - loss: 0.4059 - acc: 0.8748 - val_loss: 0.6636 - val_acc: 0.7981                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 4s - loss: 0.4046 - acc: 0.8763 - val_loss: 0.5753 - val_acc: 0.8224                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 0.4057 - acc: 0.8771 - val_loss: 0.6307 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 0.3913 - acc: 0.8812 - val_loss: 0.9600 - val_acc: 0.6378                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 4s - loss: 0.3984 - acc: 0.8810 - val_loss: 0.7561 - val_acc: 0.6231                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 4s - loss: 0.4061 - acc: 0.8795 - val_loss: 1.0400 - val_acc: 0.6212                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.5928202606343742                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.6211538461538462                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           4064                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 768)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                12304                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 18,351                                                                                                   \n",
      "Trainable params: 18,351                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  8%|███▊                                          | 10/120 [21:19<4:04:44, 133.49s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 5s - loss: 44.1407 - acc: 0.8375 - val_loss: 0.7251 - val_acc: 0.8096                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 4s - loss: 0.4219 - acc: 0.8699 - val_loss: 0.5141 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 5s - loss: 0.3516 - acc: 0.8805 - val_loss: 0.4833 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 5s - loss: 0.3256 - acc: 0.8886 - val_loss: 0.4502 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 6s - loss: 0.3197 - acc: 0.8879 - val_loss: 0.4327 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 5s - loss: 0.3187 - acc: 0.8916 - val_loss: 0.4507 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 5s - loss: 0.3112 - acc: 0.8896 - val_loss: 0.3998 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 4s - loss: 0.3163 - acc: 0.8913 - val_loss: 0.4856 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 4s - loss: 0.3217 - acc: 0.8923 - val_loss: 0.4286 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 5s - loss: 0.3134 - acc: 0.8871 - val_loss: 0.4103 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 5s - loss: 0.3072 - acc: 0.8948 - val_loss: 0.4716 - val_acc: 0.7391                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 5s - loss: 0.3086 - acc: 0.8864 - val_loss: 0.3959 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 5s - loss: 0.3080 - acc: 0.8879 - val_loss: 0.4072 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 4s - loss: 0.3047 - acc: 0.8918 - val_loss: 0.4712 - val_acc: 0.7917                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 4s - loss: 0.3030 - acc: 0.8943 - val_loss: 0.3951 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 4s - loss: 0.3030 - acc: 0.8948 - val_loss: 0.3892 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 4s - loss: 0.3143 - acc: 0.8874 - val_loss: 0.3997 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 4s - loss: 0.3021 - acc: 0.8948 - val_loss: 0.4400 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 5s - loss: 0.3027 - acc: 0.8938 - val_loss: 0.3798 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 4s - loss: 0.2988 - acc: 0.8896 - val_loss: 0.4180 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 4s - loss: 0.2926 - acc: 0.8943 - val_loss: 0.3991 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 4s - loss: 0.2986 - acc: 0.8982 - val_loss: 0.6946 - val_acc: 0.6750                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 4s - loss: 0.3160 - acc: 0.8916 - val_loss: 0.3783 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 4s - loss: 0.3008 - acc: 0.8977 - val_loss: 0.5210 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 4s - loss: 0.3009 - acc: 0.8925 - val_loss: 0.3671 - val_acc: 0.8795                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9171379395131547                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8794871794871795                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 42)           1176                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           4720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                20512                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 26,507                                                                                                   \n",
      "Trainable params: 26,507                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      "  9%|████▏                                         | 11/120 [23:10<3:50:12, 126.72s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 5s - loss: 23.7208 - acc: 0.8547 - val_loss: 4.3612 - val_acc: 0.8756                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 4s - loss: 1.4831 - acc: 0.8980 - val_loss: 0.5835 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 4s - loss: 0.3659 - acc: 0.8889 - val_loss: 0.4304 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 4s - loss: 0.3536 - acc: 0.8862 - val_loss: 0.4729 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 4s - loss: 0.3229 - acc: 0.8945 - val_loss: 0.4811 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 4s - loss: 0.3125 - acc: 0.9007 - val_loss: 0.4162 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 4s - loss: 0.2886 - acc: 0.8994 - val_loss: 0.3699 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.2981 - acc: 0.8992 - val_loss: 0.4148 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 4s - loss: 0.2933 - acc: 0.8992 - val_loss: 0.3667 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 5s - loss: 0.2914 - acc: 0.9024 - val_loss: 0.3667 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 0.2760 - acc: 0.8987 - val_loss: 0.3882 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.2975 - acc: 0.8923 - val_loss: 0.3842 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 0.2693 - acc: 0.9004 - val_loss: 0.3561 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 0.2783 - acc: 0.9061 - val_loss: 0.4090 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 4s - loss: 0.2878 - acc: 0.9048 - val_loss: 0.3847 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 0.2814 - acc: 0.8977 - val_loss: 0.4106 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 0.2856 - acc: 0.8977 - val_loss: 0.3617 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 4s - loss: 0.2669 - acc: 0.9016 - val_loss: 0.3608 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.2818 - acc: 0.8953 - val_loss: 0.3745 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.2712 - acc: 0.9019 - val_loss: 0.3459 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 4s - loss: 0.2645 - acc: 0.9048 - val_loss: 0.3664 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 4s - loss: 0.2703 - acc: 0.8970 - val_loss: 0.3577 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 4s - loss: 0.2749 - acc: 0.8972 - val_loss: 0.3539 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 4s - loss: 0.2799 - acc: 0.8994 - val_loss: 0.3580 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 4s - loss: 0.2669 - acc: 0.9009 - val_loss: 0.3563 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 4s - loss: 0.2818 - acc: 0.9012 - val_loss: 0.3802 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 4s - loss: 0.2662 - acc: 0.9048 - val_loss: 0.3401 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 4s - loss: 0.2683 - acc: 0.9019 - val_loss: 0.3751 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 4s - loss: 0.2584 - acc: 0.9046 - val_loss: 0.3569 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 4s - loss: 0.2770 - acc: 0.9004 - val_loss: 0.4812 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 4s - loss: 0.2967 - acc: 0.8938 - val_loss: 0.3691 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 0.2740 - acc: 0.9031 - val_loss: 0.3725 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 0.2579 - acc: 0.9012 - val_loss: 0.3412 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 4s - loss: 0.2562 - acc: 0.9029 - val_loss: 0.3745 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 4s - loss: 0.2539 - acc: 0.9093 - val_loss: 0.3319 - val_acc: 0.8686                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8986968281288419                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8685897435897436                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 116, 24)           7080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 116, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 58, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1392)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                44576                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 54,443                                                                                                   \n",
      "Trainable params: 54,443                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 10%|████▌                                         | 12/120 [25:40<4:00:28, 133.59s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 6s - loss: 36.5680 - acc: 0.8498 - val_loss: 0.6101 - val_acc: 0.8442                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 5s - loss: 0.4496 - acc: 0.8768 - val_loss: 0.6135 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 5s - loss: 0.4550 - acc: 0.8630 - val_loss: 0.5519 - val_acc: 0.8263                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 5s - loss: 0.4367 - acc: 0.8719 - val_loss: 0.4683 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 5s - loss: 0.3968 - acc: 0.8763 - val_loss: 0.5409 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 5s - loss: 0.3976 - acc: 0.8746 - val_loss: 0.4699 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 5s - loss: 0.3833 - acc: 0.8812 - val_loss: 0.5931 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 5s - loss: 0.3923 - acc: 0.8763 - val_loss: 0.5649 - val_acc: 0.7891                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 5s - loss: 0.3913 - acc: 0.8837 - val_loss: 0.4660 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 5s - loss: 0.4044 - acc: 0.8729 - val_loss: 0.4892 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 5s - loss: 0.4197 - acc: 0.8744 - val_loss: 0.4760 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 5s - loss: 0.3802 - acc: 0.8817 - val_loss: 0.4049 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 5s - loss: 0.3947 - acc: 0.8803 - val_loss: 0.4781 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 5s - loss: 0.3962 - acc: 0.8827 - val_loss: 0.5258 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 5s - loss: 0.3886 - acc: 0.8793 - val_loss: 0.3845 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 5s - loss: 0.3862 - acc: 0.8778 - val_loss: 0.5245 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 5s - loss: 0.3861 - acc: 0.8822 - val_loss: 0.4505 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 5s - loss: 0.4130 - acc: 0.8763 - val_loss: 0.5476 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 5s - loss: 0.3977 - acc: 0.8731 - val_loss: 0.4362 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 5s - loss: 0.3817 - acc: 0.8820 - val_loss: 0.4053 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 5s - loss: 0.3724 - acc: 0.8798 - val_loss: 0.4232 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 5s - loss: 0.3769 - acc: 0.8751 - val_loss: 0.4462 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 5s - loss: 0.3870 - acc: 0.8731 - val_loss: 0.4190 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 5s - loss: 0.3993 - acc: 0.8716 - val_loss: 0.4627 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 5s - loss: 0.3777 - acc: 0.8805 - val_loss: 0.4590 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 5s - loss: 0.4284 - acc: 0.8748 - val_loss: 0.3857 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 5s - loss: 0.3684 - acc: 0.8812 - val_loss: 0.3598 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 5s - loss: 0.3727 - acc: 0.8807 - val_loss: 0.4499 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 5s - loss: 0.3885 - acc: 0.8751 - val_loss: 0.3931 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 5s - loss: 0.3590 - acc: 0.8857 - val_loss: 0.4516 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 5s - loss: 0.3679 - acc: 0.8780 - val_loss: 0.3971 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 5s - loss: 0.3682 - acc: 0.8844 - val_loss: 0.4751 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 5s - loss: 0.3919 - acc: 0.8763 - val_loss: 0.4426 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 5s - loss: 0.3770 - acc: 0.8758 - val_loss: 0.4883 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 5s - loss: 0.3858 - acc: 0.8800 - val_loss: 0.4582 - val_acc: 0.8545                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8903368576346201                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8544871794871794                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           3864                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 98,715                                                                                                   \n",
      "Trainable params: 98,715                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 11%|████▉                                         | 13/120 [28:33<4:19:22, 145.45s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 69.8137 - acc: 0.8483 - val_loss: 8.6616 - val_acc: 0.8051                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 2.5807 - acc: 0.8849 - val_loss: 0.6380 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.4458 - acc: 0.8763 - val_loss: 0.4745 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.4308 - acc: 0.8704 - val_loss: 0.4843 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3825 - acc: 0.8884 - val_loss: 0.4985 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3799 - acc: 0.8925 - val_loss: 0.4737 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3606 - acc: 0.8849 - val_loss: 0.4113 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3849 - acc: 0.8830 - val_loss: 0.4684 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.4447 - acc: 0.8798 - val_loss: 0.5296 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3668 - acc: 0.8857 - val_loss: 0.4581 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3954 - acc: 0.8822 - val_loss: 0.4500 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3475 - acc: 0.8864 - val_loss: 0.3854 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3485 - acc: 0.8862 - val_loss: 0.3767 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3415 - acc: 0.8866 - val_loss: 0.3999 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3818 - acc: 0.8815 - val_loss: 0.4560 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3719 - acc: 0.8876 - val_loss: 0.4360 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.3742 - acc: 0.8815 - val_loss: 0.3912 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3323 - acc: 0.8874 - val_loss: 0.3811 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3250 - acc: 0.8884 - val_loss: 0.4196 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.3541 - acc: 0.8871 - val_loss: 0.4171 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3385 - acc: 0.8977 - val_loss: 0.3871 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.3119 - acc: 0.8960 - val_loss: 0.4220 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3752 - acc: 0.8790 - val_loss: 0.3719 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.3547 - acc: 0.8783 - val_loss: 0.4706 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3621 - acc: 0.8862 - val_loss: 0.4097 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.3619 - acc: 0.8871 - val_loss: 0.4232 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.3293 - acc: 0.8948 - val_loss: 0.3703 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.3444 - acc: 0.8864 - val_loss: 0.4462 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.3175 - acc: 0.8965 - val_loss: 0.4589 - val_acc: 0.8173                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.3691 - acc: 0.8704 - val_loss: 0.3705 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.3462 - acc: 0.8871 - val_loss: 0.3990 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.3285 - acc: 0.8903 - val_loss: 0.4227 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.3237 - acc: 0.8955 - val_loss: 0.3598 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.3152 - acc: 0.8933 - val_loss: 0.4413 - val_acc: 0.8263                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3262 - acc: 0.8935 - val_loss: 0.3995 - val_acc: 0.8679                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8790263093189082                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8679487176430531                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           3864                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 576)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                18464                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 23,899                                                                                                   \n",
      "Trainable params: 23,899                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 12%|█████▎                                        | 14/120 [30:22<3:57:56, 134.69s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 97.3711 - acc: 0.8569 - val_loss: 44.1452 - val_acc: 0.8827                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 23.5267 - acc: 0.8997 - val_loss: 10.4323 - val_acc: 0.8756                                              \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 5.3065 - acc: 0.9024 - val_loss: 2.4145 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 1.2252 - acc: 0.8898 - val_loss: 0.8293 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.4800 - acc: 0.8940 - val_loss: 0.5754 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3428 - acc: 0.9004 - val_loss: 0.4981 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3461 - acc: 0.8921 - val_loss: 0.4756 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3896 - acc: 0.8916 - val_loss: 0.5285 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3159 - acc: 0.9021 - val_loss: 0.5090 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3327 - acc: 0.8940 - val_loss: 0.4699 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3261 - acc: 0.8933 - val_loss: 0.5241 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3150 - acc: 0.8945 - val_loss: 0.4488 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3042 - acc: 0.8987 - val_loss: 0.4392 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3213 - acc: 0.8940 - val_loss: 0.4661 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3176 - acc: 0.8965 - val_loss: 0.4859 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.2997 - acc: 0.9009 - val_loss: 0.4288 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3081 - acc: 0.9004 - val_loss: 0.4158 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.2979 - acc: 0.8987 - val_loss: 0.4160 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2828 - acc: 0.9041 - val_loss: 0.4264 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.2978 - acc: 0.8999 - val_loss: 0.4053 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2958 - acc: 0.9061 - val_loss: 0.4130 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2761 - acc: 0.9046 - val_loss: 0.4086 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3474 - acc: 0.8876 - val_loss: 0.4289 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.2975 - acc: 0.8962 - val_loss: 0.4292 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3025 - acc: 0.8938 - val_loss: 0.4131 - val_acc: 0.8545                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9092697319891813                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8544871794871794                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                42048                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,387                                                                                                   \n",
      "Trainable params: 44,387                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 12%|█████▊                                        | 15/120 [31:42<3:26:41, 118.11s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 39.6644 - acc: 0.8478 - val_loss: 1.7633 - val_acc: 0.8397                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 0.5218 - acc: 0.8741 - val_loss: 0.4615 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.3558 - acc: 0.8820 - val_loss: 0.4534 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.3336 - acc: 0.8827 - val_loss: 0.4142 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3302 - acc: 0.8891 - val_loss: 0.3673 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3291 - acc: 0.8842 - val_loss: 0.4159 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3279 - acc: 0.8847 - val_loss: 0.3795 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3220 - acc: 0.8869 - val_loss: 0.4524 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3291 - acc: 0.8832 - val_loss: 0.4051 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3318 - acc: 0.8817 - val_loss: 0.3953 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3184 - acc: 0.8894 - val_loss: 0.6085 - val_acc: 0.6821                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3207 - acc: 0.8849 - val_loss: 0.3818 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3182 - acc: 0.8857 - val_loss: 0.3944 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3048 - acc: 0.8901 - val_loss: 0.5082 - val_acc: 0.7333                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3100 - acc: 0.8908 - val_loss: 0.3636 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3169 - acc: 0.8901 - val_loss: 0.3863 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3092 - acc: 0.8879 - val_loss: 0.4475 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3079 - acc: 0.8921 - val_loss: 0.3955 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.3111 - acc: 0.8886 - val_loss: 0.3658 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3028 - acc: 0.8884 - val_loss: 0.3479 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.3045 - acc: 0.8918 - val_loss: 0.3613 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.3100 - acc: 0.8945 - val_loss: 0.4022 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3148 - acc: 0.8854 - val_loss: 0.3912 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.3150 - acc: 0.8891 - val_loss: 0.4426 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3192 - acc: 0.8884 - val_loss: 0.4090 - val_acc: 0.8006                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.7927219080403246                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8006410256410257                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           2040                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 576)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                9232                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 13,115                                                                                                   \n",
      "Trainable params: 13,115                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 13%|██████▏                                       | 16/120 [32:52<2:59:49, 103.75s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 23.9810 - acc: 0.8308 - val_loss: 3.9873 - val_acc: 0.8038                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 2s - loss: 1.2274 - acc: 0.8677 - val_loss: 0.6294 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 2s - loss: 0.4203 - acc: 0.8729 - val_loss: 0.5214 - val_acc: 0.8038                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 2s - loss: 0.3872 - acc: 0.8803 - val_loss: 0.4702 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 2s - loss: 0.3488 - acc: 0.8891 - val_loss: 0.4683 - val_acc: 0.8141                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 2s - loss: 0.3450 - acc: 0.8832 - val_loss: 0.4660 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 2s - loss: 0.3502 - acc: 0.8837 - val_loss: 0.4759 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 2s - loss: 0.3359 - acc: 0.8935 - val_loss: 0.4577 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 2s - loss: 0.3304 - acc: 0.8962 - val_loss: 0.4631 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 2s - loss: 0.3301 - acc: 0.8923 - val_loss: 0.4149 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 2s - loss: 0.3174 - acc: 0.8903 - val_loss: 0.4360 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 2s - loss: 0.3258 - acc: 0.8953 - val_loss: 0.4038 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 2s - loss: 0.3239 - acc: 0.8906 - val_loss: 0.4302 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 2s - loss: 0.3128 - acc: 0.8945 - val_loss: 0.4787 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 2s - loss: 0.3149 - acc: 0.8903 - val_loss: 0.5072 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 2s - loss: 0.3242 - acc: 0.8918 - val_loss: 0.4003 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 2s - loss: 0.3152 - acc: 0.8975 - val_loss: 0.3793 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 2s - loss: 0.3149 - acc: 0.8913 - val_loss: 0.4343 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 2s - loss: 0.3022 - acc: 0.8960 - val_loss: 0.3875 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.3147 - acc: 0.8967 - val_loss: 0.6540 - val_acc: 0.7455                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3095 - acc: 0.8994 - val_loss: 0.3753 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 2s - loss: 0.2936 - acc: 0.9002 - val_loss: 0.4243 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 2s - loss: 0.3029 - acc: 0.8913 - val_loss: 0.3901 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 2s - loss: 0.3158 - acc: 0.8965 - val_loss: 0.4692 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 2s - loss: 0.3147 - acc: 0.8955 - val_loss: 0.4401 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 2s - loss: 0.3035 - acc: 0.8977 - val_loss: 0.4323 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 2s - loss: 0.3075 - acc: 0.8955 - val_loss: 0.4029 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 2s - loss: 0.3021 - acc: 0.8999 - val_loss: 0.3963 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 2s - loss: 0.3356 - acc: 0.8869 - val_loss: 0.4009 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 2s - loss: 0.2979 - acc: 0.9014 - val_loss: 0.3641 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 2s - loss: 0.3108 - acc: 0.8898 - val_loss: 0.4003 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 2s - loss: 0.2899 - acc: 0.9019 - val_loss: 0.3718 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 2s - loss: 0.3156 - acc: 0.8923 - val_loss: 0.3952 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 2s - loss: 0.2929 - acc: 0.8962 - val_loss: 0.3978 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 2s - loss: 0.3054 - acc: 0.8972 - val_loss: 0.3551 - val_acc: 0.9000                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9156626506024096                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9                                                                                                                    \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 32)           4512                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1888)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                60448                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 66,851                                                                                                   \n",
      "Trainable params: 66,851                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 14%|██████▋                                        | 17/120 [34:19<2:49:44, 98.88s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 16.0793 - acc: 0.8203 - val_loss: 0.8218 - val_acc: 0.7936                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 0.4710 - acc: 0.8751 - val_loss: 0.4531 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.3679 - acc: 0.8844 - val_loss: 0.4139 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.3829 - acc: 0.8896 - val_loss: 0.3952 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3482 - acc: 0.8879 - val_loss: 0.4901 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3442 - acc: 0.8896 - val_loss: 0.4276 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3286 - acc: 0.8935 - val_loss: 0.3739 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 4s - loss: 0.3334 - acc: 0.8945 - val_loss: 0.4544 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 4s - loss: 0.3238 - acc: 0.8950 - val_loss: 0.3593 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3217 - acc: 0.9019 - val_loss: 0.3518 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3130 - acc: 0.8980 - val_loss: 0.6199 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3225 - acc: 0.8967 - val_loss: 0.4955 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3409 - acc: 0.8921 - val_loss: 0.4600 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3191 - acc: 0.8985 - val_loss: 0.5557 - val_acc: 0.7276                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3244 - acc: 0.9004 - val_loss: 0.3882 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3229 - acc: 0.9024 - val_loss: 0.3552 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3198 - acc: 0.8933 - val_loss: 0.3474 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3202 - acc: 0.8980 - val_loss: 0.6068 - val_acc: 0.7199                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.3136 - acc: 0.8985 - val_loss: 0.3559 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3069 - acc: 0.9014 - val_loss: 0.3850 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.3176 - acc: 0.8948 - val_loss: 0.3747 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.3365 - acc: 0.8925 - val_loss: 0.4347 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3323 - acc: 0.8896 - val_loss: 0.3547 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.3351 - acc: 0.8903 - val_loss: 0.3462 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3312 - acc: 0.8780 - val_loss: 0.3559 - val_acc: 0.8865                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8994344725842144                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8865384615384615                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           6304                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1920)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                61472                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 68,659                                                                                                   \n",
      "Trainable params: 68,659                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 15%|███████                                        | 18/120 [35:46<2:41:39, 95.09s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 87.1548 - acc: 0.8158 - val_loss: 44.2616 - val_acc: 0.8718                                              \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 4s - loss: 27.1855 - acc: 0.9031 - val_loss: 15.8142 - val_acc: 0.8737                                              \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 4s - loss: 10.8618 - acc: 0.9031 - val_loss: 7.4064 - val_acc: 0.8782                                               \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 5.4148 - acc: 0.9075 - val_loss: 3.9796 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 2.9398 - acc: 0.9029 - val_loss: 2.2021 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 1.5947 - acc: 0.9110 - val_loss: 1.3000 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.9551 - acc: 0.8997 - val_loss: 0.8496 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.6325 - acc: 0.8908 - val_loss: 0.5791 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.4326 - acc: 0.9075 - val_loss: 0.4800 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 4s - loss: 0.3572 - acc: 0.9021 - val_loss: 0.4236 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 0.3274 - acc: 0.8972 - val_loss: 0.3888 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.2995 - acc: 0.8989 - val_loss: 0.3893 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 0.2936 - acc: 0.9058 - val_loss: 0.3609 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 0.2950 - acc: 0.8955 - val_loss: 0.3571 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 4s - loss: 0.2920 - acc: 0.9044 - val_loss: 0.3534 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 0.3172 - acc: 0.9048 - val_loss: 0.4134 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 0.2956 - acc: 0.9002 - val_loss: 0.3635 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 4s - loss: 0.2965 - acc: 0.9024 - val_loss: 0.3453 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.2791 - acc: 0.9031 - val_loss: 0.3439 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.2835 - acc: 0.9053 - val_loss: 0.3700 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 4s - loss: 0.2775 - acc: 0.9053 - val_loss: 0.3647 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 4s - loss: 0.2771 - acc: 0.9026 - val_loss: 0.3284 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 4s - loss: 0.2718 - acc: 0.9053 - val_loss: 0.3411 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 4s - loss: 0.2720 - acc: 0.9066 - val_loss: 0.3233 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 4s - loss: 0.2621 - acc: 0.9083 - val_loss: 0.3269 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 4s - loss: 0.2673 - acc: 0.9019 - val_loss: 0.3209 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 4s - loss: 0.2607 - acc: 0.9083 - val_loss: 0.3296 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2746 - acc: 0.9004 - val_loss: 0.3352 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2809 - acc: 0.9071 - val_loss: 0.3545 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2759 - acc: 0.9029 - val_loss: 0.3576 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2653 - acc: 0.9110 - val_loss: 0.3155 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 0.2661 - acc: 0.9103 - val_loss: 0.3219 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 0.2638 - acc: 0.9056 - val_loss: 0.3186 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2643 - acc: 0.9073 - val_loss: 0.3229 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 4s - loss: 0.2600 - acc: 0.9112 - val_loss: 0.3260 - val_acc: 0.8827                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9001721170395869                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8826923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           7080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 936)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                14992                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 24,055                                                                                                   \n",
      "Trainable params: 24,055                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 16%|███████▎                                      | 19/120 [37:53<2:56:31, 104.86s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 5s - loss: 76.7991 - acc: 0.8308 - val_loss: 25.2696 - val_acc: 0.8635                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 4s - loss: 11.1701 - acc: 0.8889 - val_loss: 3.5678 - val_acc: 0.8724                                               \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 5s - loss: 1.5214 - acc: 0.9002 - val_loss: 0.7501 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 4s - loss: 0.4616 - acc: 0.8879 - val_loss: 0.5245 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 4s - loss: 0.3436 - acc: 0.8935 - val_loss: 0.5217 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 4s - loss: 0.3226 - acc: 0.8965 - val_loss: 0.4447 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 4s - loss: 0.3332 - acc: 0.8938 - val_loss: 0.4171 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 4s - loss: 0.3412 - acc: 0.8965 - val_loss: 0.4713 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 4s - loss: 0.3290 - acc: 0.8962 - val_loss: 0.4838 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 4s - loss: 0.3506 - acc: 0.8940 - val_loss: 0.4477 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 4s - loss: 0.3105 - acc: 0.8960 - val_loss: 0.4388 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 4s - loss: 0.3237 - acc: 0.8903 - val_loss: 0.4172 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 4s - loss: 0.3094 - acc: 0.8992 - val_loss: 0.4128 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 4s - loss: 0.3084 - acc: 0.8977 - val_loss: 0.4551 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 4s - loss: 0.3123 - acc: 0.8935 - val_loss: 0.4347 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 4s - loss: 0.3335 - acc: 0.8913 - val_loss: 0.4365 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 4s - loss: 0.3177 - acc: 0.8962 - val_loss: 0.4201 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 4s - loss: 0.3119 - acc: 0.9016 - val_loss: 0.4445 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 4s - loss: 0.2941 - acc: 0.8953 - val_loss: 0.4058 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 4s - loss: 0.3027 - acc: 0.8945 - val_loss: 0.3892 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 4s - loss: 0.2822 - acc: 0.9053 - val_loss: 0.3948 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 4s - loss: 0.2860 - acc: 0.8972 - val_loss: 0.3661 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 4s - loss: 0.3125 - acc: 0.8938 - val_loss: 0.3746 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 4s - loss: 0.2869 - acc: 0.8930 - val_loss: 0.3907 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 4s - loss: 0.2987 - acc: 0.9012 - val_loss: 0.4685 - val_acc: 0.8423                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.880009835259405                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8423076923076923                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                42048                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,691                                                                                                   \n",
      "Trainable params: 44,691                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 17%|███████▋                                      | 20/120 [39:40<2:55:24, 105.25s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 13.0512 - acc: 0.8200 - val_loss: 5.1421 - val_acc: 0.8929                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.7555 - acc: 0.9085 - val_loss: 1.5202 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.9448 - acc: 0.9117 - val_loss: 0.7511 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5135 - acc: 0.9046 - val_loss: 0.5426 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3679 - acc: 0.9098 - val_loss: 0.4271 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.2813 - acc: 0.9191 - val_loss: 0.4015 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.2678 - acc: 0.9122 - val_loss: 0.3715 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2760 - acc: 0.9039 - val_loss: 0.3772 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2434 - acc: 0.9171 - val_loss: 0.3588 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2657 - acc: 0.9115 - val_loss: 0.3563 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2557 - acc: 0.9154 - val_loss: 0.3262 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2635 - acc: 0.9107 - val_loss: 0.3232 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2328 - acc: 0.9233 - val_loss: 0.3270 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2436 - acc: 0.9149 - val_loss: 0.3065 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2332 - acc: 0.9218 - val_loss: 0.3090 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2281 - acc: 0.9235 - val_loss: 0.3648 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2607 - acc: 0.9134 - val_loss: 0.3596 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2495 - acc: 0.9169 - val_loss: 0.3100 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2325 - acc: 0.9191 - val_loss: 0.3213 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2429 - acc: 0.9186 - val_loss: 0.2984 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2443 - acc: 0.9184 - val_loss: 0.3357 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2345 - acc: 0.9196 - val_loss: 0.3016 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2272 - acc: 0.9243 - val_loss: 0.3241 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2151 - acc: 0.9309 - val_loss: 0.2994 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2404 - acc: 0.9184 - val_loss: 0.3026 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2399 - acc: 0.9216 - val_loss: 0.3034 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2253 - acc: 0.9255 - val_loss: 0.2960 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2192 - acc: 0.9289 - val_loss: 0.3229 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2352 - acc: 0.9240 - val_loss: 0.2932 - val_acc: 0.9115                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2263 - acc: 0.9309 - val_loss: 0.2824 - val_acc: 0.9019                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9409884435701992                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9019230769230769                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 992)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                63552                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 66,195                                                                                                   \n",
      "Trainable params: 66,195                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 18%|████████▏                                      | 21/120 [40:42<2:32:14, 92.26s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 11.8197 - acc: 0.8394 - val_loss: 2.7110 - val_acc: 0.8859                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.2870 - acc: 0.8997 - val_loss: 0.7504 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4465 - acc: 0.8982 - val_loss: 0.4068 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.2970 - acc: 0.9078 - val_loss: 0.3517 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3298 - acc: 0.8921 - val_loss: 0.3601 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.2789 - acc: 0.9159 - val_loss: 0.5628 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3053 - acc: 0.8982 - val_loss: 0.3735 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2841 - acc: 0.9007 - val_loss: 0.3600 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3105 - acc: 0.9046 - val_loss: 0.3605 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2801 - acc: 0.9103 - val_loss: 0.3373 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2735 - acc: 0.9051 - val_loss: 0.3358 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2585 - acc: 0.9046 - val_loss: 0.3924 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2872 - acc: 0.9080 - val_loss: 0.3534 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2576 - acc: 0.9107 - val_loss: 0.3245 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2593 - acc: 0.9100 - val_loss: 0.3263 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2564 - acc: 0.9125 - val_loss: 0.3600 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2720 - acc: 0.9031 - val_loss: 0.3396 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2941 - acc: 0.9024 - val_loss: 0.3419 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2529 - acc: 0.9117 - val_loss: 0.3143 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2504 - acc: 0.9090 - val_loss: 0.3185 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2690 - acc: 0.9056 - val_loss: 0.3476 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2630 - acc: 0.9021 - val_loss: 0.3157 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2540 - acc: 0.9058 - val_loss: 0.3351 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2713 - acc: 0.9071 - val_loss: 0.3120 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2556 - acc: 0.9110 - val_loss: 0.3080 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2545 - acc: 0.9061 - val_loss: 0.3129 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2493 - acc: 0.9098 - val_loss: 0.3575 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2573 - acc: 0.9093 - val_loss: 0.3236 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2681 - acc: 0.9083 - val_loss: 0.3233 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2547 - acc: 0.9098 - val_loss: 0.3043 - val_acc: 0.8853                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9198426358495205                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8852564102564102                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 18%|████████▌                                      | 22/120 [41:44<2:16:13, 83.40s/it, best loss: -0.9115384615384615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 11.2518 - acc: 0.8313 - val_loss: 3.4397 - val_acc: 0.8821                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.6672 - acc: 0.9004 - val_loss: 0.8981 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.6926 - acc: 0.9014 - val_loss: 0.7147 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4987 - acc: 0.9048 - val_loss: 0.5674 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3753 - acc: 0.9142 - val_loss: 0.3995 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3432 - acc: 0.9063 - val_loss: 0.4374 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3133 - acc: 0.9189 - val_loss: 0.4387 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2886 - acc: 0.9233 - val_loss: 0.5258 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2920 - acc: 0.9164 - val_loss: 0.3302 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2724 - acc: 0.9221 - val_loss: 0.3107 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2763 - acc: 0.9208 - val_loss: 0.3283 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2469 - acc: 0.9221 - val_loss: 0.3300 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2436 - acc: 0.9240 - val_loss: 0.3239 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2427 - acc: 0.9297 - val_loss: 0.3198 - val_acc: 0.9045                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2475 - acc: 0.9267 - val_loss: 0.2888 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2258 - acc: 0.9309 - val_loss: 0.2945 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2174 - acc: 0.9371 - val_loss: 0.2903 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2170 - acc: 0.9334 - val_loss: 0.2618 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2101 - acc: 0.9309 - val_loss: 0.2433 - val_acc: 0.9231                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2284 - acc: 0.9353 - val_loss: 0.2435 - val_acc: 0.9135                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.1971 - acc: 0.9415 - val_loss: 0.4098 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2004 - acc: 0.9383 - val_loss: 0.2499 - val_acc: 0.9096                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.1962 - acc: 0.9393 - val_loss: 0.2534 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2076 - acc: 0.9358 - val_loss: 0.2408 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.1978 - acc: 0.9385 - val_loss: 0.2467 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.1969 - acc: 0.9403 - val_loss: 0.2375 - val_acc: 0.9314                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.1995 - acc: 0.9398 - val_loss: 0.2509 - val_acc: 0.9071                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.1938 - acc: 0.9430 - val_loss: 0.2257 - val_acc: 0.9263                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2070 - acc: 0.9420 - val_loss: 0.2349 - val_acc: 0.9327                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2026 - acc: 0.9417 - val_loss: 0.2478 - val_acc: 0.9263                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9486107696090484                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9262820512820513                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 19%|█████████                                      | 23/120 [42:49<2:05:41, 77.75s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 48.5250 - acc: 0.7895 - val_loss: 37.0985 - val_acc: 0.8590                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 28.8639 - acc: 0.8724 - val_loss: 21.4615 - val_acc: 0.8692                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 16.0428 - acc: 0.8994 - val_loss: 11.4430 - val_acc: 0.8635                                              \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 8.2315 - acc: 0.9078 - val_loss: 5.7518 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 3.9829 - acc: 0.9073 - val_loss: 2.7684 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 1.8746 - acc: 0.9073 - val_loss: 1.3575 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.9341 - acc: 0.9056 - val_loss: 0.7749 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.5194 - acc: 0.9095 - val_loss: 0.6129 - val_acc: 0.8147                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3888 - acc: 0.9103 - val_loss: 0.4588 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3409 - acc: 0.9085 - val_loss: 0.3911 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3131 - acc: 0.9078 - val_loss: 0.3804 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2975 - acc: 0.9100 - val_loss: 0.3843 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2854 - acc: 0.9125 - val_loss: 0.3605 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2760 - acc: 0.9132 - val_loss: 0.4085 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2753 - acc: 0.9107 - val_loss: 0.4018 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2632 - acc: 0.9125 - val_loss: 0.3364 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2575 - acc: 0.9142 - val_loss: 0.3578 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2533 - acc: 0.9154 - val_loss: 0.3309 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2576 - acc: 0.9100 - val_loss: 0.3246 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2483 - acc: 0.9154 - val_loss: 0.3325 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2461 - acc: 0.9184 - val_loss: 0.3517 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2446 - acc: 0.9139 - val_loss: 0.3345 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2431 - acc: 0.9164 - val_loss: 0.3210 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2424 - acc: 0.9181 - val_loss: 0.3121 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2391 - acc: 0.9142 - val_loss: 0.3429 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2393 - acc: 0.9122 - val_loss: 0.3051 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2304 - acc: 0.9162 - val_loss: 0.2933 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2270 - acc: 0.9206 - val_loss: 0.2912 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2301 - acc: 0.9225 - val_loss: 0.3082 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2288 - acc: 0.9228 - val_loss: 0.3075 - val_acc: 0.8897                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9050897467420703                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8897435897435897                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           2328                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 20%|█████████▍                                     | 24/120 [43:54<1:58:24, 74.00s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 85.7601 - acc: 0.8153 - val_loss: 59.7817 - val_acc: 0.8679                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 42.5279 - acc: 0.9029 - val_loss: 27.7676 - val_acc: 0.8538                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 18.3237 - acc: 0.9066 - val_loss: 10.8467 - val_acc: 0.8429                                              \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 6.5573 - acc: 0.9063 - val_loss: 3.5213 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 1.9195 - acc: 0.8980 - val_loss: 1.0584 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.5716 - acc: 0.8957 - val_loss: 0.5230 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3691 - acc: 0.8950 - val_loss: 0.4250 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3228 - acc: 0.8967 - val_loss: 0.4513 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3085 - acc: 0.9024 - val_loss: 0.3972 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3045 - acc: 0.8940 - val_loss: 0.3631 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2940 - acc: 0.8955 - val_loss: 0.3625 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2931 - acc: 0.8970 - val_loss: 0.3532 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2853 - acc: 0.9036 - val_loss: 0.3366 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2762 - acc: 0.9031 - val_loss: 0.3793 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2762 - acc: 0.9044 - val_loss: 0.3850 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2736 - acc: 0.9016 - val_loss: 0.3350 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2663 - acc: 0.9093 - val_loss: 0.3457 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2666 - acc: 0.9080 - val_loss: 0.3221 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2645 - acc: 0.9034 - val_loss: 0.3381 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2630 - acc: 0.9058 - val_loss: 0.3331 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2601 - acc: 0.9122 - val_loss: 0.3674 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2626 - acc: 0.9073 - val_loss: 0.3215 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2675 - acc: 0.9009 - val_loss: 0.3428 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2626 - acc: 0.9088 - val_loss: 0.3243 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2552 - acc: 0.9051 - val_loss: 0.3693 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2633 - acc: 0.9044 - val_loss: 0.3187 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2564 - acc: 0.9098 - val_loss: 0.3176 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2611 - acc: 0.9044 - val_loss: 0.3227 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2610 - acc: 0.9073 - val_loss: 0.3170 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2546 - acc: 0.9147 - val_loss: 0.3471 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8768133759527907                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 21%|█████████▊                                     | 25/120 [45:10<1:58:01, 74.54s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 26.2787 - acc: 0.8050 - val_loss: 8.0676 - val_acc: 0.8936                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 3.2729 - acc: 0.8682 - val_loss: 1.0125 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.5660 - acc: 0.8780 - val_loss: 0.5227 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3855 - acc: 0.8825 - val_loss: 0.4427 - val_acc: 0.8327                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3504 - acc: 0.8916 - val_loss: 0.5213 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3265 - acc: 0.8894 - val_loss: 0.4629 - val_acc: 0.8173                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3391 - acc: 0.8849 - val_loss: 0.3749 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3085 - acc: 0.8938 - val_loss: 0.4123 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3057 - acc: 0.8938 - val_loss: 0.3572 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3175 - acc: 0.8891 - val_loss: 0.4094 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3119 - acc: 0.8894 - val_loss: 0.3745 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2977 - acc: 0.8925 - val_loss: 0.3592 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2939 - acc: 0.8994 - val_loss: 0.3380 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2966 - acc: 0.8985 - val_loss: 0.4683 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2968 - acc: 0.8921 - val_loss: 0.3531 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2955 - acc: 0.8930 - val_loss: 0.3292 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2832 - acc: 0.9029 - val_loss: 0.3618 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2888 - acc: 0.8953 - val_loss: 0.5973 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2956 - acc: 0.8925 - val_loss: 0.3435 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2841 - acc: 0.8965 - val_loss: 0.3305 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2895 - acc: 0.8972 - val_loss: 0.3343 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2792 - acc: 0.8980 - val_loss: 0.3239 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2889 - acc: 0.8923 - val_loss: 0.3477 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2870 - acc: 0.8960 - val_loss: 0.3469 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2917 - acc: 0.8930 - val_loss: 0.3638 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2816 - acc: 0.8989 - val_loss: 0.4426 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2790 - acc: 0.8977 - val_loss: 0.3300 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3049 - acc: 0.8928 - val_loss: 0.3528 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2788 - acc: 0.9002 - val_loss: 0.3481 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2831 - acc: 0.8985 - val_loss: 0.3857 - val_acc: 0.8359                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8473075977378903                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8358974358974359                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           2328                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 22%|██████████▏                                    | 26/120 [46:15<1:52:11, 71.61s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 15.0112 - acc: 0.8355 - val_loss: 2.9736 - val_acc: 0.6968                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.0641 - acc: 0.8699 - val_loss: 0.5338 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4003 - acc: 0.8793 - val_loss: 0.5830 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3635 - acc: 0.8876 - val_loss: 0.4744 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3375 - acc: 0.8862 - val_loss: 0.4396 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3168 - acc: 0.8938 - val_loss: 0.4292 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3158 - acc: 0.8955 - val_loss: 0.3627 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2976 - acc: 0.8977 - val_loss: 0.4215 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2917 - acc: 0.8980 - val_loss: 0.3838 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3060 - acc: 0.8911 - val_loss: 0.3524 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2981 - acc: 0.8965 - val_loss: 0.3703 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2973 - acc: 0.8945 - val_loss: 0.3668 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2875 - acc: 0.9002 - val_loss: 0.3794 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3108 - acc: 0.8997 - val_loss: 0.3725 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3085 - acc: 0.8898 - val_loss: 0.4143 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3089 - acc: 0.8901 - val_loss: 0.3400 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2851 - acc: 0.9007 - val_loss: 0.3339 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2860 - acc: 0.8999 - val_loss: 0.3789 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2879 - acc: 0.8957 - val_loss: 0.3406 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2881 - acc: 0.8980 - val_loss: 0.5812 - val_acc: 0.7929                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2778 - acc: 0.8994 - val_loss: 0.3291 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2860 - acc: 0.8935 - val_loss: 0.6842 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3012 - acc: 0.8977 - val_loss: 0.3379 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2788 - acc: 0.9012 - val_loss: 0.3487 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2763 - acc: 0.8985 - val_loss: 0.3997 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2873 - acc: 0.8970 - val_loss: 0.3587 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2793 - acc: 0.9009 - val_loss: 0.3317 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3047 - acc: 0.8928 - val_loss: 0.3584 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2816 - acc: 0.9004 - val_loss: 0.3422 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3046 - acc: 0.8960 - val_loss: 0.3530 - val_acc: 0.8776                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8898450946643718                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8775641025641026                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 22%|██████████▌                                    | 27/120 [47:29<1:52:07, 72.34s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 50.3271 - acc: 0.7986 - val_loss: 21.5585 - val_acc: 0.8840                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 10.0065 - acc: 0.8670 - val_loss: 2.9033 - val_acc: 0.8667                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 1.1295 - acc: 0.8734 - val_loss: 0.5904 - val_acc: 0.7846                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4010 - acc: 0.8832 - val_loss: 0.5263 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3600 - acc: 0.8842 - val_loss: 0.4651 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3269 - acc: 0.8884 - val_loss: 0.3715 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3294 - acc: 0.8869 - val_loss: 0.3974 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3096 - acc: 0.8894 - val_loss: 0.4160 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3065 - acc: 0.8953 - val_loss: 0.3586 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3084 - acc: 0.8921 - val_loss: 0.4397 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3163 - acc: 0.8911 - val_loss: 0.3449 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3498 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3175 - acc: 0.8908 - val_loss: 0.3384 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2859 - acc: 0.9004 - val_loss: 0.4595 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3044 - acc: 0.8908 - val_loss: 0.3485 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2948 - acc: 0.8908 - val_loss: 0.3381 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2945 - acc: 0.8967 - val_loss: 0.3366 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3075 - acc: 0.8894 - val_loss: 0.3493 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2995 - acc: 0.8928 - val_loss: 0.3606 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2888 - acc: 0.8943 - val_loss: 0.5587 - val_acc: 0.7756                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2894 - acc: 0.8967 - val_loss: 0.3463 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2954 - acc: 0.8925 - val_loss: 0.3422 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2962 - acc: 0.8908 - val_loss: 0.3613 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2931 - acc: 0.8933 - val_loss: 0.3446 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2973 - acc: 0.8948 - val_loss: 0.3541 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2952 - acc: 0.8923 - val_loss: 0.3619 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2877 - acc: 0.8962 - val_loss: 0.3271 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2919 - acc: 0.8943 - val_loss: 0.3724 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2875 - acc: 0.8999 - val_loss: 0.3493 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2939 - acc: 0.8945 - val_loss: 0.3744 - val_acc: 0.8603                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8645193016965822                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8602564102564103                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           2328                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 23%|██████████▉                                    | 28/120 [48:33<1:47:22, 70.03s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 101.8160 - acc: 0.8171 - val_loss: 62.5121 - val_acc: 0.8731                                             \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 39.5088 - acc: 0.8985 - val_loss: 21.2652 - val_acc: 0.8724                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 11.8914 - acc: 0.8923 - val_loss: 5.3574 - val_acc: 0.8141                                               \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 2.5737 - acc: 0.8916 - val_loss: 1.1362 - val_acc: 0.7891                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.5596 - acc: 0.8852 - val_loss: 0.5771 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3666 - acc: 0.8876 - val_loss: 0.4334 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3488 - acc: 0.8862 - val_loss: 0.4135 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3253 - acc: 0.8881 - val_loss: 0.4498 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3184 - acc: 0.8970 - val_loss: 0.3896 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3136 - acc: 0.8916 - val_loss: 0.3684 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3056 - acc: 0.8923 - val_loss: 0.3573 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3035 - acc: 0.8921 - val_loss: 0.3684 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3021 - acc: 0.8953 - val_loss: 0.3651 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2875 - acc: 0.9002 - val_loss: 0.4593 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2905 - acc: 0.9004 - val_loss: 0.4326 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2919 - acc: 0.8916 - val_loss: 0.3400 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2842 - acc: 0.9024 - val_loss: 0.3522 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2798 - acc: 0.8997 - val_loss: 0.3507 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2881 - acc: 0.8928 - val_loss: 0.3620 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2824 - acc: 0.8975 - val_loss: 0.4617 - val_acc: 0.8019                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2773 - acc: 0.9056 - val_loss: 0.3892 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2844 - acc: 0.8940 - val_loss: 0.3340 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2902 - acc: 0.8935 - val_loss: 0.3378 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2760 - acc: 0.8972 - val_loss: 0.3505 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2850 - acc: 0.8928 - val_loss: 0.4192 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2854 - acc: 0.8970 - val_loss: 0.3539 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2856 - acc: 0.9007 - val_loss: 0.3538 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2857 - acc: 0.8933 - val_loss: 0.3581 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2759 - acc: 0.9034 - val_loss: 0.3405 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2793 - acc: 0.8992 - val_loss: 0.3569 - val_acc: 0.8718                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8797639537742808                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8717948717948718                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 24%|███████████▎                                   | 29/120 [49:50<1:49:05, 71.93s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 12.7084 - acc: 0.8117 - val_loss: 9.2711 - val_acc: 0.8731                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 6.9073 - acc: 0.9031 - val_loss: 4.9996 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 3.5381 - acc: 0.9147 - val_loss: 2.5107 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 1.8095 - acc: 0.9223 - val_loss: 1.5075 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 1.1134 - acc: 0.9208 - val_loss: 1.0416 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.7883 - acc: 0.9208 - val_loss: 0.7702 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.6210 - acc: 0.9253 - val_loss: 0.6992 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.5073 - acc: 0.9262 - val_loss: 0.6344 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.4339 - acc: 0.9265 - val_loss: 0.4658 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3797 - acc: 0.9265 - val_loss: 0.4387 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3381 - acc: 0.9292 - val_loss: 0.4158 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3134 - acc: 0.9302 - val_loss: 0.4669 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2950 - acc: 0.9346 - val_loss: 0.3816 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2753 - acc: 0.9385 - val_loss: 0.3440 - val_acc: 0.9096                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2619 - acc: 0.9353 - val_loss: 0.3481 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2458 - acc: 0.9398 - val_loss: 0.3321 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2451 - acc: 0.9378 - val_loss: 0.3580 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2334 - acc: 0.9405 - val_loss: 0.2945 - val_acc: 0.9045                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2306 - acc: 0.9361 - val_loss: 0.3033 - val_acc: 0.9218                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2181 - acc: 0.9447 - val_loss: 0.4544 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2121 - acc: 0.9439 - val_loss: 0.3155 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2080 - acc: 0.9459 - val_loss: 0.2711 - val_acc: 0.9237                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2002 - acc: 0.9454 - val_loss: 0.2755 - val_acc: 0.9077                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2066 - acc: 0.9449 - val_loss: 0.2667 - val_acc: 0.9218                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.1953 - acc: 0.9459 - val_loss: 0.2936 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.1885 - acc: 0.9491 - val_loss: 0.2573 - val_acc: 0.9308                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.1880 - acc: 0.9484 - val_loss: 0.3261 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.1852 - acc: 0.9493 - val_loss: 0.2856 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.1865 - acc: 0.9498 - val_loss: 0.3047 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.1758 - acc: 0.9518 - val_loss: 0.3787 - val_acc: 0.9141                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9385296287189575                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9141025641025641                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 27,859                                                                                                   \n",
      "Trainable params: 27,859                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 25%|███████████▊                                   | 30/120 [50:55<1:45:02, 70.03s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 8.4485 - acc: 0.8026 - val_loss: 1.7619 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 0.9092 - acc: 0.8822 - val_loss: 0.6631 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4356 - acc: 0.8967 - val_loss: 0.5589 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3757 - acc: 0.8977 - val_loss: 0.4520 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3243 - acc: 0.9007 - val_loss: 0.3868 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3007 - acc: 0.8987 - val_loss: 0.4080 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.2998 - acc: 0.8992 - val_loss: 0.3782 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2724 - acc: 0.9125 - val_loss: 0.3587 - val_acc: 0.9109                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2760 - acc: 0.9132 - val_loss: 0.3346 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2687 - acc: 0.9103 - val_loss: 0.4051 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2744 - acc: 0.9083 - val_loss: 0.4034 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2677 - acc: 0.9125 - val_loss: 0.3454 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2779 - acc: 0.9085 - val_loss: 0.3544 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2698 - acc: 0.9125 - val_loss: 0.3442 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2753 - acc: 0.9085 - val_loss: 0.3468 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2528 - acc: 0.9203 - val_loss: 0.3333 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2795 - acc: 0.9142 - val_loss: 0.3748 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2708 - acc: 0.9154 - val_loss: 0.3271 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2730 - acc: 0.9105 - val_loss: 0.3603 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2738 - acc: 0.9107 - val_loss: 1.1928 - val_acc: 0.6038                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3043 - acc: 0.9083 - val_loss: 0.3407 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3000 - acc: 0.9056 - val_loss: 0.3527 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3529 - acc: 0.8940 - val_loss: 0.3276 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3979 - acc: 0.8891 - val_loss: 0.6770 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.5592 - acc: 0.8689 - val_loss: 0.5477 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.8556 - acc: 0.8503 - val_loss: 0.5969 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 1.1065 - acc: 0.8670 - val_loss: 0.7390 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 1.5111 - acc: 0.8579 - val_loss: 0.6975 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 1.5792 - acc: 0.8712 - val_loss: 0.9201 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 1.6173 - acc: 0.8780 - val_loss: 1.7195 - val_acc: 0.8564                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.887140398328006                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8564102564102564                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 26%|████████████▏                                  | 31/120 [51:58<1:40:37, 67.84s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 29.9280 - acc: 0.8006 - val_loss: 18.2808 - val_acc: 0.8872                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 11.6077 - acc: 0.8913 - val_loss: 6.4509 - val_acc: 0.8788                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 3.7511 - acc: 0.9019 - val_loss: 1.9635 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 1.0670 - acc: 0.9071 - val_loss: 0.7909 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.4640 - acc: 0.8999 - val_loss: 0.5154 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3519 - acc: 0.8992 - val_loss: 0.4209 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3241 - acc: 0.8985 - val_loss: 0.4188 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2903 - acc: 0.9078 - val_loss: 0.4557 - val_acc: 0.8160                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2857 - acc: 0.9078 - val_loss: 0.3715 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2857 - acc: 0.9002 - val_loss: 0.3643 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2808 - acc: 0.9012 - val_loss: 0.3668 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2715 - acc: 0.9083 - val_loss: 0.3616 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2703 - acc: 0.9058 - val_loss: 0.3631 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2588 - acc: 0.9120 - val_loss: 0.3301 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2641 - acc: 0.9100 - val_loss: 0.3795 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2502 - acc: 0.9134 - val_loss: 0.3357 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2479 - acc: 0.9176 - val_loss: 0.3536 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2503 - acc: 0.9142 - val_loss: 0.2916 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2520 - acc: 0.9142 - val_loss: 0.2968 - val_acc: 0.9006                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2447 - acc: 0.9152 - val_loss: 0.2927 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2409 - acc: 0.9181 - val_loss: 0.3256 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2420 - acc: 0.9166 - val_loss: 0.3172 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2477 - acc: 0.9115 - val_loss: 0.3086 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2459 - acc: 0.9139 - val_loss: 0.4378 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2432 - acc: 0.9162 - val_loss: 0.4001 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2445 - acc: 0.9105 - val_loss: 0.3330 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2448 - acc: 0.9149 - val_loss: 0.3213 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2357 - acc: 0.9208 - val_loss: 0.3035 - val_acc: 0.9045                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2443 - acc: 0.9189 - val_loss: 0.3410 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2340 - acc: 0.9194 - val_loss: 0.3680 - val_acc: 0.8827                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9006638800098352                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8826923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15632                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 18,707                                                                                                   \n",
      "Trainable params: 18,707                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 27%|████████████▌                                  | 32/120 [53:03<1:38:06, 66.89s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 12.5203 - acc: 0.8235 - val_loss: 2.6158 - val_acc: 0.8615                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.0498 - acc: 0.8876 - val_loss: 0.6116 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4922 - acc: 0.8994 - val_loss: 0.5748 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4086 - acc: 0.9056 - val_loss: 0.5304 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3764 - acc: 0.9056 - val_loss: 0.4105 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3171 - acc: 0.9073 - val_loss: 0.3637 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3030 - acc: 0.9164 - val_loss: 0.3859 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2694 - acc: 0.9238 - val_loss: 0.4074 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2738 - acc: 0.9208 - val_loss: 0.3462 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2765 - acc: 0.9162 - val_loss: 0.3193 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2927 - acc: 0.9181 - val_loss: 0.3296 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2626 - acc: 0.9196 - val_loss: 0.3119 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2493 - acc: 0.9196 - val_loss: 0.3223 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2548 - acc: 0.9221 - val_loss: 0.3121 - val_acc: 0.9045                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2574 - acc: 0.9216 - val_loss: 0.3171 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2324 - acc: 0.9260 - val_loss: 0.3060 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2443 - acc: 0.9267 - val_loss: 0.3284 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2338 - acc: 0.9260 - val_loss: 0.3335 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2428 - acc: 0.9189 - val_loss: 0.3185 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2320 - acc: 0.9282 - val_loss: 0.5317 - val_acc: 0.8147                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2305 - acc: 0.9294 - val_loss: 0.3086 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2225 - acc: 0.9287 - val_loss: 0.3347 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2224 - acc: 0.9326 - val_loss: 0.2800 - val_acc: 0.9128                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2255 - acc: 0.9275 - val_loss: 0.2665 - val_acc: 0.9038                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2215 - acc: 0.9316 - val_loss: 0.2805 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2165 - acc: 0.9302 - val_loss: 0.2671 - val_acc: 0.9173                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2130 - acc: 0.9351 - val_loss: 0.2903 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2088 - acc: 0.9366 - val_loss: 0.2553 - val_acc: 0.9231                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2256 - acc: 0.9378 - val_loss: 0.3108 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2072 - acc: 0.9373 - val_loss: 0.2742 - val_acc: 0.9173                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9326284730759774                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9173076923076923                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15376                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 19,027                                                                                                   \n",
      "Trainable params: 19,027                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 28%|████████████▉                                  | 33/120 [54:06<1:35:29, 65.86s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 40.7849 - acc: 0.7792 - val_loss: 9.6113 - val_acc: 0.8391                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.9217 - acc: 0.8653 - val_loss: 0.5836 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4579 - acc: 0.8672 - val_loss: 0.5246 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4167 - acc: 0.8724 - val_loss: 0.5630 - val_acc: 0.7814                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3791 - acc: 0.8820 - val_loss: 0.4904 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3748 - acc: 0.8830 - val_loss: 0.4415 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3629 - acc: 0.8849 - val_loss: 0.4033 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3608 - acc: 0.8793 - val_loss: 0.4451 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3573 - acc: 0.8906 - val_loss: 0.4118 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3582 - acc: 0.8837 - val_loss: 0.3865 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3387 - acc: 0.8876 - val_loss: 0.4008 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3543 - acc: 0.8884 - val_loss: 0.4026 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3370 - acc: 0.8876 - val_loss: 0.3650 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3219 - acc: 0.8948 - val_loss: 0.4454 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3497 - acc: 0.8830 - val_loss: 0.4091 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3283 - acc: 0.8889 - val_loss: 0.3732 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3333 - acc: 0.8916 - val_loss: 0.3612 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3247 - acc: 0.8862 - val_loss: 0.4265 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3199 - acc: 0.8889 - val_loss: 0.3649 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3181 - acc: 0.8903 - val_loss: 0.4534 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3368 - acc: 0.8921 - val_loss: 0.4006 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3126 - acc: 0.8903 - val_loss: 0.3697 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3345 - acc: 0.8835 - val_loss: 0.3816 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3214 - acc: 0.8879 - val_loss: 0.4701 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3338 - acc: 0.8842 - val_loss: 0.4437 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3167 - acc: 0.8933 - val_loss: 0.3723 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.3206 - acc: 0.8864 - val_loss: 0.3590 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3328 - acc: 0.8852 - val_loss: 0.3740 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.3265 - acc: 0.8933 - val_loss: 0.3842 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3226 - acc: 0.8881 - val_loss: 0.3698 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9011556429800835                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15632                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 18,707                                                                                                   \n",
      "Trainable params: 18,707                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 28%|█████████████▎                                 | 34/120 [55:12<1:34:29, 65.92s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 25.7496 - acc: 0.7984 - val_loss: 9.3055 - val_acc: 0.8654                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 4.0289 - acc: 0.8746 - val_loss: 1.3149 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.6683 - acc: 0.8859 - val_loss: 0.6251 - val_acc: 0.7853                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3806 - acc: 0.8891 - val_loss: 0.5149 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3411 - acc: 0.8928 - val_loss: 0.4679 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3266 - acc: 0.8928 - val_loss: 0.3988 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3214 - acc: 0.8906 - val_loss: 0.3754 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3031 - acc: 0.8953 - val_loss: 0.4429 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3088 - acc: 0.8987 - val_loss: 0.3461 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3026 - acc: 0.8948 - val_loss: 0.3520 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2922 - acc: 0.8972 - val_loss: 0.3586 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2938 - acc: 0.8980 - val_loss: 0.3504 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2898 - acc: 0.9016 - val_loss: 0.3386 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2773 - acc: 0.9044 - val_loss: 0.3683 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2811 - acc: 0.8992 - val_loss: 0.4083 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2777 - acc: 0.9002 - val_loss: 0.3292 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2735 - acc: 0.9029 - val_loss: 0.3370 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2755 - acc: 0.8985 - val_loss: 0.3295 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2783 - acc: 0.8948 - val_loss: 0.3336 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2774 - acc: 0.8962 - val_loss: 0.3494 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2724 - acc: 0.9034 - val_loss: 0.3443 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2744 - acc: 0.8965 - val_loss: 0.3307 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2832 - acc: 0.8903 - val_loss: 0.3402 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2725 - acc: 0.8987 - val_loss: 0.3345 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2735 - acc: 0.8975 - val_loss: 0.4877 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2812 - acc: 0.8965 - val_loss: 0.3286 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2657 - acc: 0.9016 - val_loss: 0.3197 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2833 - acc: 0.8933 - val_loss: 0.3447 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2718 - acc: 0.8977 - val_loss: 0.3367 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2642 - acc: 0.9024 - val_loss: 0.3365 - val_acc: 0.8724                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9102532579296779                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8724358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                6160                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 9,235                                                                                                    \n",
      "Trainable params: 9,235                                                                                                \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 29%|█████████████▋                                 | 35/120 [56:15<1:32:14, 65.11s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 30.3417 - acc: 0.8055 - val_loss: 12.6115 - val_acc: 0.8647                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 5.7514 - acc: 0.8795 - val_loss: 2.0761 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.9953 - acc: 0.8901 - val_loss: 0.7379 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4193 - acc: 0.8982 - val_loss: 0.5877 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3563 - acc: 0.8953 - val_loss: 0.4951 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3248 - acc: 0.8948 - val_loss: 0.4470 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3164 - acc: 0.8980 - val_loss: 0.4352 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3011 - acc: 0.8962 - val_loss: 0.4488 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2933 - acc: 0.9046 - val_loss: 0.4483 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2964 - acc: 0.8972 - val_loss: 0.4093 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2850 - acc: 0.8980 - val_loss: 0.4178 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2882 - acc: 0.9014 - val_loss: 0.3821 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2806 - acc: 0.9026 - val_loss: 0.3830 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2740 - acc: 0.9039 - val_loss: 0.4567 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2742 - acc: 0.9056 - val_loss: 0.5180 - val_acc: 0.8327                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2764 - acc: 0.8992 - val_loss: 0.3864 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2714 - acc: 0.9048 - val_loss: 0.3779 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2660 - acc: 0.9048 - val_loss: 0.3776 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2720 - acc: 0.8994 - val_loss: 0.3754 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2661 - acc: 0.9046 - val_loss: 0.4800 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2599 - acc: 0.9105 - val_loss: 0.3717 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2713 - acc: 0.9002 - val_loss: 0.3997 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2706 - acc: 0.8960 - val_loss: 0.3945 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2654 - acc: 0.9016 - val_loss: 0.3900 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2636 - acc: 0.9004 - val_loss: 0.6635 - val_acc: 0.7897                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2749 - acc: 0.9014 - val_loss: 0.3753 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2617 - acc: 0.9046 - val_loss: 0.3581 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2639 - acc: 0.8985 - val_loss: 0.3730 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2702 - acc: 0.9031 - val_loss: 0.3825 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2562 - acc: 0.9090 - val_loss: 0.3956 - val_acc: 0.8506                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8505040570445046                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8506410256410256                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3600                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 944)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15120                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 20,243                                                                                                   \n",
      "Trainable params: 20,243                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 30%|██████████████                                 | 36/120 [57:18<1:30:11, 64.42s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 30.4530 - acc: 0.7583 - val_loss: 6.7733 - val_acc: 0.8160                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 2.2867 - acc: 0.8485 - val_loss: 0.5765 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4499 - acc: 0.8608 - val_loss: 0.5099 - val_acc: 0.7955                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4203 - acc: 0.8716 - val_loss: 0.5941 - val_acc: 0.7763                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3836 - acc: 0.8822 - val_loss: 0.4676 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3937 - acc: 0.8721 - val_loss: 0.4332 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3620 - acc: 0.8785 - val_loss: 0.4269 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3451 - acc: 0.8894 - val_loss: 0.4655 - val_acc: 0.8064                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3505 - acc: 0.8889 - val_loss: 0.3973 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3470 - acc: 0.8857 - val_loss: 0.4024 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3497 - acc: 0.8879 - val_loss: 0.4694 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3361 - acc: 0.8881 - val_loss: 0.3798 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3553 - acc: 0.8886 - val_loss: 0.3618 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3315 - acc: 0.8940 - val_loss: 0.6933 - val_acc: 0.7481                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3530 - acc: 0.8776 - val_loss: 0.3817 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3296 - acc: 0.8896 - val_loss: 0.3659 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3348 - acc: 0.8911 - val_loss: 0.3601 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3263 - acc: 0.8876 - val_loss: 0.3821 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3304 - acc: 0.8903 - val_loss: 0.3686 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3264 - acc: 0.8950 - val_loss: 0.8408 - val_acc: 0.6449                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3317 - acc: 0.8876 - val_loss: 0.3876 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3123 - acc: 0.8962 - val_loss: 0.3674 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3222 - acc: 0.8896 - val_loss: 0.3641 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3271 - acc: 0.8876 - val_loss: 0.3701 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3495 - acc: 0.8862 - val_loss: 0.4612 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3265 - acc: 0.8901 - val_loss: 0.3587 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3319 - acc: 0.8886 - val_loss: 0.3855 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3099 - acc: 0.8923 - val_loss: 0.3799 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3334 - acc: 0.8913 - val_loss: 0.3630 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3135 - acc: 0.8908 - val_loss: 0.4949 - val_acc: 0.8308                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8271453159577083                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8307692307692308                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1920)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                30736                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 35,939                                                                                                   \n",
      "Trainable params: 35,939                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 31%|██████████████▍                                | 37/120 [58:44<1:37:50, 70.73s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 30.6680 - acc: 0.8134 - val_loss: 6.0000 - val_acc: 0.6795                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.7901 - acc: 0.8571 - val_loss: 0.5645 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4146 - acc: 0.8731 - val_loss: 0.5415 - val_acc: 0.7929                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3602 - acc: 0.8832 - val_loss: 0.4634 - val_acc: 0.8160                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3416 - acc: 0.8874 - val_loss: 0.4336 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3259 - acc: 0.8916 - val_loss: 0.4188 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3485 - acc: 0.8837 - val_loss: 0.3849 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3335 - acc: 0.8889 - val_loss: 0.4193 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3123 - acc: 0.8965 - val_loss: 0.3799 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3309 - acc: 0.8866 - val_loss: 0.5912 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3270 - acc: 0.8891 - val_loss: 0.3999 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3168 - acc: 0.8913 - val_loss: 0.3728 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3123 - acc: 0.8948 - val_loss: 0.3513 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3221 - acc: 0.8955 - val_loss: 0.3787 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3190 - acc: 0.8916 - val_loss: 0.5071 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3216 - acc: 0.8879 - val_loss: 0.3929 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3212 - acc: 0.8943 - val_loss: 0.3613 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3068 - acc: 0.8945 - val_loss: 0.3611 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3404 - acc: 0.8825 - val_loss: 0.4194 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2959 - acc: 0.8975 - val_loss: 0.5565 - val_acc: 0.7891                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2937 - acc: 0.8975 - val_loss: 0.3849 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3163 - acc: 0.8871 - val_loss: 0.3517 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3205 - acc: 0.8901 - val_loss: 0.3443 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3278 - acc: 0.8913 - val_loss: 0.3639 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8881 - val_loss: 0.3569 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3095 - acc: 0.8896 - val_loss: 0.3891 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2873 - acc: 0.9014 - val_loss: 0.3457 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3321 - acc: 0.8896 - val_loss: 0.3643 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2987 - acc: 0.9046 - val_loss: 0.3818 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3194 - acc: 0.8921 - val_loss: 0.3490 - val_acc: 0.8859                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.895992131792476                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8858974358974359                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15376                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 19,475                                                                                                   \n",
      "Trainable params: 19,475                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 32%|██████████████▎                              | 38/120 [1:00:12<1:43:36, 75.81s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 28.3830 - acc: 0.8176 - val_loss: 3.3626 - val_acc: 0.7449                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.9341 - acc: 0.8662 - val_loss: 0.4818 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3979 - acc: 0.8739 - val_loss: 0.4429 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3691 - acc: 0.8842 - val_loss: 0.4160 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3521 - acc: 0.8854 - val_loss: 0.4081 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3548 - acc: 0.8807 - val_loss: 0.5773 - val_acc: 0.7801                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3250 - acc: 0.8869 - val_loss: 0.3705 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3363 - acc: 0.8815 - val_loss: 0.4032 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3278 - acc: 0.8953 - val_loss: 0.3756 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3195 - acc: 0.8962 - val_loss: 0.3463 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3193 - acc: 0.8891 - val_loss: 0.5710 - val_acc: 0.6923                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3221 - acc: 0.8864 - val_loss: 0.4576 - val_acc: 0.7987                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3193 - acc: 0.8894 - val_loss: 0.4581 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3218 - acc: 0.8891 - val_loss: 0.4597 - val_acc: 0.7583                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8945 - val_loss: 0.3546 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3090 - acc: 0.8930 - val_loss: 0.3494 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3149 - acc: 0.8876 - val_loss: 0.3459 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3097 - acc: 0.8913 - val_loss: 0.3929 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3135 - acc: 0.8938 - val_loss: 0.3352 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2965 - acc: 0.8997 - val_loss: 0.3543 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2974 - acc: 0.8953 - val_loss: 0.4211 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3102 - acc: 0.8908 - val_loss: 0.4050 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3135 - acc: 0.8876 - val_loss: 0.3499 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2992 - acc: 0.8925 - val_loss: 0.3310 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3459 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3108 - acc: 0.8903 - val_loss: 0.3787 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3122 - acc: 0.8864 - val_loss: 0.3799 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2952 - acc: 0.8923 - val_loss: 0.4315 - val_acc: 0.8167                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3217 - acc: 0.8898 - val_loss: 0.4048 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3012 - acc: 0.8898 - val_loss: 0.3428 - val_acc: 0.8699                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8908286206048684                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8698717948717949                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                10256                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 12,955                                                                                                   \n",
      "Trainable params: 12,955                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 32%|██████████████▋                              | 39/120 [1:01:37<1:46:09, 78.64s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 50.9444 - acc: 0.7893 - val_loss: 19.9882 - val_acc: 0.8506                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 8.4986 - acc: 0.8702 - val_loss: 2.0865 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.7880 - acc: 0.8724 - val_loss: 0.5540 - val_acc: 0.8212                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3847 - acc: 0.8827 - val_loss: 0.5145 - val_acc: 0.8045                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3565 - acc: 0.8844 - val_loss: 0.4376 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3410 - acc: 0.8879 - val_loss: 0.4418 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3436 - acc: 0.8857 - val_loss: 0.4031 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3303 - acc: 0.8894 - val_loss: 0.4311 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3328 - acc: 0.8913 - val_loss: 0.4085 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3340 - acc: 0.8827 - val_loss: 0.4105 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3173 - acc: 0.8906 - val_loss: 0.4364 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3252 - acc: 0.8913 - val_loss: 0.4044 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3173 - acc: 0.8896 - val_loss: 0.3867 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3100 - acc: 0.8987 - val_loss: 0.5207 - val_acc: 0.8000                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3239 - acc: 0.8930 - val_loss: 0.4324 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3199 - acc: 0.8898 - val_loss: 0.3853 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3224 - acc: 0.8933 - val_loss: 0.3737 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3209 - acc: 0.8886 - val_loss: 0.4005 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3112 - acc: 0.8935 - val_loss: 0.3953 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3129 - acc: 0.8871 - val_loss: 0.5713 - val_acc: 0.7763                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3110 - acc: 0.8940 - val_loss: 0.3864 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3011 - acc: 0.8955 - val_loss: 0.3864 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3181 - acc: 0.8852 - val_loss: 0.4008 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3029 - acc: 0.8921 - val_loss: 0.3933 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3103 - acc: 0.8891 - val_loss: 0.3932 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3083 - acc: 0.8918 - val_loss: 0.3834 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2996 - acc: 0.8908 - val_loss: 0.3901 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3194 - acc: 0.8894 - val_loss: 0.3939 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.3114 - acc: 0.8938 - val_loss: 0.3859 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3066 - acc: 0.8955 - val_loss: 0.4016 - val_acc: 0.8506                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8559134497172363                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8506410256410256                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 32)           7200                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 736)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                11792                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 20,515                                                                                                   \n",
      "Trainable params: 20,515                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 33%|███████████████                              | 40/120 [1:02:40<1:38:41, 74.01s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 5s - loss: 24.7644 - acc: 0.8166 - val_loss: 0.6585 - val_acc: 0.8045                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 4s - loss: 0.4425 - acc: 0.8662 - val_loss: 0.5301 - val_acc: 0.8192                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 4s - loss: 0.4132 - acc: 0.8726 - val_loss: 0.5458 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 4s - loss: 0.4080 - acc: 0.8714 - val_loss: 0.4595 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 4s - loss: 0.3966 - acc: 0.8746 - val_loss: 0.4685 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 4s - loss: 0.3865 - acc: 0.8773 - val_loss: 0.5068 - val_acc: 0.8051                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 4s - loss: 0.3794 - acc: 0.8773 - val_loss: 0.4454 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 4s - loss: 0.3898 - acc: 0.8803 - val_loss: 0.4801 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 4s - loss: 0.3828 - acc: 0.8746 - val_loss: 0.4969 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 4s - loss: 0.3858 - acc: 0.8729 - val_loss: 0.4331 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 4s - loss: 0.3754 - acc: 0.8827 - val_loss: 0.6538 - val_acc: 0.6551                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 4s - loss: 0.3850 - acc: 0.8716 - val_loss: 0.4832 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 4s - loss: 0.3756 - acc: 0.8761 - val_loss: 0.4548 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 4s - loss: 0.3721 - acc: 0.8825 - val_loss: 0.5016 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 4s - loss: 0.3794 - acc: 0.8788 - val_loss: 0.4473 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 4s - loss: 0.3839 - acc: 0.8827 - val_loss: 0.4287 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 4s - loss: 0.3803 - acc: 0.8746 - val_loss: 0.4433 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 4s - loss: 0.3619 - acc: 0.8837 - val_loss: 0.4841 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 4s - loss: 0.3688 - acc: 0.8835 - val_loss: 0.4252 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 4s - loss: 0.3565 - acc: 0.8793 - val_loss: 0.4902 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 4s - loss: 0.3573 - acc: 0.8812 - val_loss: 0.4807 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 4s - loss: 0.3687 - acc: 0.8803 - val_loss: 0.6795 - val_acc: 0.7468                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 4s - loss: 0.3748 - acc: 0.8746 - val_loss: 0.4239 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 4s - loss: 0.3652 - acc: 0.8795 - val_loss: 0.4385 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 4s - loss: 0.3589 - acc: 0.8795 - val_loss: 0.4902 - val_acc: 0.7942                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.7858372264568478                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.7942307692307692                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15376                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 20,147                                                                                                   \n",
      "Trainable params: 20,147                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 34%|███████████████▍                             | 41/120 [1:04:28<1:50:46, 84.13s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 56.3952 - acc: 0.8271 - val_loss: 7.6814 - val_acc: 0.8096                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.9000 - acc: 0.8377 - val_loss: 0.5517 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4974 - acc: 0.8515 - val_loss: 0.5311 - val_acc: 0.8045                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4427 - acc: 0.8672 - val_loss: 0.4948 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4393 - acc: 0.8630 - val_loss: 0.4523 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.4291 - acc: 0.8714 - val_loss: 0.4420 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.4358 - acc: 0.8635 - val_loss: 0.4410 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3846 - acc: 0.8812 - val_loss: 0.5780 - val_acc: 0.7788                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3963 - acc: 0.8822 - val_loss: 0.4240 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3919 - acc: 0.8805 - val_loss: 0.4126 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3858 - acc: 0.8800 - val_loss: 0.3862 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3962 - acc: 0.8758 - val_loss: 0.5163 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3779 - acc: 0.8852 - val_loss: 0.4019 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3726 - acc: 0.8898 - val_loss: 0.4972 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3613 - acc: 0.8835 - val_loss: 0.4207 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3974 - acc: 0.8758 - val_loss: 0.3994 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3686 - acc: 0.8812 - val_loss: 0.3813 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3603 - acc: 0.8820 - val_loss: 0.4166 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3785 - acc: 0.8803 - val_loss: 0.3990 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3665 - acc: 0.8854 - val_loss: 0.3982 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3519 - acc: 0.8857 - val_loss: 0.4400 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3668 - acc: 0.8859 - val_loss: 0.3808 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3717 - acc: 0.8766 - val_loss: 0.3920 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3629 - acc: 0.8798 - val_loss: 0.4483 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3577 - acc: 0.8847 - val_loss: 0.4857 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3826 - acc: 0.8788 - val_loss: 0.3982 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3551 - acc: 0.8864 - val_loss: 0.3840 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3803 - acc: 0.8800 - val_loss: 0.3830 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3615 - acc: 0.8862 - val_loss: 0.3971 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3594 - acc: 0.8842 - val_loss: 0.5101 - val_acc: 0.8333                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8450946643717728                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8333333333333334                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15376                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 19,475                                                                                                   \n",
      "Trainable params: 19,475                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 35%|███████████████▋                             | 42/120 [1:05:51<1:49:11, 84.00s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 17.5926 - acc: 0.8458 - val_loss: 0.8767 - val_acc: 0.7840                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.4364 - acc: 0.8729 - val_loss: 0.4606 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3663 - acc: 0.8830 - val_loss: 0.4791 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3529 - acc: 0.8849 - val_loss: 0.3933 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3402 - acc: 0.8866 - val_loss: 0.3754 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3314 - acc: 0.8859 - val_loss: 0.4366 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3219 - acc: 0.8862 - val_loss: 0.3700 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3282 - acc: 0.8825 - val_loss: 0.4384 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3131 - acc: 0.8970 - val_loss: 0.3392 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3075 - acc: 0.8930 - val_loss: 0.3457 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3104 - acc: 0.8891 - val_loss: 0.6222 - val_acc: 0.6827                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3125 - acc: 0.8881 - val_loss: 0.6035 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3151 - acc: 0.8849 - val_loss: 0.4261 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3151 - acc: 0.8903 - val_loss: 0.5362 - val_acc: 0.7346                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3057 - acc: 0.8928 - val_loss: 0.3437 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3164 - acc: 0.8957 - val_loss: 0.3411 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3119 - acc: 0.8881 - val_loss: 0.3376 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3158 - acc: 0.8925 - val_loss: 0.4919 - val_acc: 0.8167                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3109 - acc: 0.8876 - val_loss: 0.3397 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2960 - acc: 0.8957 - val_loss: 0.3635 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3062 - acc: 0.8948 - val_loss: 0.3548 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3078 - acc: 0.8950 - val_loss: 0.8122 - val_acc: 0.7186                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3130 - acc: 0.8849 - val_loss: 0.3333 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2992 - acc: 0.8896 - val_loss: 0.4139 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3051 - acc: 0.8953 - val_loss: 0.3454 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3316 - acc: 0.8884 - val_loss: 0.4397 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3190 - acc: 0.8933 - val_loss: 0.4624 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3087 - acc: 0.8891 - val_loss: 0.4080 - val_acc: 0.8051                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3087 - acc: 0.8928 - val_loss: 0.3834 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3065 - acc: 0.8925 - val_loss: 0.3270 - val_acc: 0.8936                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9087779690189329                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8935897435897436                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                10256                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 14,271                                                                                                   \n",
      "Trainable params: 14,271                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 36%|████████████████▏                            | 43/120 [1:07:13<1:47:00, 83.39s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 10.8849 - acc: 0.8311 - val_loss: 0.5395 - val_acc: 0.8199                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.4113 - acc: 0.8689 - val_loss: 0.4353 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3634 - acc: 0.8761 - val_loss: 0.4600 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3622 - acc: 0.8807 - val_loss: 0.3867 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3597 - acc: 0.8810 - val_loss: 0.3888 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3579 - acc: 0.8756 - val_loss: 0.4568 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3422 - acc: 0.8864 - val_loss: 0.4161 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.3436 - acc: 0.8837 - val_loss: 0.4018 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3407 - acc: 0.8862 - val_loss: 0.3699 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3511 - acc: 0.8795 - val_loss: 0.3644 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3346 - acc: 0.8871 - val_loss: 0.4962 - val_acc: 0.7385                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3526 - acc: 0.8761 - val_loss: 0.4283 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3271 - acc: 0.8810 - val_loss: 0.4041 - val_acc: 0.8288                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3320 - acc: 0.8842 - val_loss: 0.4546 - val_acc: 0.8122                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3240 - acc: 0.8869 - val_loss: 0.3816 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3387 - acc: 0.8871 - val_loss: 0.3535 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3327 - acc: 0.8859 - val_loss: 0.4799 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3241 - acc: 0.8876 - val_loss: 0.4978 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3211 - acc: 0.8871 - val_loss: 0.3489 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3262 - acc: 0.8854 - val_loss: 0.4038 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3279 - acc: 0.8854 - val_loss: 0.4519 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3297 - acc: 0.8886 - val_loss: 0.3698 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3267 - acc: 0.8798 - val_loss: 0.3718 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3296 - acc: 0.8852 - val_loss: 0.7016 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3348 - acc: 0.8798 - val_loss: 0.3418 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3145 - acc: 0.8918 - val_loss: 0.3711 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3228 - acc: 0.8921 - val_loss: 0.3714 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3167 - acc: 0.8849 - val_loss: 0.3747 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.3219 - acc: 0.8871 - val_loss: 0.3714 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3166 - acc: 0.8921 - val_loss: 0.3785 - val_acc: 0.8481                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8925497910007376                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8480769230769231                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 116, 32)           6304                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 116, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 736)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                23584                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 31,779                                                                                                   \n",
      "Trainable params: 31,779                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 37%|████████████████▌                            | 44/120 [1:08:52<1:51:23, 87.94s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 69.8809 - acc: 0.8043 - val_loss: 17.1833 - val_acc: 0.8673                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 7.0860 - acc: 0.8874 - val_loss: 2.5300 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 1.1758 - acc: 0.8982 - val_loss: 0.7734 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.4404 - acc: 0.8945 - val_loss: 0.5172 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 4s - loss: 0.3662 - acc: 0.8866 - val_loss: 0.5213 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3112 - acc: 0.8989 - val_loss: 0.4949 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3538 - acc: 0.8869 - val_loss: 0.4912 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3412 - acc: 0.8906 - val_loss: 0.4561 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3193 - acc: 0.8997 - val_loss: 0.4885 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3132 - acc: 0.8925 - val_loss: 0.4752 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3177 - acc: 0.8940 - val_loss: 0.5025 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.2962 - acc: 0.8989 - val_loss: 0.4501 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.2875 - acc: 0.9039 - val_loss: 0.4324 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.2989 - acc: 0.8970 - val_loss: 0.4160 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.2861 - acc: 0.9026 - val_loss: 0.3979 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3118 - acc: 0.9004 - val_loss: 0.4215 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 4s - loss: 0.3157 - acc: 0.8925 - val_loss: 0.4323 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3141 - acc: 0.8980 - val_loss: 0.4038 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2833 - acc: 0.9007 - val_loss: 0.4313 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3015 - acc: 0.8965 - val_loss: 0.4032 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2825 - acc: 0.8999 - val_loss: 0.4200 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2904 - acc: 0.8950 - val_loss: 0.4038 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.2829 - acc: 0.8997 - val_loss: 0.4087 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.2832 - acc: 0.8987 - val_loss: 0.3939 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.2877 - acc: 0.8970 - val_loss: 0.4216 - val_acc: 0.8712                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9095156134743054                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8711538461538462                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 38%|████████████████▉                            | 45/120 [1:10:21<1:50:13, 88.18s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 4.9169 - acc: 0.8394 - val_loss: 0.4765 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.3869 - acc: 0.8763 - val_loss: 0.3919 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3624 - acc: 0.8842 - val_loss: 0.4333 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3599 - acc: 0.8803 - val_loss: 0.3462 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.4013 - acc: 0.8712 - val_loss: 0.4112 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 1.1849 - acc: 0.8220 - val_loss: 1.3832 - val_acc: 0.8263                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 4.2454 - acc: 0.7288 - val_loss: 5.5555 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 5.5988 - acc: 0.6504 - val_loss: 5.5787 - val_acc: 0.6545                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 5.6071 - acc: 0.6494 - val_loss: 5.6431 - val_acc: 0.6526                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 5.6860 - acc: 0.6474 - val_loss: 5.5306 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 5.7983 - acc: 0.6356 - val_loss: 5.5413 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 5.6843 - acc: 0.6464 - val_loss: 5.5784 - val_acc: 0.6551                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 5.6359 - acc: 0.6484 - val_loss: 5.5952 - val_acc: 0.6558                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 8.1535 - acc: 0.4952 - val_loss: 11.0559 - val_acc: 0.3147                                               \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 11.0324 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 4s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 11.0325 - acc: 0.3162 - val_loss: 11.0560 - val_acc: 0.3147                                              \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.31620358986968283                                                                                                    \n",
      "Test accuracy:                                                                                                         \n",
      "0.31474358974358974                                                                                                    \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 42)           1176                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 992)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15888                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 19,147                                                                                                   \n",
      "Trainable params: 19,147                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 38%|█████████████████▎                           | 46/120 [1:12:18<1:59:39, 97.03s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 14.4711 - acc: 0.8112 - val_loss: 0.8476 - val_acc: 0.7840                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.4779 - acc: 0.8596 - val_loss: 0.4660 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4003 - acc: 0.8667 - val_loss: 0.4364 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3873 - acc: 0.8773 - val_loss: 0.4018 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3837 - acc: 0.8721 - val_loss: 0.4188 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3755 - acc: 0.8798 - val_loss: 0.4722 - val_acc: 0.8013                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3526 - acc: 0.8812 - val_loss: 0.3838 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3547 - acc: 0.8771 - val_loss: 0.4570 - val_acc: 0.8423                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3420 - acc: 0.8886 - val_loss: 0.4519 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3392 - acc: 0.8866 - val_loss: 0.3594 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3301 - acc: 0.8839 - val_loss: 0.6252 - val_acc: 0.6872                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3439 - acc: 0.8837 - val_loss: 0.4963 - val_acc: 0.7942                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3452 - acc: 0.8857 - val_loss: 0.4193 - val_acc: 0.8288                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3358 - acc: 0.8839 - val_loss: 0.5379 - val_acc: 0.7173                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3395 - acc: 0.8849 - val_loss: 0.4124 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3433 - acc: 0.8871 - val_loss: 0.3789 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3454 - acc: 0.8812 - val_loss: 0.4272 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3319 - acc: 0.8859 - val_loss: 0.5337 - val_acc: 0.7199                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3259 - acc: 0.8849 - val_loss: 0.3538 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3247 - acc: 0.8906 - val_loss: 0.3751 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3303 - acc: 0.8854 - val_loss: 0.3737 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3222 - acc: 0.8884 - val_loss: 0.4472 - val_acc: 0.7712                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3281 - acc: 0.8864 - val_loss: 0.3548 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3321 - acc: 0.8898 - val_loss: 0.3714 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3375 - acc: 0.8803 - val_loss: 0.3542 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3283 - acc: 0.8869 - val_loss: 0.3503 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3346 - acc: 0.8884 - val_loss: 0.4437 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3367 - acc: 0.8876 - val_loss: 0.4013 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.3406 - acc: 0.8805 - val_loss: 0.3713 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3459 - acc: 0.8778 - val_loss: 0.3807 - val_acc: 0.8500                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8770592574379149                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.85                                                                                                                   \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1280)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                40992                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 45,099                                                                                                   \n",
      "Trainable params: 45,099                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 39%|█████████████████▋                           | 47/120 [1:13:37<1:51:26, 91.60s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 81.4748 - acc: 0.8173 - val_loss: 53.6524 - val_acc: 0.8827                                              \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 37.2256 - acc: 0.9063 - val_loss: 24.0494 - val_acc: 0.8942                                              \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 16.3372 - acc: 0.9230 - val_loss: 10.3413 - val_acc: 0.8814                                              \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 6.8660 - acc: 0.9228 - val_loss: 4.3660 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 2.8637 - acc: 0.9149 - val_loss: 1.9269 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 1.2551 - acc: 0.9181 - val_loss: 0.9868 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.6778 - acc: 0.9088 - val_loss: 0.6790 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.4739 - acc: 0.9063 - val_loss: 0.5639 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3936 - acc: 0.9127 - val_loss: 0.4771 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3387 - acc: 0.9144 - val_loss: 0.4555 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3226 - acc: 0.9083 - val_loss: 0.4429 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3107 - acc: 0.9105 - val_loss: 0.4491 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3266 - acc: 0.9107 - val_loss: 0.4677 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2910 - acc: 0.9080 - val_loss: 0.3851 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2804 - acc: 0.9159 - val_loss: 0.3847 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2745 - acc: 0.9115 - val_loss: 0.3989 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2893 - acc: 0.9085 - val_loss: 0.4011 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2825 - acc: 0.9088 - val_loss: 0.3791 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.2493 - acc: 0.9174 - val_loss: 0.3646 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2529 - acc: 0.9181 - val_loss: 0.3596 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2667 - acc: 0.9157 - val_loss: 0.4188 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2595 - acc: 0.9157 - val_loss: 0.3369 - val_acc: 0.9013                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.2454 - acc: 0.9225 - val_loss: 0.3596 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2545 - acc: 0.9186 - val_loss: 0.3475 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.2626 - acc: 0.9169 - val_loss: 0.3551 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2407 - acc: 0.9206 - val_loss: 0.3398 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2407 - acc: 0.9221 - val_loss: 0.3614 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2644 - acc: 0.9147 - val_loss: 0.3474 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2464 - acc: 0.9213 - val_loss: 0.3418 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2319 - acc: 0.9221 - val_loss: 0.3200 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2326 - acc: 0.9253 - val_loss: 0.3114 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2256 - acc: 0.9250 - val_loss: 0.3098 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.2429 - acc: 0.9235 - val_loss: 0.3677 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2583 - acc: 0.9134 - val_loss: 0.3395 - val_acc: 0.9147                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.2346 - acc: 0.9275 - val_loss: 0.3452 - val_acc: 0.8750                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9245143840668798                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.875                                                                                                                  \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 116, 16)           3600                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 116, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 368)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                23616                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 29,459                                                                                                   \n",
      "Trainable params: 29,459                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 40%|██████████████████                           | 48/120 [1:15:15<1:52:13, 93.52s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 4s - loss: 55.5716 - acc: 0.7465 - val_loss: 3.9084 - val_acc: 0.8494                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 1.0705 - acc: 0.8156 - val_loss: 0.6808 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.5442 - acc: 0.8495 - val_loss: 0.5843 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.5131 - acc: 0.8596 - val_loss: 0.6413 - val_acc: 0.7782                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.5197 - acc: 0.8630 - val_loss: 0.6405 - val_acc: 0.7776                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.4664 - acc: 0.8721 - val_loss: 0.4774 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.5232 - acc: 0.8517 - val_loss: 0.5454 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.4531 - acc: 0.8687 - val_loss: 0.6663 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.4637 - acc: 0.8739 - val_loss: 0.5321 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.4537 - acc: 0.8704 - val_loss: 0.4965 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.4360 - acc: 0.8748 - val_loss: 0.5290 - val_acc: 0.7904                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.4615 - acc: 0.8628 - val_loss: 0.5244 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.4527 - acc: 0.8682 - val_loss: 0.4693 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.4429 - acc: 0.8788 - val_loss: 0.5932 - val_acc: 0.7365                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.4741 - acc: 0.8603 - val_loss: 0.6006 - val_acc: 0.7878                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.4474 - acc: 0.8761 - val_loss: 0.5351 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.4479 - acc: 0.8699 - val_loss: 0.4797 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.4427 - acc: 0.8685 - val_loss: 0.5052 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.4583 - acc: 0.8650 - val_loss: 0.5333 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.4307 - acc: 0.8716 - val_loss: 1.2558 - val_acc: 0.5397                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.4468 - acc: 0.8709 - val_loss: 0.4402 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.4557 - acc: 0.8721 - val_loss: 0.5008 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.4387 - acc: 0.8751 - val_loss: 0.4807 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.4394 - acc: 0.8682 - val_loss: 0.5081 - val_acc: 0.8212                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.4477 - acc: 0.8665 - val_loss: 0.5100 - val_acc: 0.8154                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8423899680354069                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8153846153846154                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 42)           1176                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                15632                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 20,235                                                                                                   \n",
      "Trainable params: 20,235                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 41%|██████████████████▍                          | 49/120 [1:16:29<1:43:42, 87.64s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 28.7809 - acc: 0.8316 - val_loss: 0.5454 - val_acc: 0.8032                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 0.4390 - acc: 0.8618 - val_loss: 0.4594 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.3963 - acc: 0.8712 - val_loss: 0.4625 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.3860 - acc: 0.8773 - val_loss: 0.3864 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4015 - acc: 0.8783 - val_loss: 0.4481 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.3828 - acc: 0.8766 - val_loss: 0.4848 - val_acc: 0.8096                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3810 - acc: 0.8714 - val_loss: 0.4102 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3800 - acc: 0.8780 - val_loss: 0.4649 - val_acc: 0.8173                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.3804 - acc: 0.8744 - val_loss: 0.3953 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.4043 - acc: 0.8712 - val_loss: 0.4049 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.3791 - acc: 0.8776 - val_loss: 0.7602 - val_acc: 0.6583                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.3758 - acc: 0.8724 - val_loss: 0.5317 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.3867 - acc: 0.8714 - val_loss: 0.5171 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.3693 - acc: 0.8795 - val_loss: 0.5496 - val_acc: 0.7199                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.3613 - acc: 0.8822 - val_loss: 0.4063 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 4s - loss: 0.3752 - acc: 0.8820 - val_loss: 0.5190 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.4016 - acc: 0.8766 - val_loss: 0.4857 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.3752 - acc: 0.8822 - val_loss: 0.7129 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.3672 - acc: 0.8803 - val_loss: 0.5370 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.3656 - acc: 0.8800 - val_loss: 0.6128 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.3748 - acc: 0.8736 - val_loss: 0.5184 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.3725 - acc: 0.8803 - val_loss: 0.8109 - val_acc: 0.6763                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 4s - loss: 0.3990 - acc: 0.8734 - val_loss: 0.4791 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3817 - acc: 0.8822 - val_loss: 0.5317 - val_acc: 0.7513                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.3867 - acc: 0.8766 - val_loss: 0.4152 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.3842 - acc: 0.8830 - val_loss: 0.5203 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.3828 - acc: 0.8822 - val_loss: 0.5925 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3755 - acc: 0.8788 - val_loss: 0.5181 - val_acc: 0.7853                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.4028 - acc: 0.8795 - val_loss: 0.5742 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.3961 - acc: 0.8761 - val_loss: 0.3961 - val_acc: 0.8731                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8945168428817311                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8730769230769231                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                62496                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 66,603                                                                                                   \n",
      "Trainable params: 66,603                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 42%|██████████████████▊                          | 50/120 [1:18:18<1:49:39, 94.00s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 40.1664 - acc: 0.8579 - val_loss: 8.3889 - val_acc: 0.8840                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 2.9090 - acc: 0.8985 - val_loss: 0.7629 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.4322 - acc: 0.8908 - val_loss: 0.4455 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3594 - acc: 0.8820 - val_loss: 0.4307 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3212 - acc: 0.8955 - val_loss: 0.4039 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3070 - acc: 0.8962 - val_loss: 0.4226 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3269 - acc: 0.8935 - val_loss: 0.3703 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3267 - acc: 0.8925 - val_loss: 0.4262 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3660 - acc: 0.8923 - val_loss: 0.4347 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3015 - acc: 0.8965 - val_loss: 0.3749 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3189 - acc: 0.8955 - val_loss: 0.3866 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3170 - acc: 0.8871 - val_loss: 0.3881 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2883 - acc: 0.8982 - val_loss: 0.3801 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3223 - acc: 0.8972 - val_loss: 0.3908 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2868 - acc: 0.8985 - val_loss: 0.3645 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3087 - acc: 0.8938 - val_loss: 0.3604 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.3252 - acc: 0.8906 - val_loss: 0.3471 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2988 - acc: 0.8943 - val_loss: 0.3768 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3070 - acc: 0.8953 - val_loss: 0.3666 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2905 - acc: 0.9019 - val_loss: 0.3291 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2801 - acc: 0.9016 - val_loss: 0.3519 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.3074 - acc: 0.8940 - val_loss: 0.3528 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3108 - acc: 0.8906 - val_loss: 0.3468 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2924 - acc: 0.8930 - val_loss: 0.3669 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3030 - acc: 0.8943 - val_loss: 0.3756 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.3114 - acc: 0.8921 - val_loss: 0.3545 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2856 - acc: 0.9009 - val_loss: 0.3482 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2968 - acc: 0.8972 - val_loss: 0.3421 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2758 - acc: 0.9009 - val_loss: 0.3884 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2838 - acc: 0.8957 - val_loss: 0.3297 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2944 - acc: 0.8925 - val_loss: 0.3511 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2682 - acc: 0.9031 - val_loss: 0.3541 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.2846 - acc: 0.9039 - val_loss: 0.3609 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2799 - acc: 0.8985 - val_loss: 0.3209 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8967 - val_loss: 0.3400 - val_acc: 0.8840                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8962380132776002                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8839743589743589                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                41024                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,243                                                                                                   \n",
      "Trainable params: 44,243                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 42%|███████████████████▏                         | 51/120 [1:20:04<1:52:09, 97.53s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 2s - loss: 17.1364 - acc: 0.8259 - val_loss: 4.9381 - val_acc: 0.8885                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 1.9384 - acc: 0.8953 - val_loss: 0.8102 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 0.5710 - acc: 0.9073 - val_loss: 0.5963 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.4582 - acc: 0.9085 - val_loss: 0.5794 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.4145 - acc: 0.9036 - val_loss: 0.5120 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3469 - acc: 0.9105 - val_loss: 0.4339 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 0.3531 - acc: 0.9073 - val_loss: 0.4176 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.3077 - acc: 0.9203 - val_loss: 0.3927 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.3232 - acc: 0.9149 - val_loss: 0.3745 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.2841 - acc: 0.9240 - val_loss: 0.3588 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.2743 - acc: 0.9221 - val_loss: 0.3328 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.2539 - acc: 0.9289 - val_loss: 0.3162 - val_acc: 0.9058                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.2860 - acc: 0.9213 - val_loss: 0.3247 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.2735 - acc: 0.9265 - val_loss: 0.3268 - val_acc: 0.9109                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.2536 - acc: 0.9275 - val_loss: 0.3089 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.2455 - acc: 0.9292 - val_loss: 0.3149 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.2388 - acc: 0.9304 - val_loss: 0.3417 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.2723 - acc: 0.9257 - val_loss: 0.2947 - val_acc: 0.9058                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.2413 - acc: 0.9262 - val_loss: 0.2802 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.2273 - acc: 0.9351 - val_loss: 0.2781 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.2285 - acc: 0.9353 - val_loss: 0.2912 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.2260 - acc: 0.9309 - val_loss: 0.2997 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.2142 - acc: 0.9353 - val_loss: 0.2705 - val_acc: 0.9135                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.2227 - acc: 0.9312 - val_loss: 0.2684 - val_acc: 0.9295                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.2241 - acc: 0.9314 - val_loss: 0.2822 - val_acc: 0.8910                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.940742562085075                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8910256410256411                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1440)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                23056                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 29,403                                                                                                   \n",
      "Trainable params: 29,403                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 43%|███████████████████▌                         | 52/120 [1:20:58<1:35:42, 84.44s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 6.3405 - acc: 0.8571 - val_loss: 0.5675 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 0.3634 - acc: 0.8940 - val_loss: 0.4415 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.3345 - acc: 0.8960 - val_loss: 0.3872 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.3066 - acc: 0.8989 - val_loss: 0.3700 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 4s - loss: 0.3245 - acc: 0.8953 - val_loss: 0.4354 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.2991 - acc: 0.8994 - val_loss: 0.3475 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.2875 - acc: 0.9004 - val_loss: 0.3634 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.2935 - acc: 0.8980 - val_loss: 0.3758 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.2991 - acc: 0.8982 - val_loss: 0.3613 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.2837 - acc: 0.9036 - val_loss: 0.3365 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.2694 - acc: 0.9024 - val_loss: 0.3291 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.2823 - acc: 0.8953 - val_loss: 0.3265 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.2882 - acc: 0.8970 - val_loss: 0.3229 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.2740 - acc: 0.9053 - val_loss: 0.3742 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.2927 - acc: 0.9002 - val_loss: 0.3371 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 4s - loss: 0.2799 - acc: 0.9024 - val_loss: 0.3680 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.2872 - acc: 0.8970 - val_loss: 0.3193 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.2699 - acc: 0.8997 - val_loss: 0.3666 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.2811 - acc: 0.8928 - val_loss: 0.3547 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2900 - acc: 0.8972 - val_loss: 0.3315 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.2686 - acc: 0.9031 - val_loss: 0.4501 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.2670 - acc: 0.8985 - val_loss: 0.3197 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 4s - loss: 0.2618 - acc: 0.8980 - val_loss: 0.3114 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 4s - loss: 0.2632 - acc: 0.8999 - val_loss: 0.3174 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.2693 - acc: 0.9021 - val_loss: 0.3381 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.2721 - acc: 0.8999 - val_loss: 0.3847 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.2818 - acc: 0.8999 - val_loss: 0.3561 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.2701 - acc: 0.9004 - val_loss: 0.3533 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.2730 - acc: 0.9007 - val_loss: 0.3241 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.2735 - acc: 0.8989 - val_loss: 0.3173 - val_acc: 0.8750                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.916154413572658                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.875                                                                                                                  \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 368)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                23616                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 29,875                                                                                                   \n",
      "Trainable params: 29,875                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 44%|███████████████████▉                         | 53/120 [1:22:56<1:45:33, 94.53s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 69.8062 - acc: 0.8296 - val_loss: 15.3941 - val_acc: 0.8462                                              \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 4.5084 - acc: 0.8537 - val_loss: 0.7782 - val_acc: 0.7769                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.4827 - acc: 0.8564 - val_loss: 0.5321 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.4401 - acc: 0.8721 - val_loss: 0.5663 - val_acc: 0.8000                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.4016 - acc: 0.8746 - val_loss: 0.4871 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3809 - acc: 0.8805 - val_loss: 0.4887 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3830 - acc: 0.8810 - val_loss: 0.4244 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3771 - acc: 0.8857 - val_loss: 0.5186 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3724 - acc: 0.8815 - val_loss: 0.4683 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3817 - acc: 0.8780 - val_loss: 0.4338 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3667 - acc: 0.8879 - val_loss: 0.4410 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3631 - acc: 0.8830 - val_loss: 0.4476 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3581 - acc: 0.8876 - val_loss: 0.4179 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3760 - acc: 0.8874 - val_loss: 0.4938 - val_acc: 0.8071                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3579 - acc: 0.8842 - val_loss: 0.9008 - val_acc: 0.6506                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3685 - acc: 0.8854 - val_loss: 0.3942 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.3502 - acc: 0.8874 - val_loss: 0.4257 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3765 - acc: 0.8876 - val_loss: 0.4731 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3488 - acc: 0.8859 - val_loss: 0.4374 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.3529 - acc: 0.8864 - val_loss: 0.9787 - val_acc: 0.5955                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3626 - acc: 0.8825 - val_loss: 0.4146 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.3668 - acc: 0.8847 - val_loss: 0.4154 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3498 - acc: 0.8825 - val_loss: 0.4276 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.3501 - acc: 0.8849 - val_loss: 0.4685 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3520 - acc: 0.8857 - val_loss: 0.4017 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.3446 - acc: 0.8859 - val_loss: 0.4057 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.3381 - acc: 0.8866 - val_loss: 0.4369 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.3406 - acc: 0.8864 - val_loss: 0.4688 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.3611 - acc: 0.8862 - val_loss: 0.4450 - val_acc: 0.8327                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.3430 - acc: 0.8889 - val_loss: 0.4711 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.3422 - acc: 0.8898 - val_loss: 0.3992 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.3405 - acc: 0.8889 - val_loss: 0.4405 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.3413 - acc: 0.8879 - val_loss: 0.4759 - val_acc: 0.7872                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.3360 - acc: 0.8857 - val_loss: 0.4262 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3436 - acc: 0.8923 - val_loss: 0.4092 - val_acc: 0.8788                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8861568723875092                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8788461538461538                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                62496                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 67,171                                                                                                   \n",
      "Trainable params: 67,171                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 45%|███████████████████▊                        | 54/120 [1:24:50<1:50:26, 100.41s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 25.9319 - acc: 0.8392 - val_loss: 1.5820 - val_acc: 0.7712                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.5456 - acc: 0.8726 - val_loss: 0.5079 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3733 - acc: 0.8807 - val_loss: 0.4757 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3377 - acc: 0.8896 - val_loss: 0.4281 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3503 - acc: 0.8891 - val_loss: 0.4193 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3198 - acc: 0.8928 - val_loss: 0.4577 - val_acc: 0.8096                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3249 - acc: 0.8857 - val_loss: 0.3609 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3315 - acc: 0.8820 - val_loss: 0.4642 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3039 - acc: 0.8997 - val_loss: 0.3374 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3137 - acc: 0.8928 - val_loss: 0.3600 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3045 - acc: 0.8916 - val_loss: 0.5912 - val_acc: 0.6923                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3189 - acc: 0.8903 - val_loss: 0.5053 - val_acc: 0.8135                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3167 - acc: 0.8881 - val_loss: 0.4369 - val_acc: 0.8090                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3195 - acc: 0.8911 - val_loss: 0.5164 - val_acc: 0.7295                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3109 - acc: 0.8903 - val_loss: 0.3712 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3146 - acc: 0.8930 - val_loss: 0.3616 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3099 - acc: 0.8881 - val_loss: 0.3477 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2993 - acc: 0.8960 - val_loss: 0.3465 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3081 - acc: 0.8889 - val_loss: 0.4019 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2988 - acc: 0.8945 - val_loss: 0.3813 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2914 - acc: 0.8918 - val_loss: 0.4410 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2992 - acc: 0.8925 - val_loss: 0.3705 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3003 - acc: 0.8886 - val_loss: 0.3343 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3104 - acc: 0.8884 - val_loss: 0.3386 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3031 - acc: 0.8898 - val_loss: 0.3296 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3141 - acc: 0.8925 - val_loss: 0.4114 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3100 - acc: 0.8918 - val_loss: 0.3756 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2953 - acc: 0.8940 - val_loss: 0.4587 - val_acc: 0.7462                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3131 - acc: 0.8928 - val_loss: 0.3440 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3139 - acc: 0.8891 - val_loss: 0.3345 - val_acc: 0.8763                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9055815097123187                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8762820512820513                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 640)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                41024                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 43,867                                                                                                   \n",
      "Trainable params: 43,867                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 46%|████████████████████▋                        | 55/120 [1:26:20<1:45:31, 97.40s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 46.7227 - acc: 0.7504 - val_loss: 35.9021 - val_acc: 0.8096                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 28.2558 - acc: 0.8726 - val_loss: 21.6416 - val_acc: 0.8705                                              \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 16.9094 - acc: 0.9048 - val_loss: 12.9129 - val_acc: 0.8686                                              \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 9.9939 - acc: 0.9152 - val_loss: 7.6382 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 5.8575 - acc: 0.9157 - val_loss: 4.5142 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 3.4229 - acc: 0.9198 - val_loss: 2.6946 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 2.0248 - acc: 0.9125 - val_loss: 1.6644 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 1.2480 - acc: 0.9073 - val_loss: 1.0804 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.7938 - acc: 0.9122 - val_loss: 0.7582 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.5500 - acc: 0.9100 - val_loss: 0.5772 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.4181 - acc: 0.9117 - val_loss: 0.4757 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.3463 - acc: 0.9085 - val_loss: 0.4293 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.3029 - acc: 0.9149 - val_loss: 0.3917 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.2891 - acc: 0.9095 - val_loss: 0.3763 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.2736 - acc: 0.9130 - val_loss: 0.3636 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.2671 - acc: 0.9139 - val_loss: 0.3756 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.2723 - acc: 0.9058 - val_loss: 0.3447 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.2771 - acc: 0.9071 - val_loss: 0.3447 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.2566 - acc: 0.9122 - val_loss: 0.3400 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.2537 - acc: 0.9169 - val_loss: 0.3472 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2511 - acc: 0.9132 - val_loss: 0.3586 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2576 - acc: 0.9125 - val_loss: 0.3252 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.2516 - acc: 0.9125 - val_loss: 0.3329 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.2511 - acc: 0.9142 - val_loss: 0.3257 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.2472 - acc: 0.9162 - val_loss: 0.3360 - val_acc: 0.8891                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9294320137693631                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.889102564102564                                                                                                      \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1440)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                23056                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 29,403                                                                                                   \n",
      "Trainable params: 29,403                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 47%|█████████████████████                        | 56/120 [1:27:17<1:31:04, 85.38s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 31.7427 - acc: 0.7876 - val_loss: 7.3728 - val_acc: 0.8449                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 2.3846 - acc: 0.8603 - val_loss: 0.5778 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.4320 - acc: 0.8672 - val_loss: 0.5159 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.4188 - acc: 0.8751 - val_loss: 0.5006 - val_acc: 0.7904                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 4s - loss: 0.3742 - acc: 0.8790 - val_loss: 0.4922 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.3509 - acc: 0.8849 - val_loss: 0.4804 - val_acc: 0.8032                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.3716 - acc: 0.8812 - val_loss: 0.4086 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.3463 - acc: 0.8886 - val_loss: 0.5109 - val_acc: 0.8192                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.3534 - acc: 0.8884 - val_loss: 0.3793 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.3507 - acc: 0.8837 - val_loss: 0.4632 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3453 - acc: 0.8869 - val_loss: 0.4176 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3322 - acc: 0.8908 - val_loss: 0.3727 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3350 - acc: 0.8874 - val_loss: 0.3661 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3318 - acc: 0.8903 - val_loss: 0.5162 - val_acc: 0.7846                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3494 - acc: 0.8871 - val_loss: 0.4552 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3429 - acc: 0.8862 - val_loss: 0.3588 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.3474 - acc: 0.8906 - val_loss: 0.3862 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.3296 - acc: 0.8857 - val_loss: 0.3806 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.3376 - acc: 0.8871 - val_loss: 0.3861 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.3256 - acc: 0.8898 - val_loss: 0.5097 - val_acc: 0.8141                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.3311 - acc: 0.8906 - val_loss: 0.3516 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3298 - acc: 0.8943 - val_loss: 0.3847 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3286 - acc: 0.8894 - val_loss: 0.3697 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3401 - acc: 0.8839 - val_loss: 0.4289 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.3296 - acc: 0.8925 - val_loss: 0.4733 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3312 - acc: 0.8918 - val_loss: 0.3828 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3210 - acc: 0.8862 - val_loss: 0.3948 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3367 - acc: 0.8835 - val_loss: 0.3608 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3295 - acc: 0.8930 - val_loss: 0.3938 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3215 - acc: 0.8925 - val_loss: 0.4141 - val_acc: 0.8321                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8185394639783624                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8320512820512821                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 48%|█████████████████████▍                       | 57/120 [1:29:08<1:37:37, 92.97s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 8.4518 - acc: 0.8621 - val_loss: 0.6482 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.4179 - acc: 0.8896 - val_loss: 0.4333 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3425 - acc: 0.8923 - val_loss: 0.4110 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3037 - acc: 0.8980 - val_loss: 0.3447 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3181 - acc: 0.8925 - val_loss: 0.3806 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3114 - acc: 0.8945 - val_loss: 0.4208 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.2914 - acc: 0.9021 - val_loss: 0.4517 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3024 - acc: 0.8955 - val_loss: 0.5060 - val_acc: 0.8122                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3529 - acc: 0.8832 - val_loss: 0.3831 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.5198 - acc: 0.8591 - val_loss: 1.2590 - val_acc: 0.7981                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 1.6312 - acc: 0.8358 - val_loss: 5.1387 - val_acc: 0.6538                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 1.8903 - acc: 0.8594 - val_loss: 1.3968 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 2.0245 - acc: 0.8675 - val_loss: 2.5397 - val_acc: 0.8128                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 2.9488 - acc: 0.8212 - val_loss: 5.9615 - val_acc: 0.6436                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 5.6094 - acc: 0.6528 - val_loss: 5.5522 - val_acc: 0.6564                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 5.6018 - acc: 0.6499 - val_loss: 5.5145 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 5.5767 - acc: 0.6518 - val_loss: 5.5674 - val_acc: 0.6551                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 5.5916 - acc: 0.6521 - val_loss: 5.5109 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 5.5409 - acc: 0.6531 - val_loss: 5.5266 - val_acc: 0.6577                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 5.5764 - acc: 0.6528 - val_loss: 5.5172 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 5.5403 - acc: 0.6545 - val_loss: 5.5598 - val_acc: 0.6545                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 5.5646 - acc: 0.6518 - val_loss: 5.5263 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 5.5624 - acc: 0.6543 - val_loss: 5.5476 - val_acc: 0.6577                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 5.5204 - acc: 0.6572 - val_loss: 5.5212 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 5.5322 - acc: 0.6555 - val_loss: 5.5250 - val_acc: 0.6583                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 5.5666 - acc: 0.6540 - val_loss: 5.5329 - val_acc: 0.6577                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 5.5623 - acc: 0.6523 - val_loss: 5.5152 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 5.5359 - acc: 0.6548 - val_loss: 5.5838 - val_acc: 0.6519                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 5.5777 - acc: 0.6523 - val_loss: 5.5213 - val_acc: 0.6590                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 5.5387 - acc: 0.6563 - val_loss: 5.5420 - val_acc: 0.6558                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.657487091222031                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.6557692307692308                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           5064                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 576)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                18464                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 25,559                                                                                                   \n",
      "Trainable params: 25,559                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 48%|█████████████████████▊                       | 58/120 [1:30:37<1:34:53, 91.83s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 75.7934 - acc: 0.7903 - val_loss: 38.0178 - val_acc: 0.8545                                              \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 20.6820 - acc: 0.8603 - val_loss: 8.4376 - val_acc: 0.8577                                               \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 3.5461 - acc: 0.8702 - val_loss: 1.0232 - val_acc: 0.8090                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.4945 - acc: 0.8862 - val_loss: 0.5699 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3672 - acc: 0.8849 - val_loss: 0.5196 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3469 - acc: 0.8881 - val_loss: 0.4451 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3413 - acc: 0.8911 - val_loss: 0.4050 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.3262 - acc: 0.8928 - val_loss: 0.4770 - val_acc: 0.8090                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3159 - acc: 0.8997 - val_loss: 0.4077 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3138 - acc: 0.8898 - val_loss: 0.4776 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3071 - acc: 0.8972 - val_loss: 0.4275 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.2973 - acc: 0.8970 - val_loss: 0.3800 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2990 - acc: 0.9014 - val_loss: 0.3759 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2894 - acc: 0.9034 - val_loss: 0.5147 - val_acc: 0.8263                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3001 - acc: 0.8960 - val_loss: 0.4176 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2980 - acc: 0.8930 - val_loss: 0.3688 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2864 - acc: 0.9009 - val_loss: 0.3672 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2925 - acc: 0.8972 - val_loss: 0.3735 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.2956 - acc: 0.8916 - val_loss: 0.3915 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.2941 - acc: 0.8948 - val_loss: 0.5200 - val_acc: 0.7878                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2822 - acc: 0.9014 - val_loss: 0.3715 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2873 - acc: 0.8977 - val_loss: 0.3569 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.2966 - acc: 0.8965 - val_loss: 0.3815 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2874 - acc: 0.8935 - val_loss: 0.4049 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.2905 - acc: 0.8950 - val_loss: 0.3745 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2889 - acc: 0.8925 - val_loss: 0.5240 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2815 - acc: 0.8955 - val_loss: 0.3488 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2989 - acc: 0.8930 - val_loss: 0.3820 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2872 - acc: 0.8985 - val_loss: 0.3636 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2801 - acc: 0.8997 - val_loss: 0.3738 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2908 - acc: 0.8994 - val_loss: 0.3568 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2759 - acc: 0.9007 - val_loss: 0.3425 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.2812 - acc: 0.9036 - val_loss: 0.3823 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2782 - acc: 0.8985 - val_loss: 0.3454 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.2819 - acc: 0.8975 - val_loss: 0.5107 - val_acc: 0.8609                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8932874354561101                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.860897435897436                                                                                                      \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,299                                                                                                   \n",
      "Trainable params: 65,299                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 49%|█████████████████████▋                      | 59/120 [1:32:41<1:43:10, 101.48s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 22.0908 - acc: 0.8343 - val_loss: 0.8608 - val_acc: 0.7750                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 0.4322 - acc: 0.8783 - val_loss: 0.4804 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3659 - acc: 0.8852 - val_loss: 0.3775 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3995 - acc: 0.8810 - val_loss: 0.4105 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3406 - acc: 0.8894 - val_loss: 0.3843 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3064 - acc: 0.8957 - val_loss: 0.4125 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3549 - acc: 0.8837 - val_loss: 0.3930 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3449 - acc: 0.8847 - val_loss: 0.4168 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3160 - acc: 0.8992 - val_loss: 0.3882 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3406 - acc: 0.8948 - val_loss: 0.3899 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3176 - acc: 0.8943 - val_loss: 0.4022 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3592 - acc: 0.8800 - val_loss: 0.3796 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3212 - acc: 0.8896 - val_loss: 0.4714 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3012 - acc: 0.9004 - val_loss: 0.3995 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3057 - acc: 0.8913 - val_loss: 0.3806 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3176 - acc: 0.8940 - val_loss: 0.3725 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3298 - acc: 0.8906 - val_loss: 0.3643 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3002 - acc: 0.8948 - val_loss: 0.3718 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3550 - acc: 0.8884 - val_loss: 0.3660 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3044 - acc: 0.8950 - val_loss: 0.3381 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3110 - acc: 0.9007 - val_loss: 0.3601 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3195 - acc: 0.8886 - val_loss: 0.3533 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3045 - acc: 0.8960 - val_loss: 0.4336 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3469 - acc: 0.8795 - val_loss: 0.4314 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3132 - acc: 0.8916 - val_loss: 0.3723 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3247 - acc: 0.8901 - val_loss: 0.3776 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.3079 - acc: 0.8906 - val_loss: 0.3543 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3053 - acc: 0.8903 - val_loss: 0.3530 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2925 - acc: 0.8987 - val_loss: 0.3768 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3134 - acc: 0.8908 - val_loss: 0.3958 - val_acc: 0.8705                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8763216129825424                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8705128205128205                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                10512                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 12,707                                                                                                   \n",
      "Trainable params: 12,707                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 50%|██████████████████████▌                      | 60/120 [1:33:58<1:34:10, 94.18s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 2s - loss: 34.4271 - acc: 0.8070 - val_loss: 10.2233 - val_acc: 0.8551                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 4.0434 - acc: 0.8552 - val_loss: 1.0173 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 0.5313 - acc: 0.8662 - val_loss: 0.5493 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.3933 - acc: 0.8852 - val_loss: 0.5407 - val_acc: 0.7872                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.3831 - acc: 0.8785 - val_loss: 0.4834 - val_acc: 0.8032                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3691 - acc: 0.8803 - val_loss: 0.4385 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 0.3593 - acc: 0.8817 - val_loss: 0.4103 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.3482 - acc: 0.8812 - val_loss: 0.4695 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.3532 - acc: 0.8886 - val_loss: 0.4258 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.3531 - acc: 0.8805 - val_loss: 0.3907 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.3416 - acc: 0.8869 - val_loss: 0.3870 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.3411 - acc: 0.8857 - val_loss: 0.3997 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.3355 - acc: 0.8869 - val_loss: 0.3725 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.3248 - acc: 0.8980 - val_loss: 0.4955 - val_acc: 0.8090                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.3403 - acc: 0.8854 - val_loss: 0.4454 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.3330 - acc: 0.8871 - val_loss: 0.3915 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.3280 - acc: 0.8945 - val_loss: 0.3637 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.3409 - acc: 0.8911 - val_loss: 0.3760 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.3219 - acc: 0.8898 - val_loss: 0.3898 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.3155 - acc: 0.8923 - val_loss: 0.7395 - val_acc: 0.6853                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.3221 - acc: 0.8874 - val_loss: 0.3991 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.3230 - acc: 0.8869 - val_loss: 0.3727 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.3252 - acc: 0.8793 - val_loss: 0.3891 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.3190 - acc: 0.8916 - val_loss: 0.4219 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.3249 - acc: 0.8891 - val_loss: 0.3999 - val_acc: 0.8474                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8750922055569216                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8474358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                124992                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 51%|██████████████████████▉                      | 61/120 [1:34:48<1:19:28, 80.83s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 19.2787 - acc: 0.8296 - val_loss: 2.9047 - val_acc: 0.8474                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.0381 - acc: 0.8807 - val_loss: 0.6022 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4234 - acc: 0.8822 - val_loss: 0.4586 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3903 - acc: 0.8876 - val_loss: 0.4555 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3346 - acc: 0.8886 - val_loss: 0.5039 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3337 - acc: 0.8921 - val_loss: 0.4191 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3162 - acc: 0.8903 - val_loss: 0.3777 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3032 - acc: 0.8985 - val_loss: 0.3924 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2997 - acc: 0.8982 - val_loss: 0.3499 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3068 - acc: 0.8923 - val_loss: 0.6080 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3081 - acc: 0.8928 - val_loss: 0.4063 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2966 - acc: 0.9019 - val_loss: 0.4614 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3076 - acc: 0.8992 - val_loss: 0.3480 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2899 - acc: 0.8992 - val_loss: 0.5205 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3222 - acc: 0.8898 - val_loss: 0.4104 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2911 - acc: 0.8972 - val_loss: 0.3399 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2945 - acc: 0.8982 - val_loss: 0.3464 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3073 - acc: 0.8957 - val_loss: 0.5180 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3060 - acc: 0.8921 - val_loss: 0.3540 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2984 - acc: 0.8911 - val_loss: 0.3572 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3013 - acc: 0.8928 - val_loss: 0.3541 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3033 - acc: 0.8967 - val_loss: 0.3407 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3133 - acc: 0.8923 - val_loss: 0.4266 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3080 - acc: 0.8918 - val_loss: 0.3818 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3165 - acc: 0.8881 - val_loss: 0.5161 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3247 - acc: 0.8916 - val_loss: 0.3552 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3288 - acc: 0.8869 - val_loss: 0.3637 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3592 - acc: 0.8886 - val_loss: 0.3598 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3868 - acc: 0.8807 - val_loss: 0.3945 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.5891 - acc: 0.8456 - val_loss: 1.3813 - val_acc: 0.8051                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8234570936808459                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8051282051282052                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3600                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 944)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                30240                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 35,411                                                                                                   \n",
      "Trainable params: 35,411                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 52%|███████████████████████▎                     | 62/120 [1:36:29<1:23:54, 86.81s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 114.1556 - acc: 0.7900 - val_loss: 60.5345 - val_acc: 0.8756                                             \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 35.5331 - acc: 0.8517 - val_loss: 17.0380 - val_acc: 0.8667                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 8.4875 - acc: 0.8677 - val_loss: 2.8603 - val_acc: 0.7968                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 1.1345 - acc: 0.8748 - val_loss: 0.6418 - val_acc: 0.7801                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4215 - acc: 0.8776 - val_loss: 0.5348 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3748 - acc: 0.8857 - val_loss: 0.4263 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3682 - acc: 0.8839 - val_loss: 0.4386 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3528 - acc: 0.8852 - val_loss: 0.4788 - val_acc: 0.8122                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3518 - acc: 0.8896 - val_loss: 0.4030 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3577 - acc: 0.8837 - val_loss: 0.3761 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3415 - acc: 0.8903 - val_loss: 0.4702 - val_acc: 0.7756                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3360 - acc: 0.8896 - val_loss: 0.4239 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3229 - acc: 0.8955 - val_loss: 0.3709 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3240 - acc: 0.8977 - val_loss: 0.5597 - val_acc: 0.7865                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3322 - acc: 0.8891 - val_loss: 0.3758 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3223 - acc: 0.8891 - val_loss: 0.3601 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3154 - acc: 0.8962 - val_loss: 0.3529 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3331 - acc: 0.8864 - val_loss: 0.3501 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3147 - acc: 0.8906 - val_loss: 0.3790 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3106 - acc: 0.8906 - val_loss: 0.7282 - val_acc: 0.6987                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3215 - acc: 0.8913 - val_loss: 0.3675 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3110 - acc: 0.8957 - val_loss: 0.3525 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3109 - acc: 0.8913 - val_loss: 0.3579 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3162 - acc: 0.8864 - val_loss: 0.4240 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3125 - acc: 0.8862 - val_loss: 0.4174 - val_acc: 0.8423                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3075 - acc: 0.8908 - val_loss: 0.4982 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3039 - acc: 0.8889 - val_loss: 0.3423 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3142 - acc: 0.8906 - val_loss: 0.3503 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2997 - acc: 0.8975 - val_loss: 0.3431 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3044 - acc: 0.8997 - val_loss: 0.3599 - val_acc: 0.8692                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9095156134743054                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8692307692307693                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           3864                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1440)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                92224                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 52%|███████████████████████▋                     | 63/120 [1:37:55<1:22:12, 86.54s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 19.8139 - acc: 0.8431 - val_loss: 0.5387 - val_acc: 0.8385                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 4s - loss: 0.4627 - acc: 0.8707 - val_loss: 0.5230 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 4s - loss: 0.4282 - acc: 0.8709 - val_loss: 0.4731 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 4s - loss: 0.4171 - acc: 0.8795 - val_loss: 0.4110 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 4s - loss: 0.3660 - acc: 0.8876 - val_loss: 0.4090 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 4s - loss: 0.4146 - acc: 0.8758 - val_loss: 0.3946 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 4s - loss: 0.3746 - acc: 0.8758 - val_loss: 0.4009 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 4s - loss: 0.3905 - acc: 0.8783 - val_loss: 0.5566 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 4s - loss: 0.3843 - acc: 0.8852 - val_loss: 0.4425 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 4s - loss: 0.3681 - acc: 0.8805 - val_loss: 0.5436 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 4s - loss: 0.3862 - acc: 0.8830 - val_loss: 0.4199 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.3672 - acc: 0.8788 - val_loss: 0.4069 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 4s - loss: 0.3746 - acc: 0.8768 - val_loss: 0.3802 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 4s - loss: 0.3656 - acc: 0.8852 - val_loss: 0.4720 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 4s - loss: 0.3678 - acc: 0.8859 - val_loss: 0.4030 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 4s - loss: 0.3917 - acc: 0.8798 - val_loss: 0.4460 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 4s - loss: 0.3716 - acc: 0.8894 - val_loss: 0.3986 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 4s - loss: 0.3607 - acc: 0.8830 - val_loss: 0.4749 - val_acc: 0.8128                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.3741 - acc: 0.8805 - val_loss: 0.3908 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 4s - loss: 0.3738 - acc: 0.8862 - val_loss: 0.4060 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 4s - loss: 0.3760 - acc: 0.8854 - val_loss: 0.3999 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 4s - loss: 0.3642 - acc: 0.8758 - val_loss: 0.4265 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 4s - loss: 0.3901 - acc: 0.8748 - val_loss: 0.4773 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 4s - loss: 0.3926 - acc: 0.8704 - val_loss: 0.4939 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 4s - loss: 0.3744 - acc: 0.8864 - val_loss: 0.4088 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 4s - loss: 0.3601 - acc: 0.8803 - val_loss: 0.4304 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 4s - loss: 0.3688 - acc: 0.8815 - val_loss: 0.3636 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 4s - loss: 0.3461 - acc: 0.8894 - val_loss: 0.3517 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 4s - loss: 0.3600 - acc: 0.8857 - val_loss: 0.3811 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 4s - loss: 0.3406 - acc: 0.8933 - val_loss: 0.5318 - val_acc: 0.8000                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 4s - loss: 0.3823 - acc: 0.8800 - val_loss: 0.3805 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 4s - loss: 0.3606 - acc: 0.8839 - val_loss: 0.3566 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 4s - loss: 0.3834 - acc: 0.8776 - val_loss: 0.4818 - val_acc: 0.7788                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 4s - loss: 0.3398 - acc: 0.8869 - val_loss: 0.4416 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 4s - loss: 0.3807 - acc: 0.8773 - val_loss: 0.3765 - val_acc: 0.8724                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8898450946643718                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8724358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 42)           1176                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                6160                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 9,419                                                                                                    \n",
      "Trainable params: 9,419                                                                                                \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 53%|███████████████████████▍                    | 64/120 [1:40:11<1:34:44, 101.51s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 140.6887 - acc: 0.7494 - val_loss: 104.8079 - val_acc: 0.8321                                            \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 79.8164 - acc: 0.8766 - val_loss: 57.7896 - val_acc: 0.8724                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 42.4456 - acc: 0.8989 - val_loss: 29.2746 - val_acc: 0.8686                                              \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 20.3494 - acc: 0.9061 - val_loss: 12.9538 - val_acc: 0.8500                                              \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 8.1305 - acc: 0.8930 - val_loss: 4.5100 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 2.3812 - acc: 0.8928 - val_loss: 1.1673 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.6138 - acc: 0.8901 - val_loss: 0.5471 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3965 - acc: 0.8874 - val_loss: 0.5465 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3804 - acc: 0.8903 - val_loss: 0.5058 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3749 - acc: 0.8847 - val_loss: 0.4816 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3564 - acc: 0.8918 - val_loss: 0.4760 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3572 - acc: 0.8852 - val_loss: 0.4912 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3472 - acc: 0.8928 - val_loss: 0.4388 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3400 - acc: 0.8972 - val_loss: 0.4991 - val_acc: 0.8192                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3407 - acc: 0.8923 - val_loss: 0.5091 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3409 - acc: 0.8911 - val_loss: 0.4410 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3263 - acc: 0.8960 - val_loss: 0.4306 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3260 - acc: 0.8940 - val_loss: 0.4441 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3236 - acc: 0.8955 - val_loss: 0.4201 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3158 - acc: 0.8982 - val_loss: 0.5344 - val_acc: 0.8301                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3074 - acc: 0.9019 - val_loss: 0.4191 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3105 - acc: 0.8957 - val_loss: 0.4244 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3035 - acc: 0.8938 - val_loss: 0.4079 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3002 - acc: 0.8989 - val_loss: 0.4116 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3029 - acc: 0.8955 - val_loss: 0.4175 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3018 - acc: 0.8970 - val_loss: 0.4042 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2918 - acc: 0.8989 - val_loss: 0.3889 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2937 - acc: 0.8953 - val_loss: 0.4038 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2925 - acc: 0.9016 - val_loss: 0.3924 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2910 - acc: 0.9036 - val_loss: 0.4138 - val_acc: 0.8660                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8768133759527907                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8660256410256411                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 54%|████████████████████████▍                    | 65/120 [1:41:28<1:26:10, 94.00s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 27.2689 - acc: 0.7912 - val_loss: 20.7774 - val_acc: 0.8673                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 16.1202 - acc: 0.8921 - val_loss: 12.0266 - val_acc: 0.8731                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 8.9782 - acc: 0.9085 - val_loss: 6.4713 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 4.5957 - acc: 0.9162 - val_loss: 3.2797 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 2.1832 - acc: 0.9105 - val_loss: 1.5263 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.9978 - acc: 0.9120 - val_loss: 0.7762 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.5360 - acc: 0.9137 - val_loss: 0.5396 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3740 - acc: 0.9203 - val_loss: 0.5108 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3295 - acc: 0.9154 - val_loss: 0.4095 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3032 - acc: 0.9122 - val_loss: 0.3669 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2777 - acc: 0.9164 - val_loss: 0.3603 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2693 - acc: 0.9149 - val_loss: 0.3549 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2603 - acc: 0.9169 - val_loss: 0.3479 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2524 - acc: 0.9218 - val_loss: 0.3442 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2503 - acc: 0.9169 - val_loss: 0.3602 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2396 - acc: 0.9238 - val_loss: 0.3448 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2339 - acc: 0.9235 - val_loss: 0.3493 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2324 - acc: 0.9221 - val_loss: 0.3307 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2327 - acc: 0.9235 - val_loss: 0.2999 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2308 - acc: 0.9243 - val_loss: 0.4760 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2259 - acc: 0.9270 - val_loss: 0.3356 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2206 - acc: 0.9275 - val_loss: 0.3523 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2198 - acc: 0.9292 - val_loss: 0.3136 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2188 - acc: 0.9302 - val_loss: 0.2966 - val_acc: 0.9071                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2193 - acc: 0.9243 - val_loss: 0.2973 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2158 - acc: 0.9267 - val_loss: 0.3205 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2090 - acc: 0.9321 - val_loss: 0.2980 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2104 - acc: 0.9336 - val_loss: 0.3342 - val_acc: 0.9122                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2079 - acc: 0.9316 - val_loss: 0.2747 - val_acc: 0.9077                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2052 - acc: 0.9336 - val_loss: 0.2813 - val_acc: 0.9071                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9210720432751414                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.907051282051282                                                                                                      \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 55%|████████████████████████▊                    | 66/120 [1:42:36<1:17:42, 86.34s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 12.4219 - acc: 0.8102 - val_loss: 8.8309 - val_acc: 0.8679                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 6.4286 - acc: 0.9039 - val_loss: 4.5067 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 3.1523 - acc: 0.9132 - val_loss: 2.2632 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 1.6159 - acc: 0.9198 - val_loss: 1.3644 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.9977 - acc: 0.9174 - val_loss: 0.9411 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.6736 - acc: 0.9159 - val_loss: 0.6864 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.5166 - acc: 0.9228 - val_loss: 0.5799 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.4124 - acc: 0.9284 - val_loss: 0.5353 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3634 - acc: 0.9262 - val_loss: 0.4255 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3340 - acc: 0.9213 - val_loss: 0.3844 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3004 - acc: 0.9233 - val_loss: 0.3744 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2864 - acc: 0.9280 - val_loss: 0.3591 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2697 - acc: 0.9280 - val_loss: 0.3430 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2591 - acc: 0.9326 - val_loss: 0.3115 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2473 - acc: 0.9304 - val_loss: 0.3427 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2356 - acc: 0.9324 - val_loss: 0.3209 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2361 - acc: 0.9326 - val_loss: 0.3658 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2265 - acc: 0.9343 - val_loss: 0.2873 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2261 - acc: 0.9329 - val_loss: 0.3181 - val_acc: 0.9051                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2147 - acc: 0.9388 - val_loss: 0.2976 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2092 - acc: 0.9403 - val_loss: 0.2719 - val_acc: 0.9013                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2075 - acc: 0.9405 - val_loss: 0.2603 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2023 - acc: 0.9412 - val_loss: 0.2881 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2039 - acc: 0.9385 - val_loss: 0.2574 - val_acc: 0.9244                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2001 - acc: 0.9412 - val_loss: 0.2742 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.1983 - acc: 0.9405 - val_loss: 0.2589 - val_acc: 0.9263                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.1864 - acc: 0.9449 - val_loss: 0.3027 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.1933 - acc: 0.9430 - val_loss: 0.2397 - val_acc: 0.9333                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.1925 - acc: 0.9442 - val_loss: 0.2489 - val_acc: 0.9237                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.1849 - acc: 0.9462 - val_loss: 0.2899 - val_acc: 0.9096                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9473813621834276                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9096153846153846                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 56%|█████████████████████████▏                   | 67/120 [1:43:44<1:11:24, 80.84s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 19.4908 - acc: 0.7925 - val_loss: 14.6195 - val_acc: 0.8692                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 11.1894 - acc: 0.8965 - val_loss: 8.2859 - val_acc: 0.8705                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 6.0636 - acc: 0.9068 - val_loss: 4.3533 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 3.1302 - acc: 0.9174 - val_loss: 2.3659 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 1.6916 - acc: 0.9090 - val_loss: 1.3937 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.9904 - acc: 0.9137 - val_loss: 0.8880 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.6459 - acc: 0.9132 - val_loss: 0.6515 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.4581 - acc: 0.9179 - val_loss: 0.5494 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3751 - acc: 0.9157 - val_loss: 0.4286 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3322 - acc: 0.9132 - val_loss: 0.4028 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3062 - acc: 0.9115 - val_loss: 0.3776 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2854 - acc: 0.9162 - val_loss: 0.3851 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2730 - acc: 0.9162 - val_loss: 0.3473 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2607 - acc: 0.9230 - val_loss: 0.3640 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2545 - acc: 0.9225 - val_loss: 0.3281 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2453 - acc: 0.9233 - val_loss: 0.3241 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2432 - acc: 0.9221 - val_loss: 0.3354 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2353 - acc: 0.9253 - val_loss: 0.3073 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2379 - acc: 0.9194 - val_loss: 0.3004 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2306 - acc: 0.9289 - val_loss: 0.3776 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2244 - acc: 0.9270 - val_loss: 0.3544 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2214 - acc: 0.9272 - val_loss: 0.3127 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2211 - acc: 0.9257 - val_loss: 0.2854 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2220 - acc: 0.9280 - val_loss: 0.3207 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2190 - acc: 0.9289 - val_loss: 0.2835 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2132 - acc: 0.9277 - val_loss: 0.3189 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2096 - acc: 0.9331 - val_loss: 0.3007 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2077 - acc: 0.9334 - val_loss: 0.3163 - val_acc: 0.9122                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2068 - acc: 0.9378 - val_loss: 0.2687 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2046 - acc: 0.9343 - val_loss: 0.2836 - val_acc: 0.9077                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9235308581263831                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9076923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 57%|█████████████████████████▌                   | 68/120 [1:44:52<1:06:41, 76.95s/it, best loss: -0.9262820512820513]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 10.1069 - acc: 0.8269 - val_loss: 4.0977 - val_acc: 0.8987                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.1417 - acc: 0.9007 - val_loss: 1.2337 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.8440 - acc: 0.9053 - val_loss: 0.7333 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5488 - acc: 0.9105 - val_loss: 0.5495 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.4296 - acc: 0.9095 - val_loss: 0.4775 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3614 - acc: 0.9090 - val_loss: 0.4014 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3217 - acc: 0.9171 - val_loss: 0.4509 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3001 - acc: 0.9221 - val_loss: 0.4462 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2777 - acc: 0.9250 - val_loss: 0.3300 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2717 - acc: 0.9235 - val_loss: 0.3353 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2516 - acc: 0.9280 - val_loss: 0.2988 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2489 - acc: 0.9294 - val_loss: 0.3020 - val_acc: 0.9103                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2321 - acc: 0.9353 - val_loss: 0.2976 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2281 - acc: 0.9326 - val_loss: 0.2754 - val_acc: 0.9244                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2399 - acc: 0.9331 - val_loss: 0.2752 - val_acc: 0.9058                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2263 - acc: 0.9368 - val_loss: 0.3293 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2175 - acc: 0.9358 - val_loss: 0.3176 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2217 - acc: 0.9356 - val_loss: 0.2894 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2150 - acc: 0.9343 - val_loss: 0.2844 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2187 - acc: 0.9375 - val_loss: 0.3363 - val_acc: 0.9109                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2111 - acc: 0.9361 - val_loss: 0.2579 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.1988 - acc: 0.9393 - val_loss: 0.2656 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2076 - acc: 0.9356 - val_loss: 0.2568 - val_acc: 0.9103                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2135 - acc: 0.9363 - val_loss: 0.2642 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2097 - acc: 0.9383 - val_loss: 0.2859 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2015 - acc: 0.9366 - val_loss: 0.2647 - val_acc: 0.9295                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.1955 - acc: 0.9373 - val_loss: 0.2743 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2042 - acc: 0.9412 - val_loss: 0.2403 - val_acc: 0.9288                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2067 - acc: 0.9371 - val_loss: 0.2376 - val_acc: 0.9365                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.1948 - acc: 0.9412 - val_loss: 0.2435 - val_acc: 0.9353                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9471354806983034                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9352564102564103                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 57%|█████████████████████████▊                   | 69/120 [1:46:01<1:03:14, 74.40s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 18.9274 - acc: 0.8114 - val_loss: 7.4747 - val_acc: 0.8801                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 3.5079 - acc: 0.8825 - val_loss: 1.3124 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.7217 - acc: 0.8889 - val_loss: 0.6210 - val_acc: 0.8141                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3882 - acc: 0.8921 - val_loss: 0.5194 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3313 - acc: 0.8972 - val_loss: 0.4514 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3089 - acc: 0.8960 - val_loss: 0.3794 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3156 - acc: 0.8940 - val_loss: 0.4149 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2876 - acc: 0.9031 - val_loss: 0.4560 - val_acc: 0.8096                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2906 - acc: 0.8985 - val_loss: 0.3400 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2880 - acc: 0.8933 - val_loss: 0.3327 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2741 - acc: 0.8992 - val_loss: 0.3791 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2757 - acc: 0.9080 - val_loss: 0.3782 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2693 - acc: 0.9071 - val_loss: 0.3167 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2632 - acc: 0.9105 - val_loss: 0.3785 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2685 - acc: 0.9063 - val_loss: 0.3572 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2708 - acc: 0.9066 - val_loss: 0.3310 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2554 - acc: 0.9159 - val_loss: 0.3556 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2642 - acc: 0.9051 - val_loss: 0.4055 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2680 - acc: 0.9068 - val_loss: 0.3392 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2546 - acc: 0.9125 - val_loss: 0.3056 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2601 - acc: 0.9110 - val_loss: 0.3881 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2575 - acc: 0.9078 - val_loss: 0.3219 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2678 - acc: 0.9110 - val_loss: 0.3755 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2548 - acc: 0.9127 - val_loss: 0.4293 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2657 - acc: 0.9056 - val_loss: 0.4129 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2534 - acc: 0.9110 - val_loss: 0.3249 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2490 - acc: 0.9122 - val_loss: 0.3012 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2655 - acc: 0.9117 - val_loss: 0.4253 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2387 - acc: 0.9174 - val_loss: 0.3108 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2513 - acc: 0.9134 - val_loss: 0.3425 - val_acc: 0.8942                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9095156134743054                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8942307692307693                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 58%|██████████████████████████▎                  | 70/120 [1:47:08<1:00:17, 72.36s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 16.0199 - acc: 0.8151 - val_loss: 4.8590 - val_acc: 0.8776                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.1507 - acc: 0.8785 - val_loss: 0.9466 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.5697 - acc: 0.8965 - val_loss: 0.6279 - val_acc: 0.8179                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3691 - acc: 0.9034 - val_loss: 0.5099 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3463 - acc: 0.8955 - val_loss: 0.4739 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3082 - acc: 0.9019 - val_loss: 0.3503 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3227 - acc: 0.8980 - val_loss: 0.4250 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2767 - acc: 0.9056 - val_loss: 0.5535 - val_acc: 0.8301                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2839 - acc: 0.9036 - val_loss: 0.4255 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2896 - acc: 0.8992 - val_loss: 0.3263 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2707 - acc: 0.9048 - val_loss: 0.3624 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2867 - acc: 0.9048 - val_loss: 0.3273 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2692 - acc: 0.9080 - val_loss: 0.3237 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2672 - acc: 0.9120 - val_loss: 0.3797 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2792 - acc: 0.9046 - val_loss: 0.3546 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2720 - acc: 0.9075 - val_loss: 0.3134 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2548 - acc: 0.9149 - val_loss: 0.3288 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2718 - acc: 0.9041 - val_loss: 0.3079 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2641 - acc: 0.9071 - val_loss: 0.2950 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2906 - acc: 0.9090 - val_loss: 0.3534 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2563 - acc: 0.9139 - val_loss: 0.3392 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2637 - acc: 0.9117 - val_loss: 0.3291 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2694 - acc: 0.9112 - val_loss: 0.3081 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2576 - acc: 0.9120 - val_loss: 0.3613 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2630 - acc: 0.9071 - val_loss: 0.3606 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2522 - acc: 0.9107 - val_loss: 0.3199 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2589 - acc: 0.9090 - val_loss: 0.2961 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2622 - acc: 0.9120 - val_loss: 0.3111 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2494 - acc: 0.9179 - val_loss: 0.3235 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2602 - acc: 0.9134 - val_loss: 0.2987 - val_acc: 0.8955                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9146791246619129                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8955128205128206                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 59%|███████████████████████████▊                   | 71/120 [1:48:16<57:59, 71.01s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 11.9821 - acc: 0.8323 - val_loss: 4.9371 - val_acc: 0.8865                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.6008 - acc: 0.8987 - val_loss: 1.3338 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.8978 - acc: 0.9034 - val_loss: 0.7425 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5294 - acc: 0.9051 - val_loss: 0.5303 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3989 - acc: 0.9039 - val_loss: 0.4846 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3378 - acc: 0.9088 - val_loss: 0.4430 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3147 - acc: 0.9122 - val_loss: 0.4039 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2810 - acc: 0.9228 - val_loss: 0.5340 - val_acc: 0.8288                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2908 - acc: 0.9154 - val_loss: 0.3371 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2690 - acc: 0.9169 - val_loss: 0.3424 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2713 - acc: 0.9132 - val_loss: 0.3525 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2532 - acc: 0.9179 - val_loss: 0.5251 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2526 - acc: 0.9181 - val_loss: 0.3204 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2429 - acc: 0.9262 - val_loss: 0.2996 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2370 - acc: 0.9225 - val_loss: 0.3130 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2356 - acc: 0.9206 - val_loss: 0.3121 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2372 - acc: 0.9211 - val_loss: 0.3911 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2340 - acc: 0.9260 - val_loss: 0.2782 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2391 - acc: 0.9208 - val_loss: 0.2703 - val_acc: 0.9128                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2297 - acc: 0.9265 - val_loss: 0.4518 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2230 - acc: 0.9260 - val_loss: 0.3058 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2285 - acc: 0.9262 - val_loss: 0.2717 - val_acc: 0.9071                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2233 - acc: 0.9275 - val_loss: 0.2841 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2254 - acc: 0.9272 - val_loss: 0.2832 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2279 - acc: 0.9292 - val_loss: 0.3205 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2162 - acc: 0.9294 - val_loss: 0.2702 - val_acc: 0.9231                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2122 - acc: 0.9294 - val_loss: 0.2489 - val_acc: 0.9154                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2101 - acc: 0.9366 - val_loss: 0.2548 - val_acc: 0.9269                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2106 - acc: 0.9312 - val_loss: 0.2594 - val_acc: 0.9244                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2105 - acc: 0.9346 - val_loss: 0.2657 - val_acc: 0.9218                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9252520285222523                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9217948717948717                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 60%|████████████████████████████▏                  | 72/120 [1:49:23<55:55, 69.90s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 26.1182 - acc: 0.8139 - val_loss: 12.3677 - val_acc: 0.8776                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 6.3142 - acc: 0.8876 - val_loss: 2.4362 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 1.1336 - acc: 0.8925 - val_loss: 0.6632 - val_acc: 0.8026                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4119 - acc: 0.8940 - val_loss: 0.5743 - val_acc: 0.7949                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3527 - acc: 0.8923 - val_loss: 0.3917 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3247 - acc: 0.8950 - val_loss: 0.4465 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3152 - acc: 0.8933 - val_loss: 0.4797 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2965 - acc: 0.9019 - val_loss: 0.4684 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2888 - acc: 0.9095 - val_loss: 0.3468 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2972 - acc: 0.8992 - val_loss: 0.3357 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2865 - acc: 0.9058 - val_loss: 0.3481 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2869 - acc: 0.9044 - val_loss: 0.3675 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2742 - acc: 0.9068 - val_loss: 0.3810 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2658 - acc: 0.9147 - val_loss: 0.3486 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2710 - acc: 0.9056 - val_loss: 0.3805 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2672 - acc: 0.9120 - val_loss: 0.3266 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2611 - acc: 0.9147 - val_loss: 0.5099 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2623 - acc: 0.9132 - val_loss: 0.3458 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2649 - acc: 0.9053 - val_loss: 0.3295 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2591 - acc: 0.9105 - val_loss: 0.3425 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2546 - acc: 0.9174 - val_loss: 0.3328 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2635 - acc: 0.9088 - val_loss: 0.3318 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2618 - acc: 0.9139 - val_loss: 0.3823 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2621 - acc: 0.9103 - val_loss: 0.3779 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2664 - acc: 0.9046 - val_loss: 0.4414 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2674 - acc: 0.9078 - val_loss: 0.3633 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2520 - acc: 0.9142 - val_loss: 0.4044 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2539 - acc: 0.9132 - val_loss: 0.3421 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2502 - acc: 0.9179 - val_loss: 0.3390 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2660 - acc: 0.9085 - val_loss: 0.3622 - val_acc: 0.8782                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.926235554462749                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8782051282051282                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 61%|████████████████████████████▌                  | 73/120 [1:50:32<54:30, 69.59s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 11.9985 - acc: 0.8257 - val_loss: 6.5945 - val_acc: 0.8897                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 3.8878 - acc: 0.9034 - val_loss: 2.0800 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 1.2049 - acc: 0.9048 - val_loss: 0.8631 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5174 - acc: 0.9105 - val_loss: 0.5641 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3808 - acc: 0.9051 - val_loss: 0.4525 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3193 - acc: 0.9053 - val_loss: 0.4192 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.2985 - acc: 0.9071 - val_loss: 0.4795 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2683 - acc: 0.9196 - val_loss: 0.4478 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2795 - acc: 0.9152 - val_loss: 0.3344 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2632 - acc: 0.9184 - val_loss: 0.3630 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2445 - acc: 0.9169 - val_loss: 0.3461 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2531 - acc: 0.9201 - val_loss: 0.3884 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2378 - acc: 0.9208 - val_loss: 0.3252 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2413 - acc: 0.9235 - val_loss: 0.2876 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2412 - acc: 0.9157 - val_loss: 0.3115 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2358 - acc: 0.9223 - val_loss: 0.3174 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2255 - acc: 0.9255 - val_loss: 0.3565 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2373 - acc: 0.9250 - val_loss: 0.3165 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2320 - acc: 0.9213 - val_loss: 0.2722 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2297 - acc: 0.9253 - val_loss: 0.3350 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2288 - acc: 0.9329 - val_loss: 0.4089 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2317 - acc: 0.9235 - val_loss: 0.2825 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2228 - acc: 0.9238 - val_loss: 0.3101 - val_acc: 0.9071                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2267 - acc: 0.9235 - val_loss: 0.2560 - val_acc: 0.9276                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2363 - acc: 0.9223 - val_loss: 0.3169 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2225 - acc: 0.9267 - val_loss: 0.2854 - val_acc: 0.9212                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2189 - acc: 0.9297 - val_loss: 0.3131 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2169 - acc: 0.9336 - val_loss: 0.2528 - val_acc: 0.9231                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2178 - acc: 0.9299 - val_loss: 0.2730 - val_acc: 0.9237                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2201 - acc: 0.9289 - val_loss: 0.2766 - val_acc: 0.9250                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9215638062453897                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.925                                                                                                                  \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 62%|████████████████████████████▉                  | 74/120 [1:51:40<53:01, 69.17s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 40.4107 - acc: 0.8168 - val_loss: 21.1421 - val_acc: 0.8853                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 11.5239 - acc: 0.8933 - val_loss: 4.7127 - val_acc: 0.8737                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 2.1387 - acc: 0.8918 - val_loss: 0.8920 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5104 - acc: 0.8943 - val_loss: 0.5581 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3726 - acc: 0.8906 - val_loss: 0.4393 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3244 - acc: 0.8962 - val_loss: 0.3959 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3209 - acc: 0.8950 - val_loss: 0.4912 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3094 - acc: 0.9002 - val_loss: 0.4917 - val_acc: 0.8141                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3123 - acc: 0.8970 - val_loss: 0.3738 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3035 - acc: 0.9007 - val_loss: 0.3553 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3053 - acc: 0.9024 - val_loss: 0.3919 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2887 - acc: 0.9066 - val_loss: 0.3975 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2812 - acc: 0.9024 - val_loss: 0.3294 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2828 - acc: 0.9075 - val_loss: 0.4735 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2789 - acc: 0.9024 - val_loss: 0.4870 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2809 - acc: 0.9063 - val_loss: 0.3387 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2734 - acc: 0.9083 - val_loss: 0.3960 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2761 - acc: 0.9061 - val_loss: 0.3586 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2709 - acc: 0.9046 - val_loss: 0.3501 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2699 - acc: 0.9088 - val_loss: 0.3748 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2608 - acc: 0.9152 - val_loss: 0.3392 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2716 - acc: 0.9093 - val_loss: 0.3153 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2721 - acc: 0.9051 - val_loss: 0.3183 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2680 - acc: 0.9080 - val_loss: 0.4747 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2630 - acc: 0.9095 - val_loss: 0.3773 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2594 - acc: 0.9107 - val_loss: 0.3584 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2583 - acc: 0.9149 - val_loss: 0.3687 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2658 - acc: 0.9100 - val_loss: 0.3389 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2581 - acc: 0.9162 - val_loss: 0.3285 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2618 - acc: 0.9134 - val_loss: 0.3188 - val_acc: 0.8962                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9195967543643964                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8961538461538462                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 62%|█████████████████████████████▍                 | 75/120 [1:52:47<51:14, 68.32s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 9.0042 - acc: 0.8562 - val_loss: 3.4409 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.5006 - acc: 0.9034 - val_loss: 0.8755 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4678 - acc: 0.9029 - val_loss: 0.5647 - val_acc: 0.8327                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3368 - acc: 0.9095 - val_loss: 0.4662 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.2954 - acc: 0.9088 - val_loss: 0.3420 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.2750 - acc: 0.9122 - val_loss: 0.4842 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.2643 - acc: 0.9149 - val_loss: 0.4512 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2521 - acc: 0.9179 - val_loss: 0.3912 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2485 - acc: 0.9223 - val_loss: 0.2861 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2380 - acc: 0.9272 - val_loss: 0.3519 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2324 - acc: 0.9289 - val_loss: 0.4598 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2360 - acc: 0.9255 - val_loss: 0.4190 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2328 - acc: 0.9287 - val_loss: 0.3141 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2312 - acc: 0.9277 - val_loss: 0.2774 - val_acc: 0.9135                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2333 - acc: 0.9302 - val_loss: 0.2851 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2242 - acc: 0.9309 - val_loss: 0.2642 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2259 - acc: 0.9307 - val_loss: 0.2831 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2156 - acc: 0.9336 - val_loss: 0.4318 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2239 - acc: 0.9307 - val_loss: 0.2912 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2117 - acc: 0.9366 - val_loss: 0.2503 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2146 - acc: 0.9329 - val_loss: 0.2584 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2194 - acc: 0.9324 - val_loss: 0.2605 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2056 - acc: 0.9339 - val_loss: 0.2584 - val_acc: 0.9147                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2168 - acc: 0.9329 - val_loss: 0.2505 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2066 - acc: 0.9346 - val_loss: 0.2747 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2112 - acc: 0.9356 - val_loss: 0.2480 - val_acc: 0.9353                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2107 - acc: 0.9361 - val_loss: 0.4096 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2113 - acc: 0.9356 - val_loss: 0.2227 - val_acc: 0.9429                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2137 - acc: 0.9385 - val_loss: 0.2346 - val_acc: 0.9385                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2022 - acc: 0.9434 - val_loss: 0.2785 - val_acc: 0.9077                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9481190066388001                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9076923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 28)           1792                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 24)           2040                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,531                                                                                                   \n",
      "Trainable params: 65,531                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 63%|█████████████████████████████▊                 | 76/120 [1:54:00<51:12, 69.83s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 37.7895 - acc: 0.8087 - val_loss: 23.0039 - val_acc: 0.8583                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 14.2648 - acc: 0.8950 - val_loss: 7.5864 - val_acc: 0.8756                                               \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 4.0642 - acc: 0.9021 - val_loss: 1.8024 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.8529 - acc: 0.9009 - val_loss: 0.6297 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.3904 - acc: 0.8977 - val_loss: 0.4610 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3281 - acc: 0.8953 - val_loss: 0.3978 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3026 - acc: 0.8982 - val_loss: 0.4156 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.2794 - acc: 0.9044 - val_loss: 0.4058 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.2794 - acc: 0.9014 - val_loss: 0.3605 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.2735 - acc: 0.9029 - val_loss: 0.3446 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.2710 - acc: 0.9021 - val_loss: 0.3549 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.2655 - acc: 0.9056 - val_loss: 0.3331 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.2626 - acc: 0.9068 - val_loss: 0.3317 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.2647 - acc: 0.9075 - val_loss: 0.4156 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.2604 - acc: 0.9088 - val_loss: 0.4183 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.2607 - acc: 0.9112 - val_loss: 0.3298 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.2516 - acc: 0.9159 - val_loss: 0.3365 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.2504 - acc: 0.9132 - val_loss: 0.3308 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2593 - acc: 0.9073 - val_loss: 0.3211 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.2544 - acc: 0.9110 - val_loss: 0.3568 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.2485 - acc: 0.9157 - val_loss: 0.3376 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.2536 - acc: 0.9107 - val_loss: 0.3441 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.2555 - acc: 0.9103 - val_loss: 0.3492 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.2561 - acc: 0.9137 - val_loss: 0.3577 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.2564 - acc: 0.9100 - val_loss: 0.3457 - val_acc: 0.8692                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9095156134743054                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8692307692307693                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 64%|██████████████████████████████▏                | 77/120 [1:55:05<49:02, 68.43s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 27.1766 - acc: 0.8043 - val_loss: 16.7809 - val_acc: 0.8788                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 10.7267 - acc: 0.8950 - val_loss: 6.1237 - val_acc: 0.8179                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 3.4576 - acc: 0.8997 - val_loss: 1.8011 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.9670 - acc: 0.9063 - val_loss: 0.6985 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.4540 - acc: 0.8955 - val_loss: 0.4742 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3446 - acc: 0.9014 - val_loss: 0.3938 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3178 - acc: 0.9016 - val_loss: 0.4559 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2778 - acc: 0.9152 - val_loss: 0.5010 - val_acc: 0.8205                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2889 - acc: 0.9053 - val_loss: 0.4082 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2798 - acc: 0.9068 - val_loss: 0.3367 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2698 - acc: 0.9066 - val_loss: 0.3504 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2689 - acc: 0.9085 - val_loss: 0.3568 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2602 - acc: 0.9105 - val_loss: 0.3275 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2533 - acc: 0.9149 - val_loss: 0.3600 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2534 - acc: 0.9115 - val_loss: 0.3271 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2472 - acc: 0.9203 - val_loss: 0.3576 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2488 - acc: 0.9164 - val_loss: 0.3051 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2415 - acc: 0.9240 - val_loss: 0.3283 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2526 - acc: 0.9147 - val_loss: 0.2880 - val_acc: 0.9147                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2489 - acc: 0.9201 - val_loss: 0.2914 - val_acc: 0.9000                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2422 - acc: 0.9225 - val_loss: 0.3514 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2435 - acc: 0.9144 - val_loss: 0.2964 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2466 - acc: 0.9196 - val_loss: 0.2872 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2396 - acc: 0.9230 - val_loss: 0.2842 - val_acc: 0.9147                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2382 - acc: 0.9225 - val_loss: 0.3218 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2363 - acc: 0.9221 - val_loss: 0.2941 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2320 - acc: 0.9262 - val_loss: 0.3134 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2314 - acc: 0.9267 - val_loss: 0.2759 - val_acc: 0.9231                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2311 - acc: 0.9240 - val_loss: 0.2868 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2352 - acc: 0.9253 - val_loss: 0.2695 - val_acc: 0.9077                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9274649618883698                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9076923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 32)           7200                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1888)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                120896                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 65%|██████████████████████████████▌                | 78/120 [1:56:12<47:30, 67.87s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 28.1986 - acc: 0.8178 - val_loss: 8.6408 - val_acc: 0.8609                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 4.1135 - acc: 0.8672 - val_loss: 1.6138 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.7939 - acc: 0.8896 - val_loss: 0.6660 - val_acc: 0.8186                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.3909 - acc: 0.8987 - val_loss: 0.5264 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 4s - loss: 0.3521 - acc: 0.8903 - val_loss: 0.4449 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.3241 - acc: 0.8933 - val_loss: 0.4080 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.3160 - acc: 0.8985 - val_loss: 0.3926 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.2934 - acc: 0.9041 - val_loss: 0.3775 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.2856 - acc: 0.9029 - val_loss: 0.4031 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.2920 - acc: 0.9039 - val_loss: 0.3326 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.2928 - acc: 0.9021 - val_loss: 0.3377 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.2830 - acc: 0.9021 - val_loss: 0.3361 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.2742 - acc: 0.9048 - val_loss: 0.3226 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.2796 - acc: 0.9036 - val_loss: 0.4187 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.2850 - acc: 0.8999 - val_loss: 0.3462 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 4s - loss: 0.2786 - acc: 0.9046 - val_loss: 0.3488 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.2691 - acc: 0.9068 - val_loss: 0.3504 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.2792 - acc: 0.9021 - val_loss: 0.3620 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.2777 - acc: 0.8999 - val_loss: 0.4102 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2786 - acc: 0.9039 - val_loss: 0.3867 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.2712 - acc: 0.9063 - val_loss: 0.3327 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.2737 - acc: 0.9034 - val_loss: 0.3423 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 4s - loss: 0.2902 - acc: 0.8972 - val_loss: 0.3313 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 4s - loss: 0.2670 - acc: 0.9066 - val_loss: 0.3644 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.2945 - acc: 0.8975 - val_loss: 0.3411 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.2715 - acc: 0.9046 - val_loss: 0.3268 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.2785 - acc: 0.9031 - val_loss: 0.3937 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.3005 - acc: 0.8972 - val_loss: 0.3276 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.2781 - acc: 0.9036 - val_loss: 0.4414 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.2653 - acc: 0.9058 - val_loss: 0.4955 - val_acc: 0.8558                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8787804278337841                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8557692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 27,859                                                                                                   \n",
      "Trainable params: 27,859                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 66%|██████████████████████████████▉                | 79/120 [1:58:12<57:08, 83.61s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 7.6857 - acc: 0.8345 - val_loss: 4.1217 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 2s - loss: 2.3184 - acc: 0.9009 - val_loss: 1.3114 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 2s - loss: 0.7854 - acc: 0.9098 - val_loss: 0.6902 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 2s - loss: 0.4339 - acc: 0.9093 - val_loss: 0.4912 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 2s - loss: 0.3430 - acc: 0.9083 - val_loss: 0.4066 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 2s - loss: 0.2947 - acc: 0.9144 - val_loss: 0.3589 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 2s - loss: 0.2738 - acc: 0.9186 - val_loss: 0.3593 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 2s - loss: 0.2526 - acc: 0.9287 - val_loss: 0.4173 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 2s - loss: 0.2439 - acc: 0.9257 - val_loss: 0.3249 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 2s - loss: 0.2382 - acc: 0.9277 - val_loss: 0.3241 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 2s - loss: 0.2271 - acc: 0.9284 - val_loss: 0.2847 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.2251 - acc: 0.9294 - val_loss: 0.4827 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2224 - acc: 0.9324 - val_loss: 0.3108 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 2s - loss: 0.2079 - acc: 0.9390 - val_loss: 0.2753 - val_acc: 0.9333                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 2s - loss: 0.2117 - acc: 0.9329 - val_loss: 0.2918 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 2s - loss: 0.1990 - acc: 0.9398 - val_loss: 0.2888 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 2s - loss: 0.2085 - acc: 0.9388 - val_loss: 0.2736 - val_acc: 0.9051                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 2s - loss: 0.1937 - acc: 0.9400 - val_loss: 0.2278 - val_acc: 0.9321                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 2s - loss: 0.1996 - acc: 0.9410 - val_loss: 0.2250 - val_acc: 0.9455                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 2s - loss: 0.1974 - acc: 0.9361 - val_loss: 0.2370 - val_acc: 0.9224                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 2s - loss: 0.1854 - acc: 0.9420 - val_loss: 0.2355 - val_acc: 0.9340                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 2s - loss: 0.1902 - acc: 0.9412 - val_loss: 0.2623 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 2s - loss: 0.2003 - acc: 0.9373 - val_loss: 0.2389 - val_acc: 0.9340                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 2s - loss: 0.1889 - acc: 0.9420 - val_loss: 0.2263 - val_acc: 0.9436                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 2s - loss: 0.1947 - acc: 0.9380 - val_loss: 0.2838 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 2s - loss: 0.1772 - acc: 0.9471 - val_loss: 0.2929 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 2s - loss: 0.1891 - acc: 0.9449 - val_loss: 0.2091 - val_acc: 0.9449                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 2s - loss: 0.1780 - acc: 0.9447 - val_loss: 0.2367 - val_acc: 0.9340                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 2s - loss: 0.1888 - acc: 0.9439 - val_loss: 0.2049 - val_acc: 0.9481                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 2s - loss: 0.1794 - acc: 0.9452 - val_loss: 0.2228 - val_acc: 0.9494                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 2s - loss: 0.1782 - acc: 0.9432 - val_loss: 0.2401 - val_acc: 0.9436                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 2s - loss: 0.1796 - acc: 0.9452 - val_loss: 0.2258 - val_acc: 0.9154                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 2s - loss: 0.1777 - acc: 0.9493 - val_loss: 0.2193 - val_acc: 0.9404                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 2s - loss: 0.1690 - acc: 0.9501 - val_loss: 0.2214 - val_acc: 0.9353                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 2s - loss: 0.1655 - acc: 0.9496 - val_loss: 0.2669 - val_acc: 0.9096                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.956724858618146                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.9096153846153846                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 944)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                60480                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 66,739                                                                                                   \n",
      "Trainable params: 66,739                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 67%|███████████████████████████████▎               | 80/120 [1:59:31<54:48, 82.21s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 5s - loss: 19.8783 - acc: 0.8665 - val_loss: 0.9607 - val_acc: 0.8135                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 0.4329 - acc: 0.8881 - val_loss: 0.4228 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.3498 - acc: 0.8847 - val_loss: 0.4009 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.3502 - acc: 0.8866 - val_loss: 0.4007 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 4s - loss: 0.3425 - acc: 0.8896 - val_loss: 0.4236 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.3193 - acc: 0.8950 - val_loss: 0.4272 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.3034 - acc: 0.8975 - val_loss: 0.3552 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.3132 - acc: 0.8921 - val_loss: 0.3905 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.2976 - acc: 0.8997 - val_loss: 0.3675 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.2917 - acc: 0.9019 - val_loss: 0.3426 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.3038 - acc: 0.8923 - val_loss: 0.3721 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.3025 - acc: 0.8903 - val_loss: 0.3452 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.2984 - acc: 0.8945 - val_loss: 0.3239 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.3034 - acc: 0.8933 - val_loss: 0.3618 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.3144 - acc: 0.8928 - val_loss: 0.3695 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 4s - loss: 0.3010 - acc: 0.8938 - val_loss: 0.3867 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.2987 - acc: 0.8967 - val_loss: 0.3350 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.2841 - acc: 0.8985 - val_loss: 0.3802 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.3031 - acc: 0.8901 - val_loss: 0.3909 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2927 - acc: 0.8985 - val_loss: 0.3397 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.2818 - acc: 0.8999 - val_loss: 0.3404 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.2806 - acc: 0.8945 - val_loss: 0.3163 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 4s - loss: 0.3065 - acc: 0.8891 - val_loss: 0.3705 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 4s - loss: 0.2959 - acc: 0.8921 - val_loss: 0.3500 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.2828 - acc: 0.8970 - val_loss: 0.3707 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.3218 - acc: 0.8879 - val_loss: 0.3983 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.3021 - acc: 0.8953 - val_loss: 0.3264 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.2901 - acc: 0.8955 - val_loss: 0.3415 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.2755 - acc: 0.9031 - val_loss: 0.3346 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.2982 - acc: 0.8967 - val_loss: 0.3279 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9058273911974428                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 68%|██████████████████████████████▍              | 81/120 [2:01:34<1:01:19, 94.36s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 41.5969 - acc: 0.8301 - val_loss: 9.7082 - val_acc: 0.8244                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.7988 - acc: 0.8790 - val_loss: 0.5609 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.3979 - acc: 0.8788 - val_loss: 0.4601 - val_acc: 0.8199                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3440 - acc: 0.8906 - val_loss: 0.3816 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3366 - acc: 0.8879 - val_loss: 0.4457 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3194 - acc: 0.8886 - val_loss: 0.4497 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3112 - acc: 0.8894 - val_loss: 0.3595 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3109 - acc: 0.8842 - val_loss: 0.3784 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3030 - acc: 0.8982 - val_loss: 0.3309 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3054 - acc: 0.8928 - val_loss: 0.3426 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2926 - acc: 0.8906 - val_loss: 0.6098 - val_acc: 0.6955                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2931 - acc: 0.8962 - val_loss: 0.4804 - val_acc: 0.8064                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2961 - acc: 0.8898 - val_loss: 0.3926 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2953 - acc: 0.8957 - val_loss: 0.5620 - val_acc: 0.7026                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2909 - acc: 0.8928 - val_loss: 0.3555 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2993 - acc: 0.8965 - val_loss: 0.3359 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2894 - acc: 0.8945 - val_loss: 0.3609 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2860 - acc: 0.8940 - val_loss: 0.3816 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2858 - acc: 0.8923 - val_loss: 0.3293 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2817 - acc: 0.9014 - val_loss: 0.3393 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2832 - acc: 0.8945 - val_loss: 0.3451 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2894 - acc: 0.8970 - val_loss: 0.3638 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2832 - acc: 0.8916 - val_loss: 0.3446 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2883 - acc: 0.8930 - val_loss: 0.3227 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2855 - acc: 0.8901 - val_loss: 0.3202 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2854 - acc: 0.8925 - val_loss: 0.3550 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2824 - acc: 0.8977 - val_loss: 0.3298 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2801 - acc: 0.8950 - val_loss: 0.3755 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2876 - acc: 0.8985 - val_loss: 0.3373 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2843 - acc: 0.8982 - val_loss: 0.3195 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9011556429800835                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1312)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                84032                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 87,731                                                                                                   \n",
      "Trainable params: 87,731                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 68%|████████████████████████████████               | 82/120 [2:02:47<55:43, 87.99s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 18.6160 - acc: 0.8370 - val_loss: 5.2337 - val_acc: 0.8237                                               \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 2.3166 - acc: 0.8741 - val_loss: 1.0744 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.6054 - acc: 0.8884 - val_loss: 0.6405 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.3727 - acc: 0.9012 - val_loss: 0.4824 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3094 - acc: 0.9009 - val_loss: 0.4391 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.2904 - acc: 0.9012 - val_loss: 0.3816 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.2819 - acc: 0.9051 - val_loss: 0.3751 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.2748 - acc: 0.9080 - val_loss: 0.3711 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.2708 - acc: 0.9075 - val_loss: 0.3780 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.2717 - acc: 0.9048 - val_loss: 0.3498 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.2666 - acc: 0.9024 - val_loss: 0.3452 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.2613 - acc: 0.9088 - val_loss: 0.3391 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.2607 - acc: 0.9105 - val_loss: 0.3345 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.2727 - acc: 0.9073 - val_loss: 0.3871 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.2609 - acc: 0.9103 - val_loss: 0.3437 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.2592 - acc: 0.9112 - val_loss: 0.3443 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.2617 - acc: 0.9107 - val_loss: 0.4006 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.2741 - acc: 0.9051 - val_loss: 0.3205 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2517 - acc: 0.9080 - val_loss: 0.3415 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.2527 - acc: 0.9115 - val_loss: 0.3189 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2554 - acc: 0.9154 - val_loss: 0.3231 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2489 - acc: 0.9132 - val_loss: 0.3303 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.2518 - acc: 0.9117 - val_loss: 0.3180 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.2470 - acc: 0.9147 - val_loss: 0.3237 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.2509 - acc: 0.9130 - val_loss: 0.3292 - val_acc: 0.8923                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8979591836734694                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8923076923076924                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 24)           2328                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1464)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                93760                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 69%|████████████████████████████████▌              | 83/120 [2:04:01<51:38, 83.75s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 17.6560 - acc: 0.8330 - val_loss: 6.0418 - val_acc: 0.8571                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 2.5950 - acc: 0.8903 - val_loss: 0.9455 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.5497 - acc: 0.8903 - val_loss: 0.6156 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3600 - acc: 0.8980 - val_loss: 0.4981 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3322 - acc: 0.8953 - val_loss: 0.6088 - val_acc: 0.8327                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3036 - acc: 0.8977 - val_loss: 0.4473 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3100 - acc: 0.8965 - val_loss: 0.4121 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2811 - acc: 0.9029 - val_loss: 0.4141 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2855 - acc: 0.9039 - val_loss: 0.3254 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2791 - acc: 0.9056 - val_loss: 0.3252 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2700 - acc: 0.9063 - val_loss: 0.3724 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2696 - acc: 0.9085 - val_loss: 0.3885 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2678 - acc: 0.9080 - val_loss: 0.3132 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2755 - acc: 0.9105 - val_loss: 0.3541 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2783 - acc: 0.9090 - val_loss: 0.3401 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2740 - acc: 0.9098 - val_loss: 0.3333 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2740 - acc: 0.9061 - val_loss: 0.3306 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2665 - acc: 0.9144 - val_loss: 0.3501 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2757 - acc: 0.9051 - val_loss: 0.3675 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2617 - acc: 0.9142 - val_loss: 0.6364 - val_acc: 0.7853                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2640 - acc: 0.9139 - val_loss: 0.3551 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2590 - acc: 0.9095 - val_loss: 0.3174 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2691 - acc: 0.9046 - val_loss: 0.3070 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2655 - acc: 0.9100 - val_loss: 0.3602 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2612 - acc: 0.9100 - val_loss: 0.3505 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2702 - acc: 0.9117 - val_loss: 0.3532 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2551 - acc: 0.9157 - val_loss: 0.4359 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2629 - acc: 0.9134 - val_loss: 0.3785 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2663 - acc: 0.9132 - val_loss: 0.3213 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2591 - acc: 0.9211 - val_loss: 0.3054 - val_acc: 0.9026                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9323825915908532                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9025641025641026                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3600                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 944)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                60480                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 70%|████████████████████████████████▉              | 84/120 [2:05:24<50:05, 83.49s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 30.6581 - acc: 0.8448 - val_loss: 6.4310 - val_acc: 0.8705                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 2.5141 - acc: 0.8898 - val_loss: 0.8424 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4571 - acc: 0.8943 - val_loss: 0.4117 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3456 - acc: 0.8933 - val_loss: 0.4250 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3715 - acc: 0.8916 - val_loss: 0.3934 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3020 - acc: 0.9021 - val_loss: 0.5493 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3347 - acc: 0.8921 - val_loss: 0.4231 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3363 - acc: 0.8916 - val_loss: 0.4221 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3084 - acc: 0.8994 - val_loss: 0.3987 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3262 - acc: 0.8945 - val_loss: 0.3788 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3387 - acc: 0.8943 - val_loss: 0.4420 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3030 - acc: 0.8989 - val_loss: 0.3460 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3593 - acc: 0.8980 - val_loss: 0.4557 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3287 - acc: 0.8881 - val_loss: 0.3615 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3043 - acc: 0.8980 - val_loss: 0.3607 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2998 - acc: 0.8967 - val_loss: 0.4411 - val_acc: 0.8115                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3088 - acc: 0.8953 - val_loss: 0.3728 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3215 - acc: 0.8930 - val_loss: 0.3948 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2885 - acc: 0.8975 - val_loss: 0.3373 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2970 - acc: 0.8940 - val_loss: 0.3811 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2926 - acc: 0.9007 - val_loss: 0.4406 - val_acc: 0.8186                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3070 - acc: 0.8916 - val_loss: 0.3667 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3038 - acc: 0.8957 - val_loss: 0.3992 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3202 - acc: 0.8894 - val_loss: 0.3619 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2826 - acc: 0.8955 - val_loss: 0.3886 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2953 - acc: 0.8933 - val_loss: 0.3776 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2824 - acc: 0.9004 - val_loss: 0.4173 - val_acc: 0.8263                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3179 - acc: 0.8925 - val_loss: 0.3521 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3153 - acc: 0.8980 - val_loss: 0.5153 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3225 - acc: 0.8923 - val_loss: 0.3615 - val_acc: 0.8615                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8898450946643718                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8615384615384616                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 28,883                                                                                                   \n",
      "Trainable params: 28,883                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 71%|█████████████████████████████████▎             | 85/120 [2:06:52<49:34, 85.00s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 7.4812 - acc: 0.8503 - val_loss: 0.5146 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.3755 - acc: 0.8778 - val_loss: 0.4429 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3381 - acc: 0.8886 - val_loss: 0.4413 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3139 - acc: 0.8913 - val_loss: 0.3927 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3151 - acc: 0.8869 - val_loss: 0.3570 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3072 - acc: 0.8940 - val_loss: 0.4012 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3014 - acc: 0.8923 - val_loss: 0.3559 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3047 - acc: 0.8916 - val_loss: 0.3885 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3097 - acc: 0.8898 - val_loss: 0.3491 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.2987 - acc: 0.8911 - val_loss: 0.3843 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3210 - acc: 0.8901 - val_loss: 0.5974 - val_acc: 0.6577                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3050 - acc: 0.8894 - val_loss: 0.3924 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2958 - acc: 0.8923 - val_loss: 0.3436 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2968 - acc: 0.8948 - val_loss: 0.5391 - val_acc: 0.7122                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2931 - acc: 0.8980 - val_loss: 0.3982 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3042 - acc: 0.8925 - val_loss: 0.3915 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2961 - acc: 0.8913 - val_loss: 0.3533 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3197 - acc: 0.8940 - val_loss: 0.4948 - val_acc: 0.7218                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 4s - loss: 0.2958 - acc: 0.8918 - val_loss: 0.4099 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2951 - acc: 0.8933 - val_loss: 0.5779 - val_acc: 0.7846                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3012 - acc: 0.8930 - val_loss: 0.3871 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.3016 - acc: 0.8923 - val_loss: 0.7407 - val_acc: 0.6301                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3230 - acc: 0.8832 - val_loss: 0.3497 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.3031 - acc: 0.8918 - val_loss: 0.4062 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.2969 - acc: 0.8935 - val_loss: 0.4997 - val_acc: 0.7192                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2951 - acc: 0.9007 - val_loss: 0.5371 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.3098 - acc: 0.8950 - val_loss: 0.4611 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.3151 - acc: 0.8960 - val_loss: 0.6426 - val_acc: 0.6878                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.3204 - acc: 0.8953 - val_loss: 0.5647 - val_acc: 0.7295                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.3096 - acc: 0.8940 - val_loss: 0.4903 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.3106 - acc: 0.8960 - val_loss: 0.4095 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2957 - acc: 0.8908 - val_loss: 0.7077 - val_acc: 0.7615                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.3112 - acc: 0.8876 - val_loss: 0.7016 - val_acc: 0.6635                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.3227 - acc: 0.8908 - val_loss: 0.5190 - val_acc: 0.8128                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3096 - acc: 0.8967 - val_loss: 0.6339 - val_acc: 0.7071                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.7155151217113351                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.7070512820512821                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                30752                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 35,571                                                                                                   \n",
      "Trainable params: 35,571                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 72%|█████████████████████████████████▋             | 86/120 [2:08:48<53:26, 94.30s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 6.7498 - acc: 0.8776 - val_loss: 3.0520 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.4667 - acc: 0.9110 - val_loss: 0.8025 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.5023 - acc: 0.9157 - val_loss: 0.5363 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3436 - acc: 0.9166 - val_loss: 0.4021 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.2964 - acc: 0.9159 - val_loss: 0.4055 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.2671 - acc: 0.9184 - val_loss: 0.4111 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.2505 - acc: 0.9267 - val_loss: 0.3171 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2432 - acc: 0.9208 - val_loss: 0.3586 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2317 - acc: 0.9304 - val_loss: 0.2889 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2300 - acc: 0.9307 - val_loss: 0.3590 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2215 - acc: 0.9316 - val_loss: 0.2747 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2220 - acc: 0.9280 - val_loss: 0.2767 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2196 - acc: 0.9316 - val_loss: 0.2896 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2109 - acc: 0.9336 - val_loss: 0.2758 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2048 - acc: 0.9368 - val_loss: 0.2637 - val_acc: 0.9045                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2025 - acc: 0.9366 - val_loss: 0.2685 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2041 - acc: 0.9356 - val_loss: 0.2739 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.1984 - acc: 0.9383 - val_loss: 0.3021 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2071 - acc: 0.9371 - val_loss: 0.2858 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.1970 - acc: 0.9400 - val_loss: 0.2480 - val_acc: 0.9224                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.1895 - acc: 0.9430 - val_loss: 0.2470 - val_acc: 0.9154                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.1935 - acc: 0.9422 - val_loss: 0.2500 - val_acc: 0.9256                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2017 - acc: 0.9400 - val_loss: 0.2392 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2059 - acc: 0.9400 - val_loss: 0.2746 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2048 - acc: 0.9410 - val_loss: 0.2403 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.1970 - acc: 0.9390 - val_loss: 0.2169 - val_acc: 0.9410                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.1966 - acc: 0.9410 - val_loss: 0.3123 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.1873 - acc: 0.9417 - val_loss: 0.2526 - val_acc: 0.9282                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.1996 - acc: 0.9415 - val_loss: 0.2160 - val_acc: 0.9410                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.1878 - acc: 0.9437 - val_loss: 0.2656 - val_acc: 0.9128                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9459060732726826                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9128205128205128                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                42048                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,387                                                                                                   \n",
      "Trainable params: 44,387                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 72%|██████████████████████████████████             | 87/120 [2:10:15<50:36, 92.01s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 2s - loss: 76.7023 - acc: 0.8262 - val_loss: 41.5304 - val_acc: 0.8788                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 23.4249 - acc: 0.8803 - val_loss: 10.1789 - val_acc: 0.8750                                              \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 4.5132 - acc: 0.8916 - val_loss: 1.3369 - val_acc: 0.7885                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.5756 - acc: 0.8753 - val_loss: 0.5754 - val_acc: 0.7872                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.3710 - acc: 0.8844 - val_loss: 0.5214 - val_acc: 0.8096                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3457 - acc: 0.8849 - val_loss: 0.4062 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 0.3341 - acc: 0.8879 - val_loss: 0.3818 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.3272 - acc: 0.8908 - val_loss: 0.4550 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.3192 - acc: 0.8935 - val_loss: 0.3879 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.3189 - acc: 0.8889 - val_loss: 0.3800 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.3164 - acc: 0.8906 - val_loss: 0.3939 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.3111 - acc: 0.8950 - val_loss: 0.3857 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.3089 - acc: 0.9002 - val_loss: 0.3552 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.3041 - acc: 0.8980 - val_loss: 0.5214 - val_acc: 0.7981                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.3137 - acc: 0.8955 - val_loss: 0.5586 - val_acc: 0.7692                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.3135 - acc: 0.8950 - val_loss: 0.3688 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.3018 - acc: 0.9007 - val_loss: 0.3667 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.3118 - acc: 0.8930 - val_loss: 0.3563 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.3023 - acc: 0.8965 - val_loss: 0.3600 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.3093 - acc: 0.8960 - val_loss: 0.6628 - val_acc: 0.7167                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.3015 - acc: 0.9026 - val_loss: 0.3617 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.3036 - acc: 0.8982 - val_loss: 0.3786 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.3046 - acc: 0.8962 - val_loss: 0.3713 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.2980 - acc: 0.8965 - val_loss: 0.3685 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.2994 - acc: 0.8962 - val_loss: 0.3585 - val_acc: 0.8724                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8935333169412343                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8724358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                124992                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 73%|██████████████████████████████████▍            | 88/120 [2:11:05<42:21, 79.42s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 40.2079 - acc: 0.8326 - val_loss: 23.6520 - val_acc: 0.8853                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 14.3340 - acc: 0.8997 - val_loss: 7.3128 - val_acc: 0.8571                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 3.9082 - acc: 0.8972 - val_loss: 1.8318 - val_acc: 0.8192                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.8870 - acc: 0.9024 - val_loss: 0.6732 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4056 - acc: 0.8960 - val_loss: 0.5141 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3240 - acc: 0.9004 - val_loss: 0.4493 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3060 - acc: 0.8992 - val_loss: 0.4631 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2784 - acc: 0.9056 - val_loss: 0.3918 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2772 - acc: 0.9066 - val_loss: 0.3300 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2779 - acc: 0.9048 - val_loss: 0.3533 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2701 - acc: 0.9034 - val_loss: 0.3437 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2631 - acc: 0.9093 - val_loss: 0.4053 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2632 - acc: 0.9053 - val_loss: 0.3187 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2564 - acc: 0.9157 - val_loss: 0.4081 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2617 - acc: 0.9071 - val_loss: 0.4322 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2522 - acc: 0.9103 - val_loss: 0.3458 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2544 - acc: 0.9142 - val_loss: 0.3438 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2522 - acc: 0.9127 - val_loss: 0.3387 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2555 - acc: 0.9098 - val_loss: 0.4433 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2564 - acc: 0.9149 - val_loss: 0.3026 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2477 - acc: 0.9152 - val_loss: 0.3983 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.2455 - acc: 0.9112 - val_loss: 0.3417 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2606 - acc: 0.9117 - val_loss: 0.3291 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2474 - acc: 0.9139 - val_loss: 0.3444 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2572 - acc: 0.9139 - val_loss: 0.3904 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2427 - acc: 0.9194 - val_loss: 0.3593 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2397 - acc: 0.9152 - val_loss: 0.3874 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2519 - acc: 0.9174 - val_loss: 0.2924 - val_acc: 0.9103                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2421 - acc: 0.9186 - val_loss: 0.4015 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2453 - acc: 0.9196 - val_loss: 0.4008 - val_acc: 0.8782                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9269731989181215                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8782051282051282                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                90688                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 74%|██████████████████████████████████▊            | 89/120 [2:12:45<44:15, 85.66s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 24.3906 - acc: 0.8549 - val_loss: 4.5208 - val_acc: 0.8821                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.7096 - acc: 0.8930 - val_loss: 0.7814 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.5635 - acc: 0.9103 - val_loss: 0.5919 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.5286 - acc: 0.9044 - val_loss: 0.6199 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.5008 - acc: 0.9127 - val_loss: 0.5637 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.4141 - acc: 0.9253 - val_loss: 0.4850 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3989 - acc: 0.9157 - val_loss: 0.5328 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3928 - acc: 0.9228 - val_loss: 0.4158 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3373 - acc: 0.9233 - val_loss: 0.4557 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.3426 - acc: 0.9228 - val_loss: 0.3959 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.2924 - acc: 0.9287 - val_loss: 0.3673 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2998 - acc: 0.9257 - val_loss: 0.3594 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2665 - acc: 0.9339 - val_loss: 0.3804 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2707 - acc: 0.9343 - val_loss: 0.3280 - val_acc: 0.9038                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2696 - acc: 0.9375 - val_loss: 0.3365 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2712 - acc: 0.9324 - val_loss: 0.3680 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2374 - acc: 0.9383 - val_loss: 0.3316 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2448 - acc: 0.9366 - val_loss: 0.3160 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2490 - acc: 0.9378 - val_loss: 0.3653 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2484 - acc: 0.9378 - val_loss: 0.3029 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2312 - acc: 0.9398 - val_loss: 0.2968 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2349 - acc: 0.9363 - val_loss: 0.2803 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2083 - acc: 0.9439 - val_loss: 0.2681 - val_acc: 0.9077                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2122 - acc: 0.9447 - val_loss: 0.3775 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2296 - acc: 0.9400 - val_loss: 0.2636 - val_acc: 0.9128                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2131 - acc: 0.9390 - val_loss: 0.2821 - val_acc: 0.9154                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2252 - acc: 0.9400 - val_loss: 0.2727 - val_acc: 0.9051                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.1928 - acc: 0.9405 - val_loss: 0.2687 - val_acc: 0.9263                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2024 - acc: 0.9412 - val_loss: 0.2693 - val_acc: 0.9205                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.1930 - acc: 0.9444 - val_loss: 0.3299 - val_acc: 0.9199                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9476272436685518                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9198717948717948                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                12320                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 15,443                                                                                                   \n",
      "Trainable params: 15,443                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 75%|███████████████████████████████████▎           | 90/120 [2:14:30<45:42, 91.40s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 23.6565 - acc: 0.8271 - val_loss: 0.5998 - val_acc: 0.8199                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.3952 - acc: 0.8748 - val_loss: 0.4855 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3506 - acc: 0.8839 - val_loss: 0.4592 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3362 - acc: 0.8852 - val_loss: 0.4466 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3340 - acc: 0.8849 - val_loss: 0.4164 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3322 - acc: 0.8820 - val_loss: 0.4645 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3261 - acc: 0.8859 - val_loss: 0.3918 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3198 - acc: 0.8884 - val_loss: 0.4739 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3308 - acc: 0.8908 - val_loss: 0.3991 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3285 - acc: 0.8842 - val_loss: 0.3923 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3214 - acc: 0.8894 - val_loss: 0.5300 - val_acc: 0.6974                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.3321 - acc: 0.8785 - val_loss: 0.4318 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3249 - acc: 0.8869 - val_loss: 0.4093 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3242 - acc: 0.8837 - val_loss: 0.4355 - val_acc: 0.8071                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3120 - acc: 0.8913 - val_loss: 0.4037 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.3206 - acc: 0.8871 - val_loss: 0.3867 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.3103 - acc: 0.8847 - val_loss: 0.4189 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3193 - acc: 0.8876 - val_loss: 0.3779 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3185 - acc: 0.8913 - val_loss: 0.4027 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.3216 - acc: 0.8886 - val_loss: 0.3697 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3085 - acc: 0.8908 - val_loss: 0.3592 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.3139 - acc: 0.8903 - val_loss: 0.4117 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3203 - acc: 0.8803 - val_loss: 0.4045 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.3118 - acc: 0.8908 - val_loss: 0.5339 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3112 - acc: 0.8918 - val_loss: 0.4070 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.3138 - acc: 0.8930 - val_loss: 0.3983 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.3095 - acc: 0.8935 - val_loss: 0.4229 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.3083 - acc: 0.8938 - val_loss: 0.4823 - val_acc: 0.7558                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.3059 - acc: 0.8953 - val_loss: 0.3832 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2981 - acc: 0.8933 - val_loss: 0.4429 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.3060 - acc: 0.8923 - val_loss: 0.4612 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.3006 - acc: 0.8925 - val_loss: 0.4955 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.3049 - acc: 0.8928 - val_loss: 0.5798 - val_acc: 0.6936                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2962 - acc: 0.8945 - val_loss: 0.4585 - val_acc: 0.7417                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3041 - acc: 0.8913 - val_loss: 0.6043 - val_acc: 0.6667                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.6840422916154414                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.6666666666666666                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 944)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                60480                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 66,739                                                                                                   \n",
      "Trainable params: 66,739                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 76%|███████████████████████████████████▋           | 91/120 [2:16:09<45:19, 93.76s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 75.0678 - acc: 0.8547 - val_loss: 36.9006 - val_acc: 0.8699                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 18.7516 - acc: 0.8898 - val_loss: 6.2582 - val_acc: 0.8705                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 2.3483 - acc: 0.8854 - val_loss: 0.6870 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4209 - acc: 0.8879 - val_loss: 0.4966 - val_acc: 0.7942                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3626 - acc: 0.8807 - val_loss: 0.5109 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3257 - acc: 0.8943 - val_loss: 0.4024 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3247 - acc: 0.8874 - val_loss: 0.4206 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3184 - acc: 0.8940 - val_loss: 0.4112 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.3041 - acc: 0.8945 - val_loss: 0.3894 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2999 - acc: 0.8945 - val_loss: 0.3485 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2940 - acc: 0.8985 - val_loss: 0.4451 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2944 - acc: 0.8987 - val_loss: 0.3633 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3100 - acc: 0.8950 - val_loss: 0.3916 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2841 - acc: 0.9044 - val_loss: 0.3671 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3078 - acc: 0.8955 - val_loss: 0.3499 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2840 - acc: 0.9041 - val_loss: 0.3454 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2835 - acc: 0.9019 - val_loss: 0.3337 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2767 - acc: 0.8997 - val_loss: 0.3205 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2736 - acc: 0.9039 - val_loss: 0.3738 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2778 - acc: 0.9036 - val_loss: 0.5005 - val_acc: 0.8288                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2771 - acc: 0.9061 - val_loss: 0.3266 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2703 - acc: 0.9058 - val_loss: 0.3302 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2765 - acc: 0.8980 - val_loss: 0.3266 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2700 - acc: 0.9056 - val_loss: 0.3543 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2865 - acc: 0.9021 - val_loss: 0.4421 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2794 - acc: 0.9029 - val_loss: 0.3326 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2677 - acc: 0.9066 - val_loss: 0.3399 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.2837 - acc: 0.9044 - val_loss: 0.3116 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.2644 - acc: 0.9095 - val_loss: 0.3374 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.2719 - acc: 0.9107 - val_loss: 0.3092 - val_acc: 0.8878                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9171379395131547                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8878205128205128                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 77%|████████████████████████████████████           | 92/120 [2:17:49<44:38, 95.67s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 21.4083 - acc: 0.8537 - val_loss: 2.2585 - val_acc: 0.8468                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 0.8058 - acc: 0.8793 - val_loss: 0.5066 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.3797 - acc: 0.8815 - val_loss: 0.4997 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3419 - acc: 0.8901 - val_loss: 0.4108 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3375 - acc: 0.8923 - val_loss: 0.3918 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3283 - acc: 0.8925 - val_loss: 0.4999 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3154 - acc: 0.8972 - val_loss: 0.3794 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3165 - acc: 0.8962 - val_loss: 0.4724 - val_acc: 0.8301                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3060 - acc: 0.9016 - val_loss: 0.3434 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3011 - acc: 0.9039 - val_loss: 0.3277 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2996 - acc: 0.9021 - val_loss: 0.5661 - val_acc: 0.7250                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3075 - acc: 0.9039 - val_loss: 0.4036 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3078 - acc: 0.8977 - val_loss: 0.4431 - val_acc: 0.8353                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3064 - acc: 0.8982 - val_loss: 0.6097 - val_acc: 0.7269                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2966 - acc: 0.9048 - val_loss: 0.3065 - val_acc: 0.8917                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3172 - acc: 0.9019 - val_loss: 0.3305 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3230 - acc: 0.8903 - val_loss: 0.4496 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3498 - acc: 0.8898 - val_loss: 0.6613 - val_acc: 0.7071                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3903 - acc: 0.8783 - val_loss: 0.5769 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.5957 - acc: 0.8581 - val_loss: 0.4347 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 1.1885 - acc: 0.8424 - val_loss: 1.7137 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 1.6662 - acc: 0.8510 - val_loss: 2.4907 - val_acc: 0.7974                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 1.7373 - acc: 0.8628 - val_loss: 3.3503 - val_acc: 0.7846                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 1.8253 - acc: 0.8687 - val_loss: 3.3113 - val_acc: 0.7814                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 1.8554 - acc: 0.8716 - val_loss: 3.0054 - val_acc: 0.7910                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 1.9286 - acc: 0.8707 - val_loss: 1.7052 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 1.9150 - acc: 0.8763 - val_loss: 2.5511 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 1.9320 - acc: 0.8778 - val_loss: 3.1147 - val_acc: 0.7609                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 1.8841 - acc: 0.8793 - val_loss: 2.6670 - val_acc: 0.8314                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 1.8096 - acc: 0.8864 - val_loss: 1.9587 - val_acc: 0.8699                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8620604868453405                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8698717948717949                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1312)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                84032                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 87,731                                                                                                   \n",
      "Trainable params: 87,731                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 78%|████████████████████████████████████▍          | 93/120 [2:19:02<39:59, 88.87s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 40.9582 - acc: 0.8527 - val_loss: 13.7500 - val_acc: 0.8788                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 6.4127 - acc: 0.9044 - val_loss: 2.4434 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 1.1893 - acc: 0.8989 - val_loss: 0.6717 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.4384 - acc: 0.8911 - val_loss: 0.4451 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3490 - acc: 0.8891 - val_loss: 0.4101 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3176 - acc: 0.8938 - val_loss: 0.4184 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3438 - acc: 0.8898 - val_loss: 0.4279 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.3370 - acc: 0.8938 - val_loss: 0.4052 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3015 - acc: 0.9002 - val_loss: 0.3761 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.2913 - acc: 0.9009 - val_loss: 0.3938 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.2950 - acc: 0.8967 - val_loss: 0.4137 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3399 - acc: 0.8911 - val_loss: 0.3798 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3174 - acc: 0.9002 - val_loss: 0.3896 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.2947 - acc: 0.8999 - val_loss: 0.3586 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3075 - acc: 0.8943 - val_loss: 0.4198 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.2952 - acc: 0.8992 - val_loss: 0.4011 - val_acc: 0.8391                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3075 - acc: 0.8930 - val_loss: 0.3637 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3005 - acc: 0.8967 - val_loss: 0.3817 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2996 - acc: 0.8943 - val_loss: 0.3896 - val_acc: 0.8423                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.2847 - acc: 0.8965 - val_loss: 0.3700 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2752 - acc: 0.9024 - val_loss: 0.3952 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2844 - acc: 0.8982 - val_loss: 0.3680 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.2870 - acc: 0.8985 - val_loss: 0.5013 - val_acc: 0.8250                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.2891 - acc: 0.9014 - val_loss: 0.3531 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.2799 - acc: 0.8955 - val_loss: 0.3723 - val_acc: 0.8635                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8932874354561101                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8634615384615385                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                31264                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 34,387                                                                                                   \n",
      "Trainable params: 34,387                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 78%|████████████████████████████████████▊          | 94/120 [2:20:15<36:24, 84.03s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 2s - loss: 80.8490 - acc: 0.7908 - val_loss: 20.3440 - val_acc: 0.6115                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 6.1203 - acc: 0.8419 - val_loss: 0.8102 - val_acc: 0.8090                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.5178 - acc: 0.8520 - val_loss: 0.5572 - val_acc: 0.7904                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.4405 - acc: 0.8692 - val_loss: 0.5739 - val_acc: 0.7795                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.4150 - acc: 0.8721 - val_loss: 0.5356 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.4030 - acc: 0.8716 - val_loss: 0.4853 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.4029 - acc: 0.8739 - val_loss: 0.4306 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3844 - acc: 0.8810 - val_loss: 0.5092 - val_acc: 0.7987                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3830 - acc: 0.8761 - val_loss: 0.4275 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3771 - acc: 0.8758 - val_loss: 0.4461 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3890 - acc: 0.8744 - val_loss: 0.4267 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3741 - acc: 0.8780 - val_loss: 0.4258 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3791 - acc: 0.8739 - val_loss: 0.3850 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3778 - acc: 0.8780 - val_loss: 0.4939 - val_acc: 0.8179                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3749 - acc: 0.8771 - val_loss: 0.5371 - val_acc: 0.8179                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3707 - acc: 0.8805 - val_loss: 0.3979 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3547 - acc: 0.8810 - val_loss: 0.3630 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3782 - acc: 0.8807 - val_loss: 0.4093 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3591 - acc: 0.8795 - val_loss: 0.3903 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3596 - acc: 0.8805 - val_loss: 0.7836 - val_acc: 0.6974                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3638 - acc: 0.8839 - val_loss: 0.4507 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3642 - acc: 0.8842 - val_loss: 0.4421 - val_acc: 0.8256                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3701 - acc: 0.8783 - val_loss: 0.3879 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3576 - acc: 0.8810 - val_loss: 0.4351 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3767 - acc: 0.8793 - val_loss: 0.4693 - val_acc: 0.8128                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3606 - acc: 0.8854 - val_loss: 0.4042 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.3745 - acc: 0.8798 - val_loss: 0.3700 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3807 - acc: 0.8756 - val_loss: 0.4047 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.3660 - acc: 0.8820 - val_loss: 0.4016 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3571 - acc: 0.8854 - val_loss: 0.4377 - val_acc: 0.8474                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8477993607081387                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8474358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                90688                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 79%|█████████████████████████████████████▏         | 95/120 [2:21:19<32:32, 78.08s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 21.9962 - acc: 0.8244 - val_loss: 6.7898 - val_acc: 0.8628                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 3.1916 - acc: 0.8716 - val_loss: 1.3193 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.6915 - acc: 0.8921 - val_loss: 0.6187 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3752 - acc: 0.8940 - val_loss: 0.5372 - val_acc: 0.8096                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3461 - acc: 0.8911 - val_loss: 0.4937 - val_acc: 0.8365                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3189 - acc: 0.9014 - val_loss: 0.3862 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3103 - acc: 0.9019 - val_loss: 0.3576 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2964 - acc: 0.8999 - val_loss: 0.3726 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2953 - acc: 0.9036 - val_loss: 0.4495 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2864 - acc: 0.9019 - val_loss: 0.4919 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2791 - acc: 0.8989 - val_loss: 0.3448 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2948 - acc: 0.9007 - val_loss: 0.3632 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2822 - acc: 0.9058 - val_loss: 0.3315 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.2766 - acc: 0.9034 - val_loss: 0.4491 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.2807 - acc: 0.8997 - val_loss: 0.3630 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2855 - acc: 0.9014 - val_loss: 0.3309 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2729 - acc: 0.9075 - val_loss: 0.3360 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2684 - acc: 0.9088 - val_loss: 0.3373 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2796 - acc: 0.9021 - val_loss: 0.3391 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2776 - acc: 0.9029 - val_loss: 0.6984 - val_acc: 0.7186                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2742 - acc: 0.9071 - val_loss: 0.3544 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2982 - acc: 0.9007 - val_loss: 0.3346 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2718 - acc: 0.9071 - val_loss: 0.3246 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2840 - acc: 0.9024 - val_loss: 0.3652 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2792 - acc: 0.9031 - val_loss: 0.3911 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2733 - acc: 0.9068 - val_loss: 0.3542 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2679 - acc: 0.9095 - val_loss: 0.3100 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2839 - acc: 0.9031 - val_loss: 0.3203 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2661 - acc: 0.9134 - val_loss: 0.3361 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2776 - acc: 0.9090 - val_loss: 0.3638 - val_acc: 0.8776                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9151708876321613                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8775641025641026                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 28,883                                                                                                   \n",
      "Trainable params: 28,883                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 80%|█████████████████████████████████████▌         | 96/120 [2:23:02<34:12, 85.54s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 10.1643 - acc: 0.8473 - val_loss: 0.5436 - val_acc: 0.8526                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.3834 - acc: 0.8746 - val_loss: 0.5012 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3297 - acc: 0.8859 - val_loss: 0.5099 - val_acc: 0.8269                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3298 - acc: 0.8864 - val_loss: 0.4336 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.3131 - acc: 0.8896 - val_loss: 0.3942 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.3213 - acc: 0.8859 - val_loss: 0.3886 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.3034 - acc: 0.8933 - val_loss: 0.3686 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.3138 - acc: 0.8894 - val_loss: 0.3871 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.3061 - acc: 0.8894 - val_loss: 0.4016 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.3101 - acc: 0.8896 - val_loss: 0.4063 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.3016 - acc: 0.8938 - val_loss: 0.5150 - val_acc: 0.6923                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 4s - loss: 0.3108 - acc: 0.8864 - val_loss: 0.4235 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.3034 - acc: 0.8896 - val_loss: 0.3775 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.3043 - acc: 0.8908 - val_loss: 0.5765 - val_acc: 0.6897                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.3002 - acc: 0.8928 - val_loss: 0.3985 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2967 - acc: 0.8950 - val_loss: 0.4083 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2991 - acc: 0.8921 - val_loss: 0.4259 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.3066 - acc: 0.8916 - val_loss: 0.5037 - val_acc: 0.7891                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.3166 - acc: 0.8923 - val_loss: 0.4231 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2877 - acc: 0.8957 - val_loss: 0.5755 - val_acc: 0.7199                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.3033 - acc: 0.8970 - val_loss: 0.4714 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2964 - acc: 0.8935 - val_loss: 0.6421 - val_acc: 0.6481                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.3035 - acc: 0.8859 - val_loss: 0.5050 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2925 - acc: 0.8911 - val_loss: 0.6014 - val_acc: 0.7795                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.3009 - acc: 0.8916 - val_loss: 0.5010 - val_acc: 0.6987                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.2940 - acc: 0.8975 - val_loss: 0.4950 - val_acc: 0.7250                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.2978 - acc: 0.8980 - val_loss: 0.4190 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.3000 - acc: 0.8965 - val_loss: 0.6447 - val_acc: 0.6865                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.3008 - acc: 0.8960 - val_loss: 0.5274 - val_acc: 0.6929                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2986 - acc: 0.8889 - val_loss: 0.6160 - val_acc: 0.6776                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.3152 - acc: 0.8935 - val_loss: 0.4375 - val_acc: 0.8058                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.2916 - acc: 0.8982 - val_loss: 0.4953 - val_acc: 0.7128                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.3040 - acc: 0.8911 - val_loss: 0.7902 - val_acc: 0.6564                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2990 - acc: 0.8955 - val_loss: 0.5870 - val_acc: 0.7192                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.3103 - acc: 0.8913 - val_loss: 0.7486 - val_acc: 0.6538                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.6616670764691419                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.6538461538461539                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 66,419                                                                                                   \n",
      "Trainable params: 66,419                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 81%|█████████████████████████████████████▉         | 97/120 [2:24:58<36:15, 94.58s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 19.5161 - acc: 0.8461 - val_loss: 4.7246 - val_acc: 0.8519                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.7409 - acc: 0.8835 - val_loss: 0.5382 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4089 - acc: 0.8862 - val_loss: 0.5308 - val_acc: 0.8077                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3490 - acc: 0.8935 - val_loss: 0.4615 - val_acc: 0.8224                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3192 - acc: 0.8953 - val_loss: 0.4581 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3012 - acc: 0.8960 - val_loss: 0.4118 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.2977 - acc: 0.8977 - val_loss: 0.3830 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2987 - acc: 0.8970 - val_loss: 0.4468 - val_acc: 0.8218                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2916 - acc: 0.9012 - val_loss: 0.3525 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.3012 - acc: 0.8933 - val_loss: 0.3602 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2833 - acc: 0.8989 - val_loss: 0.4081 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2836 - acc: 0.9007 - val_loss: 0.3685 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2777 - acc: 0.9058 - val_loss: 0.3315 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2786 - acc: 0.9004 - val_loss: 0.3902 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2818 - acc: 0.8965 - val_loss: 0.4993 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2780 - acc: 0.8977 - val_loss: 0.3939 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2783 - acc: 0.9026 - val_loss: 0.3368 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2705 - acc: 0.9024 - val_loss: 0.5831 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2891 - acc: 0.8898 - val_loss: 0.3232 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2663 - acc: 0.9004 - val_loss: 0.5847 - val_acc: 0.7545                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2803 - acc: 0.8994 - val_loss: 0.3469 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2735 - acc: 0.9024 - val_loss: 0.3803 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2783 - acc: 0.8908 - val_loss: 0.3361 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2659 - acc: 0.8994 - val_loss: 0.4200 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2991 - acc: 0.8950 - val_loss: 0.3397 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2759 - acc: 0.8975 - val_loss: 0.3498 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2598 - acc: 0.9029 - val_loss: 0.3348 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2758 - acc: 0.8957 - val_loss: 0.3383 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2623 - acc: 0.9066 - val_loss: 0.3317 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2761 - acc: 0.9021 - val_loss: 0.3493 - val_acc: 0.8795                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.893041553970986                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8794871794871795                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 992)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                31776                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 34,323                                                                                                   \n",
      "Trainable params: 34,323                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 82%|██████████████████████████████████████▍        | 98/120 [2:26:33<34:45, 94.79s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 39.6673 - acc: 0.8129 - val_loss: 14.8881 - val_acc: 0.8776                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 7.3374 - acc: 0.9014 - val_loss: 3.0191 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 1.5099 - acc: 0.8997 - val_loss: 0.8531 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.5213 - acc: 0.8881 - val_loss: 0.4689 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3618 - acc: 0.8891 - val_loss: 0.4203 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3235 - acc: 0.8977 - val_loss: 0.4263 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3157 - acc: 0.8975 - val_loss: 0.3751 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3246 - acc: 0.8930 - val_loss: 0.4781 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2985 - acc: 0.9012 - val_loss: 0.3761 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3161 - acc: 0.9004 - val_loss: 0.3962 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3026 - acc: 0.8960 - val_loss: 0.3823 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3001 - acc: 0.8943 - val_loss: 0.3589 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2920 - acc: 0.9024 - val_loss: 0.3671 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2829 - acc: 0.9012 - val_loss: 0.3879 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3081 - acc: 0.8967 - val_loss: 0.3799 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3049 - acc: 0.8930 - val_loss: 0.3790 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2885 - acc: 0.8960 - val_loss: 0.3464 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2731 - acc: 0.9039 - val_loss: 0.3698 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2883 - acc: 0.9019 - val_loss: 0.3755 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2799 - acc: 0.9075 - val_loss: 0.3621 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2814 - acc: 0.9014 - val_loss: 0.3691 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2895 - acc: 0.8992 - val_loss: 0.3537 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2808 - acc: 0.8955 - val_loss: 0.3465 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2710 - acc: 0.8992 - val_loss: 0.3432 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2862 - acc: 0.8992 - val_loss: 0.5128 - val_acc: 0.8045                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2792 - acc: 0.9007 - val_loss: 0.3480 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2589 - acc: 0.9085 - val_loss: 0.3623 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2861 - acc: 0.8970 - val_loss: 0.3394 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2739 - acc: 0.9019 - val_loss: 0.3596 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2686 - acc: 0.9024 - val_loss: 0.3315 - val_acc: 0.8808                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.916154413572658                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8807692307692307                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1280)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                81984                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 86,187                                                                                                   \n",
      "Trainable params: 86,187                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 82%|██████████████████████████████████████▊        | 99/120 [2:27:48<31:07, 88.93s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 60.2191 - acc: 0.8050 - val_loss: 10.1351 - val_acc: 0.8288                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 2.7267 - acc: 0.8535 - val_loss: 0.6579 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 0.4883 - acc: 0.8544 - val_loss: 0.5970 - val_acc: 0.7897                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.4205 - acc: 0.8697 - val_loss: 0.5328 - val_acc: 0.7872                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.4169 - acc: 0.8719 - val_loss: 0.4588 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.4089 - acc: 0.8736 - val_loss: 0.4603 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3865 - acc: 0.8731 - val_loss: 0.4642 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.4268 - acc: 0.8731 - val_loss: 0.4856 - val_acc: 0.8147                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.3784 - acc: 0.8805 - val_loss: 0.4909 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.3926 - acc: 0.8776 - val_loss: 0.5822 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.3856 - acc: 0.8753 - val_loss: 0.4463 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.3856 - acc: 0.8771 - val_loss: 0.4550 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.3738 - acc: 0.8817 - val_loss: 0.4124 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.3933 - acc: 0.8753 - val_loss: 0.5115 - val_acc: 0.8340                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.3840 - acc: 0.8790 - val_loss: 0.4997 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.3980 - acc: 0.8768 - val_loss: 0.4976 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.3750 - acc: 0.8788 - val_loss: 0.4538 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.3718 - acc: 0.8810 - val_loss: 0.4725 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.3589 - acc: 0.8800 - val_loss: 0.4207 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.3707 - acc: 0.8820 - val_loss: 0.5807 - val_acc: 0.7942                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.3741 - acc: 0.8817 - val_loss: 0.4051 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.3610 - acc: 0.8898 - val_loss: 0.4384 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.3679 - acc: 0.8773 - val_loss: 0.4202 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.3693 - acc: 0.8830 - val_loss: 0.4352 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.3872 - acc: 0.8798 - val_loss: 0.3913 - val_acc: 0.8686                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9014015244652077                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8685897435897436                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 83%|██████████████████████████████████████▎       | 100/120 [2:29:02<28:08, 84.44s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 14.6324 - acc: 0.8139 - val_loss: 4.3891 - val_acc: 0.8673                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 1.9008 - acc: 0.8785 - val_loss: 0.8108 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.5153 - acc: 0.8844 - val_loss: 0.5968 - val_acc: 0.7987                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3874 - acc: 0.8879 - val_loss: 0.6013 - val_acc: 0.7846                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3334 - acc: 0.8985 - val_loss: 0.4205 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3164 - acc: 0.8955 - val_loss: 0.3674 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3187 - acc: 0.8923 - val_loss: 0.3797 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2995 - acc: 0.9004 - val_loss: 0.4049 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2876 - acc: 0.9088 - val_loss: 0.4261 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2981 - acc: 0.9004 - val_loss: 0.3237 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2784 - acc: 0.9039 - val_loss: 0.4069 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2809 - acc: 0.9063 - val_loss: 0.3193 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2665 - acc: 0.9100 - val_loss: 0.3387 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2741 - acc: 0.9110 - val_loss: 0.3551 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2799 - acc: 0.9107 - val_loss: 0.3535 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2616 - acc: 0.9125 - val_loss: 0.3294 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2746 - acc: 0.9132 - val_loss: 0.3456 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2656 - acc: 0.9144 - val_loss: 0.3179 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2711 - acc: 0.9071 - val_loss: 0.3210 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2823 - acc: 0.9093 - val_loss: 0.3328 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2549 - acc: 0.9159 - val_loss: 0.3726 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2574 - acc: 0.9142 - val_loss: 0.3981 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2668 - acc: 0.9078 - val_loss: 0.3186 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2734 - acc: 0.9085 - val_loss: 0.3136 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2716 - acc: 0.9125 - val_loss: 0.5058 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2552 - acc: 0.9164 - val_loss: 0.3454 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2535 - acc: 0.9144 - val_loss: 0.3451 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2722 - acc: 0.9137 - val_loss: 0.3734 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2530 - acc: 0.9171 - val_loss: 0.3368 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2556 - acc: 0.9144 - val_loss: 0.3298 - val_acc: 0.8968                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9257437914925006                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8967948717948718                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                90688                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 84%|██████████████████████████████████████▋       | 101/120 [2:30:07<24:52, 78.57s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 23.7338 - acc: 0.8394 - val_loss: 13.6323 - val_acc: 0.8904                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 8.3485 - acc: 0.8997 - val_loss: 4.4810 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 2.5403 - acc: 0.9031 - val_loss: 1.3189 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.7414 - acc: 0.9107 - val_loss: 0.6209 - val_acc: 0.8423                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3907 - acc: 0.9058 - val_loss: 0.4175 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3169 - acc: 0.9048 - val_loss: 0.3982 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.2983 - acc: 0.9095 - val_loss: 0.3537 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.2775 - acc: 0.9142 - val_loss: 0.3998 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2678 - acc: 0.9181 - val_loss: 0.3410 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2664 - acc: 0.9152 - val_loss: 0.3105 - val_acc: 0.9109                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2440 - acc: 0.9189 - val_loss: 0.2844 - val_acc: 0.8994                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.2598 - acc: 0.9149 - val_loss: 0.2840 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2407 - acc: 0.9277 - val_loss: 0.3202 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2404 - acc: 0.9257 - val_loss: 0.2850 - val_acc: 0.9205                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.2413 - acc: 0.9245 - val_loss: 0.2697 - val_acc: 0.9006                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2345 - acc: 0.9302 - val_loss: 0.2845 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2344 - acc: 0.9240 - val_loss: 0.2927 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2218 - acc: 0.9272 - val_loss: 0.2891 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2369 - acc: 0.9221 - val_loss: 0.2546 - val_acc: 0.9269                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2321 - acc: 0.9299 - val_loss: 0.5343 - val_acc: 0.7936                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2273 - acc: 0.9309 - val_loss: 0.2643 - val_acc: 0.9038                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2187 - acc: 0.9277 - val_loss: 0.3134 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2278 - acc: 0.9272 - val_loss: 0.2571 - val_acc: 0.9218                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2237 - acc: 0.9292 - val_loss: 0.2395 - val_acc: 0.9385                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2167 - acc: 0.9297 - val_loss: 0.2725 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2096 - acc: 0.9331 - val_loss: 0.2539 - val_acc: 0.9321                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2111 - acc: 0.9331 - val_loss: 0.2200 - val_acc: 0.9378                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.2170 - acc: 0.9341 - val_loss: 0.2579 - val_acc: 0.9295                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2185 - acc: 0.9339 - val_loss: 0.2227 - val_acc: 0.9397                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2134 - acc: 0.9307 - val_loss: 0.2559 - val_acc: 0.9051                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9316449471354807                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9051282051282051                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2576                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 28,883                                                                                                   \n",
      "Trainable params: 28,883                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 85%|███████████████████████████████████████       | 102/120 [2:31:51<25:49, 86.08s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 4s - loss: 12.8041 - acc: 0.8758 - val_loss: 2.5705 - val_acc: 0.8750                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.9183 - acc: 0.9080 - val_loss: 0.5775 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3625 - acc: 0.9110 - val_loss: 0.4903 - val_acc: 0.8359                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3120 - acc: 0.9157 - val_loss: 0.3775 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.2821 - acc: 0.9174 - val_loss: 0.3552 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.2642 - acc: 0.9240 - val_loss: 0.3885 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.2531 - acc: 0.9265 - val_loss: 0.3424 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.2388 - acc: 0.9253 - val_loss: 0.3985 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.2419 - acc: 0.9302 - val_loss: 0.3303 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.2350 - acc: 0.9270 - val_loss: 0.2856 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.2245 - acc: 0.9348 - val_loss: 0.2898 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.2270 - acc: 0.9312 - val_loss: 0.3368 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2346 - acc: 0.9331 - val_loss: 0.3337 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2145 - acc: 0.9343 - val_loss: 0.3552 - val_acc: 0.9192                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2154 - acc: 0.9358 - val_loss: 0.3275 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2113 - acc: 0.9407 - val_loss: 0.3430 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2190 - acc: 0.9361 - val_loss: 0.3291 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2179 - acc: 0.9407 - val_loss: 0.3233 - val_acc: 0.9051                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.2000 - acc: 0.9407 - val_loss: 0.3551 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2010 - acc: 0.9466 - val_loss: 0.4289 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2075 - acc: 0.9439 - val_loss: 0.4034 - val_acc: 0.9160                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.1964 - acc: 0.9420 - val_loss: 0.3847 - val_acc: 0.9192                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.2015 - acc: 0.9407 - val_loss: 0.4631 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 4s - loss: 0.1997 - acc: 0.9422 - val_loss: 0.4452 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.1918 - acc: 0.9481 - val_loss: 0.3622 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.1881 - acc: 0.9484 - val_loss: 0.2599 - val_acc: 0.9295                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.1857 - acc: 0.9469 - val_loss: 0.4119 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.1875 - acc: 0.9481 - val_loss: 0.4189 - val_acc: 0.9109                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.1885 - acc: 0.9466 - val_loss: 0.3366 - val_acc: 0.9199                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2027 - acc: 0.9491 - val_loss: 0.3811 - val_acc: 0.9282                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.1919 - acc: 0.9513 - val_loss: 0.4238 - val_acc: 0.9250                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.1769 - acc: 0.9565 - val_loss: 0.5133 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.1964 - acc: 0.9459 - val_loss: 0.3463 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.1809 - acc: 0.9530 - val_loss: 0.2435 - val_acc: 0.9340                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.1912 - acc: 0.9518 - val_loss: 0.3833 - val_acc: 0.9167                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9685271699041063                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9166666666666666                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 42)           2688                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           2032                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                30752                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 35,571                                                                                                   \n",
      "Trainable params: 35,571                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 86%|███████████████████████████████████████▍      | 103/120 [2:33:46<26:52, 94.86s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 64.2781 - acc: 0.8466 - val_loss: 26.3678 - val_acc: 0.8795                                              \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 12.9861 - acc: 0.8987 - val_loss: 4.8991 - val_acc: 0.8660                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 2.3244 - acc: 0.8994 - val_loss: 1.0096 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.5773 - acc: 0.8960 - val_loss: 0.4835 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3585 - acc: 0.8896 - val_loss: 0.4035 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3030 - acc: 0.8997 - val_loss: 0.4792 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3396 - acc: 0.8876 - val_loss: 0.4086 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3278 - acc: 0.8953 - val_loss: 0.3881 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.2999 - acc: 0.8999 - val_loss: 0.3731 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3137 - acc: 0.8938 - val_loss: 0.3908 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3007 - acc: 0.8985 - val_loss: 0.3719 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2799 - acc: 0.9029 - val_loss: 0.3562 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2785 - acc: 0.9058 - val_loss: 0.3786 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2868 - acc: 0.8970 - val_loss: 0.3384 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3080 - acc: 0.8965 - val_loss: 0.4272 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3129 - acc: 0.9007 - val_loss: 0.3868 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3037 - acc: 0.8972 - val_loss: 0.3494 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8982 - val_loss: 0.3660 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2825 - acc: 0.9004 - val_loss: 0.3475 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2845 - acc: 0.9046 - val_loss: 0.3375 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2670 - acc: 0.9073 - val_loss: 0.3751 - val_acc: 0.8474                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2953 - acc: 0.8977 - val_loss: 0.3385 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2995 - acc: 0.8970 - val_loss: 0.3539 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2742 - acc: 0.9044 - val_loss: 0.3214 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2750 - acc: 0.9014 - val_loss: 0.3395 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2809 - acc: 0.8962 - val_loss: 0.3342 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2800 - acc: 0.9004 - val_loss: 0.3890 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2844 - acc: 0.9016 - val_loss: 0.3863 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2790 - acc: 0.9110 - val_loss: 0.3196 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2742 - acc: 0.8999 - val_loss: 0.3229 - val_acc: 0.8827                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9036144578313253                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8826923076923077                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 992)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                63552                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 66,195                                                                                                   \n",
      "Trainable params: 66,195                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 87%|███████████████████████████████████████▊      | 104/120 [2:35:10<24:25, 91.62s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 16.5145 - acc: 0.8458 - val_loss: 9.7144 - val_acc: 0.8513                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 5.9290 - acc: 0.9039 - val_loss: 3.3463 - val_acc: 0.8481                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 1.9368 - acc: 0.9125 - val_loss: 1.1739 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.6429 - acc: 0.9142 - val_loss: 0.5075 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3666 - acc: 0.9078 - val_loss: 0.4107 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3058 - acc: 0.9110 - val_loss: 0.4115 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.2806 - acc: 0.9127 - val_loss: 0.3814 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2668 - acc: 0.9120 - val_loss: 0.4443 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2502 - acc: 0.9147 - val_loss: 0.3329 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2447 - acc: 0.9201 - val_loss: 0.3054 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2376 - acc: 0.9196 - val_loss: 0.3540 - val_acc: 0.8891                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2371 - acc: 0.9235 - val_loss: 0.3148 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2333 - acc: 0.9171 - val_loss: 0.3881 - val_acc: 0.8404                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2278 - acc: 0.9230 - val_loss: 0.4077 - val_acc: 0.8103                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2254 - acc: 0.9223 - val_loss: 0.3138 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2205 - acc: 0.9221 - val_loss: 0.2930 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2202 - acc: 0.9240 - val_loss: 0.3064 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2143 - acc: 0.9294 - val_loss: 0.3047 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2216 - acc: 0.9243 - val_loss: 0.3407 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2089 - acc: 0.9321 - val_loss: 0.2826 - val_acc: 0.9032                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2120 - acc: 0.9277 - val_loss: 0.2860 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2080 - acc: 0.9326 - val_loss: 0.2711 - val_acc: 0.9167                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2101 - acc: 0.9289 - val_loss: 0.2843 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2107 - acc: 0.9280 - val_loss: 0.3032 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2069 - acc: 0.9326 - val_loss: 0.2626 - val_acc: 0.9013                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2049 - acc: 0.9319 - val_loss: 0.2884 - val_acc: 0.9058                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2120 - acc: 0.9304 - val_loss: 0.3549 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2040 - acc: 0.9353 - val_loss: 0.2945 - val_acc: 0.9186                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2023 - acc: 0.9351 - val_loss: 0.2792 - val_acc: 0.9205                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.1983 - acc: 0.9351 - val_loss: 0.3234 - val_acc: 0.8756                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9252520285222523                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8756410256410256                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 28)           1288                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           2720                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1280)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                81984                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 86,187                                                                                                   \n",
      "Trainable params: 86,187                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 88%|████████████████████████████████████████▎     | 105/120 [2:36:21<21:21, 85.41s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 61.7899 - acc: 0.8131 - val_loss: 29.2118 - val_acc: 0.9026                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 3s - loss: 14.4101 - acc: 0.8962 - val_loss: 4.8678 - val_acc: 0.8635                                               \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 3s - loss: 2.0708 - acc: 0.8884 - val_loss: 0.9190 - val_acc: 0.8154                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 3s - loss: 0.4834 - acc: 0.8950 - val_loss: 0.5455 - val_acc: 0.8192                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 3s - loss: 0.3400 - acc: 0.8928 - val_loss: 0.5137 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 3s - loss: 0.3110 - acc: 0.8948 - val_loss: 0.3989 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 3s - loss: 0.3081 - acc: 0.8957 - val_loss: 0.4062 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 3s - loss: 0.2871 - acc: 0.8994 - val_loss: 0.4073 - val_acc: 0.8397                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 3s - loss: 0.2909 - acc: 0.9036 - val_loss: 0.3832 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 3s - loss: 0.2900 - acc: 0.8940 - val_loss: 0.3751 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 3s - loss: 0.2827 - acc: 0.8980 - val_loss: 0.3781 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 3s - loss: 0.2821 - acc: 0.8992 - val_loss: 0.3512 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 3s - loss: 0.2832 - acc: 0.9002 - val_loss: 0.3554 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 3s - loss: 0.2737 - acc: 0.9026 - val_loss: 0.3657 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 3s - loss: 0.2772 - acc: 0.9004 - val_loss: 0.3820 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 3s - loss: 0.2792 - acc: 0.8980 - val_loss: 0.3507 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 3s - loss: 0.2749 - acc: 0.9014 - val_loss: 0.3518 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 3s - loss: 0.2691 - acc: 0.9021 - val_loss: 0.3407 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 3s - loss: 0.2727 - acc: 0.8980 - val_loss: 0.3451 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 3s - loss: 0.2766 - acc: 0.8955 - val_loss: 0.3423 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 3s - loss: 0.2630 - acc: 0.9041 - val_loss: 0.3434 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 3s - loss: 0.2697 - acc: 0.8972 - val_loss: 0.3334 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 3s - loss: 0.2734 - acc: 0.8985 - val_loss: 0.3384 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 3s - loss: 0.2716 - acc: 0.8989 - val_loss: 0.3494 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 3s - loss: 0.2654 - acc: 0.8957 - val_loss: 0.3572 - val_acc: 0.8519                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8822227686255225                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8519230769230769                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 976)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                62528                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,747                                                                                                   \n",
      "Trainable params: 65,747                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 88%|████████████████████████████████████████▋     | 106/120 [2:37:33<18:58, 81.31s/it, best loss: -0.9352564102564103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 10.2002 - acc: 0.8301 - val_loss: 5.2111 - val_acc: 0.8827                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 3.0056 - acc: 0.9031 - val_loss: 1.6707 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 1.0355 - acc: 0.9068 - val_loss: 0.8101 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.5343 - acc: 0.9080 - val_loss: 0.5732 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3948 - acc: 0.9066 - val_loss: 0.4507 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3227 - acc: 0.9093 - val_loss: 0.3971 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3013 - acc: 0.9144 - val_loss: 0.3700 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.2735 - acc: 0.9186 - val_loss: 0.5026 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2702 - acc: 0.9186 - val_loss: 0.3504 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2641 - acc: 0.9154 - val_loss: 0.3138 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.2395 - acc: 0.9179 - val_loss: 0.3235 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.2458 - acc: 0.9221 - val_loss: 0.3304 - val_acc: 0.8974                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.2378 - acc: 0.9230 - val_loss: 0.3118 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2299 - acc: 0.9282 - val_loss: 0.3341 - val_acc: 0.9013                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2284 - acc: 0.9235 - val_loss: 0.3801 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2241 - acc: 0.9307 - val_loss: 0.3024 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2222 - acc: 0.9316 - val_loss: 0.3438 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2238 - acc: 0.9289 - val_loss: 0.3058 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2133 - acc: 0.9289 - val_loss: 0.2465 - val_acc: 0.9250                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2151 - acc: 0.9366 - val_loss: 0.2407 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2098 - acc: 0.9363 - val_loss: 0.2732 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2138 - acc: 0.9351 - val_loss: 0.3454 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2071 - acc: 0.9346 - val_loss: 0.2573 - val_acc: 0.9019                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2167 - acc: 0.9314 - val_loss: 0.2436 - val_acc: 0.9276                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2061 - acc: 0.9329 - val_loss: 0.2645 - val_acc: 0.9038                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.1988 - acc: 0.9444 - val_loss: 0.2491 - val_acc: 0.9327                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.1987 - acc: 0.9368 - val_loss: 0.2270 - val_acc: 0.9321                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.1899 - acc: 0.9452 - val_loss: 0.2326 - val_acc: 0.9365                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.1926 - acc: 0.9420 - val_loss: 0.2158 - val_acc: 0.9481                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.1995 - acc: 0.9407 - val_loss: 0.2180 - val_acc: 0.9359                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9508237029751659                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9358974358974359                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                90688                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 97,755                                                                                                   \n",
      "Trainable params: 97,755                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 89%|█████████████████████████████████████████     | 107/120 [2:38:39<16:38, 76.78s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 14.6215 - acc: 0.8335 - val_loss: 7.7808 - val_acc: 0.8756                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 4.5685 - acc: 0.9014 - val_loss: 2.5505 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 1.7643 - acc: 0.9088 - val_loss: 1.3604 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.9957 - acc: 0.9179 - val_loss: 0.8801 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.6403 - acc: 0.9144 - val_loss: 0.6035 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.4486 - acc: 0.9147 - val_loss: 0.4893 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3716 - acc: 0.9179 - val_loss: 0.4575 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3162 - acc: 0.9221 - val_loss: 0.4547 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3121 - acc: 0.9223 - val_loss: 0.3407 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.2842 - acc: 0.9243 - val_loss: 0.3407 - val_acc: 0.8942                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2735 - acc: 0.9208 - val_loss: 0.3221 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2578 - acc: 0.9233 - val_loss: 0.3007 - val_acc: 0.9173                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2419 - acc: 0.9284 - val_loss: 0.3211 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2340 - acc: 0.9331 - val_loss: 0.2955 - val_acc: 0.9192                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2383 - acc: 0.9284 - val_loss: 0.2751 - val_acc: 0.9006                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2177 - acc: 0.9358 - val_loss: 0.2917 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2220 - acc: 0.9336 - val_loss: 0.3097 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2150 - acc: 0.9343 - val_loss: 0.2633 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.2099 - acc: 0.9336 - val_loss: 0.2415 - val_acc: 0.9308                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2081 - acc: 0.9368 - val_loss: 0.3251 - val_acc: 0.9064                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2011 - acc: 0.9380 - val_loss: 0.2392 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2000 - acc: 0.9363 - val_loss: 0.2597 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2055 - acc: 0.9341 - val_loss: 0.2359 - val_acc: 0.9308                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2025 - acc: 0.9390 - val_loss: 0.2496 - val_acc: 0.9321                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2041 - acc: 0.9353 - val_loss: 0.2741 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.1901 - acc: 0.9400 - val_loss: 0.2792 - val_acc: 0.9154                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.1872 - acc: 0.9380 - val_loss: 0.2461 - val_acc: 0.9103                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.1958 - acc: 0.9388 - val_loss: 0.2352 - val_acc: 0.9288                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.1890 - acc: 0.9415 - val_loss: 0.2323 - val_acc: 0.9321                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.1881 - acc: 0.9373 - val_loss: 0.2406 - val_acc: 0.9256                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9449225473321858                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.9256410256410257                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                6160                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 9,235                                                                                                    \n",
      "Trainable params: 9,235                                                                                                \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 90%|█████████████████████████████████████████▍    | 108/120 [2:40:21<16:50, 84.23s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 1.9062 - acc: 0.8731 - val_loss: 0.5846 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 3s - loss: 0.4446 - acc: 0.9031 - val_loss: 0.4859 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 3s - loss: 0.3272 - acc: 0.9115 - val_loss: 0.3997 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 3s - loss: 0.3280 - acc: 0.9105 - val_loss: 0.4052 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 3s - loss: 0.2923 - acc: 0.9166 - val_loss: 0.3675 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 3s - loss: 0.2620 - acc: 0.9171 - val_loss: 0.3733 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 3s - loss: 0.2637 - acc: 0.9186 - val_loss: 0.3101 - val_acc: 0.9071                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 3s - loss: 0.2569 - acc: 0.9171 - val_loss: 0.3130 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 3s - loss: 0.2489 - acc: 0.9257 - val_loss: 0.3769 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 3s - loss: 0.2545 - acc: 0.9267 - val_loss: 0.3754 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 3s - loss: 0.2405 - acc: 0.9255 - val_loss: 0.3109 - val_acc: 0.8962                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 3s - loss: 0.2315 - acc: 0.9245 - val_loss: 0.2920 - val_acc: 0.9160                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 3s - loss: 0.2246 - acc: 0.9348 - val_loss: 0.2900 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 3s - loss: 0.2342 - acc: 0.9282 - val_loss: 0.2822 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 3s - loss: 0.2371 - acc: 0.9329 - val_loss: 0.2899 - val_acc: 0.9141                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 3s - loss: 0.2243 - acc: 0.9336 - val_loss: 0.3220 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 3s - loss: 0.2206 - acc: 0.9339 - val_loss: 0.2859 - val_acc: 0.9077                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 3s - loss: 0.2384 - acc: 0.9324 - val_loss: 0.3942 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 3s - loss: 0.2177 - acc: 0.9361 - val_loss: 0.3288 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 3s - loss: 0.2009 - acc: 0.9403 - val_loss: 0.2633 - val_acc: 0.9051                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 3s - loss: 0.2141 - acc: 0.9405 - val_loss: 0.3008 - val_acc: 0.9006                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 3s - loss: 0.2271 - acc: 0.9339 - val_loss: 0.2798 - val_acc: 0.9205                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 3s - loss: 0.2090 - acc: 0.9363 - val_loss: 0.3720 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 3s - loss: 0.2251 - acc: 0.9309 - val_loss: 0.2840 - val_acc: 0.9090                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 3s - loss: 0.2172 - acc: 0.9353 - val_loss: 0.3075 - val_acc: 0.8981                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 3s - loss: 0.1967 - acc: 0.9366 - val_loss: 0.2664 - val_acc: 0.9083                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 3s - loss: 0.1947 - acc: 0.9417 - val_loss: 0.2719 - val_acc: 0.9103                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 3s - loss: 0.2249 - acc: 0.9329 - val_loss: 0.2715 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 3s - loss: 0.2008 - acc: 0.9388 - val_loss: 0.3090 - val_acc: 0.8897                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 3s - loss: 0.2120 - acc: 0.9307 - val_loss: 0.2996 - val_acc: 0.9038                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 3s - loss: 0.2146 - acc: 0.9339 - val_loss: 0.2751 - val_acc: 0.9205                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 3s - loss: 0.1929 - acc: 0.9403 - val_loss: 0.2582 - val_acc: 0.9186                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 3s - loss: 0.1945 - acc: 0.9415 - val_loss: 0.2705 - val_acc: 0.9179                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 3s - loss: 0.2157 - acc: 0.9385 - val_loss: 0.2655 - val_acc: 0.9135                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 3s - loss: 0.1894 - acc: 0.9427 - val_loss: 0.2766 - val_acc: 0.9160                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9522989918859109                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.916025641025641                                                                                                      \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                30752                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 36,159                                                                                                   \n",
      "Trainable params: 36,159                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 91%|█████████████████████████████████████████▊    | 109/120 [2:42:01<16:18, 88.92s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 144.3685 - acc: 0.8291 - val_loss: 89.4709 - val_acc: 0.8429                                             \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 57.3549 - acc: 0.8716 - val_loss: 31.3525 - val_acc: 0.8699                                              \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 17.2278 - acc: 0.8928 - val_loss: 7.0032 - val_acc: 0.8442                                               \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 2.9180 - acc: 0.8815 - val_loss: 0.9003 - val_acc: 0.8051                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4752 - acc: 0.8790 - val_loss: 0.4792 - val_acc: 0.8487                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3583 - acc: 0.8886 - val_loss: 0.4272 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3466 - acc: 0.8881 - val_loss: 0.3856 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3299 - acc: 0.8889 - val_loss: 0.4691 - val_acc: 0.8045                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3206 - acc: 0.8955 - val_loss: 0.3597 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3189 - acc: 0.8918 - val_loss: 0.3659 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3065 - acc: 0.8938 - val_loss: 0.3886 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2984 - acc: 0.8985 - val_loss: 0.3713 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.2971 - acc: 0.9026 - val_loss: 0.3594 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2895 - acc: 0.9039 - val_loss: 0.4120 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3006 - acc: 0.8940 - val_loss: 0.3788 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.2974 - acc: 0.8960 - val_loss: 0.3516 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.2899 - acc: 0.8985 - val_loss: 0.3418 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.2866 - acc: 0.9019 - val_loss: 0.3409 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3024 - acc: 0.8923 - val_loss: 0.3501 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.2935 - acc: 0.8921 - val_loss: 0.4100 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2864 - acc: 0.9029 - val_loss: 0.3641 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2805 - acc: 0.8977 - val_loss: 0.3293 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2977 - acc: 0.8913 - val_loss: 0.3543 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2856 - acc: 0.8950 - val_loss: 0.3525 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2877 - acc: 0.8950 - val_loss: 0.3673 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2863 - acc: 0.8970 - val_loss: 0.3476 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2837 - acc: 0.8913 - val_loss: 0.3428 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2823 - acc: 0.8935 - val_loss: 0.3448 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2849 - acc: 0.8975 - val_loss: 0.3409 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2803 - acc: 0.9031 - val_loss: 0.3315 - val_acc: 0.8821                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9198426358495205                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.882051282051282                                                                                                      \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,299                                                                                                   \n",
      "Trainable params: 65,299                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 92%|██████████████████████████████████████████▏   | 110/120 [2:43:37<15:11, 91.10s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 20.3999 - acc: 0.8289 - val_loss: 1.8019 - val_acc: 0.7968                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 0.5802 - acc: 0.8753 - val_loss: 0.4227 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.3392 - acc: 0.8847 - val_loss: 0.4417 - val_acc: 0.8410                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3196 - acc: 0.8925 - val_loss: 0.3933 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3139 - acc: 0.8908 - val_loss: 0.3397 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3093 - acc: 0.8940 - val_loss: 0.4343 - val_acc: 0.8109                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3071 - acc: 0.8913 - val_loss: 0.3602 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3009 - acc: 0.8923 - val_loss: 0.4163 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.2928 - acc: 0.8985 - val_loss: 0.3216 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.2966 - acc: 0.9004 - val_loss: 0.3326 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.2916 - acc: 0.8948 - val_loss: 0.5614 - val_acc: 0.7295                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.2919 - acc: 0.8967 - val_loss: 0.4077 - val_acc: 0.8333                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3064 - acc: 0.8935 - val_loss: 0.4351 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.2827 - acc: 0.8994 - val_loss: 0.4126 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.2879 - acc: 0.9012 - val_loss: 0.3460 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.2968 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.2900 - acc: 0.8928 - val_loss: 0.3410 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.2959 - acc: 0.8999 - val_loss: 0.7126 - val_acc: 0.7179                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.2940 - acc: 0.8962 - val_loss: 0.3172 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.2819 - acc: 0.9012 - val_loss: 0.3224 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.2810 - acc: 0.9048 - val_loss: 0.3328 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.2865 - acc: 0.9029 - val_loss: 0.4442 - val_acc: 0.7686                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.2791 - acc: 0.9016 - val_loss: 0.3311 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.2859 - acc: 0.9024 - val_loss: 0.3187 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.2872 - acc: 0.9031 - val_loss: 0.3618 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.2887 - acc: 0.9016 - val_loss: 0.3326 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.2881 - acc: 0.9026 - val_loss: 0.3967 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.2803 - acc: 0.9063 - val_loss: 0.3476 - val_acc: 0.8865                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.2865 - acc: 0.9068 - val_loss: 0.3179 - val_acc: 0.8878                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.2917 - acc: 0.9007 - val_loss: 0.3442 - val_acc: 0.8718                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9198426358495205                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8717948717948718                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                42048                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,387                                                                                                   \n",
      "Trainable params: 44,387                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 92%|██████████████████████████████████████████▌   | 111/120 [2:44:52<12:58, 86.47s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 2s - loss: 38.3772 - acc: 0.8279 - val_loss: 11.9872 - val_acc: 0.8840                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 4.4407 - acc: 0.8670 - val_loss: 0.8439 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 0.4812 - acc: 0.8643 - val_loss: 0.5066 - val_acc: 0.8019                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.3655 - acc: 0.8835 - val_loss: 0.5707 - val_acc: 0.7750                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.3573 - acc: 0.8842 - val_loss: 0.4327 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3513 - acc: 0.8835 - val_loss: 0.4529 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 0.3552 - acc: 0.8795 - val_loss: 0.4373 - val_acc: 0.8641                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.3312 - acc: 0.8866 - val_loss: 0.4333 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.3334 - acc: 0.8862 - val_loss: 0.4189 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.3449 - acc: 0.8862 - val_loss: 0.3828 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.3373 - acc: 0.8908 - val_loss: 0.4105 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.3289 - acc: 0.8869 - val_loss: 0.3594 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.3225 - acc: 0.8911 - val_loss: 0.3746 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.3103 - acc: 0.8948 - val_loss: 0.6838 - val_acc: 0.6647                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.3409 - acc: 0.8820 - val_loss: 0.4238 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.3414 - acc: 0.8852 - val_loss: 0.3842 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.3252 - acc: 0.8898 - val_loss: 0.3452 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.3312 - acc: 0.8854 - val_loss: 0.3861 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.3235 - acc: 0.8898 - val_loss: 0.3857 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.3156 - acc: 0.8911 - val_loss: 0.5823 - val_acc: 0.7910                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.3133 - acc: 0.8925 - val_loss: 0.3833 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.3144 - acc: 0.8960 - val_loss: 0.3694 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.3223 - acc: 0.8844 - val_loss: 0.3886 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.3202 - acc: 0.8881 - val_loss: 0.3876 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.3246 - acc: 0.8925 - val_loss: 0.5538 - val_acc: 0.8378                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.861322842389968                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8378205128205128                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                124992                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 93%|██████████████████████████████████████████▉   | 112/120 [2:45:43<10:06, 75.78s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 55.1219 - acc: 0.8065 - val_loss: 7.9078 - val_acc: 0.8308                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 2.0051 - acc: 0.8530 - val_loss: 0.6064 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4937 - acc: 0.8598 - val_loss: 0.5688 - val_acc: 0.8026                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4476 - acc: 0.8704 - val_loss: 0.5866 - val_acc: 0.7795                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.4348 - acc: 0.8623 - val_loss: 0.6138 - val_acc: 0.7904                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.4202 - acc: 0.8768 - val_loss: 0.4416 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.3928 - acc: 0.8807 - val_loss: 0.4253 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.4083 - acc: 0.8773 - val_loss: 0.5388 - val_acc: 0.8000                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 5s - loss: 0.3909 - acc: 0.8800 - val_loss: 0.4514 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.3883 - acc: 0.8795 - val_loss: 0.4149 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.3804 - acc: 0.8820 - val_loss: 0.4234 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3996 - acc: 0.8830 - val_loss: 0.4403 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.3885 - acc: 0.8766 - val_loss: 0.3744 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.3658 - acc: 0.8849 - val_loss: 0.4873 - val_acc: 0.8346                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3887 - acc: 0.8820 - val_loss: 0.4459 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3892 - acc: 0.8758 - val_loss: 0.4389 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.3759 - acc: 0.8746 - val_loss: 0.4440 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3787 - acc: 0.8812 - val_loss: 0.4181 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3792 - acc: 0.8790 - val_loss: 0.3873 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3590 - acc: 0.8810 - val_loss: 0.5829 - val_acc: 0.8135                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3678 - acc: 0.8817 - val_loss: 0.3949 - val_acc: 0.8686                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3596 - acc: 0.8874 - val_loss: 0.3934 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3761 - acc: 0.8807 - val_loss: 0.4235 - val_acc: 0.8519                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3626 - acc: 0.8839 - val_loss: 0.4274 - val_acc: 0.8615                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3672 - acc: 0.8815 - val_loss: 0.5013 - val_acc: 0.8378                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.3719 - acc: 0.8734 - val_loss: 0.4117 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.3696 - acc: 0.8771 - val_loss: 0.3957 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.3744 - acc: 0.8817 - val_loss: 0.3810 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.3829 - acc: 0.8761 - val_loss: 0.4128 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.3852 - acc: 0.8748 - val_loss: 0.4378 - val_acc: 0.8686                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8652569461519548                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8685897435897436                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                22672                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 29,595                                                                                                   \n",
      "Trainable params: 29,595                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 94%|███████████████████████████████████████████▎  | 113/120 [2:47:34<10:02, 86.04s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 24.8907 - acc: 0.7893 - val_loss: 5.6108 - val_acc: 0.8558                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 1.8847 - acc: 0.8633 - val_loss: 0.5141 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.4352 - acc: 0.8739 - val_loss: 0.4905 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4136 - acc: 0.8773 - val_loss: 0.5204 - val_acc: 0.8212                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3813 - acc: 0.8790 - val_loss: 0.5326 - val_acc: 0.8083                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3427 - acc: 0.8879 - val_loss: 0.4600 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3727 - acc: 0.8810 - val_loss: 0.4430 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3438 - acc: 0.8884 - val_loss: 0.4707 - val_acc: 0.8321                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3487 - acc: 0.8923 - val_loss: 0.3851 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3251 - acc: 0.8886 - val_loss: 0.3704 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3245 - acc: 0.8903 - val_loss: 0.3847 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3214 - acc: 0.8881 - val_loss: 0.3732 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3143 - acc: 0.8960 - val_loss: 0.3492 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3062 - acc: 0.8967 - val_loss: 0.4180 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3161 - acc: 0.8908 - val_loss: 0.4336 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3168 - acc: 0.8913 - val_loss: 0.3838 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.3064 - acc: 0.8930 - val_loss: 0.3679 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3076 - acc: 0.8948 - val_loss: 0.4192 - val_acc: 0.8596                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3194 - acc: 0.8886 - val_loss: 0.3590 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3163 - acc: 0.8916 - val_loss: 0.3421 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2980 - acc: 0.8994 - val_loss: 0.3539 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3024 - acc: 0.8933 - val_loss: 0.3443 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3178 - acc: 0.8898 - val_loss: 0.3658 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3063 - acc: 0.8903 - val_loss: 0.3689 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3073 - acc: 0.8953 - val_loss: 0.5079 - val_acc: 0.8372                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3041 - acc: 0.8933 - val_loss: 0.3830 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2966 - acc: 0.8918 - val_loss: 0.3339 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3166 - acc: 0.8896 - val_loss: 0.3621 - val_acc: 0.8801                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3006 - acc: 0.9024 - val_loss: 0.3406 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3130 - acc: 0.8901 - val_loss: 0.3776 - val_acc: 0.8776                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8964838947627244                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8775641025641026                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 384)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                24640                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 27,859                                                                                                   \n",
      "Trainable params: 27,859                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 95%|███████████████████████████████████████████▋  | 114/120 [2:49:12<08:58, 89.77s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/35                                                                                                             \n",
      " - 3s - loss: 30.7523 - acc: 0.8210 - val_loss: 7.5254 - val_acc: 0.8590                                               \n",
      "\n",
      "Epoch 2/35                                                                                                             \n",
      " - 2s - loss: 2.8562 - acc: 0.8921 - val_loss: 0.9450 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 3/35                                                                                                             \n",
      " - 2s - loss: 0.4760 - acc: 0.9014 - val_loss: 0.4738 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 4/35                                                                                                             \n",
      " - 2s - loss: 0.3396 - acc: 0.8992 - val_loss: 0.4561 - val_acc: 0.8436                                                \n",
      "\n",
      "Epoch 5/35                                                                                                             \n",
      " - 2s - loss: 0.3425 - acc: 0.8881 - val_loss: 0.4345 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 6/35                                                                                                             \n",
      " - 2s - loss: 0.3040 - acc: 0.9016 - val_loss: 0.4548 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 7/35                                                                                                             \n",
      " - 2s - loss: 0.3343 - acc: 0.8948 - val_loss: 0.4632 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 8/35                                                                                                             \n",
      " - 2s - loss: 0.3158 - acc: 0.8985 - val_loss: 0.4636 - val_acc: 0.8590                                                \n",
      "\n",
      "Epoch 9/35                                                                                                             \n",
      " - 2s - loss: 0.3325 - acc: 0.9016 - val_loss: 0.4150 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 10/35                                                                                                            \n",
      " - 2s - loss: 0.2990 - acc: 0.8982 - val_loss: 0.4448 - val_acc: 0.8237                                                \n",
      "\n",
      "Epoch 11/35                                                                                                            \n",
      " - 2s - loss: 0.3028 - acc: 0.9007 - val_loss: 0.4222 - val_acc: 0.8660                                                \n",
      "\n",
      "Epoch 12/35                                                                                                            \n",
      " - 2s - loss: 0.2993 - acc: 0.8987 - val_loss: 0.4104 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 13/35                                                                                                            \n",
      " - 2s - loss: 0.3131 - acc: 0.9031 - val_loss: 0.4038 - val_acc: 0.8647                                                \n",
      "\n",
      "Epoch 14/35                                                                                                            \n",
      " - 2s - loss: 0.2891 - acc: 0.9031 - val_loss: 0.3949 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 15/35                                                                                                            \n",
      " - 2s - loss: 0.2863 - acc: 0.9041 - val_loss: 0.3812 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 16/35                                                                                                            \n",
      " - 2s - loss: 0.3007 - acc: 0.8994 - val_loss: 0.4309 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 17/35                                                                                                            \n",
      " - 2s - loss: 0.3085 - acc: 0.8987 - val_loss: 0.3607 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 18/35                                                                                                            \n",
      " - 2s - loss: 0.3001 - acc: 0.9007 - val_loss: 0.3647 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 19/35                                                                                                            \n",
      " - 2s - loss: 0.2813 - acc: 0.9024 - val_loss: 0.3930 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 20/35                                                                                                            \n",
      " - 2s - loss: 0.2851 - acc: 0.9012 - val_loss: 0.3835 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 21/35                                                                                                            \n",
      " - 2s - loss: 0.2902 - acc: 0.9046 - val_loss: 0.4107 - val_acc: 0.8564                                                \n",
      "\n",
      "Epoch 22/35                                                                                                            \n",
      " - 2s - loss: 0.2754 - acc: 0.9009 - val_loss: 0.3862 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 23/35                                                                                                            \n",
      " - 2s - loss: 0.2783 - acc: 0.9041 - val_loss: 0.3525 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 24/35                                                                                                            \n",
      " - 2s - loss: 0.2828 - acc: 0.9036 - val_loss: 0.3580 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 25/35                                                                                                            \n",
      " - 2s - loss: 0.2827 - acc: 0.9016 - val_loss: 0.3629 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 26/35                                                                                                            \n",
      " - 2s - loss: 0.3065 - acc: 0.9009 - val_loss: 0.3828 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 27/35                                                                                                            \n",
      " - 2s - loss: 0.2678 - acc: 0.9098 - val_loss: 0.3783 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 28/35                                                                                                            \n",
      " - 2s - loss: 0.2857 - acc: 0.9034 - val_loss: 0.4210 - val_acc: 0.8231                                                \n",
      "\n",
      "Epoch 29/35                                                                                                            \n",
      " - 2s - loss: 0.2998 - acc: 0.9044 - val_loss: 0.4703 - val_acc: 0.8500                                                \n",
      "\n",
      "Epoch 30/35                                                                                                            \n",
      " - 2s - loss: 0.2942 - acc: 0.9016 - val_loss: 0.3844 - val_acc: 0.8853                                                \n",
      "\n",
      "Epoch 31/35                                                                                                            \n",
      " - 2s - loss: 0.2821 - acc: 0.9100 - val_loss: 0.3785 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 32/35                                                                                                            \n",
      " - 2s - loss: 0.2819 - acc: 0.9080 - val_loss: 0.3769 - val_acc: 0.8769                                                \n",
      "\n",
      "Epoch 33/35                                                                                                            \n",
      " - 2s - loss: 0.2694 - acc: 0.9132 - val_loss: 0.4135 - val_acc: 0.8385                                                \n",
      "\n",
      "Epoch 34/35                                                                                                            \n",
      " - 2s - loss: 0.2928 - acc: 0.8999 - val_loss: 0.4013 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 35/35                                                                                                            \n",
      " - 2s - loss: 0.2673 - acc: 0.9115 - val_loss: 0.3589 - val_acc: 0.8846                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.9068109171379395                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8846153846153846                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 42)           1932                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           3376                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                30752                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 99                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 36,159                                                                                                   \n",
      "Trainable params: 36,159                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 96%|████████████████████████████████████████████  | 115/120 [2:50:27<07:07, 85.45s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 4.8467 - acc: 0.8672 - val_loss: 0.5698 - val_acc: 0.8667                                                \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 4s - loss: 0.3805 - acc: 0.8960 - val_loss: 0.3767 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 4s - loss: 0.3173 - acc: 0.8994 - val_loss: 0.3968 - val_acc: 0.8506                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 4s - loss: 0.2881 - acc: 0.9056 - val_loss: 0.3418 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 4s - loss: 0.2875 - acc: 0.9053 - val_loss: 0.3334 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 4s - loss: 0.2796 - acc: 0.9066 - val_loss: 0.3633 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 4s - loss: 0.2731 - acc: 0.9100 - val_loss: 0.3886 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 4s - loss: 0.2704 - acc: 0.9122 - val_loss: 0.5853 - val_acc: 0.8276                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 4s - loss: 0.2685 - acc: 0.9103 - val_loss: 0.3140 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 4s - loss: 0.2670 - acc: 0.9098 - val_loss: 0.3075 - val_acc: 0.8936                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 4s - loss: 0.2655 - acc: 0.9144 - val_loss: 0.3259 - val_acc: 0.8929                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 4s - loss: 0.2905 - acc: 0.9105 - val_loss: 0.3812 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 4s - loss: 0.2870 - acc: 0.9127 - val_loss: 0.3503 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 4s - loss: 0.2739 - acc: 0.9169 - val_loss: 0.2993 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 4s - loss: 0.2661 - acc: 0.9203 - val_loss: 0.3704 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 4s - loss: 0.2693 - acc: 0.9149 - val_loss: 0.3115 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 4s - loss: 0.2742 - acc: 0.9110 - val_loss: 0.3154 - val_acc: 0.8955                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 4s - loss: 0.2631 - acc: 0.9184 - val_loss: 0.4670 - val_acc: 0.8545                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 4s - loss: 0.2725 - acc: 0.9117 - val_loss: 0.3000 - val_acc: 0.8987                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2704 - acc: 0.9127 - val_loss: 0.3961 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 4s - loss: 0.2833 - acc: 0.9132 - val_loss: 0.4147 - val_acc: 0.8603                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 4s - loss: 0.2768 - acc: 0.9149 - val_loss: 0.3293 - val_acc: 0.8923                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 4s - loss: 0.2825 - acc: 0.9071 - val_loss: 0.3839 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 4s - loss: 0.2818 - acc: 0.9115 - val_loss: 0.3145 - val_acc: 0.8910                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 4s - loss: 0.3172 - acc: 0.9036 - val_loss: 0.2962 - val_acc: 0.8968                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 4s - loss: 0.3133 - acc: 0.9068 - val_loss: 0.3261 - val_acc: 0.9026                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 4s - loss: 0.3074 - acc: 0.9103 - val_loss: 0.3875 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 4s - loss: 0.3889 - acc: 0.8842 - val_loss: 0.3683 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 4s - loss: 0.4232 - acc: 0.8864 - val_loss: 0.3873 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 4s - loss: 0.5990 - acc: 0.8635 - val_loss: 0.5464 - val_acc: 0.8750                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8962380132776002                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.875                                                                                                                  \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 122, 32)           2048                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 120, 16)           1552                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 120, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 60, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 960)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                61504                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 65,299                                                                                                   \n",
      "Trainable params: 65,299                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 97%|████████████████████████████████████████████▍ | 116/120 [2:52:25<06:21, 95.31s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 33.2658 - acc: 0.7969 - val_loss: 6.9987 - val_acc: 0.8423                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 2s - loss: 2.0605 - acc: 0.8621 - val_loss: 0.5374 - val_acc: 0.8532                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 2s - loss: 0.4020 - acc: 0.8780 - val_loss: 0.4936 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 2s - loss: 0.3751 - acc: 0.8835 - val_loss: 0.5841 - val_acc: 0.7737                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 2s - loss: 0.3596 - acc: 0.8869 - val_loss: 0.4486 - val_acc: 0.8468                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 2s - loss: 0.3603 - acc: 0.8847 - val_loss: 0.3965 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 2s - loss: 0.3494 - acc: 0.8854 - val_loss: 0.3642 - val_acc: 0.8840                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 2s - loss: 0.3394 - acc: 0.8859 - val_loss: 0.4456 - val_acc: 0.8179                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 2s - loss: 0.3298 - acc: 0.8953 - val_loss: 0.3548 - val_acc: 0.8833                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 2s - loss: 0.3345 - acc: 0.8886 - val_loss: 0.3533 - val_acc: 0.8827                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 2s - loss: 0.3299 - acc: 0.8879 - val_loss: 0.3830 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 2s - loss: 0.3295 - acc: 0.8916 - val_loss: 0.3499 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 2s - loss: 0.3304 - acc: 0.8953 - val_loss: 0.3468 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 2s - loss: 0.3198 - acc: 0.8928 - val_loss: 0.4040 - val_acc: 0.8526                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 2s - loss: 0.3385 - acc: 0.8894 - val_loss: 0.3849 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 2s - loss: 0.3247 - acc: 0.8891 - val_loss: 0.3489 - val_acc: 0.8872                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 2s - loss: 0.3179 - acc: 0.8933 - val_loss: 0.3539 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 2s - loss: 0.3289 - acc: 0.8925 - val_loss: 0.4105 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 2s - loss: 0.3198 - acc: 0.8953 - val_loss: 0.3583 - val_acc: 0.8859                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 2s - loss: 0.3299 - acc: 0.8977 - val_loss: 0.3906 - val_acc: 0.8763                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 2s - loss: 0.3151 - acc: 0.8957 - val_loss: 0.3691 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 2s - loss: 0.3083 - acc: 0.8967 - val_loss: 0.3897 - val_acc: 0.8654                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 2s - loss: 0.3282 - acc: 0.8891 - val_loss: 0.3553 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 2s - loss: 0.3159 - acc: 0.8962 - val_loss: 0.3704 - val_acc: 0.8885                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 2s - loss: 0.3225 - acc: 0.8938 - val_loss: 0.4390 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 2s - loss: 0.3132 - acc: 0.8975 - val_loss: 0.3856 - val_acc: 0.8821                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 2s - loss: 0.3122 - acc: 0.9009 - val_loss: 0.3353 - val_acc: 0.8949                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 2s - loss: 0.3438 - acc: 0.8940 - val_loss: 0.3624 - val_acc: 0.8904                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 2s - loss: 0.3357 - acc: 0.8965 - val_loss: 0.3791 - val_acc: 0.8846                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 2s - loss: 0.3134 - acc: 0.8967 - val_loss: 0.4798 - val_acc: 0.8724                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.906073272682567                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8724358974358974                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 126, 28)           784                                                             \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 124, 16)           1360                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 124, 16)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 41, 16)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 656)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                42048                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 44,387                                                                                                   \n",
      "Trainable params: 44,387                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 98%|████████████████████████████████████████████▊ | 117/120 [2:53:34<04:22, 87.34s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/25                                                                                                             \n",
      " - 3s - loss: 85.6068 - acc: 0.8520 - val_loss: 27.8886 - val_acc: 0.8160                                              \n",
      "\n",
      "Epoch 2/25                                                                                                             \n",
      " - 2s - loss: 9.7005 - acc: 0.8795 - val_loss: 1.0812 - val_acc: 0.8538                                                \n",
      "\n",
      "Epoch 3/25                                                                                                             \n",
      " - 2s - loss: 0.4890 - acc: 0.8685 - val_loss: 0.4694 - val_acc: 0.8295                                                \n",
      "\n",
      "Epoch 4/25                                                                                                             \n",
      " - 2s - loss: 0.3792 - acc: 0.8830 - val_loss: 0.4488 - val_acc: 0.8699                                                \n",
      "\n",
      "Epoch 5/25                                                                                                             \n",
      " - 2s - loss: 0.3596 - acc: 0.8852 - val_loss: 0.4256 - val_acc: 0.8429                                                \n",
      "\n",
      "Epoch 6/25                                                                                                             \n",
      " - 2s - loss: 0.3450 - acc: 0.8864 - val_loss: 0.4879 - val_acc: 0.7904                                                \n",
      "\n",
      "Epoch 7/25                                                                                                             \n",
      " - 2s - loss: 0.3328 - acc: 0.8862 - val_loss: 0.3698 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 8/25                                                                                                             \n",
      " - 2s - loss: 0.3306 - acc: 0.8825 - val_loss: 0.4087 - val_acc: 0.8417                                                \n",
      "\n",
      "Epoch 9/25                                                                                                             \n",
      " - 2s - loss: 0.3239 - acc: 0.8889 - val_loss: 0.3546 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 10/25                                                                                                            \n",
      " - 2s - loss: 0.3233 - acc: 0.8896 - val_loss: 0.3758 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 11/25                                                                                                            \n",
      " - 2s - loss: 0.3184 - acc: 0.8847 - val_loss: 0.6194 - val_acc: 0.6917                                                \n",
      "\n",
      "Epoch 12/25                                                                                                            \n",
      " - 2s - loss: 0.3215 - acc: 0.8884 - val_loss: 0.5071 - val_acc: 0.7872                                                \n",
      "\n",
      "Epoch 13/25                                                                                                            \n",
      " - 2s - loss: 0.3268 - acc: 0.8842 - val_loss: 0.4583 - val_acc: 0.8006                                                \n",
      "\n",
      "Epoch 14/25                                                                                                            \n",
      " - 2s - loss: 0.3176 - acc: 0.8879 - val_loss: 0.6435 - val_acc: 0.6731                                                \n",
      "\n",
      "Epoch 15/25                                                                                                            \n",
      " - 2s - loss: 0.3194 - acc: 0.8859 - val_loss: 0.3659 - val_acc: 0.8679                                                \n",
      "\n",
      "Epoch 16/25                                                                                                            \n",
      " - 2s - loss: 0.3224 - acc: 0.8889 - val_loss: 0.3519 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 17/25                                                                                                            \n",
      " - 2s - loss: 0.3173 - acc: 0.8820 - val_loss: 0.3386 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 18/25                                                                                                            \n",
      " - 2s - loss: 0.3146 - acc: 0.8891 - val_loss: 0.4197 - val_acc: 0.8442                                                \n",
      "\n",
      "Epoch 19/25                                                                                                            \n",
      " - 2s - loss: 0.3134 - acc: 0.8891 - val_loss: 0.3421 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 20/25                                                                                                            \n",
      " - 2s - loss: 0.3077 - acc: 0.8950 - val_loss: 0.3586 - val_acc: 0.8808                                                \n",
      "\n",
      "Epoch 21/25                                                                                                            \n",
      " - 2s - loss: 0.3017 - acc: 0.8933 - val_loss: 0.3408 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 22/25                                                                                                            \n",
      " - 2s - loss: 0.3108 - acc: 0.8898 - val_loss: 0.3713 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 23/25                                                                                                            \n",
      " - 2s - loss: 0.3132 - acc: 0.8864 - val_loss: 0.3528 - val_acc: 0.8744                                                \n",
      "\n",
      "Epoch 24/25                                                                                                            \n",
      " - 2s - loss: 0.3007 - acc: 0.8894 - val_loss: 0.3509 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/25                                                                                                            \n",
      " - 2s - loss: 0.3034 - acc: 0.8881 - val_loss: 0.4233 - val_acc: 0.8397                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8554216867469879                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8397435897435898                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 122, 32)           3104                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 122, 32)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 61, 32)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1952)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 64)                124992                                                          \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 195                                                             \n",
      "=================================================================                                                      \n",
      "Total params: 129,763                                                                                                  \n",
      "Trainable params: 129,763                                                                                              \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 98%|█████████████████████████████████████████████▏| 118/120 [2:54:33<02:37, 78.72s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 3s - loss: 106.2231 - acc: 0.8220 - val_loss: 42.4383 - val_acc: 0.8673                                             \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 18.6512 - acc: 0.8665 - val_loss: 4.7978 - val_acc: 0.7865                                               \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 1.5645 - acc: 0.8655 - val_loss: 0.6181 - val_acc: 0.7878                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.4252 - acc: 0.8751 - val_loss: 0.6038 - val_acc: 0.7712                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3862 - acc: 0.8800 - val_loss: 0.5390 - val_acc: 0.8449                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3691 - acc: 0.8849 - val_loss: 0.4511 - val_acc: 0.8692                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3815 - acc: 0.8726 - val_loss: 0.4203 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3707 - acc: 0.8807 - val_loss: 0.5560 - val_acc: 0.8013                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3660 - acc: 0.8844 - val_loss: 0.4154 - val_acc: 0.8551                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3511 - acc: 0.8864 - val_loss: 0.4460 - val_acc: 0.8558                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3453 - acc: 0.8827 - val_loss: 0.3678 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3548 - acc: 0.8847 - val_loss: 0.4156 - val_acc: 0.8462                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3391 - acc: 0.8908 - val_loss: 0.3617 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.3225 - acc: 0.8921 - val_loss: 0.7364 - val_acc: 0.7103                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.3408 - acc: 0.8874 - val_loss: 0.5723 - val_acc: 0.7699                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3355 - acc: 0.8825 - val_loss: 0.3683 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3293 - acc: 0.8913 - val_loss: 0.3745 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3296 - acc: 0.8916 - val_loss: 0.3844 - val_acc: 0.8577                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.3234 - acc: 0.8869 - val_loss: 0.3609 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 3s - loss: 0.3282 - acc: 0.8864 - val_loss: 0.5982 - val_acc: 0.7635                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.3174 - acc: 0.8930 - val_loss: 0.4721 - val_acc: 0.8282                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.3285 - acc: 0.8938 - val_loss: 0.3735 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.3292 - acc: 0.8812 - val_loss: 0.3627 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.3334 - acc: 0.8835 - val_loss: 0.3853 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.3332 - acc: 0.8842 - val_loss: 0.7546 - val_acc: 0.8019                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.3280 - acc: 0.8894 - val_loss: 0.3720 - val_acc: 0.8718                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.3225 - acc: 0.8898 - val_loss: 0.3668 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.3126 - acc: 0.8916 - val_loss: 0.3696 - val_acc: 0.8795                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.3290 - acc: 0.8894 - val_loss: 0.3761 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.3133 - acc: 0.8881 - val_loss: 0.3865 - val_acc: 0.8782                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.8832062945660192                                                                                                     \n",
      "Test accuracy:                                                                                                         \n",
      "0.8782051282051282                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "conv1d_1 (Conv1D)            (None, 124, 32)           1472                                                            \n",
      "_________________________________________________________________                                                      \n",
      "conv1d_2 (Conv1D)            (None, 118, 24)           5400                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 118, 24)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 24)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "flatten_1 (Flatten)          (None, 1416)              0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 16)                22672                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 3)                 51                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 29,595                                                                                                   \n",
      "Trainable params: 29,595                                                                                               \n",
      "Non-trainable params: 0                                                                                                \n",
      "_________________________________________________________________                                                      \n",
      "None                                                                                                                   \n",
      " 99%|█████████████████████████████████████████████▌| 119/120 [2:56:05<01:22, 82.86s/it, best loss: -0.9358974358974359]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Downloads\\human activity\\HAR\\temp_model.py:623: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  validation_data=(X_val_s, Y_val_s))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples                                                                        \n",
      "Epoch 1/30                                                                                                             \n",
      " - 4s - loss: 30.9736 - acc: 0.8286 - val_loss: 7.0226 - val_acc: 0.8705                                               \n",
      "\n",
      "Epoch 2/30                                                                                                             \n",
      " - 3s - loss: 3.2555 - acc: 0.8817 - val_loss: 1.3723 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 3/30                                                                                                             \n",
      " - 3s - loss: 0.7001 - acc: 0.9019 - val_loss: 0.5209 - val_acc: 0.8635                                                \n",
      "\n",
      "Epoch 4/30                                                                                                             \n",
      " - 3s - loss: 0.3629 - acc: 0.8985 - val_loss: 0.4228 - val_acc: 0.8712                                                \n",
      "\n",
      "Epoch 5/30                                                                                                             \n",
      " - 3s - loss: 0.3609 - acc: 0.8832 - val_loss: 0.4110 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 6/30                                                                                                             \n",
      " - 3s - loss: 0.3000 - acc: 0.9061 - val_loss: 0.5435 - val_acc: 0.8308                                                \n",
      "\n",
      "Epoch 7/30                                                                                                             \n",
      " - 3s - loss: 0.3289 - acc: 0.8916 - val_loss: 0.4238 - val_acc: 0.8609                                                \n",
      "\n",
      "Epoch 8/30                                                                                                             \n",
      " - 3s - loss: 0.3238 - acc: 0.8970 - val_loss: 0.3833 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 9/30                                                                                                             \n",
      " - 3s - loss: 0.3037 - acc: 0.8999 - val_loss: 0.4966 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 10/30                                                                                                            \n",
      " - 3s - loss: 0.3009 - acc: 0.9007 - val_loss: 0.3899 - val_acc: 0.8622                                                \n",
      "\n",
      "Epoch 11/30                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8994 - val_loss: 0.3785 - val_acc: 0.8731                                                \n",
      "\n",
      "Epoch 12/30                                                                                                            \n",
      " - 3s - loss: 0.3040 - acc: 0.8965 - val_loss: 0.4003 - val_acc: 0.8583                                                \n",
      "\n",
      "Epoch 13/30                                                                                                            \n",
      " - 3s - loss: 0.3720 - acc: 0.8970 - val_loss: 0.3614 - val_acc: 0.8814                                                \n",
      "\n",
      "Epoch 14/30                                                                                                            \n",
      " - 3s - loss: 0.2797 - acc: 0.9036 - val_loss: 0.3498 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 15/30                                                                                                            \n",
      " - 3s - loss: 0.2748 - acc: 0.9061 - val_loss: 0.3528 - val_acc: 0.8724                                                \n",
      "\n",
      "Epoch 16/30                                                                                                            \n",
      " - 3s - loss: 0.3041 - acc: 0.8950 - val_loss: 0.4264 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 17/30                                                                                                            \n",
      " - 3s - loss: 0.3113 - acc: 0.8953 - val_loss: 0.4056 - val_acc: 0.8673                                                \n",
      "\n",
      "Epoch 18/30                                                                                                            \n",
      " - 3s - loss: 0.3148 - acc: 0.9024 - val_loss: 0.3933 - val_acc: 0.8494                                                \n",
      "\n",
      "Epoch 19/30                                                                                                            \n",
      " - 3s - loss: 0.2807 - acc: 0.9041 - val_loss: 0.3395 - val_acc: 0.8737                                                \n",
      "\n",
      "Epoch 20/30                                                                                                            \n",
      " - 4s - loss: 0.2824 - acc: 0.9009 - val_loss: 0.3390 - val_acc: 0.8782                                                \n",
      "\n",
      "Epoch 21/30                                                                                                            \n",
      " - 3s - loss: 0.2795 - acc: 0.9026 - val_loss: 0.4088 - val_acc: 0.8455                                                \n",
      "\n",
      "Epoch 22/30                                                                                                            \n",
      " - 3s - loss: 0.2868 - acc: 0.8997 - val_loss: 0.3272 - val_acc: 0.8705                                                \n",
      "\n",
      "Epoch 23/30                                                                                                            \n",
      " - 3s - loss: 0.2765 - acc: 0.8960 - val_loss: 0.3822 - val_acc: 0.8571                                                \n",
      "\n",
      "Epoch 24/30                                                                                                            \n",
      " - 3s - loss: 0.2817 - acc: 0.9024 - val_loss: 0.3325 - val_acc: 0.8776                                                \n",
      "\n",
      "Epoch 25/30                                                                                                            \n",
      " - 3s - loss: 0.2681 - acc: 0.9026 - val_loss: 0.4619 - val_acc: 0.8628                                                \n",
      "\n",
      "Epoch 26/30                                                                                                            \n",
      " - 3s - loss: 0.2945 - acc: 0.8989 - val_loss: 0.3545 - val_acc: 0.8788                                                \n",
      "\n",
      "Epoch 27/30                                                                                                            \n",
      " - 3s - loss: 0.2622 - acc: 0.9090 - val_loss: 0.3667 - val_acc: 0.8513                                                \n",
      "\n",
      "Epoch 28/30                                                                                                            \n",
      " - 3s - loss: 0.2802 - acc: 0.9019 - val_loss: 0.3589 - val_acc: 0.8750                                                \n",
      "\n",
      "Epoch 29/30                                                                                                            \n",
      " - 3s - loss: 0.2804 - acc: 0.9073 - val_loss: 0.3516 - val_acc: 0.8756                                                \n",
      "\n",
      "Epoch 30/30                                                                                                            \n",
      " - 3s - loss: 0.2744 - acc: 0.9004 - val_loss: 0.3440 - val_acc: 0.8737                                                \n",
      "\n",
      "Train accuracy                                                                                                         \n",
      "0.895992131792476                                                                                                      \n",
      "Test accuracy:                                                                                                         \n",
      "0.8737179487179487                                                                                                     \n",
      "-------------------------------------------------------------------------------------                                  \n",
      "100%|██████████████████████████████████████████████| 120/120 [2:57:49<00:00, 89.15s/it, best loss: -0.9358974358974359]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperas.utils import eval_hyperopt_space\n",
    "X_train, Y_train, X_val, Y_val = data_scaled_static()\n",
    "trials = Trials()\n",
    "best_run, best_model, space = optim.minimize(model=model_cnn,\n",
    "                                      data=data_scaled_static,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=120,rseed = 0,                                           \n",
    "                                      trials=trials,notebook_name = 'ranasinghiitkgpemail.com_21LSTM',\n",
    "                                      return_space = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense': 2,\n",
       " 'Dense_1': 2,\n",
       " 'Dropout': 0.49529853850718253,\n",
       " 'choiceval': 1,\n",
       " 'filters': 1,\n",
       " 'filters_1': 0,\n",
       " 'kernel_size': 1,\n",
       " 'kernel_size_1': 0,\n",
       " 'l2': 0.08310635905151897,\n",
       " 'l2_1': 0.39453880352012505,\n",
       " 'lr': 0.0026702733143963197,\n",
       " 'lr_1': 0.0017324123448968834,\n",
       " 'nb_epoch': 1,\n",
       " 'pool_size': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense': 64,\n",
       " 'Dense_1': 64,\n",
       " 'Dropout': 0.49529853850718253,\n",
       " 'choiceval': 'rmsprop',\n",
       " 'filters': 32,\n",
       " 'filters_1': 16,\n",
       " 'kernel_size': 5,\n",
       " 'kernel_size_1': 3,\n",
       " 'l2': 0.08310635905151897,\n",
       " 'l2_1': 0.39453880352012505,\n",
       " 'lr': 0.0026702733143963197,\n",
       " 'lr_1': 0.0017324123448968834,\n",
       " 'nb_epoch': 30,\n",
       " 'pool_size': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperas.utils import eval_hyperopt_space\n",
    "total_trials = dict()\n",
    "total_list = []\n",
    "for t, trial in enumerate(trials):\n",
    "        vals = trial.get('misc').get('vals')\n",
    "        z = eval_hyperopt_space(space, vals)\n",
    "        total_trials['M'+str(t+1)] = z\n",
    "\n",
    "\n",
    "#best Hyper params from hyperas\n",
    "best_params = eval_hyperopt_space(space, best_run)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 124, 32)           1472      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 122, 16)           1552      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 122, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 61, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 976)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                62528     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 65,747\n",
      "Trainable params: 65,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/30\n",
      "4067/4067 [==============================] - ETA: 52s - loss: 19.5415 - acc: 0.296 - ETA: 18s - loss: 18.4986 - acc: 0.479 - ETA: 11s - loss: 17.8039 - acc: 0.562 - ETA: 8s - loss: 17.2049 - acc: 0.607 - ETA: 7s - loss: 16.7269 - acc: 0.64 - ETA: 5s - loss: 16.2621 - acc: 0.68 - ETA: 5s - loss: 15.8877 - acc: 0.69 - ETA: 4s - loss: 15.5174 - acc: 0.71 - ETA: 4s - loss: 15.1806 - acc: 0.73 - ETA: 3s - loss: 14.8748 - acc: 0.74 - ETA: 3s - loss: 14.7297 - acc: 0.74 - ETA: 3s - loss: 14.4392 - acc: 0.75 - ETA: 2s - loss: 14.1693 - acc: 0.76 - ETA: 2s - loss: 13.9090 - acc: 0.77 - ETA: 2s - loss: 13.6712 - acc: 0.77 - ETA: 2s - loss: 13.4227 - acc: 0.78 - ETA: 2s - loss: 13.1875 - acc: 0.78 - ETA: 1s - loss: 12.9583 - acc: 0.79 - ETA: 1s - loss: 12.7373 - acc: 0.79 - ETA: 1s - loss: 12.5263 - acc: 0.80 - ETA: 1s - loss: 12.3212 - acc: 0.80 - ETA: 1s - loss: 12.1181 - acc: 0.80 - ETA: 1s - loss: 11.9298 - acc: 0.81 - ETA: 1s - loss: 11.8332 - acc: 0.81 - ETA: 0s - loss: 11.6438 - acc: 0.81 - ETA: 0s - loss: 11.4551 - acc: 0.81 - ETA: 0s - loss: 11.3626 - acc: 0.81 - ETA: 0s - loss: 11.2778 - acc: 0.81 - ETA: 0s - loss: 11.1926 - acc: 0.81 - ETA: 0s - loss: 11.1077 - acc: 0.81 - ETA: 0s - loss: 11.0212 - acc: 0.81 - ETA: 0s - loss: 10.8616 - acc: 0.81 - ETA: 0s - loss: 10.6940 - acc: 0.82 - ETA: 0s - loss: 10.5303 - acc: 0.82 - ETA: 0s - loss: 10.3732 - acc: 0.82 - 4s 904us/step - loss: 10.2547 - acc: 0.8274 - val_loss: 5.3539 - val_acc: 0.8833\n",
      "Epoch 2/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 5.2461 - acc: 0.937 - ETA: 2s - loss: 5.0824 - acc: 0.953 - ETA: 2s - loss: 5.0230 - acc: 0.931 - ETA: 2s - loss: 4.9373 - acc: 0.924 - ETA: 2s - loss: 4.8960 - acc: 0.906 - ETA: 2s - loss: 4.8572 - acc: 0.904 - ETA: 2s - loss: 4.7721 - acc: 0.902 - ETA: 2s - loss: 4.7082 - acc: 0.900 - ETA: 1s - loss: 4.6197 - acc: 0.905 - ETA: 1s - loss: 4.5503 - acc: 0.898 - ETA: 1s - loss: 4.4723 - acc: 0.899 - ETA: 1s - loss: 4.3916 - acc: 0.903 - ETA: 1s - loss: 4.3236 - acc: 0.899 - ETA: 1s - loss: 4.2448 - acc: 0.902 - ETA: 1s - loss: 4.1721 - acc: 0.904 - ETA: 1s - loss: 4.0998 - acc: 0.905 - ETA: 1s - loss: 4.0306 - acc: 0.904 - ETA: 1s - loss: 3.9630 - acc: 0.905 - ETA: 1s - loss: 3.8976 - acc: 0.905 - ETA: 1s - loss: 3.8302 - acc: 0.907 - ETA: 0s - loss: 3.7710 - acc: 0.907 - ETA: 0s - loss: 3.7063 - acc: 0.908 - ETA: 0s - loss: 3.6439 - acc: 0.909 - ETA: 0s - loss: 3.5925 - acc: 0.906 - ETA: 0s - loss: 3.5372 - acc: 0.904 - ETA: 0s - loss: 3.4798 - acc: 0.905 - ETA: 0s - loss: 3.4235 - acc: 0.906 - ETA: 0s - loss: 3.3692 - acc: 0.907 - ETA: 0s - loss: 3.3224 - acc: 0.906 - ETA: 0s - loss: 3.2744 - acc: 0.905 - ETA: 0s - loss: 3.2265 - acc: 0.904 - ETA: 0s - loss: 3.1855 - acc: 0.903 - ETA: 0s - loss: 3.1634 - acc: 0.902 - 3s 686us/step - loss: 3.1502 - acc: 0.9031 - val_loss: 1.7943 - val_acc: 0.8737\n",
      "Epoch 3/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 1.6847 - acc: 0.921 - ETA: 2s - loss: 1.7393 - acc: 0.890 - ETA: 2s - loss: 1.7524 - acc: 0.875 - ETA: 2s - loss: 1.6579 - acc: 0.903 - ETA: 2s - loss: 1.6246 - acc: 0.908 - ETA: 2s - loss: 1.6012 - acc: 0.914 - ETA: 2s - loss: 1.5792 - acc: 0.915 - ETA: 2s - loss: 1.5603 - acc: 0.919 - ETA: 2s - loss: 1.5440 - acc: 0.918 - ETA: 2s - loss: 1.5193 - acc: 0.918 - ETA: 2s - loss: 1.5053 - acc: 0.920 - ETA: 2s - loss: 1.4994 - acc: 0.917 - ETA: 2s - loss: 1.4901 - acc: 0.917 - ETA: 2s - loss: 1.4793 - acc: 0.917 - ETA: 1s - loss: 1.4644 - acc: 0.913 - ETA: 1s - loss: 1.4521 - acc: 0.913 - ETA: 1s - loss: 1.4294 - acc: 0.916 - ETA: 1s - loss: 1.4090 - acc: 0.917 - ETA: 1s - loss: 1.3970 - acc: 0.917 - ETA: 1s - loss: 1.3836 - acc: 0.914 - ETA: 1s - loss: 1.3747 - acc: 0.914 - ETA: 1s - loss: 1.3573 - acc: 0.914 - ETA: 1s - loss: 1.3494 - acc: 0.913 - ETA: 1s - loss: 1.3349 - acc: 0.911 - ETA: 1s - loss: 1.3175 - acc: 0.912 - ETA: 1s - loss: 1.3010 - acc: 0.912 - ETA: 1s - loss: 1.2880 - acc: 0.911 - ETA: 0s - loss: 1.2705 - acc: 0.912 - ETA: 0s - loss: 1.2525 - acc: 0.913 - ETA: 0s - loss: 1.2420 - acc: 0.911 - ETA: 0s - loss: 1.2282 - acc: 0.912 - ETA: 0s - loss: 1.2206 - acc: 0.912 - ETA: 0s - loss: 1.2058 - acc: 0.912 - ETA: 0s - loss: 1.1966 - acc: 0.911 - ETA: 0s - loss: 1.1843 - acc: 0.911 - ETA: 0s - loss: 1.1751 - acc: 0.909 - ETA: 0s - loss: 1.1644 - acc: 0.907 - ETA: 0s - loss: 1.1586 - acc: 0.907 - ETA: 0s - loss: 1.1526 - acc: 0.907 - 3s 754us/step - loss: 1.1499 - acc: 0.9068 - val_loss: 0.8690 - val_acc: 0.8712\n",
      "Epoch 4/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.7699 - acc: 0.875 - ETA: 2s - loss: 0.7474 - acc: 0.901 - ETA: 2s - loss: 0.7418 - acc: 0.915 - ETA: 2s - loss: 0.7343 - acc: 0.912 - ETA: 2s - loss: 0.7294 - acc: 0.914 - ETA: 2s - loss: 0.7175 - acc: 0.911 - ETA: 2s - loss: 0.7037 - acc: 0.915 - ETA: 2s - loss: 0.7044 - acc: 0.915 - ETA: 1s - loss: 0.6914 - acc: 0.918 - ETA: 1s - loss: 0.6961 - acc: 0.916 - ETA: 1s - loss: 0.6965 - acc: 0.915 - ETA: 1s - loss: 0.6913 - acc: 0.916 - ETA: 1s - loss: 0.6876 - acc: 0.917 - ETA: 1s - loss: 0.6823 - acc: 0.919 - ETA: 1s - loss: 0.6770 - acc: 0.919 - ETA: 1s - loss: 0.6667 - acc: 0.921 - ETA: 1s - loss: 0.6591 - acc: 0.923 - ETA: 1s - loss: 0.6516 - acc: 0.922 - ETA: 1s - loss: 0.6512 - acc: 0.921 - ETA: 1s - loss: 0.6436 - acc: 0.921 - ETA: 1s - loss: 0.6356 - acc: 0.921 - ETA: 1s - loss: 0.6279 - acc: 0.922 - ETA: 1s - loss: 0.6259 - acc: 0.922 - ETA: 1s - loss: 0.6218 - acc: 0.922 - ETA: 1s - loss: 0.6219 - acc: 0.921 - ETA: 0s - loss: 0.6150 - acc: 0.923 - ETA: 0s - loss: 0.6114 - acc: 0.921 - ETA: 0s - loss: 0.6257 - acc: 0.916 - ETA: 0s - loss: 0.6204 - acc: 0.915 - ETA: 0s - loss: 0.6170 - acc: 0.914 - ETA: 0s - loss: 0.6134 - acc: 0.912 - ETA: 0s - loss: 0.6115 - acc: 0.911 - ETA: 0s - loss: 0.6079 - acc: 0.910 - ETA: 0s - loss: 0.6058 - acc: 0.909 - ETA: 0s - loss: 0.5998 - acc: 0.910 - ETA: 0s - loss: 0.5990 - acc: 0.910 - ETA: 0s - loss: 0.5934 - acc: 0.911 - ETA: 0s - loss: 0.5897 - acc: 0.910 - 3s 714us/step - loss: 0.5893 - acc: 0.9100 - val_loss: 0.5806 - val_acc: 0.8513\n",
      "Epoch 5/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.4794 - acc: 0.859 - ETA: 2s - loss: 0.4873 - acc: 0.859 - ETA: 2s - loss: 0.4594 - acc: 0.887 - ETA: 2s - loss: 0.4733 - acc: 0.888 - ETA: 2s - loss: 0.4702 - acc: 0.888 - ETA: 2s - loss: 0.4657 - acc: 0.898 - ETA: 2s - loss: 0.4526 - acc: 0.903 - ETA: 2s - loss: 0.4615 - acc: 0.901 - ETA: 2s - loss: 0.4533 - acc: 0.904 - ETA: 1s - loss: 0.4495 - acc: 0.903 - ETA: 1s - loss: 0.4520 - acc: 0.901 - ETA: 1s - loss: 0.4530 - acc: 0.899 - ETA: 1s - loss: 0.4455 - acc: 0.903 - ETA: 1s - loss: 0.4400 - acc: 0.903 - ETA: 1s - loss: 0.4384 - acc: 0.903 - ETA: 1s - loss: 0.4362 - acc: 0.903 - ETA: 1s - loss: 0.4312 - acc: 0.903 - ETA: 1s - loss: 0.4317 - acc: 0.903 - ETA: 1s - loss: 0.4335 - acc: 0.900 - ETA: 1s - loss: 0.4296 - acc: 0.902 - ETA: 1s - loss: 0.4263 - acc: 0.904 - ETA: 0s - loss: 0.4263 - acc: 0.904 - ETA: 0s - loss: 0.4232 - acc: 0.906 - ETA: 0s - loss: 0.4234 - acc: 0.904 - ETA: 0s - loss: 0.4206 - acc: 0.905 - ETA: 0s - loss: 0.4207 - acc: 0.905 - ETA: 0s - loss: 0.4194 - acc: 0.905 - ETA: 0s - loss: 0.4199 - acc: 0.904 - ETA: 0s - loss: 0.4249 - acc: 0.903 - ETA: 0s - loss: 0.4245 - acc: 0.903 - ETA: 0s - loss: 0.4209 - acc: 0.904 - ETA: 0s - loss: 0.4195 - acc: 0.903 - ETA: 0s - loss: 0.4186 - acc: 0.904 - ETA: 0s - loss: 0.4184 - acc: 0.904 - ETA: 0s - loss: 0.4149 - acc: 0.905 - ETA: 0s - loss: 0.4130 - acc: 0.904 - 3s 717us/step - loss: 0.4121 - acc: 0.9051 - val_loss: 0.4978 - val_acc: 0.8635\n",
      "Epoch 6/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.4283 - acc: 0.875 - ETA: 3s - loss: 0.4151 - acc: 0.882 - ETA: 3s - loss: 0.3951 - acc: 0.890 - ETA: 3s - loss: 0.3743 - acc: 0.902 - ETA: 3s - loss: 0.3733 - acc: 0.909 - ETA: 2s - loss: 0.3593 - acc: 0.912 - ETA: 2s - loss: 0.3584 - acc: 0.910 - ETA: 2s - loss: 0.3567 - acc: 0.908 - ETA: 2s - loss: 0.3498 - acc: 0.912 - ETA: 2s - loss: 0.3435 - acc: 0.914 - ETA: 2s - loss: 0.3351 - acc: 0.918 - ETA: 2s - loss: 0.3348 - acc: 0.917 - ETA: 2s - loss: 0.3409 - acc: 0.915 - ETA: 2s - loss: 0.3491 - acc: 0.908 - ETA: 2s - loss: 0.3501 - acc: 0.905 - ETA: 1s - loss: 0.3520 - acc: 0.901 - ETA: 1s - loss: 0.3519 - acc: 0.902 - ETA: 1s - loss: 0.3539 - acc: 0.901 - ETA: 1s - loss: 0.3481 - acc: 0.905 - ETA: 1s - loss: 0.3497 - acc: 0.903 - ETA: 1s - loss: 0.3491 - acc: 0.903 - ETA: 1s - loss: 0.3481 - acc: 0.902 - ETA: 1s - loss: 0.3426 - acc: 0.905 - ETA: 1s - loss: 0.3412 - acc: 0.905 - ETA: 1s - loss: 0.3426 - acc: 0.903 - ETA: 1s - loss: 0.3423 - acc: 0.902 - ETA: 1s - loss: 0.3393 - acc: 0.904 - ETA: 0s - loss: 0.3371 - acc: 0.905 - ETA: 0s - loss: 0.3369 - acc: 0.903 - ETA: 0s - loss: 0.3349 - acc: 0.905 - ETA: 0s - loss: 0.3340 - acc: 0.906 - ETA: 0s - loss: 0.3389 - acc: 0.903 - ETA: 0s - loss: 0.3393 - acc: 0.903 - ETA: 0s - loss: 0.3390 - acc: 0.903 - ETA: 0s - loss: 0.3382 - acc: 0.903 - ETA: 0s - loss: 0.3364 - acc: 0.905 - ETA: 0s - loss: 0.3370 - acc: 0.905 - ETA: 0s - loss: 0.3362 - acc: 0.905 - 3s 718us/step - loss: 0.3351 - acc: 0.9056 - val_loss: 0.3969 - val_acc: 0.8641\n",
      "Epoch 7/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2746 - acc: 0.953 - ETA: 2s - loss: 0.2735 - acc: 0.927 - ETA: 2s - loss: 0.3592 - acc: 0.884 - ETA: 2s - loss: 0.3330 - acc: 0.899 - ETA: 2s - loss: 0.3199 - acc: 0.911 - ETA: 2s - loss: 0.3095 - acc: 0.913 - ETA: 1s - loss: 0.3052 - acc: 0.914 - ETA: 1s - loss: 0.3007 - acc: 0.915 - ETA: 1s - loss: 0.3029 - acc: 0.914 - ETA: 1s - loss: 0.3101 - acc: 0.912 - ETA: 1s - loss: 0.3111 - acc: 0.909 - ETA: 1s - loss: 0.3133 - acc: 0.910 - ETA: 1s - loss: 0.3093 - acc: 0.910 - ETA: 1s - loss: 0.3077 - acc: 0.911 - ETA: 1s - loss: 0.3109 - acc: 0.911 - ETA: 1s - loss: 0.3081 - acc: 0.912 - ETA: 1s - loss: 0.3103 - acc: 0.911 - ETA: 1s - loss: 0.3117 - acc: 0.910 - ETA: 1s - loss: 0.3139 - acc: 0.909 - ETA: 0s - loss: 0.3218 - acc: 0.906 - ETA: 0s - loss: 0.3165 - acc: 0.909 - ETA: 0s - loss: 0.3188 - acc: 0.907 - ETA: 0s - loss: 0.3182 - acc: 0.908 - ETA: 0s - loss: 0.3175 - acc: 0.908 - ETA: 0s - loss: 0.3146 - acc: 0.909 - ETA: 0s - loss: 0.3129 - acc: 0.911 - ETA: 0s - loss: 0.3128 - acc: 0.911 - ETA: 0s - loss: 0.3087 - acc: 0.912 - ETA: 0s - loss: 0.3080 - acc: 0.913 - ETA: 0s - loss: 0.3053 - acc: 0.913 - ETA: 0s - loss: 0.3032 - acc: 0.914 - ETA: 0s - loss: 0.3024 - acc: 0.914 - ETA: 0s - loss: 0.3018 - acc: 0.914 - 3s 667us/step - loss: 0.3016 - acc: 0.9152 - val_loss: 0.4453 - val_acc: 0.8577\n",
      "Epoch 8/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3038 - acc: 0.921 - ETA: 2s - loss: 0.3711 - acc: 0.885 - ETA: 2s - loss: 0.3258 - acc: 0.900 - ETA: 2s - loss: 0.3025 - acc: 0.915 - ETA: 2s - loss: 0.2942 - acc: 0.914 - ETA: 2s - loss: 0.2899 - acc: 0.918 - ETA: 2s - loss: 0.2916 - acc: 0.916 - ETA: 2s - loss: 0.2896 - acc: 0.919 - ETA: 2s - loss: 0.2725 - acc: 0.925 - ETA: 1s - loss: 0.2689 - acc: 0.926 - ETA: 1s - loss: 0.2806 - acc: 0.921 - ETA: 1s - loss: 0.2858 - acc: 0.918 - ETA: 1s - loss: 0.2880 - acc: 0.915 - ETA: 1s - loss: 0.2872 - acc: 0.915 - ETA: 1s - loss: 0.2853 - acc: 0.915 - ETA: 1s - loss: 0.2830 - acc: 0.916 - ETA: 1s - loss: 0.2819 - acc: 0.915 - ETA: 1s - loss: 0.2845 - acc: 0.915 - ETA: 1s - loss: 0.2850 - acc: 0.916 - ETA: 1s - loss: 0.2829 - acc: 0.918 - ETA: 1s - loss: 0.2804 - acc: 0.920 - ETA: 1s - loss: 0.2805 - acc: 0.919 - ETA: 0s - loss: 0.2786 - acc: 0.919 - ETA: 0s - loss: 0.2756 - acc: 0.921 - ETA: 0s - loss: 0.2836 - acc: 0.919 - ETA: 0s - loss: 0.2817 - acc: 0.919 - ETA: 0s - loss: 0.2786 - acc: 0.920 - ETA: 0s - loss: 0.2772 - acc: 0.921 - ETA: 0s - loss: 0.2738 - acc: 0.923 - ETA: 0s - loss: 0.2710 - acc: 0.923 - ETA: 0s - loss: 0.2721 - acc: 0.923 - ETA: 0s - loss: 0.2714 - acc: 0.924 - ETA: 0s - loss: 0.2762 - acc: 0.921 - ETA: 0s - loss: 0.2749 - acc: 0.921 - 3s 668us/step - loss: 0.2755 - acc: 0.9211 - val_loss: 0.3793 - val_acc: 0.8596\n",
      "Epoch 9/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3444 - acc: 0.890 - ETA: 2s - loss: 0.3272 - acc: 0.921 - ETA: 2s - loss: 0.3468 - acc: 0.896 - ETA: 2s - loss: 0.3336 - acc: 0.901 - ETA: 2s - loss: 0.3177 - acc: 0.906 - ETA: 2s - loss: 0.3099 - acc: 0.909 - ETA: 1s - loss: 0.2978 - acc: 0.911 - ETA: 1s - loss: 0.3062 - acc: 0.907 - ETA: 1s - loss: 0.2996 - acc: 0.907 - ETA: 1s - loss: 0.3002 - acc: 0.907 - ETA: 1s - loss: 0.2915 - acc: 0.911 - ETA: 1s - loss: 0.3021 - acc: 0.909 - ETA: 1s - loss: 0.2997 - acc: 0.909 - ETA: 1s - loss: 0.2938 - acc: 0.913 - ETA: 1s - loss: 0.2912 - acc: 0.913 - ETA: 1s - loss: 0.2924 - acc: 0.913 - ETA: 1s - loss: 0.2904 - acc: 0.914 - ETA: 1s - loss: 0.2882 - acc: 0.915 - ETA: 1s - loss: 0.2849 - acc: 0.916 - ETA: 0s - loss: 0.2862 - acc: 0.915 - ETA: 0s - loss: 0.2870 - acc: 0.914 - ETA: 0s - loss: 0.2858 - acc: 0.915 - ETA: 0s - loss: 0.2893 - acc: 0.911 - ETA: 0s - loss: 0.2889 - acc: 0.912 - ETA: 0s - loss: 0.2875 - acc: 0.912 - ETA: 0s - loss: 0.2845 - acc: 0.912 - ETA: 0s - loss: 0.2852 - acc: 0.913 - ETA: 0s - loss: 0.2845 - acc: 0.914 - ETA: 0s - loss: 0.2812 - acc: 0.916 - ETA: 0s - loss: 0.2797 - acc: 0.916 - ETA: 0s - loss: 0.2770 - acc: 0.918 - ETA: 0s - loss: 0.2767 - acc: 0.917 - ETA: 0s - loss: 0.2752 - acc: 0.918 - 3s 650us/step - loss: 0.2743 - acc: 0.9189 - val_loss: 0.3273 - val_acc: 0.8897\n",
      "Epoch 10/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3999 - acc: 0.843 - ETA: 2s - loss: 0.2960 - acc: 0.901 - ETA: 2s - loss: 0.2850 - acc: 0.915 - ETA: 2s - loss: 0.2775 - acc: 0.912 - ETA: 2s - loss: 0.2764 - acc: 0.913 - ETA: 2s - loss: 0.2844 - acc: 0.907 - ETA: 1s - loss: 0.2784 - acc: 0.908 - ETA: 1s - loss: 0.2678 - acc: 0.913 - ETA: 1s - loss: 0.2627 - acc: 0.915 - ETA: 1s - loss: 0.2569 - acc: 0.916 - ETA: 1s - loss: 0.2527 - acc: 0.920 - ETA: 1s - loss: 0.2568 - acc: 0.918 - ETA: 1s - loss: 0.2560 - acc: 0.918 - ETA: 1s - loss: 0.2492 - acc: 0.921 - ETA: 1s - loss: 0.2497 - acc: 0.919 - ETA: 1s - loss: 0.2490 - acc: 0.919 - ETA: 1s - loss: 0.2539 - acc: 0.916 - ETA: 1s - loss: 0.2553 - acc: 0.915 - ETA: 1s - loss: 0.2549 - acc: 0.916 - ETA: 1s - loss: 0.2548 - acc: 0.917 - ETA: 0s - loss: 0.2542 - acc: 0.918 - ETA: 0s - loss: 0.2572 - acc: 0.918 - ETA: 0s - loss: 0.2585 - acc: 0.917 - ETA: 0s - loss: 0.2590 - acc: 0.917 - ETA: 0s - loss: 0.2597 - acc: 0.918 - ETA: 0s - loss: 0.2605 - acc: 0.918 - ETA: 0s - loss: 0.2590 - acc: 0.918 - ETA: 0s - loss: 0.2596 - acc: 0.918 - ETA: 0s - loss: 0.2594 - acc: 0.918 - ETA: 0s - loss: 0.2598 - acc: 0.918 - ETA: 0s - loss: 0.2598 - acc: 0.918 - ETA: 0s - loss: 0.2611 - acc: 0.918 - ETA: 0s - loss: 0.2607 - acc: 0.918 - ETA: 0s - loss: 0.2610 - acc: 0.918 - ETA: 0s - loss: 0.2611 - acc: 0.918 - ETA: 0s - loss: 0.2637 - acc: 0.916 - ETA: 0s - loss: 0.2637 - acc: 0.916 - ETA: 0s - loss: 0.2625 - acc: 0.917 - 3s 698us/step - loss: 0.2616 - acc: 0.9184 - val_loss: 0.3271 - val_acc: 0.8795\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2257 - acc: 0.921 - ETA: 2s - loss: 0.2246 - acc: 0.921 - ETA: 2s - loss: 0.2704 - acc: 0.909 - ETA: 2s - loss: 0.2792 - acc: 0.901 - ETA: 2s - loss: 0.2636 - acc: 0.909 - ETA: 2s - loss: 0.2793 - acc: 0.906 - ETA: 2s - loss: 0.2607 - acc: 0.917 - ETA: 1s - loss: 0.2645 - acc: 0.914 - ETA: 1s - loss: 0.2671 - acc: 0.910 - ETA: 1s - loss: 0.2663 - acc: 0.910 - ETA: 1s - loss: 0.2641 - acc: 0.911 - ETA: 1s - loss: 0.2602 - acc: 0.912 - ETA: 1s - loss: 0.2634 - acc: 0.910 - ETA: 1s - loss: 0.2672 - acc: 0.910 - ETA: 1s - loss: 0.2617 - acc: 0.912 - ETA: 1s - loss: 0.2583 - acc: 0.914 - ETA: 1s - loss: 0.2592 - acc: 0.913 - ETA: 1s - loss: 0.2716 - acc: 0.908 - ETA: 1s - loss: 0.2715 - acc: 0.909 - ETA: 0s - loss: 0.2686 - acc: 0.910 - ETA: 0s - loss: 0.2655 - acc: 0.912 - ETA: 0s - loss: 0.2615 - acc: 0.913 - ETA: 0s - loss: 0.2628 - acc: 0.914 - ETA: 0s - loss: 0.2614 - acc: 0.915 - ETA: 0s - loss: 0.2586 - acc: 0.916 - ETA: 0s - loss: 0.2604 - acc: 0.915 - ETA: 0s - loss: 0.2597 - acc: 0.914 - ETA: 0s - loss: 0.2582 - acc: 0.915 - ETA: 0s - loss: 0.2587 - acc: 0.916 - ETA: 0s - loss: 0.2564 - acc: 0.916 - ETA: 0s - loss: 0.2565 - acc: 0.916 - ETA: 0s - loss: 0.2554 - acc: 0.916 - 3s 647us/step - loss: 0.2543 - acc: 0.9171 - val_loss: 0.3147 - val_acc: 0.8737\n",
      "Epoch 12/30\n",
      "4067/4067 [==============================] - ETA: 3s - loss: 0.1634 - acc: 0.937 - ETA: 2s - loss: 0.2768 - acc: 0.906 - ETA: 2s - loss: 0.2540 - acc: 0.909 - ETA: 2s - loss: 0.2736 - acc: 0.899 - ETA: 2s - loss: 0.2534 - acc: 0.911 - ETA: 2s - loss: 0.2498 - acc: 0.917 - ETA: 2s - loss: 0.2397 - acc: 0.921 - ETA: 1s - loss: 0.2427 - acc: 0.915 - ETA: 1s - loss: 0.2400 - acc: 0.917 - ETA: 1s - loss: 0.2326 - acc: 0.922 - ETA: 1s - loss: 0.2312 - acc: 0.926 - ETA: 1s - loss: 0.2364 - acc: 0.925 - ETA: 1s - loss: 0.2372 - acc: 0.926 - ETA: 1s - loss: 0.2385 - acc: 0.924 - ETA: 1s - loss: 0.2392 - acc: 0.923 - ETA: 1s - loss: 0.2389 - acc: 0.922 - ETA: 1s - loss: 0.2397 - acc: 0.921 - ETA: 1s - loss: 0.2384 - acc: 0.924 - ETA: 1s - loss: 0.2433 - acc: 0.922 - ETA: 0s - loss: 0.2458 - acc: 0.921 - ETA: 0s - loss: 0.2480 - acc: 0.919 - ETA: 0s - loss: 0.2492 - acc: 0.919 - ETA: 0s - loss: 0.2450 - acc: 0.921 - ETA: 0s - loss: 0.2466 - acc: 0.920 - ETA: 0s - loss: 0.2482 - acc: 0.920 - ETA: 0s - loss: 0.2476 - acc: 0.919 - ETA: 0s - loss: 0.2454 - acc: 0.920 - ETA: 0s - loss: 0.2469 - acc: 0.920 - ETA: 0s - loss: 0.2456 - acc: 0.920 - ETA: 0s - loss: 0.2442 - acc: 0.921 - ETA: 0s - loss: 0.2424 - acc: 0.921 - ETA: 0s - loss: 0.2423 - acc: 0.922 - 3s 643us/step - loss: 0.2417 - acc: 0.9228 - val_loss: 0.3155 - val_acc: 0.8987\n",
      "Epoch 13/30\n",
      "4067/4067 [==============================] - ETA: 3s - loss: 0.1252 - acc: 0.984 - ETA: 2s - loss: 0.1818 - acc: 0.947 - ETA: 2s - loss: 0.2044 - acc: 0.937 - ETA: 2s - loss: 0.1988 - acc: 0.942 - ETA: 2s - loss: 0.1974 - acc: 0.942 - ETA: 2s - loss: 0.2043 - acc: 0.941 - ETA: 1s - loss: 0.2198 - acc: 0.938 - ETA: 1s - loss: 0.2223 - acc: 0.933 - ETA: 1s - loss: 0.2301 - acc: 0.926 - ETA: 1s - loss: 0.2333 - acc: 0.926 - ETA: 1s - loss: 0.2329 - acc: 0.926 - ETA: 1s - loss: 0.2322 - acc: 0.923 - ETA: 1s - loss: 0.2284 - acc: 0.926 - ETA: 1s - loss: 0.2260 - acc: 0.927 - ETA: 1s - loss: 0.2242 - acc: 0.928 - ETA: 1s - loss: 0.2269 - acc: 0.926 - ETA: 1s - loss: 0.2319 - acc: 0.926 - ETA: 1s - loss: 0.2297 - acc: 0.926 - ETA: 1s - loss: 0.2286 - acc: 0.927 - ETA: 0s - loss: 0.2293 - acc: 0.925 - ETA: 0s - loss: 0.2361 - acc: 0.924 - ETA: 0s - loss: 0.2359 - acc: 0.924 - ETA: 0s - loss: 0.2331 - acc: 0.927 - ETA: 0s - loss: 0.2319 - acc: 0.927 - ETA: 0s - loss: 0.2382 - acc: 0.927 - ETA: 0s - loss: 0.2374 - acc: 0.928 - ETA: 0s - loss: 0.2370 - acc: 0.927 - ETA: 0s - loss: 0.2394 - acc: 0.926 - ETA: 0s - loss: 0.2386 - acc: 0.927 - ETA: 0s - loss: 0.2378 - acc: 0.928 - ETA: 0s - loss: 0.2361 - acc: 0.928 - ETA: 0s - loss: 0.2345 - acc: 0.929 - ETA: 0s - loss: 0.2344 - acc: 0.929 - ETA: 0s - loss: 0.2354 - acc: 0.928 - 3s 676us/step - loss: 0.2350 - acc: 0.9287 - val_loss: 0.3044 - val_acc: 0.8840\n",
      "Epoch 14/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2561 - acc: 0.906 - ETA: 3s - loss: 0.2587 - acc: 0.914 - ETA: 3s - loss: 0.2154 - acc: 0.942 - ETA: 2s - loss: 0.2131 - acc: 0.937 - ETA: 2s - loss: 0.1995 - acc: 0.940 - ETA: 2s - loss: 0.2062 - acc: 0.939 - ETA: 2s - loss: 0.2149 - acc: 0.935 - ETA: 2s - loss: 0.2147 - acc: 0.938 - ETA: 2s - loss: 0.2109 - acc: 0.944 - ETA: 2s - loss: 0.2074 - acc: 0.946 - ETA: 1s - loss: 0.2223 - acc: 0.940 - ETA: 1s - loss: 0.2353 - acc: 0.935 - ETA: 1s - loss: 0.2277 - acc: 0.939 - ETA: 1s - loss: 0.2261 - acc: 0.938 - ETA: 1s - loss: 0.2242 - acc: 0.939 - ETA: 1s - loss: 0.2253 - acc: 0.940 - ETA: 1s - loss: 0.2231 - acc: 0.941 - ETA: 1s - loss: 0.2270 - acc: 0.938 - ETA: 1s - loss: 0.2260 - acc: 0.937 - ETA: 1s - loss: 0.2277 - acc: 0.936 - ETA: 1s - loss: 0.2250 - acc: 0.938 - ETA: 1s - loss: 0.2221 - acc: 0.939 - ETA: 1s - loss: 0.2205 - acc: 0.939 - ETA: 1s - loss: 0.2206 - acc: 0.938 - ETA: 1s - loss: 0.2209 - acc: 0.938 - ETA: 1s - loss: 0.2191 - acc: 0.938 - ETA: 1s - loss: 0.2178 - acc: 0.939 - ETA: 0s - loss: 0.2195 - acc: 0.939 - ETA: 0s - loss: 0.2191 - acc: 0.938 - ETA: 0s - loss: 0.2204 - acc: 0.937 - ETA: 0s - loss: 0.2202 - acc: 0.936 - ETA: 0s - loss: 0.2210 - acc: 0.935 - ETA: 0s - loss: 0.2188 - acc: 0.936 - ETA: 0s - loss: 0.2199 - acc: 0.935 - ETA: 0s - loss: 0.2221 - acc: 0.934 - ETA: 0s - loss: 0.2218 - acc: 0.934 - ETA: 0s - loss: 0.2315 - acc: 0.932 - ETA: 0s - loss: 0.2308 - acc: 0.933 - ETA: 0s - loss: 0.2299 - acc: 0.934 - ETA: 0s - loss: 0.2330 - acc: 0.933 - ETA: 0s - loss: 0.2337 - acc: 0.933 - ETA: 0s - loss: 0.2351 - acc: 0.932 - 3s 731us/step - loss: 0.2356 - acc: 0.9324 - val_loss: 0.2938 - val_acc: 0.9147\n",
      "Epoch 15/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2384 - acc: 0.937 - ETA: 2s - loss: 0.1930 - acc: 0.958 - ETA: 2s - loss: 0.1813 - acc: 0.962 - ETA: 2s - loss: 0.2038 - acc: 0.944 - ETA: 2s - loss: 0.2130 - acc: 0.941 - ETA: 2s - loss: 0.2197 - acc: 0.940 - ETA: 2s - loss: 0.2259 - acc: 0.938 - ETA: 1s - loss: 0.2266 - acc: 0.937 - ETA: 1s - loss: 0.2270 - acc: 0.935 - ETA: 1s - loss: 0.2230 - acc: 0.936 - ETA: 1s - loss: 0.2283 - acc: 0.935 - ETA: 1s - loss: 0.2319 - acc: 0.934 - ETA: 1s - loss: 0.2283 - acc: 0.936 - ETA: 1s - loss: 0.2286 - acc: 0.935 - ETA: 1s - loss: 0.2231 - acc: 0.937 - ETA: 1s - loss: 0.2245 - acc: 0.933 - ETA: 1s - loss: 0.2275 - acc: 0.930 - ETA: 1s - loss: 0.2256 - acc: 0.931 - ETA: 1s - loss: 0.2289 - acc: 0.929 - ETA: 1s - loss: 0.2287 - acc: 0.930 - ETA: 1s - loss: 0.2291 - acc: 0.929 - ETA: 1s - loss: 0.2287 - acc: 0.928 - ETA: 1s - loss: 0.2297 - acc: 0.928 - ETA: 1s - loss: 0.2299 - acc: 0.928 - ETA: 0s - loss: 0.2297 - acc: 0.928 - ETA: 0s - loss: 0.2259 - acc: 0.930 - ETA: 0s - loss: 0.2260 - acc: 0.930 - ETA: 0s - loss: 0.2329 - acc: 0.927 - ETA: 0s - loss: 0.2308 - acc: 0.928 - ETA: 0s - loss: 0.2324 - acc: 0.928 - ETA: 0s - loss: 0.2310 - acc: 0.929 - ETA: 0s - loss: 0.2299 - acc: 0.930 - ETA: 0s - loss: 0.2273 - acc: 0.930 - ETA: 0s - loss: 0.2320 - acc: 0.929 - ETA: 0s - loss: 0.2321 - acc: 0.929 - 3s 684us/step - loss: 0.2299 - acc: 0.9304 - val_loss: 0.3300 - val_acc: 0.8833\n",
      "Epoch 16/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1893 - acc: 0.921 - ETA: 2s - loss: 0.1971 - acc: 0.937 - ETA: 2s - loss: 0.1837 - acc: 0.940 - ETA: 2s - loss: 0.1806 - acc: 0.942 - ETA: 2s - loss: 0.1787 - acc: 0.944 - ETA: 2s - loss: 0.1919 - acc: 0.943 - ETA: 2s - loss: 0.1905 - acc: 0.947 - ETA: 1s - loss: 0.1872 - acc: 0.946 - ETA: 1s - loss: 0.1943 - acc: 0.943 - ETA: 1s - loss: 0.1953 - acc: 0.940 - ETA: 1s - loss: 0.1927 - acc: 0.940 - ETA: 1s - loss: 0.1940 - acc: 0.938 - ETA: 1s - loss: 0.1919 - acc: 0.940 - ETA: 1s - loss: 0.1916 - acc: 0.940 - ETA: 1s - loss: 0.1946 - acc: 0.940 - ETA: 1s - loss: 0.1955 - acc: 0.940 - ETA: 1s - loss: 0.1969 - acc: 0.938 - ETA: 1s - loss: 0.2010 - acc: 0.937 - ETA: 1s - loss: 0.2028 - acc: 0.937 - ETA: 0s - loss: 0.2075 - acc: 0.934 - ETA: 0s - loss: 0.2074 - acc: 0.935 - ETA: 0s - loss: 0.2075 - acc: 0.935 - ETA: 0s - loss: 0.2062 - acc: 0.935 - ETA: 0s - loss: 0.2068 - acc: 0.935 - ETA: 0s - loss: 0.2087 - acc: 0.934 - ETA: 0s - loss: 0.2060 - acc: 0.935 - ETA: 0s - loss: 0.2059 - acc: 0.935 - ETA: 0s - loss: 0.2099 - acc: 0.933 - ETA: 0s - loss: 0.2148 - acc: 0.932 - ETA: 0s - loss: 0.2150 - acc: 0.932 - ETA: 0s - loss: 0.2173 - acc: 0.931 - ETA: 0s - loss: 0.2170 - acc: 0.931 - 3s 655us/step - loss: 0.2165 - acc: 0.9319 - val_loss: 0.3085 - val_acc: 0.8846\n",
      "Epoch 17/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2337 - acc: 0.921 - ETA: 2s - loss: 0.2379 - acc: 0.916 - ETA: 2s - loss: 0.2192 - acc: 0.929 - ETA: 2s - loss: 0.2216 - acc: 0.928 - ETA: 2s - loss: 0.2185 - acc: 0.932 - ETA: 2s - loss: 0.2164 - acc: 0.933 - ETA: 2s - loss: 0.2093 - acc: 0.937 - ETA: 2s - loss: 0.2079 - acc: 0.937 - ETA: 2s - loss: 0.2134 - acc: 0.934 - ETA: 2s - loss: 0.2064 - acc: 0.939 - ETA: 2s - loss: 0.2086 - acc: 0.939 - ETA: 2s - loss: 0.2065 - acc: 0.940 - ETA: 2s - loss: 0.2045 - acc: 0.942 - ETA: 2s - loss: 0.2061 - acc: 0.942 - ETA: 2s - loss: 0.2100 - acc: 0.941 - ETA: 1s - loss: 0.2075 - acc: 0.943 - ETA: 1s - loss: 0.2063 - acc: 0.943 - ETA: 1s - loss: 0.2074 - acc: 0.942 - ETA: 1s - loss: 0.2157 - acc: 0.939 - ETA: 1s - loss: 0.2154 - acc: 0.939 - ETA: 1s - loss: 0.2145 - acc: 0.938 - ETA: 1s - loss: 0.2170 - acc: 0.937 - ETA: 1s - loss: 0.2168 - acc: 0.936 - ETA: 1s - loss: 0.2156 - acc: 0.936 - ETA: 1s - loss: 0.2160 - acc: 0.934 - ETA: 1s - loss: 0.2148 - acc: 0.934 - ETA: 1s - loss: 0.2170 - acc: 0.933 - ETA: 1s - loss: 0.2202 - acc: 0.932 - ETA: 1s - loss: 0.2195 - acc: 0.933 - ETA: 1s - loss: 0.2200 - acc: 0.933 - ETA: 0s - loss: 0.2176 - acc: 0.934 - ETA: 0s - loss: 0.2199 - acc: 0.933 - ETA: 0s - loss: 0.2215 - acc: 0.933 - ETA: 0s - loss: 0.2227 - acc: 0.932 - ETA: 0s - loss: 0.2224 - acc: 0.932 - ETA: 0s - loss: 0.2242 - acc: 0.932 - ETA: 0s - loss: 0.2236 - acc: 0.932 - ETA: 0s - loss: 0.2233 - acc: 0.932 - ETA: 0s - loss: 0.2226 - acc: 0.933 - ETA: 0s - loss: 0.2234 - acc: 0.933 - ETA: 0s - loss: 0.2224 - acc: 0.933 - ETA: 0s - loss: 0.2220 - acc: 0.933 - ETA: 0s - loss: 0.2212 - acc: 0.934 - 3s 752us/step - loss: 0.2193 - acc: 0.9348 - val_loss: 0.3221 - val_acc: 0.8801\n",
      "Epoch 18/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1474 - acc: 0.953 - ETA: 2s - loss: 0.1397 - acc: 0.958 - ETA: 2s - loss: 0.1645 - acc: 0.949 - ETA: 2s - loss: 0.1551 - acc: 0.956 - ETA: 2s - loss: 0.1683 - acc: 0.945 - ETA: 2s - loss: 0.2234 - acc: 0.937 - ETA: 2s - loss: 0.2215 - acc: 0.935 - ETA: 2s - loss: 0.2276 - acc: 0.928 - ETA: 2s - loss: 0.2350 - acc: 0.924 - ETA: 1s - loss: 0.2277 - acc: 0.928 - ETA: 1s - loss: 0.2250 - acc: 0.931 - ETA: 1s - loss: 0.2209 - acc: 0.933 - ETA: 1s - loss: 0.2199 - acc: 0.933 - ETA: 1s - loss: 0.2273 - acc: 0.933 - ETA: 1s - loss: 0.2222 - acc: 0.936 - ETA: 1s - loss: 0.2268 - acc: 0.932 - ETA: 1s - loss: 0.2292 - acc: 0.931 - ETA: 1s - loss: 0.2313 - acc: 0.930 - ETA: 1s - loss: 0.2285 - acc: 0.931 - ETA: 1s - loss: 0.2275 - acc: 0.931 - ETA: 1s - loss: 0.2256 - acc: 0.931 - ETA: 1s - loss: 0.2244 - acc: 0.932 - ETA: 1s - loss: 0.2241 - acc: 0.931 - ETA: 1s - loss: 0.2221 - acc: 0.932 - ETA: 0s - loss: 0.2225 - acc: 0.932 - ETA: 0s - loss: 0.2224 - acc: 0.931 - ETA: 0s - loss: 0.2183 - acc: 0.933 - ETA: 0s - loss: 0.2233 - acc: 0.930 - ETA: 0s - loss: 0.2242 - acc: 0.930 - ETA: 0s - loss: 0.2245 - acc: 0.929 - ETA: 0s - loss: 0.2247 - acc: 0.929 - ETA: 0s - loss: 0.2255 - acc: 0.928 - ETA: 0s - loss: 0.2250 - acc: 0.929 - ETA: 0s - loss: 0.2244 - acc: 0.929 - ETA: 0s - loss: 0.2235 - acc: 0.929 - ETA: 0s - loss: 0.2221 - acc: 0.929 - ETA: 0s - loss: 0.2223 - acc: 0.929 - ETA: 0s - loss: 0.2228 - acc: 0.929 - ETA: 0s - loss: 0.2210 - acc: 0.930 - 3s 716us/step - loss: 0.2193 - acc: 0.9312 - val_loss: 0.2514 - val_acc: 0.9051\n",
      "Epoch 19/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2138 - acc: 0.921 - ETA: 2s - loss: 0.1750 - acc: 0.942 - ETA: 2s - loss: 0.2139 - acc: 0.915 - ETA: 2s - loss: 0.2029 - acc: 0.926 - ETA: 2s - loss: 0.2283 - acc: 0.918 - ETA: 2s - loss: 0.2169 - acc: 0.926 - ETA: 1s - loss: 0.2158 - acc: 0.925 - ETA: 1s - loss: 0.2118 - acc: 0.929 - ETA: 1s - loss: 0.2062 - acc: 0.932 - ETA: 1s - loss: 0.2008 - acc: 0.935 - ETA: 1s - loss: 0.1970 - acc: 0.938 - ETA: 1s - loss: 0.1955 - acc: 0.940 - ETA: 1s - loss: 0.2013 - acc: 0.939 - ETA: 1s - loss: 0.2089 - acc: 0.934 - ETA: 1s - loss: 0.2114 - acc: 0.933 - ETA: 1s - loss: 0.2123 - acc: 0.933 - ETA: 1s - loss: 0.2117 - acc: 0.934 - ETA: 1s - loss: 0.2129 - acc: 0.934 - ETA: 1s - loss: 0.2135 - acc: 0.934 - ETA: 0s - loss: 0.2146 - acc: 0.933 - ETA: 0s - loss: 0.2180 - acc: 0.930 - ETA: 0s - loss: 0.2172 - acc: 0.929 - ETA: 0s - loss: 0.2169 - acc: 0.930 - ETA: 0s - loss: 0.2174 - acc: 0.929 - ETA: 0s - loss: 0.2164 - acc: 0.930 - ETA: 0s - loss: 0.2142 - acc: 0.931 - ETA: 0s - loss: 0.2145 - acc: 0.930 - ETA: 0s - loss: 0.2168 - acc: 0.929 - ETA: 0s - loss: 0.2170 - acc: 0.929 - ETA: 0s - loss: 0.2154 - acc: 0.930 - ETA: 0s - loss: 0.2165 - acc: 0.930 - ETA: 0s - loss: 0.2154 - acc: 0.931 - 3s 642us/step - loss: 0.2150 - acc: 0.9314 - val_loss: 0.2582 - val_acc: 0.9327\n",
      "Epoch 20/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2188 - acc: 0.937 - ETA: 2s - loss: 0.1933 - acc: 0.942 - ETA: 2s - loss: 0.2140 - acc: 0.934 - ETA: 2s - loss: 0.2157 - acc: 0.933 - ETA: 2s - loss: 0.2279 - acc: 0.927 - ETA: 2s - loss: 0.2222 - acc: 0.929 - ETA: 1s - loss: 0.2214 - acc: 0.932 - ETA: 1s - loss: 0.2157 - acc: 0.936 - ETA: 1s - loss: 0.2155 - acc: 0.934 - ETA: 1s - loss: 0.2185 - acc: 0.932 - ETA: 1s - loss: 0.2140 - acc: 0.935 - ETA: 1s - loss: 0.2102 - acc: 0.936 - ETA: 1s - loss: 0.2085 - acc: 0.937 - ETA: 1s - loss: 0.2112 - acc: 0.936 - ETA: 1s - loss: 0.2114 - acc: 0.937 - ETA: 1s - loss: 0.2065 - acc: 0.939 - ETA: 1s - loss: 0.2102 - acc: 0.939 - ETA: 1s - loss: 0.2189 - acc: 0.934 - ETA: 1s - loss: 0.2172 - acc: 0.934 - ETA: 1s - loss: 0.2153 - acc: 0.936 - ETA: 0s - loss: 0.2138 - acc: 0.936 - ETA: 0s - loss: 0.2103 - acc: 0.938 - ETA: 0s - loss: 0.2084 - acc: 0.939 - ETA: 0s - loss: 0.2056 - acc: 0.940 - ETA: 0s - loss: 0.2069 - acc: 0.939 - ETA: 0s - loss: 0.2083 - acc: 0.937 - ETA: 0s - loss: 0.2079 - acc: 0.937 - ETA: 0s - loss: 0.2098 - acc: 0.937 - ETA: 0s - loss: 0.2103 - acc: 0.937 - ETA: 0s - loss: 0.2140 - acc: 0.936 - ETA: 0s - loss: 0.2131 - acc: 0.936 - ETA: 0s - loss: 0.2146 - acc: 0.935 - 3s 648us/step - loss: 0.2155 - acc: 0.9361 - val_loss: 0.3048 - val_acc: 0.8904\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2530 - acc: 0.937 - ETA: 2s - loss: 0.2920 - acc: 0.932 - ETA: 2s - loss: 0.2822 - acc: 0.931 - ETA: 2s - loss: 0.2538 - acc: 0.942 - ETA: 2s - loss: 0.2281 - acc: 0.946 - ETA: 2s - loss: 0.2256 - acc: 0.941 - ETA: 1s - loss: 0.2142 - acc: 0.943 - ETA: 1s - loss: 0.2056 - acc: 0.945 - ETA: 1s - loss: 0.1989 - acc: 0.949 - ETA: 1s - loss: 0.1966 - acc: 0.948 - ETA: 1s - loss: 0.2085 - acc: 0.944 - ETA: 1s - loss: 0.2056 - acc: 0.942 - ETA: 1s - loss: 0.2135 - acc: 0.940 - ETA: 1s - loss: 0.2146 - acc: 0.939 - ETA: 1s - loss: 0.2158 - acc: 0.938 - ETA: 1s - loss: 0.2165 - acc: 0.938 - ETA: 1s - loss: 0.2221 - acc: 0.935 - ETA: 1s - loss: 0.2216 - acc: 0.936 - ETA: 1s - loss: 0.2191 - acc: 0.938 - ETA: 0s - loss: 0.2187 - acc: 0.937 - ETA: 0s - loss: 0.2163 - acc: 0.937 - ETA: 0s - loss: 0.2115 - acc: 0.940 - ETA: 0s - loss: 0.2150 - acc: 0.938 - ETA: 0s - loss: 0.2121 - acc: 0.939 - ETA: 0s - loss: 0.2117 - acc: 0.940 - ETA: 0s - loss: 0.2125 - acc: 0.939 - ETA: 0s - loss: 0.2137 - acc: 0.939 - ETA: 0s - loss: 0.2117 - acc: 0.939 - ETA: 0s - loss: 0.2140 - acc: 0.936 - ETA: 0s - loss: 0.2111 - acc: 0.938 - ETA: 0s - loss: 0.2116 - acc: 0.937 - ETA: 0s - loss: 0.2092 - acc: 0.938 - 3s 640us/step - loss: 0.2089 - acc: 0.9385 - val_loss: 0.2511 - val_acc: 0.9167\n",
      "Epoch 22/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1745 - acc: 0.937 - ETA: 2s - loss: 0.1378 - acc: 0.974 - ETA: 2s - loss: 0.2003 - acc: 0.937 - ETA: 2s - loss: 0.2057 - acc: 0.933 - ETA: 2s - loss: 0.2163 - acc: 0.930 - ETA: 2s - loss: 0.2122 - acc: 0.931 - ETA: 1s - loss: 0.2118 - acc: 0.931 - ETA: 1s - loss: 0.2179 - acc: 0.930 - ETA: 1s - loss: 0.2121 - acc: 0.935 - ETA: 1s - loss: 0.2072 - acc: 0.937 - ETA: 1s - loss: 0.2000 - acc: 0.941 - ETA: 1s - loss: 0.1965 - acc: 0.942 - ETA: 1s - loss: 0.1920 - acc: 0.944 - ETA: 1s - loss: 0.1987 - acc: 0.939 - ETA: 1s - loss: 0.1980 - acc: 0.940 - ETA: 1s - loss: 0.1976 - acc: 0.940 - ETA: 1s - loss: 0.1963 - acc: 0.941 - ETA: 1s - loss: 0.1967 - acc: 0.939 - ETA: 1s - loss: 0.1962 - acc: 0.940 - ETA: 1s - loss: 0.1965 - acc: 0.939 - ETA: 1s - loss: 0.2051 - acc: 0.935 - ETA: 1s - loss: 0.2047 - acc: 0.934 - ETA: 0s - loss: 0.2035 - acc: 0.935 - ETA: 0s - loss: 0.2059 - acc: 0.935 - ETA: 0s - loss: 0.2050 - acc: 0.934 - ETA: 0s - loss: 0.2060 - acc: 0.935 - ETA: 0s - loss: 0.2055 - acc: 0.935 - ETA: 0s - loss: 0.2046 - acc: 0.936 - ETA: 0s - loss: 0.2035 - acc: 0.936 - ETA: 0s - loss: 0.2025 - acc: 0.937 - ETA: 0s - loss: 0.2045 - acc: 0.936 - ETA: 0s - loss: 0.2045 - acc: 0.936 - ETA: 0s - loss: 0.2021 - acc: 0.938 - ETA: 0s - loss: 0.2037 - acc: 0.937 - ETA: 0s - loss: 0.2033 - acc: 0.937 - 3s 688us/step - loss: 0.2037 - acc: 0.9373 - val_loss: 0.2708 - val_acc: 0.8994\n",
      "Epoch 23/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2134 - acc: 0.875 - ETA: 2s - loss: 0.2121 - acc: 0.916 - ETA: 2s - loss: 0.1958 - acc: 0.931 - ETA: 2s - loss: 0.1956 - acc: 0.930 - ETA: 2s - loss: 0.1821 - acc: 0.939 - ETA: 2s - loss: 0.1918 - acc: 0.936 - ETA: 1s - loss: 0.1899 - acc: 0.935 - ETA: 1s - loss: 0.1938 - acc: 0.935 - ETA: 1s - loss: 0.1957 - acc: 0.934 - ETA: 1s - loss: 0.2029 - acc: 0.930 - ETA: 1s - loss: 0.2073 - acc: 0.930 - ETA: 1s - loss: 0.1987 - acc: 0.934 - ETA: 1s - loss: 0.1965 - acc: 0.935 - ETA: 1s - loss: 0.1904 - acc: 0.938 - ETA: 1s - loss: 0.1958 - acc: 0.933 - ETA: 1s - loss: 0.1995 - acc: 0.932 - ETA: 1s - loss: 0.1986 - acc: 0.932 - ETA: 1s - loss: 0.1994 - acc: 0.932 - ETA: 1s - loss: 0.1988 - acc: 0.933 - ETA: 0s - loss: 0.1986 - acc: 0.933 - ETA: 0s - loss: 0.1998 - acc: 0.933 - ETA: 0s - loss: 0.1984 - acc: 0.933 - ETA: 0s - loss: 0.2059 - acc: 0.930 - ETA: 0s - loss: 0.2057 - acc: 0.931 - ETA: 0s - loss: 0.2087 - acc: 0.931 - ETA: 0s - loss: 0.2115 - acc: 0.930 - ETA: 0s - loss: 0.2082 - acc: 0.930 - ETA: 0s - loss: 0.2054 - acc: 0.932 - ETA: 0s - loss: 0.2068 - acc: 0.933 - ETA: 0s - loss: 0.2099 - acc: 0.932 - ETA: 0s - loss: 0.2113 - acc: 0.932 - ETA: 0s - loss: 0.2088 - acc: 0.934 - 3s 649us/step - loss: 0.2088 - acc: 0.9341 - val_loss: 0.2337 - val_acc: 0.9327\n",
      "Epoch 24/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1851 - acc: 0.968 - ETA: 2s - loss: 0.1743 - acc: 0.963 - ETA: 2s - loss: 0.1882 - acc: 0.956 - ETA: 2s - loss: 0.2071 - acc: 0.948 - ETA: 2s - loss: 0.2064 - acc: 0.942 - ETA: 2s - loss: 0.2038 - acc: 0.946 - ETA: 1s - loss: 0.1955 - acc: 0.945 - ETA: 2s - loss: 0.2073 - acc: 0.940 - ETA: 2s - loss: 0.2204 - acc: 0.937 - ETA: 1s - loss: 0.2102 - acc: 0.940 - ETA: 1s - loss: 0.2056 - acc: 0.939 - ETA: 1s - loss: 0.2156 - acc: 0.934 - ETA: 1s - loss: 0.2141 - acc: 0.934 - ETA: 1s - loss: 0.2098 - acc: 0.937 - ETA: 1s - loss: 0.2094 - acc: 0.936 - ETA: 1s - loss: 0.2109 - acc: 0.934 - ETA: 1s - loss: 0.2107 - acc: 0.934 - ETA: 1s - loss: 0.2128 - acc: 0.933 - ETA: 1s - loss: 0.2137 - acc: 0.932 - ETA: 1s - loss: 0.2104 - acc: 0.933 - ETA: 0s - loss: 0.2063 - acc: 0.935 - ETA: 0s - loss: 0.2039 - acc: 0.936 - ETA: 0s - loss: 0.2034 - acc: 0.936 - ETA: 0s - loss: 0.2078 - acc: 0.933 - ETA: 0s - loss: 0.2152 - acc: 0.930 - ETA: 0s - loss: 0.2126 - acc: 0.932 - ETA: 0s - loss: 0.2118 - acc: 0.932 - ETA: 0s - loss: 0.2114 - acc: 0.932 - ETA: 0s - loss: 0.2109 - acc: 0.932 - ETA: 0s - loss: 0.2103 - acc: 0.932 - ETA: 0s - loss: 0.2098 - acc: 0.933 - ETA: 0s - loss: 0.2084 - acc: 0.933 - ETA: 0s - loss: 0.2085 - acc: 0.932 - ETA: 0s - loss: 0.2080 - acc: 0.932 - ETA: 0s - loss: 0.2100 - acc: 0.932 - 3s 679us/step - loss: 0.2099 - acc: 0.9321 - val_loss: 0.2332 - val_acc: 0.9385\n",
      "Epoch 25/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1288 - acc: 0.968 - ETA: 2s - loss: 0.1733 - acc: 0.953 - ETA: 2s - loss: 0.1598 - acc: 0.959 - ETA: 2s - loss: 0.1615 - acc: 0.955 - ETA: 2s - loss: 0.1566 - acc: 0.960 - ETA: 2s - loss: 0.1584 - acc: 0.958 - ETA: 1s - loss: 0.1805 - acc: 0.944 - ETA: 1s - loss: 0.1821 - acc: 0.944 - ETA: 1s - loss: 0.1769 - acc: 0.946 - ETA: 1s - loss: 0.1774 - acc: 0.945 - ETA: 1s - loss: 0.1807 - acc: 0.944 - ETA: 1s - loss: 0.1832 - acc: 0.945 - ETA: 1s - loss: 0.1805 - acc: 0.946 - ETA: 1s - loss: 0.1838 - acc: 0.945 - ETA: 1s - loss: 0.1808 - acc: 0.945 - ETA: 1s - loss: 0.1816 - acc: 0.945 - ETA: 1s - loss: 0.1875 - acc: 0.941 - ETA: 1s - loss: 0.1907 - acc: 0.939 - ETA: 1s - loss: 0.1968 - acc: 0.936 - ETA: 0s - loss: 0.1986 - acc: 0.935 - ETA: 0s - loss: 0.1972 - acc: 0.936 - ETA: 0s - loss: 0.1979 - acc: 0.936 - ETA: 0s - loss: 0.1962 - acc: 0.937 - ETA: 0s - loss: 0.2000 - acc: 0.935 - ETA: 0s - loss: 0.2012 - acc: 0.935 - ETA: 0s - loss: 0.2044 - acc: 0.936 - ETA: 0s - loss: 0.2030 - acc: 0.936 - ETA: 0s - loss: 0.2026 - acc: 0.936 - ETA: 0s - loss: 0.2037 - acc: 0.937 - ETA: 0s - loss: 0.2021 - acc: 0.938 - ETA: 0s - loss: 0.2030 - acc: 0.937 - ETA: 0s - loss: 0.2031 - acc: 0.937 - 3s 635us/step - loss: 0.2053 - acc: 0.9371 - val_loss: 0.3010 - val_acc: 0.8923\n",
      "Epoch 26/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1899 - acc: 0.953 - ETA: 2s - loss: 0.1792 - acc: 0.953 - ETA: 2s - loss: 0.1675 - acc: 0.946 - ETA: 2s - loss: 0.1637 - acc: 0.948 - ETA: 2s - loss: 0.1679 - acc: 0.949 - ETA: 2s - loss: 0.1662 - acc: 0.950 - ETA: 1s - loss: 0.1698 - acc: 0.949 - ETA: 1s - loss: 0.1746 - acc: 0.947 - ETA: 1s - loss: 0.1700 - acc: 0.950 - ETA: 1s - loss: 0.1705 - acc: 0.949 - ETA: 1s - loss: 0.1874 - acc: 0.944 - ETA: 1s - loss: 0.1899 - acc: 0.944 - ETA: 1s - loss: 0.1818 - acc: 0.948 - ETA: 1s - loss: 0.1815 - acc: 0.949 - ETA: 1s - loss: 0.1828 - acc: 0.947 - ETA: 1s - loss: 0.1833 - acc: 0.947 - ETA: 1s - loss: 0.1849 - acc: 0.947 - ETA: 1s - loss: 0.1878 - acc: 0.947 - ETA: 1s - loss: 0.1895 - acc: 0.947 - ETA: 0s - loss: 0.1938 - acc: 0.945 - ETA: 0s - loss: 0.2023 - acc: 0.941 - ETA: 0s - loss: 0.2007 - acc: 0.942 - ETA: 0s - loss: 0.2020 - acc: 0.941 - ETA: 0s - loss: 0.2013 - acc: 0.941 - ETA: 0s - loss: 0.2000 - acc: 0.942 - ETA: 0s - loss: 0.2028 - acc: 0.940 - ETA: 0s - loss: 0.2042 - acc: 0.939 - ETA: 0s - loss: 0.2042 - acc: 0.939 - ETA: 0s - loss: 0.2029 - acc: 0.940 - ETA: 0s - loss: 0.2013 - acc: 0.940 - ETA: 0s - loss: 0.2006 - acc: 0.941 - ETA: 0s - loss: 0.1981 - acc: 0.942 - 3s 652us/step - loss: 0.1989 - acc: 0.9420 - val_loss: 0.2470 - val_acc: 0.9340\n",
      "Epoch 27/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2393 - acc: 0.937 - ETA: 2s - loss: 0.2445 - acc: 0.911 - ETA: 2s - loss: 0.2077 - acc: 0.931 - ETA: 2s - loss: 0.2028 - acc: 0.928 - ETA: 2s - loss: 0.2021 - acc: 0.932 - ETA: 2s - loss: 0.2051 - acc: 0.933 - ETA: 1s - loss: 0.1952 - acc: 0.937 - ETA: 1s - loss: 0.1902 - acc: 0.937 - ETA: 1s - loss: 0.2003 - acc: 0.934 - ETA: 1s - loss: 0.1948 - acc: 0.937 - ETA: 1s - loss: 0.1952 - acc: 0.937 - ETA: 1s - loss: 0.2097 - acc: 0.933 - ETA: 1s - loss: 0.2061 - acc: 0.936 - ETA: 1s - loss: 0.2069 - acc: 0.936 - ETA: 1s - loss: 0.2040 - acc: 0.937 - ETA: 1s - loss: 0.1999 - acc: 0.939 - ETA: 1s - loss: 0.1969 - acc: 0.940 - ETA: 1s - loss: 0.2021 - acc: 0.937 - ETA: 1s - loss: 0.2009 - acc: 0.938 - ETA: 0s - loss: 0.2002 - acc: 0.938 - ETA: 0s - loss: 0.2019 - acc: 0.937 - ETA: 0s - loss: 0.2028 - acc: 0.938 - ETA: 0s - loss: 0.2035 - acc: 0.936 - ETA: 0s - loss: 0.2030 - acc: 0.935 - ETA: 0s - loss: 0.2009 - acc: 0.935 - ETA: 0s - loss: 0.2021 - acc: 0.934 - ETA: 0s - loss: 0.2010 - acc: 0.934 - ETA: 0s - loss: 0.2007 - acc: 0.935 - ETA: 0s - loss: 0.2000 - acc: 0.936 - ETA: 0s - loss: 0.2001 - acc: 0.936 - ETA: 0s - loss: 0.2005 - acc: 0.936 - ETA: 0s - loss: 0.1997 - acc: 0.936 - 3s 638us/step - loss: 0.1990 - acc: 0.9373 - val_loss: 0.2212 - val_acc: 0.9429\n",
      "Epoch 28/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1547 - acc: 0.937 - ETA: 2s - loss: 0.1652 - acc: 0.942 - ETA: 2s - loss: 0.1842 - acc: 0.928 - ETA: 2s - loss: 0.1741 - acc: 0.933 - ETA: 2s - loss: 0.1881 - acc: 0.927 - ETA: 2s - loss: 0.2043 - acc: 0.927 - ETA: 1s - loss: 0.1922 - acc: 0.935 - ETA: 1s - loss: 0.1907 - acc: 0.935 - ETA: 1s - loss: 0.1865 - acc: 0.937 - ETA: 1s - loss: 0.1848 - acc: 0.937 - ETA: 1s - loss: 0.1796 - acc: 0.939 - ETA: 1s - loss: 0.1790 - acc: 0.940 - ETA: 1s - loss: 0.1755 - acc: 0.943 - ETA: 1s - loss: 0.1742 - acc: 0.942 - ETA: 1s - loss: 0.1723 - acc: 0.942 - ETA: 1s - loss: 0.1791 - acc: 0.942 - ETA: 1s - loss: 0.1864 - acc: 0.939 - ETA: 1s - loss: 0.1879 - acc: 0.940 - ETA: 1s - loss: 0.1890 - acc: 0.939 - ETA: 0s - loss: 0.1899 - acc: 0.939 - ETA: 0s - loss: 0.1905 - acc: 0.939 - ETA: 0s - loss: 0.1895 - acc: 0.940 - ETA: 0s - loss: 0.1902 - acc: 0.941 - ETA: 0s - loss: 0.1894 - acc: 0.940 - ETA: 0s - loss: 0.1874 - acc: 0.942 - ETA: 0s - loss: 0.1897 - acc: 0.941 - ETA: 0s - loss: 0.1917 - acc: 0.940 - ETA: 0s - loss: 0.1932 - acc: 0.940 - ETA: 0s - loss: 0.1925 - acc: 0.940 - ETA: 0s - loss: 0.1927 - acc: 0.940 - ETA: 0s - loss: 0.1941 - acc: 0.939 - ETA: 0s - loss: 0.1939 - acc: 0.940 - 3s 636us/step - loss: 0.1934 - acc: 0.9403 - val_loss: 0.2150 - val_acc: 0.9455\n",
      "Epoch 29/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1992 - acc: 0.953 - ETA: 2s - loss: 0.1871 - acc: 0.947 - ETA: 2s - loss: 0.2160 - acc: 0.928 - ETA: 2s - loss: 0.2208 - acc: 0.924 - ETA: 2s - loss: 0.2258 - acc: 0.925 - ETA: 2s - loss: 0.2193 - acc: 0.929 - ETA: 1s - loss: 0.2115 - acc: 0.931 - ETA: 1s - loss: 0.2088 - acc: 0.933 - ETA: 1s - loss: 0.2090 - acc: 0.934 - ETA: 1s - loss: 0.2057 - acc: 0.935 - ETA: 1s - loss: 0.2020 - acc: 0.937 - ETA: 1s - loss: 0.1987 - acc: 0.937 - ETA: 1s - loss: 0.1978 - acc: 0.938 - ETA: 1s - loss: 0.2017 - acc: 0.935 - ETA: 1s - loss: 0.1976 - acc: 0.937 - ETA: 1s - loss: 0.1964 - acc: 0.938 - ETA: 1s - loss: 0.1986 - acc: 0.937 - ETA: 1s - loss: 0.1985 - acc: 0.936 - ETA: 1s - loss: 0.1985 - acc: 0.937 - ETA: 0s - loss: 0.2020 - acc: 0.935 - ETA: 0s - loss: 0.1988 - acc: 0.937 - ETA: 0s - loss: 0.1993 - acc: 0.937 - ETA: 0s - loss: 0.1983 - acc: 0.936 - ETA: 0s - loss: 0.2047 - acc: 0.936 - ETA: 0s - loss: 0.2090 - acc: 0.935 - ETA: 0s - loss: 0.2121 - acc: 0.935 - ETA: 0s - loss: 0.2108 - acc: 0.936 - ETA: 0s - loss: 0.2090 - acc: 0.936 - ETA: 0s - loss: 0.2067 - acc: 0.938 - ETA: 0s - loss: 0.2055 - acc: 0.938 - ETA: 0s - loss: 0.2043 - acc: 0.938 - ETA: 0s - loss: 0.2063 - acc: 0.938 - 3s 640us/step - loss: 0.2057 - acc: 0.9388 - val_loss: 0.2189 - val_acc: 0.9449\n",
      "Epoch 30/30\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2871 - acc: 0.875 - ETA: 2s - loss: 0.2380 - acc: 0.911 - ETA: 2s - loss: 0.2129 - acc: 0.925 - ETA: 2s - loss: 0.2108 - acc: 0.928 - ETA: 2s - loss: 0.2033 - acc: 0.932 - ETA: 1s - loss: 0.1962 - acc: 0.936 - ETA: 1s - loss: 0.1974 - acc: 0.933 - ETA: 1s - loss: 0.1970 - acc: 0.936 - ETA: 1s - loss: 0.1993 - acc: 0.933 - ETA: 1s - loss: 0.1966 - acc: 0.933 - ETA: 1s - loss: 0.1910 - acc: 0.935 - ETA: 1s - loss: 0.1958 - acc: 0.933 - ETA: 1s - loss: 0.1915 - acc: 0.936 - ETA: 1s - loss: 0.1886 - acc: 0.938 - ETA: 1s - loss: 0.1895 - acc: 0.938 - ETA: 1s - loss: 0.1920 - acc: 0.937 - ETA: 1s - loss: 0.1927 - acc: 0.936 - ETA: 1s - loss: 0.1904 - acc: 0.937 - ETA: 1s - loss: 0.1935 - acc: 0.936 - ETA: 0s - loss: 0.1923 - acc: 0.937 - ETA: 0s - loss: 0.1974 - acc: 0.935 - ETA: 0s - loss: 0.1989 - acc: 0.935 - ETA: 0s - loss: 0.1971 - acc: 0.935 - ETA: 0s - loss: 0.1981 - acc: 0.934 - ETA: 0s - loss: 0.1949 - acc: 0.935 - ETA: 0s - loss: 0.1937 - acc: 0.936 - ETA: 0s - loss: 0.1933 - acc: 0.937 - ETA: 0s - loss: 0.1929 - acc: 0.938 - ETA: 0s - loss: 0.1951 - acc: 0.936 - ETA: 0s - loss: 0.1941 - acc: 0.936 - ETA: 0s - loss: 0.1925 - acc: 0.937 - ETA: 0s - loss: 0.1926 - acc: 0.937 - ETA: 0s - loss: 0.1918 - acc: 0.937 - ETA: 0s - loss: 0.1916 - acc: 0.937 - 3s 668us/step - loss: 0.1942 - acc: 0.9361 - val_loss: 0.2254 - val_acc: 0.9346\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "##model from hyperas\n",
    "def keras_fmin_fnct(space,verbose=1):   \n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session(graph=tf.get_default_graph())\n",
    "    K.set_session(sess)\n",
    "    # Initiliazing the sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=space['filters'], kernel_size=space['kernel_size'],activation='relu',\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_regularizer=l2(space['l2']),input_shape=(128,9)))\n",
    "    model.add(Conv1D(filters=space['filters_1'], kernel_size=space['kernel_size_1'], \n",
    "                activation='relu',kernel_regularizer=l2(space['l2_1']),kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(space['Dropout']))\n",
    "    model.add(MaxPooling1D(pool_size=space['pool_size']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(space['Dense'], activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    adam = keras.optimizers.Adam(lr=space['lr'])\n",
    "    rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'])\n",
    "    choiceval = space['choiceval']\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    else:\n",
    "        optim = rmsprop\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "    result = model.fit(X_train_s, Y_train_s,\n",
    "                    batch_size=space['Dense_1'],\n",
    "                    nb_epoch=space['nb_epoch'],\n",
    "                    verbose=verbose,\n",
    "                    validation_data=(X_val_s, Y_val_s))\n",
    "    #K.clear_session()\n",
    "    return model,result\n",
    "\n",
    "best_model,result = keras_fmin_fnct(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 0.9478731251536759 test_accuracy 0.9211538461538461\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = best_model.evaluate(X_val_s,Y_val_s,verbose=0)\n",
    "_,acc_train = best_model.evaluate(X_train_s,Y_train_s,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i can observe that 21st model is also giving good scores in runtime so will try once wit that params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense': 64,\n",
       " 'Dense_1': 64,\n",
       " 'Dropout': 0.4741451827698069,\n",
       " 'choiceval': 'adam',\n",
       " 'filters': 32,\n",
       " 'filters_1': 16,\n",
       " 'kernel_size': 3,\n",
       " 'kernel_size_1': 3,\n",
       " 'l2': 0.08502848012041342,\n",
       " 'l2_1': 0.6001993227170167,\n",
       " 'lr': 0.0023858522033030054,\n",
       " 'lr_1': 0.002242977232104179,\n",
       " 'nb_epoch': 30,\n",
       " 'pool_size': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_param = total_trials['M21']\n",
    "runtime_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 124, 16)           1552      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 124, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 41, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                42048     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 44,691\n",
      "Trainable params: 44,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/150\n",
      "4067/4067 [==============================] - ETA: 46s - loss: 26.5661 - acc: 0.265 - ETA: 16s - loss: 25.5133 - acc: 0.489 - ETA: 10s - loss: 25.0042 - acc: 0.546 - ETA: 7s - loss: 24.3814 - acc: 0.584 - ETA: 6s - loss: 23.7875 - acc: 0.61 - ETA: 5s - loss: 23.1779 - acc: 0.65 - ETA: 4s - loss: 22.6571 - acc: 0.66 - ETA: 4s - loss: 22.1302 - acc: 0.68 - ETA: 3s - loss: 21.5956 - acc: 0.70 - ETA: 3s - loss: 21.0872 - acc: 0.71 - ETA: 3s - loss: 20.5983 - acc: 0.73 - ETA: 2s - loss: 20.1313 - acc: 0.73 - ETA: 2s - loss: 19.6791 - acc: 0.73 - ETA: 2s - loss: 19.4554 - acc: 0.74 - ETA: 2s - loss: 19.0148 - acc: 0.74 - ETA: 2s - loss: 18.5833 - acc: 0.75 - ETA: 1s - loss: 18.1652 - acc: 0.76 - ETA: 1s - loss: 17.7614 - acc: 0.77 - ETA: 1s - loss: 17.3776 - acc: 0.77 - ETA: 1s - loss: 16.9999 - acc: 0.77 - ETA: 1s - loss: 16.6365 - acc: 0.78 - ETA: 1s - loss: 16.2832 - acc: 0.78 - ETA: 1s - loss: 15.9403 - acc: 0.78 - ETA: 0s - loss: 15.6045 - acc: 0.79 - ETA: 0s - loss: 15.2838 - acc: 0.79 - ETA: 0s - loss: 14.9682 - acc: 0.80 - ETA: 0s - loss: 14.6642 - acc: 0.80 - ETA: 0s - loss: 14.3695 - acc: 0.80 - ETA: 0s - loss: 14.0849 - acc: 0.81 - ETA: 0s - loss: 13.8077 - acc: 0.81 - ETA: 0s - loss: 13.5383 - acc: 0.81 - ETA: 0s - loss: 13.2772 - acc: 0.82 - 3s 858us/step - loss: 13.0824 - acc: 0.8222 - val_loss: 5.1062 - val_acc: 0.9000\n",
      "Epoch 2/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 4.9540 - acc: 0.921 - ETA: 2s - loss: 4.8177 - acc: 0.953 - ETA: 2s - loss: 4.7299 - acc: 0.931 - ETA: 2s - loss: 4.6190 - acc: 0.924 - ETA: 2s - loss: 4.5373 - acc: 0.906 - ETA: 2s - loss: 4.4483 - acc: 0.904 - ETA: 2s - loss: 4.3969 - acc: 0.906 - ETA: 2s - loss: 4.3076 - acc: 0.906 - ETA: 1s - loss: 4.2053 - acc: 0.909 - ETA: 2s - loss: 4.1134 - acc: 0.908 - ETA: 2s - loss: 4.0682 - acc: 0.909 - ETA: 1s - loss: 3.9850 - acc: 0.908 - ETA: 1s - loss: 3.9085 - acc: 0.906 - ETA: 1s - loss: 3.8247 - acc: 0.908 - ETA: 1s - loss: 3.7454 - acc: 0.910 - ETA: 1s - loss: 3.6658 - acc: 0.912 - ETA: 1s - loss: 3.5942 - acc: 0.911 - ETA: 1s - loss: 3.5254 - acc: 0.910 - ETA: 1s - loss: 3.4589 - acc: 0.911 - ETA: 1s - loss: 3.3914 - acc: 0.913 - ETA: 1s - loss: 3.3283 - acc: 0.914 - ETA: 0s - loss: 3.2653 - acc: 0.915 - ETA: 0s - loss: 3.2344 - acc: 0.915 - ETA: 0s - loss: 3.1759 - acc: 0.914 - ETA: 0s - loss: 3.1220 - acc: 0.912 - ETA: 0s - loss: 3.0686 - acc: 0.912 - ETA: 0s - loss: 3.0160 - acc: 0.912 - ETA: 0s - loss: 2.9647 - acc: 0.912 - ETA: 0s - loss: 2.9157 - acc: 0.912 - ETA: 0s - loss: 2.8673 - acc: 0.913 - ETA: 0s - loss: 2.8233 - acc: 0.913 - ETA: 0s - loss: 2.7782 - acc: 0.912 - ETA: 0s - loss: 2.7373 - acc: 0.911 - 3s 672us/step - loss: 2.7043 - acc: 0.9110 - val_loss: 1.4675 - val_acc: 0.8782\n",
      "Epoch 3/150\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 1.3037 - acc: 0.953 - ETA: 2s - loss: 1.3371 - acc: 0.921 - ETA: 2s - loss: 1.2655 - acc: 0.946 - ETA: 2s - loss: 1.2638 - acc: 0.935 - ETA: 2s - loss: 1.2389 - acc: 0.934 - ETA: 2s - loss: 1.2307 - acc: 0.926 - ETA: 2s - loss: 1.2053 - acc: 0.931 - ETA: 1s - loss: 1.1925 - acc: 0.927 - ETA: 1s - loss: 1.1683 - acc: 0.927 - ETA: 1s - loss: 1.1563 - acc: 0.924 - ETA: 1s - loss: 1.1439 - acc: 0.921 - ETA: 1s - loss: 1.1404 - acc: 0.914 - ETA: 1s - loss: 1.1219 - acc: 0.915 - ETA: 1s - loss: 1.1092 - acc: 0.914 - ETA: 1s - loss: 1.0913 - acc: 0.916 - ETA: 1s - loss: 1.0773 - acc: 0.916 - ETA: 1s - loss: 1.0653 - acc: 0.917 - ETA: 1s - loss: 1.0534 - acc: 0.915 - ETA: 1s - loss: 1.0424 - acc: 0.915 - ETA: 0s - loss: 1.0303 - acc: 0.915 - ETA: 0s - loss: 1.0212 - acc: 0.916 - ETA: 0s - loss: 1.0109 - acc: 0.915 - ETA: 0s - loss: 0.9981 - acc: 0.916 - ETA: 0s - loss: 0.9867 - acc: 0.915 - ETA: 0s - loss: 0.9817 - acc: 0.915 - ETA: 0s - loss: 0.9697 - acc: 0.915 - ETA: 0s - loss: 0.9603 - acc: 0.914 - ETA: 0s - loss: 0.9513 - acc: 0.913 - ETA: 0s - loss: 0.9401 - acc: 0.913 - ETA: 0s - loss: 0.9347 - acc: 0.913 - ETA: 0s - loss: 0.9249 - acc: 0.913 - ETA: 0s - loss: 0.9170 - acc: 0.913 - ETA: 0s - loss: 0.9077 - acc: 0.913 - 3s 648us/step - loss: 0.9059 - acc: 0.9130 - val_loss: 0.7278 - val_acc: 0.8718\n",
      "Epoch 4/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.6001 - acc: 0.843 - ETA: 2s - loss: 0.5859 - acc: 0.885 - ETA: 2s - loss: 0.5791 - acc: 0.896 - ETA: 2s - loss: 0.5734 - acc: 0.897 - ETA: 2s - loss: 0.5691 - acc: 0.899 - ETA: 2s - loss: 0.5582 - acc: 0.907 - ETA: 1s - loss: 0.5526 - acc: 0.906 - ETA: 2s - loss: 0.5543 - acc: 0.902 - ETA: 1s - loss: 0.5396 - acc: 0.910 - ETA: 1s - loss: 0.5443 - acc: 0.907 - ETA: 1s - loss: 0.5426 - acc: 0.904 - ETA: 1s - loss: 0.5404 - acc: 0.904 - ETA: 1s - loss: 0.5392 - acc: 0.905 - ETA: 1s - loss: 0.5339 - acc: 0.907 - ETA: 1s - loss: 0.5286 - acc: 0.908 - ETA: 1s - loss: 0.5204 - acc: 0.911 - ETA: 1s - loss: 0.5159 - acc: 0.913 - ETA: 1s - loss: 0.5087 - acc: 0.914 - ETA: 1s - loss: 0.5036 - acc: 0.915 - ETA: 1s - loss: 0.4996 - acc: 0.914 - ETA: 1s - loss: 0.4959 - acc: 0.914 - ETA: 0s - loss: 0.4923 - acc: 0.913 - ETA: 0s - loss: 0.4913 - acc: 0.912 - ETA: 0s - loss: 0.4927 - acc: 0.911 - ETA: 0s - loss: 0.4913 - acc: 0.912 - ETA: 0s - loss: 0.4905 - acc: 0.910 - ETA: 0s - loss: 0.4894 - acc: 0.908 - ETA: 0s - loss: 0.4916 - acc: 0.905 - ETA: 0s - loss: 0.4928 - acc: 0.905 - ETA: 0s - loss: 0.4937 - acc: 0.903 - ETA: 0s - loss: 0.4957 - acc: 0.903 - ETA: 0s - loss: 0.4946 - acc: 0.903 - ETA: 0s - loss: 0.4908 - acc: 0.904 - 3s 632us/step - loss: 0.4895 - acc: 0.9041 - val_loss: 0.5262 - val_acc: 0.8641\n",
      "Epoch 5/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3577 - acc: 0.937 - ETA: 2s - loss: 0.4480 - acc: 0.890 - ETA: 2s - loss: 0.4296 - acc: 0.900 - ETA: 2s - loss: 0.4130 - acc: 0.899 - ETA: 2s - loss: 0.3936 - acc: 0.908 - ETA: 1s - loss: 0.3864 - acc: 0.911 - ETA: 1s - loss: 0.3890 - acc: 0.911 - ETA: 1s - loss: 0.3819 - acc: 0.910 - ETA: 1s - loss: 0.3784 - acc: 0.912 - ETA: 1s - loss: 0.3859 - acc: 0.907 - ETA: 1s - loss: 0.3859 - acc: 0.908 - ETA: 1s - loss: 0.3913 - acc: 0.906 - ETA: 1s - loss: 0.3845 - acc: 0.909 - ETA: 1s - loss: 0.3891 - acc: 0.907 - ETA: 1s - loss: 0.3909 - acc: 0.904 - ETA: 1s - loss: 0.3864 - acc: 0.906 - ETA: 1s - loss: 0.3916 - acc: 0.902 - ETA: 1s - loss: 0.3887 - acc: 0.903 - ETA: 0s - loss: 0.3818 - acc: 0.905 - ETA: 0s - loss: 0.3877 - acc: 0.901 - ETA: 0s - loss: 0.3843 - acc: 0.902 - ETA: 0s - loss: 0.3830 - acc: 0.902 - ETA: 0s - loss: 0.3786 - acc: 0.904 - ETA: 0s - loss: 0.3789 - acc: 0.904 - ETA: 0s - loss: 0.3769 - acc: 0.904 - ETA: 0s - loss: 0.3751 - acc: 0.904 - ETA: 0s - loss: 0.3737 - acc: 0.903 - ETA: 0s - loss: 0.3707 - acc: 0.903 - ETA: 0s - loss: 0.3696 - acc: 0.903 - ETA: 0s - loss: 0.3696 - acc: 0.903 - ETA: 0s - loss: 0.3686 - acc: 0.904 - ETA: 0s - loss: 0.3663 - acc: 0.904 - 2s 612us/step - loss: 0.3644 - acc: 0.9061 - val_loss: 0.4310 - val_acc: 0.8679\n",
      "Epoch 6/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3648 - acc: 0.843 - ETA: 2s - loss: 0.3230 - acc: 0.895 - ETA: 2s - loss: 0.3043 - acc: 0.909 - ETA: 2s - loss: 0.2957 - acc: 0.915 - ETA: 2s - loss: 0.3000 - acc: 0.912 - ETA: 2s - loss: 0.2967 - acc: 0.913 - ETA: 2s - loss: 0.2923 - acc: 0.917 - ETA: 2s - loss: 0.2873 - acc: 0.919 - ETA: 2s - loss: 0.2920 - acc: 0.912 - ETA: 1s - loss: 0.2895 - acc: 0.915 - ETA: 1s - loss: 0.2907 - acc: 0.914 - ETA: 1s - loss: 0.2903 - acc: 0.916 - ETA: 1s - loss: 0.2919 - acc: 0.915 - ETA: 1s - loss: 0.2944 - acc: 0.914 - ETA: 1s - loss: 0.2939 - acc: 0.914 - ETA: 1s - loss: 0.2935 - acc: 0.914 - ETA: 1s - loss: 0.2892 - acc: 0.915 - ETA: 1s - loss: 0.2865 - acc: 0.916 - ETA: 1s - loss: 0.2817 - acc: 0.918 - ETA: 1s - loss: 0.2823 - acc: 0.917 - ETA: 0s - loss: 0.2847 - acc: 0.914 - ETA: 0s - loss: 0.2835 - acc: 0.915 - ETA: 0s - loss: 0.2816 - acc: 0.915 - ETA: 0s - loss: 0.2791 - acc: 0.918 - ETA: 0s - loss: 0.2794 - acc: 0.918 - ETA: 0s - loss: 0.2779 - acc: 0.919 - ETA: 0s - loss: 0.2774 - acc: 0.918 - ETA: 0s - loss: 0.2779 - acc: 0.919 - ETA: 0s - loss: 0.2778 - acc: 0.919 - ETA: 0s - loss: 0.2780 - acc: 0.919 - ETA: 0s - loss: 0.2773 - acc: 0.919 - ETA: 0s - loss: 0.2794 - acc: 0.918 - ETA: 0s - loss: 0.2787 - acc: 0.918 - ETA: 0s - loss: 0.2768 - acc: 0.919 - ETA: 0s - loss: 0.2783 - acc: 0.918 - ETA: 0s - loss: 0.2772 - acc: 0.919 - 3s 676us/step - loss: 0.2763 - acc: 0.9194 - val_loss: 0.3869 - val_acc: 0.8705\n",
      "Epoch 7/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2495 - acc: 0.921 - ETA: 2s - loss: 0.2642 - acc: 0.911 - ETA: 2s - loss: 0.2862 - acc: 0.906 - ETA: 2s - loss: 0.2652 - acc: 0.919 - ETA: 2s - loss: 0.2559 - acc: 0.923 - ETA: 2s - loss: 0.2566 - acc: 0.923 - ETA: 2s - loss: 0.2540 - acc: 0.924 - ETA: 2s - loss: 0.2527 - acc: 0.923 - ETA: 2s - loss: 0.2553 - acc: 0.920 - ETA: 2s - loss: 0.2583 - acc: 0.919 - ETA: 1s - loss: 0.2549 - acc: 0.920 - ETA: 1s - loss: 0.2578 - acc: 0.916 - ETA: 1s - loss: 0.2584 - acc: 0.918 - ETA: 1s - loss: 0.2567 - acc: 0.918 - ETA: 1s - loss: 0.2538 - acc: 0.920 - ETA: 1s - loss: 0.2545 - acc: 0.918 - ETA: 1s - loss: 0.2558 - acc: 0.918 - ETA: 1s - loss: 0.2552 - acc: 0.916 - ETA: 1s - loss: 0.2569 - acc: 0.915 - ETA: 1s - loss: 0.2589 - acc: 0.914 - ETA: 1s - loss: 0.2631 - acc: 0.912 - ETA: 0s - loss: 0.2683 - acc: 0.911 - ETA: 0s - loss: 0.2628 - acc: 0.913 - ETA: 0s - loss: 0.2688 - acc: 0.909 - ETA: 0s - loss: 0.2705 - acc: 0.908 - ETA: 0s - loss: 0.2727 - acc: 0.906 - ETA: 0s - loss: 0.2722 - acc: 0.907 - ETA: 0s - loss: 0.2723 - acc: 0.908 - ETA: 0s - loss: 0.2739 - acc: 0.907 - ETA: 0s - loss: 0.2702 - acc: 0.908 - ETA: 0s - loss: 0.2702 - acc: 0.907 - ETA: 0s - loss: 0.2680 - acc: 0.909 - ETA: 0s - loss: 0.2667 - acc: 0.910 - ETA: 0s - loss: 0.2655 - acc: 0.911 - 3s 641us/step - loss: 0.2656 - acc: 0.9105 - val_loss: 0.3773 - val_acc: 0.8647\n",
      "Epoch 8/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2611 - acc: 0.921 - ETA: 2s - loss: 0.3385 - acc: 0.869 - ETA: 2s - loss: 0.2996 - acc: 0.896 - ETA: 2s - loss: 0.3059 - acc: 0.890 - ETA: 2s - loss: 0.2866 - acc: 0.897 - ETA: 2s - loss: 0.2843 - acc: 0.897 - ETA: 1s - loss: 0.2876 - acc: 0.899 - ETA: 1s - loss: 0.2770 - acc: 0.902 - ETA: 1s - loss: 0.2718 - acc: 0.906 - ETA: 1s - loss: 0.2798 - acc: 0.904 - ETA: 1s - loss: 0.2863 - acc: 0.901 - ETA: 1s - loss: 0.2850 - acc: 0.902 - ETA: 1s - loss: 0.2831 - acc: 0.902 - ETA: 1s - loss: 0.2831 - acc: 0.902 - ETA: 1s - loss: 0.2849 - acc: 0.899 - ETA: 1s - loss: 0.2846 - acc: 0.900 - ETA: 1s - loss: 0.2854 - acc: 0.901 - ETA: 1s - loss: 0.2840 - acc: 0.901 - ETA: 1s - loss: 0.2840 - acc: 0.899 - ETA: 0s - loss: 0.2842 - acc: 0.899 - ETA: 0s - loss: 0.2868 - acc: 0.898 - ETA: 0s - loss: 0.2852 - acc: 0.899 - ETA: 0s - loss: 0.2864 - acc: 0.898 - ETA: 0s - loss: 0.2881 - acc: 0.897 - ETA: 0s - loss: 0.2868 - acc: 0.898 - ETA: 0s - loss: 0.2848 - acc: 0.899 - ETA: 0s - loss: 0.2854 - acc: 0.899 - ETA: 0s - loss: 0.2853 - acc: 0.899 - ETA: 0s - loss: 0.2844 - acc: 0.899 - ETA: 0s - loss: 0.2851 - acc: 0.899 - ETA: 0s - loss: 0.2838 - acc: 0.900 - ETA: 0s - loss: 0.2831 - acc: 0.900 - 3s 634us/step - loss: 0.2837 - acc: 0.9004 - val_loss: 0.3628 - val_acc: 0.8712\n",
      "Epoch 9/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2399 - acc: 0.890 - ETA: 2s - loss: 0.2225 - acc: 0.921 - ETA: 2s - loss: 0.2670 - acc: 0.906 - ETA: 2s - loss: 0.2718 - acc: 0.901 - ETA: 1s - loss: 0.2743 - acc: 0.901 - ETA: 1s - loss: 0.2724 - acc: 0.902 - ETA: 1s - loss: 0.2655 - acc: 0.908 - ETA: 1s - loss: 0.2825 - acc: 0.904 - ETA: 1s - loss: 0.2844 - acc: 0.906 - ETA: 1s - loss: 0.2855 - acc: 0.904 - ETA: 1s - loss: 0.2806 - acc: 0.907 - ETA: 1s - loss: 0.2870 - acc: 0.903 - ETA: 1s - loss: 0.2846 - acc: 0.903 - ETA: 1s - loss: 0.2819 - acc: 0.906 - ETA: 1s - loss: 0.2800 - acc: 0.907 - ETA: 1s - loss: 0.2807 - acc: 0.908 - ETA: 1s - loss: 0.2820 - acc: 0.908 - ETA: 1s - loss: 0.2821 - acc: 0.908 - ETA: 1s - loss: 0.2796 - acc: 0.910 - ETA: 1s - loss: 0.2788 - acc: 0.911 - ETA: 1s - loss: 0.2776 - acc: 0.911 - ETA: 0s - loss: 0.2760 - acc: 0.912 - ETA: 0s - loss: 0.2763 - acc: 0.912 - ETA: 0s - loss: 0.2794 - acc: 0.909 - ETA: 0s - loss: 0.2772 - acc: 0.910 - ETA: 0s - loss: 0.2812 - acc: 0.909 - ETA: 0s - loss: 0.2792 - acc: 0.909 - ETA: 0s - loss: 0.2760 - acc: 0.910 - ETA: 0s - loss: 0.2774 - acc: 0.910 - ETA: 0s - loss: 0.2769 - acc: 0.910 - ETA: 0s - loss: 0.2739 - acc: 0.912 - ETA: 0s - loss: 0.2718 - acc: 0.913 - ETA: 0s - loss: 0.2695 - acc: 0.914 - ETA: 0s - loss: 0.2689 - acc: 0.913 - ETA: 0s - loss: 0.2683 - acc: 0.914 - 3s 638us/step - loss: 0.2676 - acc: 0.9144 - val_loss: 0.3490 - val_acc: 0.8744\n",
      "Epoch 10/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2797 - acc: 0.890 - ETA: 2s - loss: 0.2164 - acc: 0.927 - ETA: 2s - loss: 0.2416 - acc: 0.918 - ETA: 2s - loss: 0.2421 - acc: 0.917 - ETA: 2s - loss: 0.2528 - acc: 0.913 - ETA: 2s - loss: 0.2536 - acc: 0.911 - ETA: 1s - loss: 0.2575 - acc: 0.908 - ETA: 1s - loss: 0.2484 - acc: 0.913 - ETA: 1s - loss: 0.2496 - acc: 0.909 - ETA: 1s - loss: 0.2441 - acc: 0.912 - ETA: 1s - loss: 0.2377 - acc: 0.916 - ETA: 1s - loss: 0.2379 - acc: 0.915 - ETA: 1s - loss: 0.2346 - acc: 0.916 - ETA: 1s - loss: 0.2287 - acc: 0.919 - ETA: 1s - loss: 0.2324 - acc: 0.917 - ETA: 1s - loss: 0.2329 - acc: 0.918 - ETA: 1s - loss: 0.2360 - acc: 0.916 - ETA: 1s - loss: 0.2355 - acc: 0.917 - ETA: 1s - loss: 0.2368 - acc: 0.917 - ETA: 0s - loss: 0.2357 - acc: 0.919 - ETA: 0s - loss: 0.2382 - acc: 0.918 - ETA: 0s - loss: 0.2415 - acc: 0.917 - ETA: 0s - loss: 0.2430 - acc: 0.917 - ETA: 0s - loss: 0.2437 - acc: 0.916 - ETA: 0s - loss: 0.2437 - acc: 0.917 - ETA: 0s - loss: 0.2465 - acc: 0.915 - ETA: 0s - loss: 0.2470 - acc: 0.915 - ETA: 0s - loss: 0.2476 - acc: 0.914 - ETA: 0s - loss: 0.2494 - acc: 0.914 - ETA: 0s - loss: 0.2505 - acc: 0.915 - ETA: 0s - loss: 0.2512 - acc: 0.914 - ETA: 0s - loss: 0.2507 - acc: 0.914 - 3s 645us/step - loss: 0.2502 - acc: 0.9152 - val_loss: 0.3316 - val_acc: 0.8859\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2236 - acc: 0.953 - ETA: 2s - loss: 0.2240 - acc: 0.947 - ETA: 2s - loss: 0.2511 - acc: 0.925 - ETA: 2s - loss: 0.2571 - acc: 0.924 - ETA: 2s - loss: 0.2549 - acc: 0.921 - ETA: 2s - loss: 0.2573 - acc: 0.920 - ETA: 2s - loss: 0.2583 - acc: 0.918 - ETA: 2s - loss: 0.2531 - acc: 0.920 - ETA: 1s - loss: 0.2519 - acc: 0.921 - ETA: 1s - loss: 0.2497 - acc: 0.921 - ETA: 1s - loss: 0.2534 - acc: 0.915 - ETA: 1s - loss: 0.2477 - acc: 0.915 - ETA: 1s - loss: 0.2482 - acc: 0.914 - ETA: 1s - loss: 0.2512 - acc: 0.913 - ETA: 1s - loss: 0.2471 - acc: 0.915 - ETA: 1s - loss: 0.2428 - acc: 0.917 - ETA: 1s - loss: 0.2422 - acc: 0.917 - ETA: 1s - loss: 0.2426 - acc: 0.916 - ETA: 1s - loss: 0.2455 - acc: 0.914 - ETA: 1s - loss: 0.2472 - acc: 0.912 - ETA: 1s - loss: 0.2463 - acc: 0.913 - ETA: 1s - loss: 0.2458 - acc: 0.913 - ETA: 0s - loss: 0.2444 - acc: 0.914 - ETA: 0s - loss: 0.2425 - acc: 0.915 - ETA: 0s - loss: 0.2450 - acc: 0.916 - ETA: 0s - loss: 0.2456 - acc: 0.916 - ETA: 0s - loss: 0.2448 - acc: 0.917 - ETA: 0s - loss: 0.2481 - acc: 0.915 - ETA: 0s - loss: 0.2477 - acc: 0.915 - ETA: 0s - loss: 0.2467 - acc: 0.916 - ETA: 0s - loss: 0.2475 - acc: 0.916 - ETA: 0s - loss: 0.2480 - acc: 0.916 - ETA: 0s - loss: 0.2460 - acc: 0.917 - ETA: 0s - loss: 0.2485 - acc: 0.916 - ETA: 0s - loss: 0.2484 - acc: 0.915 - 3s 667us/step - loss: 0.2476 - acc: 0.9164 - val_loss: 0.3229 - val_acc: 0.8859\n",
      "Epoch 12/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1871 - acc: 0.937 - ETA: 2s - loss: 0.2138 - acc: 0.937 - ETA: 2s - loss: 0.2226 - acc: 0.934 - ETA: 2s - loss: 0.2378 - acc: 0.917 - ETA: 1s - loss: 0.2290 - acc: 0.921 - ETA: 1s - loss: 0.2275 - acc: 0.924 - ETA: 1s - loss: 0.2279 - acc: 0.926 - ETA: 1s - loss: 0.2425 - acc: 0.916 - ETA: 1s - loss: 0.2462 - acc: 0.912 - ETA: 1s - loss: 0.2426 - acc: 0.913 - ETA: 1s - loss: 0.2411 - acc: 0.915 - ETA: 1s - loss: 0.2467 - acc: 0.913 - ETA: 1s - loss: 0.2455 - acc: 0.915 - ETA: 1s - loss: 0.2439 - acc: 0.913 - ETA: 1s - loss: 0.2457 - acc: 0.913 - ETA: 1s - loss: 0.2446 - acc: 0.914 - ETA: 1s - loss: 0.2483 - acc: 0.914 - ETA: 1s - loss: 0.2477 - acc: 0.914 - ETA: 0s - loss: 0.2519 - acc: 0.912 - ETA: 0s - loss: 0.2549 - acc: 0.911 - ETA: 0s - loss: 0.2597 - acc: 0.907 - ETA: 0s - loss: 0.2603 - acc: 0.907 - ETA: 0s - loss: 0.2580 - acc: 0.908 - ETA: 0s - loss: 0.2594 - acc: 0.907 - ETA: 0s - loss: 0.2606 - acc: 0.907 - ETA: 0s - loss: 0.2586 - acc: 0.907 - ETA: 0s - loss: 0.2584 - acc: 0.906 - ETA: 0s - loss: 0.2585 - acc: 0.906 - ETA: 0s - loss: 0.2598 - acc: 0.907 - ETA: 0s - loss: 0.2594 - acc: 0.907 - ETA: 0s - loss: 0.2595 - acc: 0.907 - ETA: 0s - loss: 0.2597 - acc: 0.907 - 2s 597us/step - loss: 0.2635 - acc: 0.9073 - val_loss: 0.3311 - val_acc: 0.8776\n",
      "Epoch 13/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1561 - acc: 0.953 - ETA: 2s - loss: 0.2396 - acc: 0.911 - ETA: 1s - loss: 0.2445 - acc: 0.909 - ETA: 1s - loss: 0.2516 - acc: 0.915 - ETA: 1s - loss: 0.2410 - acc: 0.921 - ETA: 1s - loss: 0.2449 - acc: 0.919 - ETA: 1s - loss: 0.2481 - acc: 0.917 - ETA: 1s - loss: 0.2521 - acc: 0.912 - ETA: 1s - loss: 0.2497 - acc: 0.912 - ETA: 1s - loss: 0.2532 - acc: 0.912 - ETA: 1s - loss: 0.2558 - acc: 0.912 - ETA: 1s - loss: 0.2609 - acc: 0.912 - ETA: 1s - loss: 0.2565 - acc: 0.913 - ETA: 1s - loss: 0.2577 - acc: 0.913 - ETA: 1s - loss: 0.2531 - acc: 0.915 - ETA: 1s - loss: 0.2542 - acc: 0.915 - ETA: 1s - loss: 0.2561 - acc: 0.913 - ETA: 1s - loss: 0.2606 - acc: 0.911 - ETA: 1s - loss: 0.2613 - acc: 0.912 - ETA: 0s - loss: 0.2607 - acc: 0.911 - ETA: 0s - loss: 0.2587 - acc: 0.913 - ETA: 0s - loss: 0.2636 - acc: 0.910 - ETA: 0s - loss: 0.2624 - acc: 0.911 - ETA: 0s - loss: 0.2591 - acc: 0.913 - ETA: 0s - loss: 0.2589 - acc: 0.913 - ETA: 0s - loss: 0.2587 - acc: 0.913 - ETA: 0s - loss: 0.2611 - acc: 0.912 - ETA: 0s - loss: 0.2588 - acc: 0.913 - ETA: 0s - loss: 0.2584 - acc: 0.913 - ETA: 0s - loss: 0.2583 - acc: 0.913 - ETA: 0s - loss: 0.2564 - acc: 0.914 - ETA: 0s - loss: 0.2548 - acc: 0.915 - ETA: 0s - loss: 0.2530 - acc: 0.916 - ETA: 0s - loss: 0.2515 - acc: 0.917 - ETA: 0s - loss: 0.2523 - acc: 0.916 - 3s 632us/step - loss: 0.2518 - acc: 0.9169 - val_loss: 0.3266 - val_acc: 0.8821\n",
      "Epoch 14/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2405 - acc: 0.890 - ETA: 3s - loss: 0.2203 - acc: 0.914 - ETA: 2s - loss: 0.2009 - acc: 0.929 - ETA: 2s - loss: 0.1896 - acc: 0.937 - ETA: 2s - loss: 0.2165 - acc: 0.923 - ETA: 2s - loss: 0.2164 - acc: 0.925 - ETA: 1s - loss: 0.2168 - acc: 0.924 - ETA: 1s - loss: 0.2156 - acc: 0.925 - ETA: 1s - loss: 0.2191 - acc: 0.924 - ETA: 1s - loss: 0.2290 - acc: 0.920 - ETA: 1s - loss: 0.2374 - acc: 0.916 - ETA: 1s - loss: 0.2315 - acc: 0.919 - ETA: 1s - loss: 0.2317 - acc: 0.919 - ETA: 1s - loss: 0.2304 - acc: 0.922 - ETA: 1s - loss: 0.2323 - acc: 0.921 - ETA: 1s - loss: 0.2345 - acc: 0.920 - ETA: 1s - loss: 0.2313 - acc: 0.922 - ETA: 1s - loss: 0.2291 - acc: 0.923 - ETA: 1s - loss: 0.2304 - acc: 0.924 - ETA: 1s - loss: 0.2356 - acc: 0.921 - ETA: 0s - loss: 0.2351 - acc: 0.922 - ETA: 0s - loss: 0.2347 - acc: 0.922 - ETA: 0s - loss: 0.2376 - acc: 0.921 - ETA: 0s - loss: 0.2374 - acc: 0.921 - ETA: 0s - loss: 0.2388 - acc: 0.920 - ETA: 0s - loss: 0.2399 - acc: 0.920 - ETA: 0s - loss: 0.2407 - acc: 0.918 - ETA: 0s - loss: 0.2406 - acc: 0.919 - ETA: 0s - loss: 0.2415 - acc: 0.919 - ETA: 0s - loss: 0.2420 - acc: 0.918 - ETA: 0s - loss: 0.2414 - acc: 0.918 - ETA: 0s - loss: 0.2449 - acc: 0.916 - ETA: 0s - loss: 0.2439 - acc: 0.917 - ETA: 0s - loss: 0.2443 - acc: 0.916 - ETA: 0s - loss: 0.2441 - acc: 0.916 - ETA: 0s - loss: 0.2453 - acc: 0.916 - ETA: 0s - loss: 0.2452 - acc: 0.915 - 3s 678us/step - loss: 0.2454 - acc: 0.9154 - val_loss: 0.3620 - val_acc: 0.8699\n",
      "Epoch 15/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1785 - acc: 0.937 - ETA: 2s - loss: 0.1807 - acc: 0.958 - ETA: 2s - loss: 0.1807 - acc: 0.959 - ETA: 2s - loss: 0.2136 - acc: 0.945 - ETA: 2s - loss: 0.2174 - acc: 0.939 - ETA: 2s - loss: 0.2363 - acc: 0.930 - ETA: 2s - loss: 0.2328 - acc: 0.927 - ETA: 2s - loss: 0.2352 - acc: 0.927 - ETA: 2s - loss: 0.2418 - acc: 0.920 - ETA: 1s - loss: 0.2404 - acc: 0.923 - ETA: 1s - loss: 0.2474 - acc: 0.920 - ETA: 1s - loss: 0.2443 - acc: 0.923 - ETA: 1s - loss: 0.2442 - acc: 0.923 - ETA: 1s - loss: 0.2432 - acc: 0.925 - ETA: 1s - loss: 0.2413 - acc: 0.925 - ETA: 1s - loss: 0.2387 - acc: 0.927 - ETA: 1s - loss: 0.2396 - acc: 0.927 - ETA: 1s - loss: 0.2443 - acc: 0.926 - ETA: 1s - loss: 0.2474 - acc: 0.923 - ETA: 1s - loss: 0.2489 - acc: 0.920 - ETA: 0s - loss: 0.2547 - acc: 0.917 - ETA: 0s - loss: 0.2529 - acc: 0.918 - ETA: 0s - loss: 0.2506 - acc: 0.919 - ETA: 0s - loss: 0.2494 - acc: 0.919 - ETA: 0s - loss: 0.2488 - acc: 0.919 - ETA: 0s - loss: 0.2502 - acc: 0.919 - ETA: 0s - loss: 0.2483 - acc: 0.920 - ETA: 0s - loss: 0.2490 - acc: 0.920 - ETA: 0s - loss: 0.2496 - acc: 0.919 - ETA: 0s - loss: 0.2505 - acc: 0.919 - ETA: 0s - loss: 0.2486 - acc: 0.920 - ETA: 0s - loss: 0.2479 - acc: 0.920 - ETA: 0s - loss: 0.2468 - acc: 0.921 - ETA: 0s - loss: 0.2466 - acc: 0.921 - ETA: 0s - loss: 0.2469 - acc: 0.921 - ETA: 0s - loss: 0.2472 - acc: 0.920 - 3s 691us/step - loss: 0.2464 - acc: 0.9206 - val_loss: 0.3158 - val_acc: 0.8904\n",
      "Epoch 16/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1996 - acc: 0.890 - ETA: 2s - loss: 0.2492 - acc: 0.901 - ETA: 2s - loss: 0.2426 - acc: 0.906 - ETA: 2s - loss: 0.2265 - acc: 0.921 - ETA: 2s - loss: 0.2142 - acc: 0.930 - ETA: 2s - loss: 0.2115 - acc: 0.934 - ETA: 2s - loss: 0.2102 - acc: 0.933 - ETA: 2s - loss: 0.2077 - acc: 0.936 - ETA: 1s - loss: 0.2058 - acc: 0.936 - ETA: 1s - loss: 0.2151 - acc: 0.931 - ETA: 1s - loss: 0.2123 - acc: 0.932 - ETA: 1s - loss: 0.2124 - acc: 0.933 - ETA: 1s - loss: 0.2124 - acc: 0.931 - ETA: 1s - loss: 0.2138 - acc: 0.930 - ETA: 1s - loss: 0.2135 - acc: 0.931 - ETA: 1s - loss: 0.2163 - acc: 0.932 - ETA: 1s - loss: 0.2158 - acc: 0.933 - ETA: 1s - loss: 0.2179 - acc: 0.931 - ETA: 1s - loss: 0.2219 - acc: 0.929 - ETA: 1s - loss: 0.2220 - acc: 0.931 - ETA: 0s - loss: 0.2231 - acc: 0.931 - ETA: 0s - loss: 0.2212 - acc: 0.931 - ETA: 0s - loss: 0.2226 - acc: 0.930 - ETA: 0s - loss: 0.2213 - acc: 0.930 - ETA: 0s - loss: 0.2236 - acc: 0.929 - ETA: 0s - loss: 0.2265 - acc: 0.928 - ETA: 0s - loss: 0.2249 - acc: 0.929 - ETA: 0s - loss: 0.2249 - acc: 0.929 - ETA: 0s - loss: 0.2269 - acc: 0.929 - ETA: 0s - loss: 0.2298 - acc: 0.928 - ETA: 0s - loss: 0.2303 - acc: 0.927 - ETA: 0s - loss: 0.2299 - acc: 0.928 - ETA: 0s - loss: 0.2337 - acc: 0.927 - ETA: 0s - loss: 0.2339 - acc: 0.927 - 3s 672us/step - loss: 0.2342 - acc: 0.9262 - val_loss: 0.3594 - val_acc: 0.8487\n",
      "Epoch 17/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2836 - acc: 0.875 - ETA: 2s - loss: 0.3324 - acc: 0.864 - ETA: 2s - loss: 0.3038 - acc: 0.884 - ETA: 2s - loss: 0.2824 - acc: 0.897 - ETA: 2s - loss: 0.2700 - acc: 0.902 - ETA: 2s - loss: 0.2661 - acc: 0.902 - ETA: 2s - loss: 0.2639 - acc: 0.900 - ETA: 1s - loss: 0.2638 - acc: 0.896 - ETA: 1s - loss: 0.2565 - acc: 0.903 - ETA: 1s - loss: 0.2574 - acc: 0.903 - ETA: 1s - loss: 0.2527 - acc: 0.908 - ETA: 1s - loss: 0.2483 - acc: 0.909 - ETA: 1s - loss: 0.2491 - acc: 0.911 - ETA: 1s - loss: 0.2487 - acc: 0.910 - ETA: 1s - loss: 0.2494 - acc: 0.907 - ETA: 1s - loss: 0.2491 - acc: 0.908 - ETA: 1s - loss: 0.2457 - acc: 0.910 - ETA: 1s - loss: 0.2448 - acc: 0.910 - ETA: 1s - loss: 0.2429 - acc: 0.911 - ETA: 1s - loss: 0.2436 - acc: 0.911 - ETA: 0s - loss: 0.2444 - acc: 0.912 - ETA: 0s - loss: 0.2419 - acc: 0.912 - ETA: 0s - loss: 0.2423 - acc: 0.911 - ETA: 0s - loss: 0.2427 - acc: 0.912 - ETA: 0s - loss: 0.2414 - acc: 0.913 - ETA: 0s - loss: 0.2409 - acc: 0.914 - ETA: 0s - loss: 0.2411 - acc: 0.913 - ETA: 0s - loss: 0.2404 - acc: 0.914 - ETA: 0s - loss: 0.2396 - acc: 0.914 - ETA: 0s - loss: 0.2394 - acc: 0.915 - ETA: 0s - loss: 0.2384 - acc: 0.916 - ETA: 0s - loss: 0.2386 - acc: 0.915 - 3s 641us/step - loss: 0.2361 - acc: 0.9174 - val_loss: 0.3309 - val_acc: 0.8686\n",
      "Epoch 18/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1474 - acc: 0.953 - ETA: 2s - loss: 0.1536 - acc: 0.942 - ETA: 2s - loss: 0.1948 - acc: 0.918 - ETA: 1s - loss: 0.2119 - acc: 0.919 - ETA: 1s - loss: 0.2151 - acc: 0.920 - ETA: 1s - loss: 0.2199 - acc: 0.919 - ETA: 1s - loss: 0.2295 - acc: 0.914 - ETA: 1s - loss: 0.2512 - acc: 0.908 - ETA: 1s - loss: 0.2511 - acc: 0.909 - ETA: 1s - loss: 0.2483 - acc: 0.912 - ETA: 1s - loss: 0.2443 - acc: 0.915 - ETA: 1s - loss: 0.2413 - acc: 0.919 - ETA: 1s - loss: 0.2446 - acc: 0.918 - ETA: 1s - loss: 0.2470 - acc: 0.916 - ETA: 1s - loss: 0.2549 - acc: 0.912 - ETA: 1s - loss: 0.2540 - acc: 0.914 - ETA: 1s - loss: 0.2541 - acc: 0.916 - ETA: 0s - loss: 0.2545 - acc: 0.915 - ETA: 0s - loss: 0.2560 - acc: 0.914 - ETA: 0s - loss: 0.2569 - acc: 0.913 - ETA: 0s - loss: 0.2583 - acc: 0.913 - ETA: 0s - loss: 0.2597 - acc: 0.911 - ETA: 0s - loss: 0.2589 - acc: 0.911 - ETA: 0s - loss: 0.2586 - acc: 0.912 - ETA: 0s - loss: 0.2584 - acc: 0.912 - ETA: 0s - loss: 0.2582 - acc: 0.912 - ETA: 0s - loss: 0.2576 - acc: 0.913 - ETA: 0s - loss: 0.2565 - acc: 0.913 - ETA: 0s - loss: 0.2552 - acc: 0.914 - ETA: 0s - loss: 0.2540 - acc: 0.915 - ETA: 0s - loss: 0.2545 - acc: 0.915 - ETA: 0s - loss: 0.2529 - acc: 0.915 - 2s 592us/step - loss: 0.2523 - acc: 0.9164 - val_loss: 0.3366 - val_acc: 0.8737\n",
      "Epoch 19/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3206 - acc: 0.890 - ETA: 2s - loss: 0.2492 - acc: 0.906 - ETA: 2s - loss: 0.2253 - acc: 0.928 - ETA: 2s - loss: 0.2273 - acc: 0.928 - ETA: 2s - loss: 0.2302 - acc: 0.923 - ETA: 1s - loss: 0.2278 - acc: 0.924 - ETA: 1s - loss: 0.2271 - acc: 0.925 - ETA: 1s - loss: 0.2249 - acc: 0.926 - ETA: 1s - loss: 0.2198 - acc: 0.927 - ETA: 1s - loss: 0.2162 - acc: 0.930 - ETA: 1s - loss: 0.2118 - acc: 0.930 - ETA: 1s - loss: 0.2059 - acc: 0.932 - ETA: 1s - loss: 0.2099 - acc: 0.928 - ETA: 1s - loss: 0.2159 - acc: 0.925 - ETA: 1s - loss: 0.2227 - acc: 0.921 - ETA: 1s - loss: 0.2259 - acc: 0.920 - ETA: 1s - loss: 0.2257 - acc: 0.921 - ETA: 1s - loss: 0.2285 - acc: 0.921 - ETA: 1s - loss: 0.2312 - acc: 0.921 - ETA: 0s - loss: 0.2319 - acc: 0.921 - ETA: 0s - loss: 0.2323 - acc: 0.920 - ETA: 0s - loss: 0.2346 - acc: 0.919 - ETA: 0s - loss: 0.2343 - acc: 0.919 - ETA: 0s - loss: 0.2339 - acc: 0.920 - ETA: 0s - loss: 0.2317 - acc: 0.922 - ETA: 0s - loss: 0.2306 - acc: 0.922 - ETA: 0s - loss: 0.2288 - acc: 0.922 - ETA: 0s - loss: 0.2292 - acc: 0.922 - ETA: 0s - loss: 0.2294 - acc: 0.923 - ETA: 0s - loss: 0.2291 - acc: 0.923 - ETA: 0s - loss: 0.2293 - acc: 0.923 - ETA: 0s - loss: 0.2304 - acc: 0.922 - ETA: 0s - loss: 0.2291 - acc: 0.923 - 3s 644us/step - loss: 0.2283 - acc: 0.9235 - val_loss: 0.3030 - val_acc: 0.8936\n",
      "Epoch 20/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1869 - acc: 0.937 - ETA: 2s - loss: 0.1881 - acc: 0.953 - ETA: 2s - loss: 0.2322 - acc: 0.928 - ETA: 2s - loss: 0.2313 - acc: 0.928 - ETA: 2s - loss: 0.2286 - acc: 0.930 - ETA: 2s - loss: 0.2286 - acc: 0.930 - ETA: 1s - loss: 0.2325 - acc: 0.927 - ETA: 1s - loss: 0.2240 - acc: 0.929 - ETA: 1s - loss: 0.2225 - acc: 0.928 - ETA: 1s - loss: 0.2256 - acc: 0.928 - ETA: 1s - loss: 0.2295 - acc: 0.928 - ETA: 1s - loss: 0.2293 - acc: 0.928 - ETA: 1s - loss: 0.2298 - acc: 0.926 - ETA: 1s - loss: 0.2375 - acc: 0.924 - ETA: 1s - loss: 0.2367 - acc: 0.923 - ETA: 1s - loss: 0.2315 - acc: 0.925 - ETA: 1s - loss: 0.2358 - acc: 0.923 - ETA: 1s - loss: 0.2423 - acc: 0.919 - ETA: 1s - loss: 0.2456 - acc: 0.919 - ETA: 0s - loss: 0.2460 - acc: 0.919 - ETA: 0s - loss: 0.2469 - acc: 0.918 - ETA: 0s - loss: 0.2479 - acc: 0.919 - ETA: 0s - loss: 0.2458 - acc: 0.918 - ETA: 0s - loss: 0.2458 - acc: 0.917 - ETA: 0s - loss: 0.2448 - acc: 0.917 - ETA: 0s - loss: 0.2476 - acc: 0.915 - ETA: 0s - loss: 0.2451 - acc: 0.916 - ETA: 0s - loss: 0.2464 - acc: 0.916 - ETA: 0s - loss: 0.2471 - acc: 0.916 - ETA: 0s - loss: 0.2486 - acc: 0.916 - ETA: 0s - loss: 0.2500 - acc: 0.916 - ETA: 0s - loss: 0.2476 - acc: 0.917 - 3s 625us/step - loss: 0.2471 - acc: 0.9174 - val_loss: 0.3040 - val_acc: 0.8833\n",
      "Epoch 21/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2380 - acc: 0.921 - ETA: 2s - loss: 0.2324 - acc: 0.927 - ETA: 2s - loss: 0.2298 - acc: 0.937 - ETA: 2s - loss: 0.2429 - acc: 0.933 - ETA: 2s - loss: 0.2245 - acc: 0.937 - ETA: 1s - loss: 0.2290 - acc: 0.933 - ETA: 1s - loss: 0.2174 - acc: 0.937 - ETA: 1s - loss: 0.2136 - acc: 0.936 - ETA: 1s - loss: 0.2089 - acc: 0.938 - ETA: 1s - loss: 0.2092 - acc: 0.935 - ETA: 1s - loss: 0.2179 - acc: 0.933 - ETA: 1s - loss: 0.2161 - acc: 0.932 - ETA: 1s - loss: 0.2249 - acc: 0.926 - ETA: 1s - loss: 0.2273 - acc: 0.923 - ETA: 1s - loss: 0.2298 - acc: 0.922 - ETA: 1s - loss: 0.2316 - acc: 0.922 - ETA: 1s - loss: 0.2318 - acc: 0.921 - ETA: 1s - loss: 0.2330 - acc: 0.921 - ETA: 1s - loss: 0.2320 - acc: 0.923 - ETA: 0s - loss: 0.2329 - acc: 0.921 - ETA: 0s - loss: 0.2313 - acc: 0.923 - ETA: 0s - loss: 0.2271 - acc: 0.925 - ETA: 0s - loss: 0.2321 - acc: 0.924 - ETA: 0s - loss: 0.2300 - acc: 0.924 - ETA: 0s - loss: 0.2298 - acc: 0.923 - ETA: 0s - loss: 0.2317 - acc: 0.923 - ETA: 0s - loss: 0.2336 - acc: 0.922 - ETA: 0s - loss: 0.2333 - acc: 0.922 - ETA: 0s - loss: 0.2328 - acc: 0.922 - ETA: 0s - loss: 0.2308 - acc: 0.923 - ETA: 0s - loss: 0.2310 - acc: 0.922 - ETA: 0s - loss: 0.2293 - acc: 0.922 - 3s 633us/step - loss: 0.2290 - acc: 0.9228 - val_loss: 0.3473 - val_acc: 0.8577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3362 - acc: 0.875 - ETA: 2s - loss: 0.2493 - acc: 0.916 - ETA: 2s - loss: 0.2505 - acc: 0.921 - ETA: 2s - loss: 0.2483 - acc: 0.926 - ETA: 2s - loss: 0.2460 - acc: 0.925 - ETA: 1s - loss: 0.2493 - acc: 0.921 - ETA: 1s - loss: 0.2458 - acc: 0.920 - ETA: 1s - loss: 0.2474 - acc: 0.916 - ETA: 1s - loss: 0.2414 - acc: 0.920 - ETA: 1s - loss: 0.2417 - acc: 0.918 - ETA: 1s - loss: 0.2368 - acc: 0.921 - ETA: 1s - loss: 0.2328 - acc: 0.922 - ETA: 1s - loss: 0.2314 - acc: 0.923 - ETA: 1s - loss: 0.2406 - acc: 0.919 - ETA: 1s - loss: 0.2391 - acc: 0.919 - ETA: 1s - loss: 0.2433 - acc: 0.918 - ETA: 1s - loss: 0.2428 - acc: 0.919 - ETA: 1s - loss: 0.2461 - acc: 0.918 - ETA: 1s - loss: 0.2447 - acc: 0.918 - ETA: 1s - loss: 0.2478 - acc: 0.917 - ETA: 1s - loss: 0.2475 - acc: 0.917 - ETA: 0s - loss: 0.2492 - acc: 0.917 - ETA: 0s - loss: 0.2491 - acc: 0.917 - ETA: 0s - loss: 0.2503 - acc: 0.917 - ETA: 0s - loss: 0.2528 - acc: 0.917 - ETA: 0s - loss: 0.2502 - acc: 0.918 - ETA: 0s - loss: 0.2518 - acc: 0.917 - ETA: 0s - loss: 0.2509 - acc: 0.917 - ETA: 0s - loss: 0.2515 - acc: 0.916 - ETA: 0s - loss: 0.2512 - acc: 0.916 - ETA: 0s - loss: 0.2503 - acc: 0.917 - ETA: 0s - loss: 0.2499 - acc: 0.918 - ETA: 0s - loss: 0.2499 - acc: 0.918 - 3s 669us/step - loss: 0.2505 - acc: 0.9174 - val_loss: 0.3167 - val_acc: 0.8853\n",
      "Epoch 23/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2247 - acc: 0.937 - ETA: 2s - loss: 0.2619 - acc: 0.921 - ETA: 2s - loss: 0.2783 - acc: 0.915 - ETA: 2s - loss: 0.2848 - acc: 0.897 - ETA: 2s - loss: 0.2774 - acc: 0.909 - ETA: 2s - loss: 0.2874 - acc: 0.903 - ETA: 1s - loss: 0.2950 - acc: 0.906 - ETA: 1s - loss: 0.3014 - acc: 0.906 - ETA: 1s - loss: 0.3128 - acc: 0.898 - ETA: 1s - loss: 0.3088 - acc: 0.901 - ETA: 1s - loss: 0.3053 - acc: 0.901 - ETA: 1s - loss: 0.2921 - acc: 0.906 - ETA: 1s - loss: 0.2904 - acc: 0.906 - ETA: 1s - loss: 0.2831 - acc: 0.909 - ETA: 1s - loss: 0.2836 - acc: 0.907 - ETA: 1s - loss: 0.2837 - acc: 0.907 - ETA: 1s - loss: 0.2804 - acc: 0.908 - ETA: 1s - loss: 0.2790 - acc: 0.907 - ETA: 0s - loss: 0.2761 - acc: 0.907 - ETA: 0s - loss: 0.2734 - acc: 0.908 - ETA: 0s - loss: 0.2720 - acc: 0.908 - ETA: 0s - loss: 0.2699 - acc: 0.908 - ETA: 0s - loss: 0.2689 - acc: 0.908 - ETA: 0s - loss: 0.2654 - acc: 0.909 - ETA: 0s - loss: 0.2652 - acc: 0.910 - ETA: 0s - loss: 0.2648 - acc: 0.910 - ETA: 0s - loss: 0.2602 - acc: 0.912 - ETA: 0s - loss: 0.2592 - acc: 0.912 - ETA: 0s - loss: 0.2586 - acc: 0.912 - ETA: 0s - loss: 0.2579 - acc: 0.911 - ETA: 0s - loss: 0.2591 - acc: 0.910 - ETA: 0s - loss: 0.2584 - acc: 0.912 - 2s 591us/step - loss: 0.2580 - acc: 0.9120 - val_loss: 0.3110 - val_acc: 0.8814\n",
      "Epoch 24/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2508 - acc: 0.890 - ETA: 2s - loss: 0.2417 - acc: 0.901 - ETA: 2s - loss: 0.2406 - acc: 0.906 - ETA: 2s - loss: 0.2541 - acc: 0.906 - ETA: 2s - loss: 0.2549 - acc: 0.909 - ETA: 2s - loss: 0.2428 - acc: 0.917 - ETA: 1s - loss: 0.2363 - acc: 0.920 - ETA: 1s - loss: 0.2378 - acc: 0.922 - ETA: 1s - loss: 0.2307 - acc: 0.924 - ETA: 1s - loss: 0.2379 - acc: 0.922 - ETA: 1s - loss: 0.2363 - acc: 0.924 - ETA: 1s - loss: 0.2330 - acc: 0.926 - ETA: 1s - loss: 0.2289 - acc: 0.927 - ETA: 1s - loss: 0.2295 - acc: 0.926 - ETA: 1s - loss: 0.2275 - acc: 0.926 - ETA: 1s - loss: 0.2317 - acc: 0.923 - ETA: 1s - loss: 0.2345 - acc: 0.921 - ETA: 1s - loss: 0.2342 - acc: 0.921 - ETA: 0s - loss: 0.2323 - acc: 0.922 - ETA: 0s - loss: 0.2294 - acc: 0.923 - ETA: 0s - loss: 0.2271 - acc: 0.924 - ETA: 0s - loss: 0.2261 - acc: 0.925 - ETA: 0s - loss: 0.2286 - acc: 0.924 - ETA: 0s - loss: 0.2284 - acc: 0.924 - ETA: 0s - loss: 0.2269 - acc: 0.925 - ETA: 0s - loss: 0.2260 - acc: 0.924 - ETA: 0s - loss: 0.2283 - acc: 0.923 - ETA: 0s - loss: 0.2287 - acc: 0.923 - ETA: 0s - loss: 0.2285 - acc: 0.923 - ETA: 0s - loss: 0.2299 - acc: 0.923 - ETA: 0s - loss: 0.2300 - acc: 0.922 - ETA: 0s - loss: 0.2306 - acc: 0.923 - 2s 599us/step - loss: 0.2307 - acc: 0.9235 - val_loss: 0.2901 - val_acc: 0.8929\n",
      "Epoch 25/150\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2000 - acc: 0.953 - ETA: 2s - loss: 0.1994 - acc: 0.953 - ETA: 1s - loss: 0.2030 - acc: 0.946 - ETA: 1s - loss: 0.1982 - acc: 0.946 - ETA: 1s - loss: 0.1910 - acc: 0.946 - ETA: 1s - loss: 0.2101 - acc: 0.936 - ETA: 1s - loss: 0.2106 - acc: 0.935 - ETA: 1s - loss: 0.2146 - acc: 0.932 - ETA: 1s - loss: 0.2131 - acc: 0.932 - ETA: 1s - loss: 0.2169 - acc: 0.930 - ETA: 1s - loss: 0.2246 - acc: 0.927 - ETA: 1s - loss: 0.2268 - acc: 0.926 - ETA: 1s - loss: 0.2261 - acc: 0.927 - ETA: 1s - loss: 0.2292 - acc: 0.924 - ETA: 1s - loss: 0.2252 - acc: 0.926 - ETA: 1s - loss: 0.2278 - acc: 0.923 - ETA: 1s - loss: 0.2301 - acc: 0.922 - ETA: 1s - loss: 0.2304 - acc: 0.921 - ETA: 1s - loss: 0.2352 - acc: 0.920 - ETA: 0s - loss: 0.2375 - acc: 0.919 - ETA: 0s - loss: 0.2340 - acc: 0.920 - ETA: 0s - loss: 0.2376 - acc: 0.919 - ETA: 0s - loss: 0.2373 - acc: 0.919 - ETA: 0s - loss: 0.2361 - acc: 0.919 - ETA: 0s - loss: 0.2355 - acc: 0.918 - ETA: 0s - loss: 0.2376 - acc: 0.917 - ETA: 0s - loss: 0.2361 - acc: 0.918 - ETA: 0s - loss: 0.2364 - acc: 0.917 - ETA: 0s - loss: 0.2349 - acc: 0.918 - ETA: 0s - loss: 0.2329 - acc: 0.920 - ETA: 0s - loss: 0.2339 - acc: 0.917 - ETA: 0s - loss: 0.2345 - acc: 0.917 - ETA: 0s - loss: 0.2348 - acc: 0.918 - ETA: 0s - loss: 0.2345 - acc: 0.917 - 3s 658us/step - loss: 0.2346 - acc: 0.9179 - val_loss: 0.3031 - val_acc: 0.8910\n",
      "Epoch 26/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3065 - acc: 0.875 - ETA: 2s - loss: 0.2298 - acc: 0.911 - ETA: 2s - loss: 0.2416 - acc: 0.915 - ETA: 2s - loss: 0.2229 - acc: 0.921 - ETA: 2s - loss: 0.2208 - acc: 0.920 - ETA: 1s - loss: 0.2345 - acc: 0.911 - ETA: 1s - loss: 0.2356 - acc: 0.911 - ETA: 1s - loss: 0.2353 - acc: 0.910 - ETA: 1s - loss: 0.2318 - acc: 0.912 - ETA: 1s - loss: 0.2345 - acc: 0.908 - ETA: 1s - loss: 0.2370 - acc: 0.909 - ETA: 1s - loss: 0.2425 - acc: 0.907 - ETA: 1s - loss: 0.2340 - acc: 0.912 - ETA: 1s - loss: 0.2290 - acc: 0.914 - ETA: 1s - loss: 0.2334 - acc: 0.912 - ETA: 1s - loss: 0.2313 - acc: 0.913 - ETA: 1s - loss: 0.2317 - acc: 0.914 - ETA: 1s - loss: 0.2314 - acc: 0.914 - ETA: 0s - loss: 0.2294 - acc: 0.915 - ETA: 0s - loss: 0.2323 - acc: 0.914 - ETA: 0s - loss: 0.2344 - acc: 0.913 - ETA: 0s - loss: 0.2328 - acc: 0.913 - ETA: 0s - loss: 0.2337 - acc: 0.912 - ETA: 0s - loss: 0.2365 - acc: 0.910 - ETA: 0s - loss: 0.2345 - acc: 0.912 - ETA: 0s - loss: 0.2380 - acc: 0.911 - ETA: 0s - loss: 0.2380 - acc: 0.911 - ETA: 0s - loss: 0.2380 - acc: 0.911 - ETA: 0s - loss: 0.2372 - acc: 0.912 - ETA: 0s - loss: 0.2368 - acc: 0.912 - ETA: 0s - loss: 0.2371 - acc: 0.913 - ETA: 0s - loss: 0.2366 - acc: 0.913 - 2s 610us/step - loss: 0.2364 - acc: 0.9139 - val_loss: 0.2970 - val_acc: 0.8904\n",
      "Epoch 27/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2387 - acc: 0.890 - ETA: 2s - loss: 0.2672 - acc: 0.901 - ETA: 2s - loss: 0.2344 - acc: 0.918 - ETA: 2s - loss: 0.2314 - acc: 0.919 - ETA: 2s - loss: 0.2230 - acc: 0.925 - ETA: 1s - loss: 0.2175 - acc: 0.933 - ETA: 1s - loss: 0.2141 - acc: 0.932 - ETA: 1s - loss: 0.2081 - acc: 0.935 - ETA: 1s - loss: 0.2153 - acc: 0.931 - ETA: 1s - loss: 0.2128 - acc: 0.929 - ETA: 1s - loss: 0.2084 - acc: 0.932 - ETA: 1s - loss: 0.2129 - acc: 0.929 - ETA: 1s - loss: 0.2142 - acc: 0.928 - ETA: 1s - loss: 0.2132 - acc: 0.930 - ETA: 1s - loss: 0.2121 - acc: 0.930 - ETA: 1s - loss: 0.2096 - acc: 0.931 - ETA: 1s - loss: 0.2101 - acc: 0.931 - ETA: 1s - loss: 0.2134 - acc: 0.928 - ETA: 1s - loss: 0.2167 - acc: 0.926 - ETA: 0s - loss: 0.2158 - acc: 0.927 - ETA: 0s - loss: 0.2189 - acc: 0.927 - ETA: 0s - loss: 0.2199 - acc: 0.926 - ETA: 0s - loss: 0.2201 - acc: 0.925 - ETA: 0s - loss: 0.2228 - acc: 0.923 - ETA: 0s - loss: 0.2214 - acc: 0.923 - ETA: 0s - loss: 0.2215 - acc: 0.923 - ETA: 0s - loss: 0.2232 - acc: 0.923 - ETA: 0s - loss: 0.2222 - acc: 0.923 - ETA: 0s - loss: 0.2218 - acc: 0.924 - ETA: 0s - loss: 0.2239 - acc: 0.923 - ETA: 0s - loss: 0.2263 - acc: 0.921 - ETA: 0s - loss: 0.2281 - acc: 0.920 - ETA: 0s - loss: 0.2288 - acc: 0.920 - 3s 656us/step - loss: 0.2276 - acc: 0.9208 - val_loss: 0.3096 - val_acc: 0.8878\n",
      "Epoch 28/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2050 - acc: 0.937 - ETA: 2s - loss: 0.2479 - acc: 0.901 - ETA: 2s - loss: 0.2409 - acc: 0.915 - ETA: 2s - loss: 0.2207 - acc: 0.928 - ETA: 2s - loss: 0.2261 - acc: 0.925 - ETA: 1s - loss: 0.2199 - acc: 0.929 - ETA: 1s - loss: 0.2095 - acc: 0.936 - ETA: 1s - loss: 0.2126 - acc: 0.932 - ETA: 1s - loss: 0.2064 - acc: 0.935 - ETA: 1s - loss: 0.2025 - acc: 0.937 - ETA: 1s - loss: 0.1976 - acc: 0.939 - ETA: 1s - loss: 0.1989 - acc: 0.938 - ETA: 1s - loss: 0.1993 - acc: 0.937 - ETA: 1s - loss: 0.1991 - acc: 0.937 - ETA: 1s - loss: 0.2008 - acc: 0.936 - ETA: 1s - loss: 0.2026 - acc: 0.936 - ETA: 1s - loss: 0.2054 - acc: 0.933 - ETA: 1s - loss: 0.2113 - acc: 0.930 - ETA: 1s - loss: 0.2100 - acc: 0.930 - ETA: 0s - loss: 0.2120 - acc: 0.929 - ETA: 0s - loss: 0.2136 - acc: 0.928 - ETA: 0s - loss: 0.2177 - acc: 0.928 - ETA: 0s - loss: 0.2171 - acc: 0.928 - ETA: 0s - loss: 0.2181 - acc: 0.928 - ETA: 0s - loss: 0.2211 - acc: 0.927 - ETA: 0s - loss: 0.2207 - acc: 0.927 - ETA: 0s - loss: 0.2247 - acc: 0.926 - ETA: 0s - loss: 0.2237 - acc: 0.927 - ETA: 0s - loss: 0.2241 - acc: 0.927 - ETA: 0s - loss: 0.2239 - acc: 0.927 - ETA: 0s - loss: 0.2265 - acc: 0.925 - ETA: 0s - loss: 0.2249 - acc: 0.926 - ETA: 0s - loss: 0.2250 - acc: 0.926 - 2s 602us/step - loss: 0.2245 - acc: 0.9265 - val_loss: 0.3070 - val_acc: 0.9013\n",
      "Epoch 29/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2574 - acc: 0.906 - ETA: 2s - loss: 0.2763 - acc: 0.901 - ETA: 2s - loss: 0.2509 - acc: 0.918 - ETA: 2s - loss: 0.2294 - acc: 0.921 - ETA: 2s - loss: 0.2434 - acc: 0.911 - ETA: 2s - loss: 0.2316 - acc: 0.919 - ETA: 1s - loss: 0.2267 - acc: 0.921 - ETA: 1s - loss: 0.2290 - acc: 0.919 - ETA: 1s - loss: 0.2300 - acc: 0.920 - ETA: 1s - loss: 0.2348 - acc: 0.921 - ETA: 1s - loss: 0.2403 - acc: 0.919 - ETA: 1s - loss: 0.2412 - acc: 0.920 - ETA: 1s - loss: 0.2368 - acc: 0.923 - ETA: 1s - loss: 0.2341 - acc: 0.925 - ETA: 1s - loss: 0.2309 - acc: 0.926 - ETA: 1s - loss: 0.2307 - acc: 0.926 - ETA: 1s - loss: 0.2310 - acc: 0.925 - ETA: 1s - loss: 0.2361 - acc: 0.923 - ETA: 0s - loss: 0.2361 - acc: 0.924 - ETA: 0s - loss: 0.2433 - acc: 0.921 - ETA: 0s - loss: 0.2405 - acc: 0.922 - ETA: 0s - loss: 0.2437 - acc: 0.920 - ETA: 0s - loss: 0.2467 - acc: 0.919 - ETA: 0s - loss: 0.2471 - acc: 0.918 - ETA: 0s - loss: 0.2481 - acc: 0.918 - ETA: 0s - loss: 0.2475 - acc: 0.917 - ETA: 0s - loss: 0.2448 - acc: 0.919 - ETA: 0s - loss: 0.2435 - acc: 0.920 - ETA: 0s - loss: 0.2409 - acc: 0.921 - ETA: 0s - loss: 0.2414 - acc: 0.920 - ETA: 0s - loss: 0.2411 - acc: 0.920 - ETA: 0s - loss: 0.2447 - acc: 0.918 - 2s 597us/step - loss: 0.2445 - acc: 0.9189 - val_loss: 0.3112 - val_acc: 0.9000\n",
      "Epoch 30/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3969 - acc: 0.875 - ETA: 2s - loss: 0.2593 - acc: 0.927 - ETA: 2s - loss: 0.2549 - acc: 0.921 - ETA: 1s - loss: 0.2474 - acc: 0.921 - ETA: 1s - loss: 0.2368 - acc: 0.927 - ETA: 1s - loss: 0.2396 - acc: 0.926 - ETA: 1s - loss: 0.2543 - acc: 0.918 - ETA: 1s - loss: 0.2465 - acc: 0.920 - ETA: 1s - loss: 0.2487 - acc: 0.915 - ETA: 1s - loss: 0.2517 - acc: 0.912 - ETA: 1s - loss: 0.2523 - acc: 0.912 - ETA: 1s - loss: 0.2514 - acc: 0.913 - ETA: 1s - loss: 0.2480 - acc: 0.915 - ETA: 1s - loss: 0.2458 - acc: 0.916 - ETA: 1s - loss: 0.2492 - acc: 0.915 - ETA: 1s - loss: 0.2524 - acc: 0.912 - ETA: 1s - loss: 0.2518 - acc: 0.912 - ETA: 1s - loss: 0.2496 - acc: 0.913 - ETA: 0s - loss: 0.2469 - acc: 0.915 - ETA: 0s - loss: 0.2449 - acc: 0.915 - ETA: 0s - loss: 0.2453 - acc: 0.914 - ETA: 0s - loss: 0.2426 - acc: 0.915 - ETA: 0s - loss: 0.2406 - acc: 0.916 - ETA: 0s - loss: 0.2392 - acc: 0.917 - ETA: 0s - loss: 0.2368 - acc: 0.918 - ETA: 0s - loss: 0.2345 - acc: 0.918 - ETA: 0s - loss: 0.2339 - acc: 0.918 - ETA: 0s - loss: 0.2343 - acc: 0.918 - ETA: 0s - loss: 0.2337 - acc: 0.919 - ETA: 0s - loss: 0.2313 - acc: 0.920 - ETA: 0s - loss: 0.2292 - acc: 0.921 - ETA: 0s - loss: 0.2300 - acc: 0.921 - 2s 591us/step - loss: 0.2307 - acc: 0.9213 - val_loss: 0.3057 - val_acc: 0.8923\n",
      "Epoch 31/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1436 - acc: 0.937 - ETA: 2s - loss: 0.2168 - acc: 0.916 - ETA: 2s - loss: 0.2062 - acc: 0.934 - ETA: 2s - loss: 0.2105 - acc: 0.933 - ETA: 1s - loss: 0.2156 - acc: 0.927 - ETA: 1s - loss: 0.2061 - acc: 0.931 - ETA: 1s - loss: 0.2089 - acc: 0.930 - ETA: 1s - loss: 0.2070 - acc: 0.932 - ETA: 1s - loss: 0.2108 - acc: 0.929 - ETA: 1s - loss: 0.2119 - acc: 0.926 - ETA: 1s - loss: 0.2116 - acc: 0.925 - ETA: 1s - loss: 0.2155 - acc: 0.923 - ETA: 1s - loss: 0.2192 - acc: 0.922 - ETA: 1s - loss: 0.2200 - acc: 0.923 - ETA: 1s - loss: 0.2201 - acc: 0.921 - ETA: 1s - loss: 0.2199 - acc: 0.922 - ETA: 1s - loss: 0.2209 - acc: 0.922 - ETA: 1s - loss: 0.2187 - acc: 0.925 - ETA: 1s - loss: 0.2196 - acc: 0.924 - ETA: 1s - loss: 0.2223 - acc: 0.924 - ETA: 0s - loss: 0.2214 - acc: 0.924 - ETA: 0s - loss: 0.2226 - acc: 0.924 - ETA: 0s - loss: 0.2216 - acc: 0.925 - ETA: 0s - loss: 0.2185 - acc: 0.926 - ETA: 0s - loss: 0.2172 - acc: 0.927 - ETA: 0s - loss: 0.2139 - acc: 0.928 - ETA: 0s - loss: 0.2141 - acc: 0.928 - ETA: 0s - loss: 0.2146 - acc: 0.928 - ETA: 0s - loss: 0.2151 - acc: 0.927 - ETA: 0s - loss: 0.2161 - acc: 0.927 - ETA: 0s - loss: 0.2145 - acc: 0.927 - ETA: 0s - loss: 0.2173 - acc: 0.926 - ETA: 0s - loss: 0.2174 - acc: 0.926 - ETA: 0s - loss: 0.2167 - acc: 0.926 - 3s 672us/step - loss: 0.2176 - acc: 0.9257 - val_loss: 0.2992 - val_acc: 0.8987\n",
      "Epoch 32/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1814 - acc: 0.953 - ETA: 2s - loss: 0.1654 - acc: 0.942 - ETA: 2s - loss: 0.1685 - acc: 0.950 - ETA: 2s - loss: 0.1986 - acc: 0.937 - ETA: 1s - loss: 0.2091 - acc: 0.932 - ETA: 1s - loss: 0.2094 - acc: 0.929 - ETA: 1s - loss: 0.2050 - acc: 0.931 - ETA: 1s - loss: 0.2088 - acc: 0.929 - ETA: 1s - loss: 0.2082 - acc: 0.930 - ETA: 1s - loss: 0.2067 - acc: 0.929 - ETA: 1s - loss: 0.2039 - acc: 0.930 - ETA: 1s - loss: 0.2021 - acc: 0.930 - ETA: 1s - loss: 0.2027 - acc: 0.929 - ETA: 1s - loss: 0.2012 - acc: 0.928 - ETA: 1s - loss: 0.2002 - acc: 0.930 - ETA: 1s - loss: 0.2007 - acc: 0.930 - ETA: 1s - loss: 0.1983 - acc: 0.931 - ETA: 1s - loss: 0.1986 - acc: 0.931 - ETA: 0s - loss: 0.1985 - acc: 0.932 - ETA: 0s - loss: 0.2050 - acc: 0.929 - ETA: 0s - loss: 0.2049 - acc: 0.929 - ETA: 0s - loss: 0.2083 - acc: 0.928 - ETA: 0s - loss: 0.2103 - acc: 0.927 - ETA: 0s - loss: 0.2087 - acc: 0.927 - ETA: 0s - loss: 0.2082 - acc: 0.927 - ETA: 0s - loss: 0.2064 - acc: 0.928 - ETA: 0s - loss: 0.2058 - acc: 0.929 - ETA: 0s - loss: 0.2066 - acc: 0.929 - ETA: 0s - loss: 0.2082 - acc: 0.928 - ETA: 0s - loss: 0.2102 - acc: 0.928 - ETA: 0s - loss: 0.2107 - acc: 0.927 - ETA: 0s - loss: 0.2140 - acc: 0.925 - 2s 609us/step - loss: 0.2131 - acc: 0.9265 - val_loss: 0.3024 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2253 - acc: 0.906 - ETA: 2s - loss: 0.1905 - acc: 0.942 - ETA: 2s - loss: 0.1907 - acc: 0.937 - ETA: 2s - loss: 0.1897 - acc: 0.939 - ETA: 2s - loss: 0.1993 - acc: 0.927 - ETA: 1s - loss: 0.2039 - acc: 0.930 - ETA: 1s - loss: 0.2034 - acc: 0.931 - ETA: 1s - loss: 0.2073 - acc: 0.933 - ETA: 1s - loss: 0.2138 - acc: 0.931 - ETA: 1s - loss: 0.2147 - acc: 0.930 - ETA: 1s - loss: 0.2132 - acc: 0.932 - ETA: 1s - loss: 0.2150 - acc: 0.929 - ETA: 1s - loss: 0.2107 - acc: 0.931 - ETA: 1s - loss: 0.2089 - acc: 0.930 - ETA: 1s - loss: 0.2078 - acc: 0.930 - ETA: 1s - loss: 0.2112 - acc: 0.928 - ETA: 1s - loss: 0.2139 - acc: 0.927 - ETA: 1s - loss: 0.2148 - acc: 0.926 - ETA: 1s - loss: 0.2159 - acc: 0.926 - ETA: 0s - loss: 0.2158 - acc: 0.927 - ETA: 0s - loss: 0.2179 - acc: 0.926 - ETA: 0s - loss: 0.2166 - acc: 0.926 - ETA: 0s - loss: 0.2179 - acc: 0.927 - ETA: 0s - loss: 0.2186 - acc: 0.927 - ETA: 0s - loss: 0.2178 - acc: 0.928 - ETA: 0s - loss: 0.2197 - acc: 0.926 - ETA: 0s - loss: 0.2198 - acc: 0.927 - ETA: 0s - loss: 0.2192 - acc: 0.928 - ETA: 0s - loss: 0.2198 - acc: 0.927 - ETA: 0s - loss: 0.2200 - acc: 0.927 - ETA: 0s - loss: 0.2202 - acc: 0.926 - ETA: 0s - loss: 0.2244 - acc: 0.926 - ETA: 0s - loss: 0.2271 - acc: 0.925 - 3s 631us/step - loss: 0.2267 - acc: 0.9250 - val_loss: 0.2824 - val_acc: 0.8987\n",
      "Epoch 34/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2503 - acc: 0.906 - ETA: 2s - loss: 0.2786 - acc: 0.890 - ETA: 2s - loss: 0.2426 - acc: 0.912 - ETA: 2s - loss: 0.2238 - acc: 0.926 - ETA: 2s - loss: 0.2245 - acc: 0.925 - ETA: 1s - loss: 0.2248 - acc: 0.921 - ETA: 1s - loss: 0.2204 - acc: 0.924 - ETA: 1s - loss: 0.2190 - acc: 0.927 - ETA: 1s - loss: 0.2282 - acc: 0.923 - ETA: 1s - loss: 0.2251 - acc: 0.924 - ETA: 1s - loss: 0.2267 - acc: 0.923 - ETA: 1s - loss: 0.2280 - acc: 0.922 - ETA: 1s - loss: 0.2336 - acc: 0.920 - ETA: 1s - loss: 0.2364 - acc: 0.920 - ETA: 1s - loss: 0.2386 - acc: 0.920 - ETA: 1s - loss: 0.2339 - acc: 0.923 - ETA: 1s - loss: 0.2335 - acc: 0.925 - ETA: 1s - loss: 0.2341 - acc: 0.925 - ETA: 0s - loss: 0.2323 - acc: 0.925 - ETA: 0s - loss: 0.2322 - acc: 0.925 - ETA: 0s - loss: 0.2333 - acc: 0.924 - ETA: 0s - loss: 0.2349 - acc: 0.923 - ETA: 0s - loss: 0.2365 - acc: 0.922 - ETA: 0s - loss: 0.2349 - acc: 0.922 - ETA: 0s - loss: 0.2348 - acc: 0.922 - ETA: 0s - loss: 0.2354 - acc: 0.921 - ETA: 0s - loss: 0.2357 - acc: 0.920 - ETA: 0s - loss: 0.2350 - acc: 0.921 - ETA: 0s - loss: 0.2338 - acc: 0.921 - ETA: 0s - loss: 0.2333 - acc: 0.922 - ETA: 0s - loss: 0.2327 - acc: 0.922 - ETA: 0s - loss: 0.2324 - acc: 0.922 - ETA: 0s - loss: 0.2338 - acc: 0.921 - ETA: 0s - loss: 0.2340 - acc: 0.921 - 3s 651us/step - loss: 0.2336 - acc: 0.9216 - val_loss: 0.3300 - val_acc: 0.8917\n",
      "Epoch 35/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3227 - acc: 0.875 - ETA: 2s - loss: 0.2664 - acc: 0.911 - ETA: 2s - loss: 0.2647 - acc: 0.903 - ETA: 2s - loss: 0.2379 - acc: 0.919 - ETA: 2s - loss: 0.2536 - acc: 0.911 - ETA: 1s - loss: 0.2362 - acc: 0.921 - ETA: 1s - loss: 0.2240 - acc: 0.927 - ETA: 1s - loss: 0.2271 - acc: 0.929 - ETA: 1s - loss: 0.2235 - acc: 0.932 - ETA: 1s - loss: 0.2231 - acc: 0.930 - ETA: 1s - loss: 0.2199 - acc: 0.930 - ETA: 1s - loss: 0.2277 - acc: 0.928 - ETA: 1s - loss: 0.2221 - acc: 0.930 - ETA: 1s - loss: 0.2173 - acc: 0.934 - ETA: 1s - loss: 0.2190 - acc: 0.932 - ETA: 1s - loss: 0.2217 - acc: 0.932 - ETA: 1s - loss: 0.2185 - acc: 0.932 - ETA: 1s - loss: 0.2190 - acc: 0.932 - ETA: 1s - loss: 0.2200 - acc: 0.931 - ETA: 0s - loss: 0.2211 - acc: 0.929 - ETA: 0s - loss: 0.2196 - acc: 0.930 - ETA: 0s - loss: 0.2164 - acc: 0.932 - ETA: 0s - loss: 0.2170 - acc: 0.931 - ETA: 0s - loss: 0.2186 - acc: 0.931 - ETA: 0s - loss: 0.2195 - acc: 0.931 - ETA: 0s - loss: 0.2179 - acc: 0.931 - ETA: 0s - loss: 0.2154 - acc: 0.932 - ETA: 0s - loss: 0.2146 - acc: 0.933 - ETA: 0s - loss: 0.2160 - acc: 0.932 - ETA: 0s - loss: 0.2165 - acc: 0.932 - ETA: 0s - loss: 0.2173 - acc: 0.932 - ETA: 0s - loss: 0.2158 - acc: 0.932 - 3s 628us/step - loss: 0.2152 - acc: 0.9326 - val_loss: 0.2994 - val_acc: 0.8897\n",
      "Epoch 36/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2659 - acc: 0.921 - ETA: 2s - loss: 0.2298 - acc: 0.927 - ETA: 2s - loss: 0.2174 - acc: 0.925 - ETA: 2s - loss: 0.2056 - acc: 0.930 - ETA: 2s - loss: 0.1958 - acc: 0.934 - ETA: 2s - loss: 0.1923 - acc: 0.934 - ETA: 2s - loss: 0.1994 - acc: 0.931 - ETA: 2s - loss: 0.2024 - acc: 0.929 - ETA: 2s - loss: 0.2062 - acc: 0.926 - ETA: 2s - loss: 0.2013 - acc: 0.929 - ETA: 2s - loss: 0.2078 - acc: 0.925 - ETA: 2s - loss: 0.2088 - acc: 0.923 - ETA: 2s - loss: 0.2091 - acc: 0.923 - ETA: 2s - loss: 0.2064 - acc: 0.925 - ETA: 2s - loss: 0.2050 - acc: 0.925 - ETA: 2s - loss: 0.2062 - acc: 0.923 - ETA: 2s - loss: 0.2080 - acc: 0.921 - ETA: 1s - loss: 0.2093 - acc: 0.921 - ETA: 1s - loss: 0.2075 - acc: 0.923 - ETA: 1s - loss: 0.2077 - acc: 0.924 - ETA: 1s - loss: 0.2070 - acc: 0.924 - ETA: 1s - loss: 0.2064 - acc: 0.925 - ETA: 1s - loss: 0.2023 - acc: 0.928 - ETA: 1s - loss: 0.2020 - acc: 0.928 - ETA: 1s - loss: 0.2077 - acc: 0.927 - ETA: 1s - loss: 0.2066 - acc: 0.928 - ETA: 1s - loss: 0.2081 - acc: 0.927 - ETA: 1s - loss: 0.2068 - acc: 0.927 - ETA: 1s - loss: 0.2064 - acc: 0.928 - ETA: 1s - loss: 0.2053 - acc: 0.929 - ETA: 1s - loss: 0.2038 - acc: 0.930 - ETA: 1s - loss: 0.2073 - acc: 0.928 - ETA: 0s - loss: 0.2067 - acc: 0.929 - ETA: 0s - loss: 0.2085 - acc: 0.928 - ETA: 0s - loss: 0.2075 - acc: 0.927 - ETA: 0s - loss: 0.2094 - acc: 0.927 - ETA: 0s - loss: 0.2083 - acc: 0.927 - ETA: 0s - loss: 0.2081 - acc: 0.928 - ETA: 0s - loss: 0.2071 - acc: 0.928 - ETA: 0s - loss: 0.2113 - acc: 0.927 - ETA: 0s - loss: 0.2100 - acc: 0.927 - ETA: 0s - loss: 0.2103 - acc: 0.927 - ETA: 0s - loss: 0.2110 - acc: 0.927 - ETA: 0s - loss: 0.2125 - acc: 0.926 - 3s 802us/step - loss: 0.2117 - acc: 0.9277 - val_loss: 0.3326 - val_acc: 0.8910\n",
      "Epoch 37/150\n",
      "4067/4067 [==============================] - ETA: 3s - loss: 0.2504 - acc: 0.875 - ETA: 3s - loss: 0.2485 - acc: 0.898 - ETA: 3s - loss: 0.2653 - acc: 0.890 - ETA: 3s - loss: 0.2540 - acc: 0.915 - ETA: 2s - loss: 0.2298 - acc: 0.921 - ETA: 2s - loss: 0.2422 - acc: 0.914 - ETA: 2s - loss: 0.2398 - acc: 0.914 - ETA: 2s - loss: 0.2369 - acc: 0.918 - ETA: 2s - loss: 0.2387 - acc: 0.917 - ETA: 1s - loss: 0.2448 - acc: 0.914 - ETA: 1s - loss: 0.2474 - acc: 0.914 - ETA: 1s - loss: 0.2486 - acc: 0.915 - ETA: 1s - loss: 0.2463 - acc: 0.916 - ETA: 1s - loss: 0.2436 - acc: 0.918 - ETA: 1s - loss: 0.2453 - acc: 0.919 - ETA: 1s - loss: 0.2446 - acc: 0.919 - ETA: 1s - loss: 0.2454 - acc: 0.919 - ETA: 1s - loss: 0.2433 - acc: 0.920 - ETA: 1s - loss: 0.2412 - acc: 0.921 - ETA: 1s - loss: 0.2387 - acc: 0.923 - ETA: 0s - loss: 0.2388 - acc: 0.921 - ETA: 0s - loss: 0.2404 - acc: 0.921 - ETA: 0s - loss: 0.2395 - acc: 0.921 - ETA: 0s - loss: 0.2396 - acc: 0.921 - ETA: 0s - loss: 0.2386 - acc: 0.920 - ETA: 0s - loss: 0.2437 - acc: 0.919 - ETA: 0s - loss: 0.2394 - acc: 0.921 - ETA: 0s - loss: 0.2413 - acc: 0.920 - ETA: 0s - loss: 0.2424 - acc: 0.920 - ETA: 0s - loss: 0.2394 - acc: 0.921 - ETA: 0s - loss: 0.2367 - acc: 0.922 - ETA: 0s - loss: 0.2370 - acc: 0.923 - ETA: 0s - loss: 0.2380 - acc: 0.922 - 3s 629us/step - loss: 0.2382 - acc: 0.9221 - val_loss: 0.2897 - val_acc: 0.9090\n",
      "Epoch 38/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2750 - acc: 0.875 - ETA: 2s - loss: 0.2358 - acc: 0.901 - ETA: 2s - loss: 0.2207 - acc: 0.912 - ETA: 2s - loss: 0.2500 - acc: 0.904 - ETA: 2s - loss: 0.2468 - acc: 0.906 - ETA: 1s - loss: 0.2438 - acc: 0.913 - ETA: 1s - loss: 0.2678 - acc: 0.908 - ETA: 1s - loss: 0.2608 - acc: 0.911 - ETA: 1s - loss: 0.2624 - acc: 0.909 - ETA: 1s - loss: 0.2528 - acc: 0.914 - ETA: 1s - loss: 0.2533 - acc: 0.913 - ETA: 1s - loss: 0.2552 - acc: 0.915 - ETA: 1s - loss: 0.2640 - acc: 0.912 - ETA: 1s - loss: 0.2573 - acc: 0.916 - ETA: 1s - loss: 0.2530 - acc: 0.917 - ETA: 1s - loss: 0.2509 - acc: 0.916 - ETA: 1s - loss: 0.2446 - acc: 0.920 - ETA: 1s - loss: 0.2456 - acc: 0.919 - ETA: 0s - loss: 0.2404 - acc: 0.921 - ETA: 0s - loss: 0.2416 - acc: 0.922 - ETA: 0s - loss: 0.2450 - acc: 0.921 - ETA: 0s - loss: 0.2446 - acc: 0.921 - ETA: 0s - loss: 0.2438 - acc: 0.921 - ETA: 0s - loss: 0.2455 - acc: 0.920 - ETA: 0s - loss: 0.2450 - acc: 0.920 - ETA: 0s - loss: 0.2412 - acc: 0.922 - ETA: 0s - loss: 0.2401 - acc: 0.923 - ETA: 0s - loss: 0.2407 - acc: 0.923 - ETA: 0s - loss: 0.2406 - acc: 0.923 - ETA: 0s - loss: 0.2402 - acc: 0.923 - ETA: 0s - loss: 0.2396 - acc: 0.921 - ETA: 0s - loss: 0.2379 - acc: 0.922 - ETA: 0s - loss: 0.2391 - acc: 0.921 - ETA: 0s - loss: 0.2409 - acc: 0.920 - ETA: 0s - loss: 0.2400 - acc: 0.920 - ETA: 0s - loss: 0.2388 - acc: 0.921 - 3s 677us/step - loss: 0.2387 - acc: 0.9216 - val_loss: 0.2877 - val_acc: 0.8929\n",
      "Epoch 39/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2307 - acc: 0.906 - ETA: 2s - loss: 0.2387 - acc: 0.895 - ETA: 2s - loss: 0.2253 - acc: 0.912 - ETA: 2s - loss: 0.2071 - acc: 0.924 - ETA: 2s - loss: 0.2089 - acc: 0.926 - ETA: 2s - loss: 0.2037 - acc: 0.927 - ETA: 2s - loss: 0.2039 - acc: 0.932 - ETA: 2s - loss: 0.1924 - acc: 0.940 - ETA: 2s - loss: 0.1948 - acc: 0.943 - ETA: 2s - loss: 0.1942 - acc: 0.941 - ETA: 2s - loss: 0.1927 - acc: 0.941 - ETA: 2s - loss: 0.1973 - acc: 0.939 - ETA: 1s - loss: 0.1987 - acc: 0.939 - ETA: 1s - loss: 0.1958 - acc: 0.941 - ETA: 1s - loss: 0.1963 - acc: 0.942 - ETA: 1s - loss: 0.1966 - acc: 0.941 - ETA: 1s - loss: 0.1958 - acc: 0.939 - ETA: 1s - loss: 0.1953 - acc: 0.940 - ETA: 1s - loss: 0.1948 - acc: 0.939 - ETA: 1s - loss: 0.1969 - acc: 0.938 - ETA: 1s - loss: 0.2005 - acc: 0.937 - ETA: 1s - loss: 0.1983 - acc: 0.937 - ETA: 1s - loss: 0.2003 - acc: 0.936 - ETA: 1s - loss: 0.2022 - acc: 0.935 - ETA: 1s - loss: 0.2019 - acc: 0.935 - ETA: 1s - loss: 0.2054 - acc: 0.934 - ETA: 1s - loss: 0.2053 - acc: 0.933 - ETA: 0s - loss: 0.2077 - acc: 0.932 - ETA: 0s - loss: 0.2091 - acc: 0.932 - ETA: 0s - loss: 0.2066 - acc: 0.933 - ETA: 0s - loss: 0.2074 - acc: 0.934 - ETA: 0s - loss: 0.2071 - acc: 0.934 - ETA: 0s - loss: 0.2060 - acc: 0.935 - ETA: 0s - loss: 0.2061 - acc: 0.935 - ETA: 0s - loss: 0.2080 - acc: 0.934 - ETA: 0s - loss: 0.2081 - acc: 0.934 - ETA: 0s - loss: 0.2069 - acc: 0.934 - ETA: 0s - loss: 0.2058 - acc: 0.934 - ETA: 0s - loss: 0.2045 - acc: 0.934 - ETA: 0s - loss: 0.2045 - acc: 0.934 - 3s 722us/step - loss: 0.2048 - acc: 0.9346 - val_loss: 0.2821 - val_acc: 0.9096\n",
      "Epoch 40/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1490 - acc: 0.968 - ETA: 2s - loss: 0.1426 - acc: 0.968 - ETA: 2s - loss: 0.1332 - acc: 0.971 - ETA: 2s - loss: 0.1662 - acc: 0.950 - ETA: 2s - loss: 0.1710 - acc: 0.943 - ETA: 2s - loss: 0.1878 - acc: 0.935 - ETA: 2s - loss: 0.1811 - acc: 0.941 - ETA: 2s - loss: 0.1910 - acc: 0.936 - ETA: 2s - loss: 0.1946 - acc: 0.935 - ETA: 1s - loss: 0.1893 - acc: 0.936 - ETA: 1s - loss: 0.1940 - acc: 0.934 - ETA: 1s - loss: 0.1972 - acc: 0.933 - ETA: 1s - loss: 0.1964 - acc: 0.933 - ETA: 1s - loss: 0.1947 - acc: 0.933 - ETA: 1s - loss: 0.1943 - acc: 0.932 - ETA: 1s - loss: 0.1899 - acc: 0.935 - ETA: 1s - loss: 0.1905 - acc: 0.933 - ETA: 1s - loss: 0.1873 - acc: 0.935 - ETA: 1s - loss: 0.1850 - acc: 0.937 - ETA: 1s - loss: 0.1891 - acc: 0.935 - ETA: 1s - loss: 0.1899 - acc: 0.935 - ETA: 0s - loss: 0.1903 - acc: 0.936 - ETA: 0s - loss: 0.1942 - acc: 0.935 - ETA: 0s - loss: 0.1981 - acc: 0.934 - ETA: 0s - loss: 0.1975 - acc: 0.934 - ETA: 0s - loss: 0.1958 - acc: 0.935 - ETA: 0s - loss: 0.1938 - acc: 0.936 - ETA: 0s - loss: 0.1957 - acc: 0.936 - ETA: 0s - loss: 0.1960 - acc: 0.936 - ETA: 0s - loss: 0.2013 - acc: 0.934 - ETA: 0s - loss: 0.2032 - acc: 0.933 - ETA: 0s - loss: 0.2043 - acc: 0.933 - ETA: 0s - loss: 0.2065 - acc: 0.932 - ETA: 0s - loss: 0.2087 - acc: 0.932 - 3s 642us/step - loss: 0.2101 - acc: 0.9314 - val_loss: 0.2949 - val_acc: 0.9019\n",
      "Epoch 41/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2181 - acc: 0.953 - ETA: 2s - loss: 0.2178 - acc: 0.942 - ETA: 2s - loss: 0.2055 - acc: 0.950 - ETA: 2s - loss: 0.1973 - acc: 0.946 - ETA: 2s - loss: 0.1861 - acc: 0.949 - ETA: 1s - loss: 0.1842 - acc: 0.948 - ETA: 1s - loss: 0.1908 - acc: 0.943 - ETA: 1s - loss: 0.1951 - acc: 0.942 - ETA: 1s - loss: 0.2006 - acc: 0.941 - ETA: 1s - loss: 0.1954 - acc: 0.942 - ETA: 1s - loss: 0.1979 - acc: 0.940 - ETA: 1s - loss: 0.2047 - acc: 0.936 - ETA: 1s - loss: 0.2008 - acc: 0.938 - ETA: 1s - loss: 0.1983 - acc: 0.940 - ETA: 1s - loss: 0.1997 - acc: 0.939 - ETA: 1s - loss: 0.1981 - acc: 0.941 - ETA: 1s - loss: 0.1968 - acc: 0.941 - ETA: 1s - loss: 0.1970 - acc: 0.941 - ETA: 1s - loss: 0.1992 - acc: 0.939 - ETA: 0s - loss: 0.2040 - acc: 0.937 - ETA: 0s - loss: 0.2029 - acc: 0.937 - ETA: 0s - loss: 0.2045 - acc: 0.937 - ETA: 0s - loss: 0.2078 - acc: 0.936 - ETA: 0s - loss: 0.2074 - acc: 0.936 - ETA: 0s - loss: 0.2109 - acc: 0.934 - ETA: 0s - loss: 0.2184 - acc: 0.932 - ETA: 0s - loss: 0.2227 - acc: 0.931 - ETA: 0s - loss: 0.2235 - acc: 0.931 - ETA: 0s - loss: 0.2253 - acc: 0.930 - ETA: 0s - loss: 0.2309 - acc: 0.929 - ETA: 0s - loss: 0.2334 - acc: 0.927 - ETA: 0s - loss: 0.2323 - acc: 0.928 - 3s 622us/step - loss: 0.2318 - acc: 0.9282 - val_loss: 0.3183 - val_acc: 0.9058\n",
      "Epoch 42/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1828 - acc: 0.937 - ETA: 2s - loss: 0.2128 - acc: 0.932 - ETA: 2s - loss: 0.2107 - acc: 0.941 - ETA: 2s - loss: 0.2154 - acc: 0.940 - ETA: 2s - loss: 0.2198 - acc: 0.937 - ETA: 2s - loss: 0.2141 - acc: 0.939 - ETA: 2s - loss: 0.2199 - acc: 0.934 - ETA: 2s - loss: 0.2102 - acc: 0.938 - ETA: 1s - loss: 0.2214 - acc: 0.932 - ETA: 1s - loss: 0.2142 - acc: 0.936 - ETA: 1s - loss: 0.2119 - acc: 0.935 - ETA: 1s - loss: 0.2124 - acc: 0.936 - ETA: 1s - loss: 0.2138 - acc: 0.933 - ETA: 1s - loss: 0.2192 - acc: 0.931 - ETA: 1s - loss: 0.2167 - acc: 0.932 - ETA: 1s - loss: 0.2192 - acc: 0.930 - ETA: 1s - loss: 0.2180 - acc: 0.930 - ETA: 1s - loss: 0.2195 - acc: 0.928 - ETA: 1s - loss: 0.2176 - acc: 0.929 - ETA: 1s - loss: 0.2160 - acc: 0.931 - ETA: 1s - loss: 0.2160 - acc: 0.932 - ETA: 1s - loss: 0.2136 - acc: 0.932 - ETA: 1s - loss: 0.2145 - acc: 0.931 - ETA: 0s - loss: 0.2157 - acc: 0.931 - ETA: 0s - loss: 0.2168 - acc: 0.931 - ETA: 0s - loss: 0.2165 - acc: 0.932 - ETA: 0s - loss: 0.2154 - acc: 0.932 - ETA: 0s - loss: 0.2151 - acc: 0.932 - ETA: 0s - loss: 0.2154 - acc: 0.932 - ETA: 0s - loss: 0.2159 - acc: 0.932 - ETA: 0s - loss: 0.2144 - acc: 0.933 - ETA: 0s - loss: 0.2153 - acc: 0.932 - ETA: 0s - loss: 0.2120 - acc: 0.933 - ETA: 0s - loss: 0.2117 - acc: 0.934 - ETA: 0s - loss: 0.2123 - acc: 0.933 - 3s 666us/step - loss: 0.2121 - acc: 0.9336 - val_loss: 0.2677 - val_acc: 0.9167\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.0949 - acc: 1.000 - ETA: 2s - loss: 0.1297 - acc: 0.984 - ETA: 2s - loss: 0.1725 - acc: 0.953 - ETA: 2s - loss: 0.1891 - acc: 0.944 - ETA: 2s - loss: 0.1880 - acc: 0.946 - ETA: 2s - loss: 0.1873 - acc: 0.946 - ETA: 1s - loss: 0.1801 - acc: 0.949 - ETA: 1s - loss: 0.1853 - acc: 0.945 - ETA: 1s - loss: 0.1816 - acc: 0.947 - ETA: 1s - loss: 0.1875 - acc: 0.946 - ETA: 1s - loss: 0.1853 - acc: 0.948 - ETA: 1s - loss: 0.1891 - acc: 0.946 - ETA: 1s - loss: 0.1905 - acc: 0.945 - ETA: 1s - loss: 0.1916 - acc: 0.941 - ETA: 1s - loss: 0.1958 - acc: 0.940 - ETA: 1s - loss: 0.1987 - acc: 0.940 - ETA: 1s - loss: 0.2008 - acc: 0.938 - ETA: 1s - loss: 0.2034 - acc: 0.937 - ETA: 1s - loss: 0.2033 - acc: 0.937 - ETA: 0s - loss: 0.2038 - acc: 0.935 - ETA: 0s - loss: 0.2036 - acc: 0.937 - ETA: 0s - loss: 0.2028 - acc: 0.936 - ETA: 0s - loss: 0.2030 - acc: 0.935 - ETA: 0s - loss: 0.2046 - acc: 0.935 - ETA: 0s - loss: 0.2081 - acc: 0.933 - ETA: 0s - loss: 0.2069 - acc: 0.934 - ETA: 0s - loss: 0.2057 - acc: 0.935 - ETA: 0s - loss: 0.2087 - acc: 0.934 - ETA: 0s - loss: 0.2095 - acc: 0.933 - ETA: 0s - loss: 0.2097 - acc: 0.933 - ETA: 0s - loss: 0.2093 - acc: 0.933 - ETA: 0s - loss: 0.2082 - acc: 0.933 - 3s 624us/step - loss: 0.2094 - acc: 0.9326 - val_loss: 0.2769 - val_acc: 0.9083\n",
      "Epoch 44/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2650 - acc: 0.906 - ETA: 2s - loss: 0.2447 - acc: 0.906 - ETA: 2s - loss: 0.2205 - acc: 0.931 - ETA: 2s - loss: 0.2256 - acc: 0.924 - ETA: 2s - loss: 0.2235 - acc: 0.925 - ETA: 2s - loss: 0.2213 - acc: 0.926 - ETA: 2s - loss: 0.2168 - acc: 0.925 - ETA: 1s - loss: 0.2078 - acc: 0.928 - ETA: 1s - loss: 0.2165 - acc: 0.925 - ETA: 1s - loss: 0.2144 - acc: 0.928 - ETA: 1s - loss: 0.2119 - acc: 0.928 - ETA: 1s - loss: 0.2105 - acc: 0.929 - ETA: 1s - loss: 0.2102 - acc: 0.930 - ETA: 1s - loss: 0.2176 - acc: 0.926 - ETA: 1s - loss: 0.2199 - acc: 0.924 - ETA: 1s - loss: 0.2202 - acc: 0.925 - ETA: 1s - loss: 0.2188 - acc: 0.926 - ETA: 1s - loss: 0.2190 - acc: 0.925 - ETA: 1s - loss: 0.2208 - acc: 0.925 - ETA: 1s - loss: 0.2172 - acc: 0.927 - ETA: 1s - loss: 0.2176 - acc: 0.927 - ETA: 1s - loss: 0.2164 - acc: 0.927 - ETA: 1s - loss: 0.2144 - acc: 0.928 - ETA: 0s - loss: 0.2129 - acc: 0.929 - ETA: 0s - loss: 0.2121 - acc: 0.930 - ETA: 0s - loss: 0.2109 - acc: 0.931 - ETA: 0s - loss: 0.2123 - acc: 0.932 - ETA: 0s - loss: 0.2107 - acc: 0.931 - ETA: 0s - loss: 0.2081 - acc: 0.932 - ETA: 0s - loss: 0.2090 - acc: 0.932 - ETA: 0s - loss: 0.2061 - acc: 0.933 - ETA: 0s - loss: 0.2035 - acc: 0.935 - ETA: 0s - loss: 0.2042 - acc: 0.934 - ETA: 0s - loss: 0.2023 - acc: 0.935 - ETA: 0s - loss: 0.2045 - acc: 0.935 - 3s 659us/step - loss: 0.2045 - acc: 0.9358 - val_loss: 0.2712 - val_acc: 0.9128\n",
      "Epoch 45/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1846 - acc: 0.953 - ETA: 2s - loss: 0.2115 - acc: 0.927 - ETA: 2s - loss: 0.2072 - acc: 0.931 - ETA: 2s - loss: 0.1864 - acc: 0.939 - ETA: 1s - loss: 0.1994 - acc: 0.930 - ETA: 1s - loss: 0.1953 - acc: 0.930 - ETA: 1s - loss: 0.1949 - acc: 0.933 - ETA: 1s - loss: 0.1971 - acc: 0.932 - ETA: 1s - loss: 0.1963 - acc: 0.934 - ETA: 1s - loss: 0.1892 - acc: 0.938 - ETA: 1s - loss: 0.1908 - acc: 0.938 - ETA: 1s - loss: 0.1900 - acc: 0.938 - ETA: 1s - loss: 0.1893 - acc: 0.938 - ETA: 1s - loss: 0.1852 - acc: 0.942 - ETA: 1s - loss: 0.1853 - acc: 0.942 - ETA: 1s - loss: 0.1868 - acc: 0.943 - ETA: 1s - loss: 0.1880 - acc: 0.942 - ETA: 1s - loss: 0.1926 - acc: 0.940 - ETA: 1s - loss: 0.1937 - acc: 0.939 - ETA: 0s - loss: 0.1982 - acc: 0.938 - ETA: 0s - loss: 0.1968 - acc: 0.938 - ETA: 0s - loss: 0.1949 - acc: 0.939 - ETA: 0s - loss: 0.1965 - acc: 0.938 - ETA: 0s - loss: 0.1957 - acc: 0.939 - ETA: 0s - loss: 0.1972 - acc: 0.937 - ETA: 0s - loss: 0.1961 - acc: 0.938 - ETA: 0s - loss: 0.1968 - acc: 0.937 - ETA: 0s - loss: 0.1988 - acc: 0.936 - ETA: 0s - loss: 0.2017 - acc: 0.935 - ETA: 0s - loss: 0.1983 - acc: 0.937 - ETA: 0s - loss: 0.1967 - acc: 0.937 - ETA: 0s - loss: 0.1958 - acc: 0.938 - 3s 623us/step - loss: 0.1955 - acc: 0.9383 - val_loss: 0.2682 - val_acc: 0.9064\n",
      "Epoch 46/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3502 - acc: 0.859 - ETA: 2s - loss: 0.2054 - acc: 0.942 - ETA: 2s - loss: 0.1854 - acc: 0.946 - ETA: 2s - loss: 0.2214 - acc: 0.930 - ETA: 2s - loss: 0.2158 - acc: 0.932 - ETA: 1s - loss: 0.2420 - acc: 0.921 - ETA: 1s - loss: 0.2398 - acc: 0.921 - ETA: 1s - loss: 0.2396 - acc: 0.920 - ETA: 1s - loss: 0.2312 - acc: 0.921 - ETA: 1s - loss: 0.2300 - acc: 0.921 - ETA: 1s - loss: 0.2328 - acc: 0.918 - ETA: 1s - loss: 0.2291 - acc: 0.920 - ETA: 1s - loss: 0.2324 - acc: 0.917 - ETA: 1s - loss: 0.2348 - acc: 0.914 - ETA: 1s - loss: 0.2330 - acc: 0.914 - ETA: 1s - loss: 0.2353 - acc: 0.913 - ETA: 1s - loss: 0.2347 - acc: 0.914 - ETA: 1s - loss: 0.2312 - acc: 0.917 - ETA: 0s - loss: 0.2317 - acc: 0.916 - ETA: 0s - loss: 0.2322 - acc: 0.916 - ETA: 0s - loss: 0.2290 - acc: 0.919 - ETA: 0s - loss: 0.2295 - acc: 0.919 - ETA: 0s - loss: 0.2280 - acc: 0.920 - ETA: 0s - loss: 0.2246 - acc: 0.921 - ETA: 0s - loss: 0.2240 - acc: 0.922 - ETA: 0s - loss: 0.2229 - acc: 0.923 - ETA: 0s - loss: 0.2231 - acc: 0.923 - ETA: 0s - loss: 0.2203 - acc: 0.925 - ETA: 0s - loss: 0.2193 - acc: 0.926 - ETA: 0s - loss: 0.2176 - acc: 0.927 - ETA: 0s - loss: 0.2169 - acc: 0.927 - ETA: 0s - loss: 0.2187 - acc: 0.926 - 3s 619us/step - loss: 0.2205 - acc: 0.9260 - val_loss: 0.2653 - val_acc: 0.9103\n",
      "Epoch 47/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2823 - acc: 0.921 - ETA: 2s - loss: 0.1829 - acc: 0.963 - ETA: 2s - loss: 0.2135 - acc: 0.946 - ETA: 2s - loss: 0.1980 - acc: 0.950 - ETA: 2s - loss: 0.2008 - acc: 0.946 - ETA: 1s - loss: 0.2032 - acc: 0.944 - ETA: 1s - loss: 0.2099 - acc: 0.941 - ETA: 1s - loss: 0.2186 - acc: 0.934 - ETA: 1s - loss: 0.2268 - acc: 0.930 - ETA: 1s - loss: 0.2260 - acc: 0.928 - ETA: 1s - loss: 0.2200 - acc: 0.931 - ETA: 1s - loss: 0.2253 - acc: 0.928 - ETA: 1s - loss: 0.2257 - acc: 0.926 - ETA: 1s - loss: 0.2268 - acc: 0.927 - ETA: 1s - loss: 0.2290 - acc: 0.925 - ETA: 1s - loss: 0.2276 - acc: 0.926 - ETA: 1s - loss: 0.2258 - acc: 0.927 - ETA: 1s - loss: 0.2239 - acc: 0.928 - ETA: 0s - loss: 0.2224 - acc: 0.929 - ETA: 0s - loss: 0.2213 - acc: 0.929 - ETA: 0s - loss: 0.2194 - acc: 0.929 - ETA: 0s - loss: 0.2188 - acc: 0.929 - ETA: 0s - loss: 0.2212 - acc: 0.928 - ETA: 0s - loss: 0.2203 - acc: 0.929 - ETA: 0s - loss: 0.2209 - acc: 0.929 - ETA: 0s - loss: 0.2187 - acc: 0.930 - ETA: 0s - loss: 0.2202 - acc: 0.929 - ETA: 0s - loss: 0.2182 - acc: 0.930 - ETA: 0s - loss: 0.2176 - acc: 0.931 - ETA: 0s - loss: 0.2175 - acc: 0.930 - ETA: 0s - loss: 0.2165 - acc: 0.932 - ETA: 0s - loss: 0.2166 - acc: 0.932 - 2s 614us/step - loss: 0.2179 - acc: 0.9316 - val_loss: 0.2852 - val_acc: 0.9186\n",
      "Epoch 48/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1685 - acc: 0.968 - ETA: 2s - loss: 0.1940 - acc: 0.947 - ETA: 2s - loss: 0.2150 - acc: 0.937 - ETA: 2s - loss: 0.2163 - acc: 0.942 - ETA: 2s - loss: 0.2013 - acc: 0.946 - ETA: 1s - loss: 0.1976 - acc: 0.948 - ETA: 1s - loss: 0.1917 - acc: 0.950 - ETA: 1s - loss: 0.1966 - acc: 0.949 - ETA: 1s - loss: 0.1956 - acc: 0.950 - ETA: 1s - loss: 0.1912 - acc: 0.951 - ETA: 1s - loss: 0.1902 - acc: 0.949 - ETA: 1s - loss: 0.1903 - acc: 0.948 - ETA: 1s - loss: 0.1921 - acc: 0.947 - ETA: 1s - loss: 0.1896 - acc: 0.948 - ETA: 1s - loss: 0.1895 - acc: 0.947 - ETA: 1s - loss: 0.1894 - acc: 0.948 - ETA: 1s - loss: 0.1924 - acc: 0.946 - ETA: 1s - loss: 0.1927 - acc: 0.945 - ETA: 1s - loss: 0.1935 - acc: 0.945 - ETA: 0s - loss: 0.1945 - acc: 0.944 - ETA: 0s - loss: 0.1956 - acc: 0.943 - ETA: 0s - loss: 0.1961 - acc: 0.943 - ETA: 0s - loss: 0.1978 - acc: 0.941 - ETA: 0s - loss: 0.1989 - acc: 0.941 - ETA: 0s - loss: 0.1967 - acc: 0.942 - ETA: 0s - loss: 0.1971 - acc: 0.942 - ETA: 0s - loss: 0.1960 - acc: 0.942 - ETA: 0s - loss: 0.1975 - acc: 0.941 - ETA: 0s - loss: 0.1986 - acc: 0.940 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1984 - acc: 0.940 - ETA: 0s - loss: 0.1972 - acc: 0.940 - 3s 625us/step - loss: 0.1967 - acc: 0.9407 - val_loss: 0.3645 - val_acc: 0.8827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.0843 - acc: 1.000 - ETA: 2s - loss: 0.2647 - acc: 0.916 - ETA: 2s - loss: 0.2293 - acc: 0.921 - ETA: 2s - loss: 0.2320 - acc: 0.926 - ETA: 2s - loss: 0.2458 - acc: 0.916 - ETA: 1s - loss: 0.2398 - acc: 0.920 - ETA: 1s - loss: 0.2503 - acc: 0.921 - ETA: 1s - loss: 0.2392 - acc: 0.929 - ETA: 1s - loss: 0.2289 - acc: 0.932 - ETA: 1s - loss: 0.2211 - acc: 0.936 - ETA: 1s - loss: 0.2248 - acc: 0.934 - ETA: 1s - loss: 0.2225 - acc: 0.936 - ETA: 1s - loss: 0.2170 - acc: 0.938 - ETA: 1s - loss: 0.2139 - acc: 0.937 - ETA: 1s - loss: 0.2150 - acc: 0.935 - ETA: 1s - loss: 0.2140 - acc: 0.934 - ETA: 1s - loss: 0.2117 - acc: 0.934 - ETA: 1s - loss: 0.2095 - acc: 0.936 - ETA: 0s - loss: 0.2076 - acc: 0.937 - ETA: 0s - loss: 0.2095 - acc: 0.936 - ETA: 0s - loss: 0.2093 - acc: 0.936 - ETA: 0s - loss: 0.2078 - acc: 0.935 - ETA: 0s - loss: 0.2099 - acc: 0.934 - ETA: 0s - loss: 0.2104 - acc: 0.935 - ETA: 0s - loss: 0.2093 - acc: 0.935 - ETA: 0s - loss: 0.2105 - acc: 0.934 - ETA: 0s - loss: 0.2101 - acc: 0.934 - ETA: 0s - loss: 0.2096 - acc: 0.933 - ETA: 0s - loss: 0.2085 - acc: 0.933 - ETA: 0s - loss: 0.2075 - acc: 0.934 - ETA: 0s - loss: 0.2073 - acc: 0.934 - ETA: 0s - loss: 0.2076 - acc: 0.933 - 2s 613us/step - loss: 0.2072 - acc: 0.9339 - val_loss: 0.2750 - val_acc: 0.9141\n",
      "Epoch 50/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2569 - acc: 0.921 - ETA: 2s - loss: 0.1985 - acc: 0.947 - ETA: 2s - loss: 0.2234 - acc: 0.928 - ETA: 2s - loss: 0.2291 - acc: 0.933 - ETA: 2s - loss: 0.2261 - acc: 0.934 - ETA: 2s - loss: 0.2287 - acc: 0.934 - ETA: 1s - loss: 0.2260 - acc: 0.935 - ETA: 1s - loss: 0.2266 - acc: 0.935 - ETA: 1s - loss: 0.2308 - acc: 0.932 - ETA: 1s - loss: 0.2336 - acc: 0.932 - ETA: 1s - loss: 0.2319 - acc: 0.933 - ETA: 1s - loss: 0.2323 - acc: 0.932 - ETA: 1s - loss: 0.2275 - acc: 0.935 - ETA: 1s - loss: 0.2265 - acc: 0.935 - ETA: 1s - loss: 0.2365 - acc: 0.933 - ETA: 1s - loss: 0.2348 - acc: 0.933 - ETA: 1s - loss: 0.2385 - acc: 0.929 - ETA: 1s - loss: 0.2378 - acc: 0.930 - ETA: 1s - loss: 0.2351 - acc: 0.932 - ETA: 0s - loss: 0.2355 - acc: 0.930 - ETA: 0s - loss: 0.2358 - acc: 0.932 - ETA: 0s - loss: 0.2349 - acc: 0.932 - ETA: 0s - loss: 0.2353 - acc: 0.932 - ETA: 0s - loss: 0.2331 - acc: 0.933 - ETA: 0s - loss: 0.2304 - acc: 0.934 - ETA: 0s - loss: 0.2345 - acc: 0.934 - ETA: 0s - loss: 0.2331 - acc: 0.934 - ETA: 0s - loss: 0.2348 - acc: 0.934 - ETA: 0s - loss: 0.2358 - acc: 0.934 - ETA: 0s - loss: 0.2371 - acc: 0.934 - ETA: 0s - loss: 0.2393 - acc: 0.934 - ETA: 0s - loss: 0.2394 - acc: 0.933 - 3s 617us/step - loss: 0.2397 - acc: 0.9336 - val_loss: 0.2731 - val_acc: 0.9071\n",
      "Epoch 51/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1570 - acc: 0.968 - ETA: 2s - loss: 0.2269 - acc: 0.916 - ETA: 2s - loss: 0.1961 - acc: 0.940 - ETA: 2s - loss: 0.2199 - acc: 0.921 - ETA: 2s - loss: 0.2030 - acc: 0.934 - ETA: 1s - loss: 0.2047 - acc: 0.933 - ETA: 1s - loss: 0.1981 - acc: 0.938 - ETA: 1s - loss: 0.1965 - acc: 0.941 - ETA: 1s - loss: 0.1899 - acc: 0.945 - ETA: 1s - loss: 0.1923 - acc: 0.944 - ETA: 1s - loss: 0.1986 - acc: 0.944 - ETA: 1s - loss: 0.2031 - acc: 0.942 - ETA: 1s - loss: 0.2049 - acc: 0.941 - ETA: 1s - loss: 0.2037 - acc: 0.943 - ETA: 1s - loss: 0.2074 - acc: 0.941 - ETA: 1s - loss: 0.2140 - acc: 0.937 - ETA: 1s - loss: 0.2111 - acc: 0.937 - ETA: 1s - loss: 0.2110 - acc: 0.937 - ETA: 0s - loss: 0.2097 - acc: 0.937 - ETA: 0s - loss: 0.2081 - acc: 0.939 - ETA: 0s - loss: 0.2067 - acc: 0.939 - ETA: 0s - loss: 0.2069 - acc: 0.940 - ETA: 0s - loss: 0.2053 - acc: 0.939 - ETA: 0s - loss: 0.2073 - acc: 0.937 - ETA: 0s - loss: 0.2067 - acc: 0.937 - ETA: 0s - loss: 0.2066 - acc: 0.937 - ETA: 0s - loss: 0.2063 - acc: 0.938 - ETA: 0s - loss: 0.2049 - acc: 0.937 - ETA: 0s - loss: 0.2048 - acc: 0.937 - ETA: 0s - loss: 0.2035 - acc: 0.938 - ETA: 0s - loss: 0.2009 - acc: 0.939 - ETA: 0s - loss: 0.1991 - acc: 0.940 - 3s 623us/step - loss: 0.1987 - acc: 0.9407 - val_loss: 0.2637 - val_acc: 0.9231\n",
      "Epoch 52/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1359 - acc: 0.968 - ETA: 2s - loss: 0.1414 - acc: 0.963 - ETA: 2s - loss: 0.1565 - acc: 0.959 - ETA: 2s - loss: 0.1630 - acc: 0.953 - ETA: 2s - loss: 0.1723 - acc: 0.947 - ETA: 1s - loss: 0.1845 - acc: 0.947 - ETA: 1s - loss: 0.1809 - acc: 0.949 - ETA: 1s - loss: 0.1864 - acc: 0.947 - ETA: 1s - loss: 0.1819 - acc: 0.948 - ETA: 1s - loss: 0.1812 - acc: 0.947 - ETA: 1s - loss: 0.1833 - acc: 0.944 - ETA: 1s - loss: 0.1877 - acc: 0.944 - ETA: 1s - loss: 0.1877 - acc: 0.944 - ETA: 1s - loss: 0.1863 - acc: 0.945 - ETA: 1s - loss: 0.1890 - acc: 0.942 - ETA: 1s - loss: 0.1941 - acc: 0.939 - ETA: 1s - loss: 0.1922 - acc: 0.940 - ETA: 1s - loss: 0.1919 - acc: 0.941 - ETA: 0s - loss: 0.1923 - acc: 0.940 - ETA: 0s - loss: 0.1938 - acc: 0.939 - ETA: 0s - loss: 0.1942 - acc: 0.939 - ETA: 0s - loss: 0.1942 - acc: 0.938 - ETA: 0s - loss: 0.1980 - acc: 0.936 - ETA: 0s - loss: 0.1985 - acc: 0.936 - ETA: 0s - loss: 0.2019 - acc: 0.934 - ETA: 0s - loss: 0.2009 - acc: 0.935 - ETA: 0s - loss: 0.2005 - acc: 0.935 - ETA: 0s - loss: 0.2029 - acc: 0.934 - ETA: 0s - loss: 0.2025 - acc: 0.935 - ETA: 0s - loss: 0.2021 - acc: 0.935 - ETA: 0s - loss: 0.2008 - acc: 0.936 - ETA: 0s - loss: 0.2009 - acc: 0.936 - 3s 620us/step - loss: 0.2007 - acc: 0.9358 - val_loss: 0.2816 - val_acc: 0.9231\n",
      "Epoch 53/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1162 - acc: 0.984 - ETA: 2s - loss: 0.1795 - acc: 0.942 - ETA: 2s - loss: 0.2005 - acc: 0.934 - ETA: 2s - loss: 0.1947 - acc: 0.937 - ETA: 2s - loss: 0.2091 - acc: 0.930 - ETA: 2s - loss: 0.2078 - acc: 0.930 - ETA: 2s - loss: 0.2031 - acc: 0.932 - ETA: 2s - loss: 0.1974 - acc: 0.936 - ETA: 1s - loss: 0.1889 - acc: 0.940 - ETA: 1s - loss: 0.1969 - acc: 0.935 - ETA: 1s - loss: 0.2022 - acc: 0.933 - ETA: 1s - loss: 0.2012 - acc: 0.934 - ETA: 1s - loss: 0.1979 - acc: 0.936 - ETA: 1s - loss: 0.1968 - acc: 0.938 - ETA: 1s - loss: 0.2042 - acc: 0.936 - ETA: 1s - loss: 0.2038 - acc: 0.937 - ETA: 1s - loss: 0.2005 - acc: 0.938 - ETA: 1s - loss: 0.2035 - acc: 0.936 - ETA: 1s - loss: 0.2034 - acc: 0.937 - ETA: 1s - loss: 0.2028 - acc: 0.937 - ETA: 0s - loss: 0.2021 - acc: 0.939 - ETA: 0s - loss: 0.2019 - acc: 0.939 - ETA: 0s - loss: 0.2029 - acc: 0.939 - ETA: 0s - loss: 0.2018 - acc: 0.939 - ETA: 0s - loss: 0.1997 - acc: 0.940 - ETA: 0s - loss: 0.1984 - acc: 0.941 - ETA: 0s - loss: 0.1970 - acc: 0.941 - ETA: 0s - loss: 0.1950 - acc: 0.941 - ETA: 0s - loss: 0.1955 - acc: 0.941 - ETA: 0s - loss: 0.1940 - acc: 0.942 - ETA: 0s - loss: 0.1921 - acc: 0.943 - ETA: 0s - loss: 0.1908 - acc: 0.943 - ETA: 0s - loss: 0.1915 - acc: 0.943 - 3s 623us/step - loss: 0.1923 - acc: 0.9430 - val_loss: 0.2628 - val_acc: 0.9231\n",
      "Epoch 54/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1113 - acc: 0.984 - ETA: 2s - loss: 0.1539 - acc: 0.947 - ETA: 2s - loss: 0.1481 - acc: 0.956 - ETA: 2s - loss: 0.1512 - acc: 0.959 - ETA: 2s - loss: 0.1762 - acc: 0.949 - ETA: 2s - loss: 0.1783 - acc: 0.948 - ETA: 1s - loss: 0.1904 - acc: 0.942 - ETA: 1s - loss: 0.1884 - acc: 0.943 - ETA: 1s - loss: 0.1881 - acc: 0.944 - ETA: 1s - loss: 0.1966 - acc: 0.943 - ETA: 1s - loss: 0.2000 - acc: 0.941 - ETA: 1s - loss: 0.2006 - acc: 0.940 - ETA: 1s - loss: 0.2024 - acc: 0.938 - ETA: 1s - loss: 0.2047 - acc: 0.935 - ETA: 1s - loss: 0.2103 - acc: 0.934 - ETA: 1s - loss: 0.2069 - acc: 0.936 - ETA: 1s - loss: 0.2058 - acc: 0.937 - ETA: 1s - loss: 0.2120 - acc: 0.936 - ETA: 1s - loss: 0.2100 - acc: 0.937 - ETA: 0s - loss: 0.2102 - acc: 0.938 - ETA: 0s - loss: 0.2103 - acc: 0.937 - ETA: 0s - loss: 0.2112 - acc: 0.936 - ETA: 0s - loss: 0.2124 - acc: 0.936 - ETA: 0s - loss: 0.2127 - acc: 0.935 - ETA: 0s - loss: 0.2125 - acc: 0.934 - ETA: 0s - loss: 0.2121 - acc: 0.934 - ETA: 0s - loss: 0.2113 - acc: 0.935 - ETA: 0s - loss: 0.2132 - acc: 0.934 - ETA: 0s - loss: 0.2129 - acc: 0.935 - ETA: 0s - loss: 0.2138 - acc: 0.935 - ETA: 0s - loss: 0.2129 - acc: 0.935 - ETA: 0s - loss: 0.2119 - acc: 0.936 - 3s 643us/step - loss: 0.2116 - acc: 0.9366 - val_loss: 0.3392 - val_acc: 0.8788\n",
      "Epoch 55/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2478 - acc: 0.906 - ETA: 2s - loss: 0.2850 - acc: 0.906 - ETA: 2s - loss: 0.2389 - acc: 0.918 - ETA: 2s - loss: 0.2284 - acc: 0.919 - ETA: 2s - loss: 0.2308 - acc: 0.911 - ETA: 1s - loss: 0.2383 - acc: 0.910 - ETA: 1s - loss: 0.2309 - acc: 0.914 - ETA: 1s - loss: 0.2278 - acc: 0.916 - ETA: 1s - loss: 0.2252 - acc: 0.917 - ETA: 1s - loss: 0.2188 - acc: 0.921 - ETA: 1s - loss: 0.2145 - acc: 0.925 - ETA: 1s - loss: 0.2206 - acc: 0.925 - ETA: 1s - loss: 0.2174 - acc: 0.927 - ETA: 1s - loss: 0.2126 - acc: 0.930 - ETA: 1s - loss: 0.2151 - acc: 0.930 - ETA: 1s - loss: 0.2157 - acc: 0.929 - ETA: 1s - loss: 0.2137 - acc: 0.929 - ETA: 1s - loss: 0.2175 - acc: 0.927 - ETA: 1s - loss: 0.2175 - acc: 0.928 - ETA: 0s - loss: 0.2139 - acc: 0.930 - ETA: 0s - loss: 0.2157 - acc: 0.928 - ETA: 0s - loss: 0.2186 - acc: 0.927 - ETA: 0s - loss: 0.2185 - acc: 0.927 - ETA: 0s - loss: 0.2170 - acc: 0.928 - ETA: 0s - loss: 0.2174 - acc: 0.928 - ETA: 0s - loss: 0.2163 - acc: 0.929 - ETA: 0s - loss: 0.2145 - acc: 0.931 - ETA: 0s - loss: 0.2151 - acc: 0.931 - ETA: 0s - loss: 0.2149 - acc: 0.931 - ETA: 0s - loss: 0.2130 - acc: 0.932 - ETA: 0s - loss: 0.2132 - acc: 0.931 - ETA: 0s - loss: 0.2119 - acc: 0.932 - 3s 623us/step - loss: 0.2115 - acc: 0.9326 - val_loss: 0.2714 - val_acc: 0.9058\n",
      "Epoch 56/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1987 - acc: 0.968 - ETA: 2s - loss: 0.1794 - acc: 0.958 - ETA: 2s - loss: 0.2026 - acc: 0.943 - ETA: 2s - loss: 0.2047 - acc: 0.944 - ETA: 2s - loss: 0.1953 - acc: 0.946 - ETA: 2s - loss: 0.2063 - acc: 0.940 - ETA: 1s - loss: 0.2139 - acc: 0.933 - ETA: 1s - loss: 0.2063 - acc: 0.939 - ETA: 1s - loss: 0.2026 - acc: 0.940 - ETA: 1s - loss: 0.2041 - acc: 0.937 - ETA: 1s - loss: 0.2014 - acc: 0.939 - ETA: 1s - loss: 0.1994 - acc: 0.939 - ETA: 1s - loss: 0.1971 - acc: 0.939 - ETA: 1s - loss: 0.1959 - acc: 0.940 - ETA: 1s - loss: 0.1982 - acc: 0.939 - ETA: 1s - loss: 0.1917 - acc: 0.942 - ETA: 1s - loss: 0.1913 - acc: 0.941 - ETA: 1s - loss: 0.1911 - acc: 0.941 - ETA: 1s - loss: 0.1912 - acc: 0.941 - ETA: 1s - loss: 0.1885 - acc: 0.942 - ETA: 1s - loss: 0.1879 - acc: 0.943 - ETA: 1s - loss: 0.1871 - acc: 0.943 - ETA: 0s - loss: 0.1916 - acc: 0.942 - ETA: 0s - loss: 0.1940 - acc: 0.940 - ETA: 0s - loss: 0.1944 - acc: 0.939 - ETA: 0s - loss: 0.1917 - acc: 0.941 - ETA: 0s - loss: 0.1947 - acc: 0.939 - ETA: 0s - loss: 0.1938 - acc: 0.939 - ETA: 0s - loss: 0.1935 - acc: 0.939 - ETA: 0s - loss: 0.1929 - acc: 0.939 - ETA: 0s - loss: 0.1923 - acc: 0.940 - ETA: 0s - loss: 0.1912 - acc: 0.940 - ETA: 0s - loss: 0.1912 - acc: 0.940 - ETA: 0s - loss: 0.1927 - acc: 0.940 - ETA: 0s - loss: 0.1944 - acc: 0.940 - 3s 681us/step - loss: 0.1947 - acc: 0.9395 - val_loss: 0.2724 - val_acc: 0.9212\n",
      "Epoch 57/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1498 - acc: 0.968 - ETA: 2s - loss: 0.2364 - acc: 0.927 - ETA: 2s - loss: 0.2336 - acc: 0.931 - ETA: 2s - loss: 0.2471 - acc: 0.924 - ETA: 2s - loss: 0.2517 - acc: 0.925 - ETA: 2s - loss: 0.2401 - acc: 0.931 - ETA: 1s - loss: 0.2721 - acc: 0.930 - ETA: 1s - loss: 0.2736 - acc: 0.926 - ETA: 1s - loss: 0.2759 - acc: 0.929 - ETA: 1s - loss: 0.2780 - acc: 0.930 - ETA: 1s - loss: 0.2787 - acc: 0.930 - ETA: 1s - loss: 0.2780 - acc: 0.930 - ETA: 1s - loss: 0.2770 - acc: 0.930 - ETA: 1s - loss: 0.2702 - acc: 0.932 - ETA: 1s - loss: 0.2663 - acc: 0.933 - ETA: 1s - loss: 0.2680 - acc: 0.932 - ETA: 1s - loss: 0.2662 - acc: 0.933 - ETA: 1s - loss: 0.2624 - acc: 0.934 - ETA: 0s - loss: 0.2608 - acc: 0.934 - ETA: 0s - loss: 0.2591 - acc: 0.935 - ETA: 0s - loss: 0.2560 - acc: 0.935 - ETA: 0s - loss: 0.2562 - acc: 0.935 - ETA: 0s - loss: 0.2538 - acc: 0.935 - ETA: 0s - loss: 0.2529 - acc: 0.935 - ETA: 0s - loss: 0.2529 - acc: 0.934 - ETA: 0s - loss: 0.2518 - acc: 0.933 - ETA: 0s - loss: 0.2517 - acc: 0.933 - ETA: 0s - loss: 0.2482 - acc: 0.934 - ETA: 0s - loss: 0.2455 - acc: 0.936 - ETA: 0s - loss: 0.2479 - acc: 0.934 - ETA: 0s - loss: 0.2463 - acc: 0.934 - ETA: 0s - loss: 0.2474 - acc: 0.934 - 2s 611us/step - loss: 0.2477 - acc: 0.9339 - val_loss: 0.2710 - val_acc: 0.9276\n",
      "Epoch 58/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1852 - acc: 0.953 - ETA: 2s - loss: 0.1547 - acc: 0.979 - ETA: 2s - loss: 0.1671 - acc: 0.975 - ETA: 2s - loss: 0.1783 - acc: 0.968 - ETA: 1s - loss: 0.1895 - acc: 0.965 - ETA: 1s - loss: 0.1908 - acc: 0.960 - ETA: 1s - loss: 0.1876 - acc: 0.960 - ETA: 1s - loss: 0.1913 - acc: 0.956 - ETA: 1s - loss: 0.1926 - acc: 0.955 - ETA: 1s - loss: 0.1923 - acc: 0.953 - ETA: 1s - loss: 0.1930 - acc: 0.952 - ETA: 1s - loss: 0.1952 - acc: 0.951 - ETA: 1s - loss: 0.2026 - acc: 0.947 - ETA: 1s - loss: 0.2035 - acc: 0.946 - ETA: 1s - loss: 0.2084 - acc: 0.942 - ETA: 1s - loss: 0.2048 - acc: 0.943 - ETA: 1s - loss: 0.2044 - acc: 0.941 - ETA: 1s - loss: 0.2101 - acc: 0.939 - ETA: 0s - loss: 0.2121 - acc: 0.937 - ETA: 0s - loss: 0.2104 - acc: 0.937 - ETA: 0s - loss: 0.2132 - acc: 0.937 - ETA: 0s - loss: 0.2171 - acc: 0.936 - ETA: 0s - loss: 0.2166 - acc: 0.936 - ETA: 0s - loss: 0.2159 - acc: 0.935 - ETA: 0s - loss: 0.2158 - acc: 0.935 - ETA: 0s - loss: 0.2152 - acc: 0.936 - ETA: 0s - loss: 0.2128 - acc: 0.937 - ETA: 0s - loss: 0.2130 - acc: 0.937 - ETA: 0s - loss: 0.2110 - acc: 0.938 - ETA: 0s - loss: 0.2084 - acc: 0.939 - ETA: 0s - loss: 0.2079 - acc: 0.939 - ETA: 0s - loss: 0.2080 - acc: 0.938 - 2s 593us/step - loss: 0.2085 - acc: 0.9383 - val_loss: 0.2645 - val_acc: 0.9179\n",
      "Epoch 59/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1123 - acc: 1.000 - ETA: 2s - loss: 0.1528 - acc: 0.968 - ETA: 2s - loss: 0.1851 - acc: 0.953 - ETA: 2s - loss: 0.1964 - acc: 0.948 - ETA: 1s - loss: 0.1920 - acc: 0.949 - ETA: 1s - loss: 0.1822 - acc: 0.953 - ETA: 1s - loss: 0.1882 - acc: 0.949 - ETA: 1s - loss: 0.1905 - acc: 0.949 - ETA: 1s - loss: 0.1839 - acc: 0.949 - ETA: 1s - loss: 0.1939 - acc: 0.944 - ETA: 1s - loss: 0.1896 - acc: 0.946 - ETA: 1s - loss: 0.1888 - acc: 0.945 - ETA: 1s - loss: 0.1866 - acc: 0.946 - ETA: 1s - loss: 0.1875 - acc: 0.946 - ETA: 1s - loss: 0.1903 - acc: 0.943 - ETA: 1s - loss: 0.1884 - acc: 0.944 - ETA: 1s - loss: 0.1867 - acc: 0.946 - ETA: 1s - loss: 0.1861 - acc: 0.945 - ETA: 0s - loss: 0.1859 - acc: 0.944 - ETA: 0s - loss: 0.1860 - acc: 0.944 - ETA: 0s - loss: 0.1864 - acc: 0.943 - ETA: 0s - loss: 0.1857 - acc: 0.944 - ETA: 0s - loss: 0.1862 - acc: 0.944 - ETA: 0s - loss: 0.1861 - acc: 0.944 - ETA: 0s - loss: 0.1868 - acc: 0.943 - ETA: 0s - loss: 0.1846 - acc: 0.943 - ETA: 0s - loss: 0.1886 - acc: 0.941 - ETA: 0s - loss: 0.1871 - acc: 0.942 - ETA: 0s - loss: 0.1888 - acc: 0.941 - ETA: 0s - loss: 0.1909 - acc: 0.940 - ETA: 0s - loss: 0.1900 - acc: 0.941 - ETA: 0s - loss: 0.1916 - acc: 0.940 - 2s 606us/step - loss: 0.1918 - acc: 0.9403 - val_loss: 0.2681 - val_acc: 0.9282\n",
      "Epoch 60/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2433 - acc: 0.937 - ETA: 2s - loss: 0.2179 - acc: 0.932 - ETA: 2s - loss: 0.1904 - acc: 0.946 - ETA: 2s - loss: 0.1920 - acc: 0.942 - ETA: 1s - loss: 0.1802 - acc: 0.944 - ETA: 1s - loss: 0.1708 - acc: 0.950 - ETA: 1s - loss: 0.1671 - acc: 0.950 - ETA: 1s - loss: 0.1767 - acc: 0.945 - ETA: 1s - loss: 0.1726 - acc: 0.947 - ETA: 1s - loss: 0.1798 - acc: 0.944 - ETA: 1s - loss: 0.1845 - acc: 0.942 - ETA: 1s - loss: 0.1862 - acc: 0.940 - ETA: 1s - loss: 0.1831 - acc: 0.941 - ETA: 1s - loss: 0.1838 - acc: 0.939 - ETA: 1s - loss: 0.1823 - acc: 0.940 - ETA: 1s - loss: 0.1814 - acc: 0.941 - ETA: 1s - loss: 0.1807 - acc: 0.942 - ETA: 1s - loss: 0.1828 - acc: 0.941 - ETA: 0s - loss: 0.1818 - acc: 0.942 - ETA: 0s - loss: 0.1816 - acc: 0.942 - ETA: 0s - loss: 0.1820 - acc: 0.942 - ETA: 0s - loss: 0.1797 - acc: 0.944 - ETA: 0s - loss: 0.1779 - acc: 0.945 - ETA: 0s - loss: 0.1785 - acc: 0.945 - ETA: 0s - loss: 0.1789 - acc: 0.945 - ETA: 0s - loss: 0.1779 - acc: 0.945 - ETA: 0s - loss: 0.1774 - acc: 0.946 - ETA: 0s - loss: 0.1780 - acc: 0.946 - ETA: 0s - loss: 0.1812 - acc: 0.946 - ETA: 0s - loss: 0.1813 - acc: 0.946 - ETA: 0s - loss: 0.1827 - acc: 0.945 - ETA: 0s - loss: 0.1847 - acc: 0.944 - 2s 601us/step - loss: 0.1847 - acc: 0.9447 - val_loss: 0.2589 - val_acc: 0.9256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1688 - acc: 0.968 - ETA: 2s - loss: 0.1815 - acc: 0.947 - ETA: 2s - loss: 0.1914 - acc: 0.940 - ETA: 1s - loss: 0.1845 - acc: 0.944 - ETA: 1s - loss: 0.1801 - acc: 0.944 - ETA: 1s - loss: 0.1808 - acc: 0.943 - ETA: 1s - loss: 0.1812 - acc: 0.945 - ETA: 1s - loss: 0.1849 - acc: 0.945 - ETA: 1s - loss: 0.1872 - acc: 0.943 - ETA: 1s - loss: 0.2045 - acc: 0.935 - ETA: 1s - loss: 0.2003 - acc: 0.937 - ETA: 1s - loss: 0.1969 - acc: 0.939 - ETA: 1s - loss: 0.1955 - acc: 0.940 - ETA: 1s - loss: 0.1938 - acc: 0.939 - ETA: 1s - loss: 0.1883 - acc: 0.942 - ETA: 1s - loss: 0.1935 - acc: 0.940 - ETA: 1s - loss: 0.2004 - acc: 0.938 - ETA: 1s - loss: 0.2058 - acc: 0.934 - ETA: 1s - loss: 0.2066 - acc: 0.934 - ETA: 0s - loss: 0.2088 - acc: 0.933 - ETA: 0s - loss: 0.2053 - acc: 0.935 - ETA: 0s - loss: 0.2067 - acc: 0.935 - ETA: 0s - loss: 0.2087 - acc: 0.934 - ETA: 0s - loss: 0.2064 - acc: 0.935 - ETA: 0s - loss: 0.2061 - acc: 0.935 - ETA: 0s - loss: 0.2108 - acc: 0.934 - ETA: 0s - loss: 0.2133 - acc: 0.934 - ETA: 0s - loss: 0.2143 - acc: 0.934 - ETA: 0s - loss: 0.2135 - acc: 0.934 - ETA: 0s - loss: 0.2127 - acc: 0.934 - ETA: 0s - loss: 0.2116 - acc: 0.934 - ETA: 0s - loss: 0.2119 - acc: 0.934 - ETA: 0s - loss: 0.2109 - acc: 0.934 - 3s 626us/step - loss: 0.2107 - acc: 0.9346 - val_loss: 0.3527 - val_acc: 0.9026\n",
      "Epoch 62/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2100 - acc: 0.937 - ETA: 2s - loss: 0.1933 - acc: 0.953 - ETA: 2s - loss: 0.1879 - acc: 0.962 - ETA: 2s - loss: 0.1885 - acc: 0.957 - ETA: 1s - loss: 0.1942 - acc: 0.953 - ETA: 1s - loss: 0.1947 - acc: 0.951 - ETA: 1s - loss: 0.1961 - acc: 0.947 - ETA: 1s - loss: 0.1951 - acc: 0.945 - ETA: 1s - loss: 0.1923 - acc: 0.946 - ETA: 1s - loss: 0.1906 - acc: 0.947 - ETA: 1s - loss: 0.1876 - acc: 0.949 - ETA: 1s - loss: 0.1922 - acc: 0.948 - ETA: 1s - loss: 0.1948 - acc: 0.947 - ETA: 1s - loss: 0.1932 - acc: 0.947 - ETA: 1s - loss: 0.1931 - acc: 0.946 - ETA: 1s - loss: 0.1903 - acc: 0.947 - ETA: 1s - loss: 0.1922 - acc: 0.946 - ETA: 1s - loss: 0.1904 - acc: 0.946 - ETA: 0s - loss: 0.1921 - acc: 0.946 - ETA: 0s - loss: 0.1935 - acc: 0.946 - ETA: 0s - loss: 0.1939 - acc: 0.945 - ETA: 0s - loss: 0.1925 - acc: 0.945 - ETA: 0s - loss: 0.1937 - acc: 0.945 - ETA: 0s - loss: 0.1951 - acc: 0.944 - ETA: 0s - loss: 0.1962 - acc: 0.944 - ETA: 0s - loss: 0.1962 - acc: 0.944 - ETA: 0s - loss: 0.1979 - acc: 0.944 - ETA: 0s - loss: 0.1959 - acc: 0.944 - ETA: 0s - loss: 0.1991 - acc: 0.943 - ETA: 0s - loss: 0.2006 - acc: 0.942 - ETA: 0s - loss: 0.1992 - acc: 0.942 - ETA: 0s - loss: 0.1987 - acc: 0.943 - 2s 594us/step - loss: 0.1999 - acc: 0.9425 - val_loss: 0.2571 - val_acc: 0.9263\n",
      "Epoch 63/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2012 - acc: 0.921 - ETA: 2s - loss: 0.2049 - acc: 0.927 - ETA: 2s - loss: 0.2045 - acc: 0.934 - ETA: 1s - loss: 0.2190 - acc: 0.928 - ETA: 1s - loss: 0.2022 - acc: 0.934 - ETA: 1s - loss: 0.1932 - acc: 0.937 - ETA: 1s - loss: 0.2013 - acc: 0.929 - ETA: 1s - loss: 0.1953 - acc: 0.932 - ETA: 1s - loss: 0.1875 - acc: 0.935 - ETA: 1s - loss: 0.1873 - acc: 0.935 - ETA: 1s - loss: 0.1855 - acc: 0.935 - ETA: 1s - loss: 0.1840 - acc: 0.937 - ETA: 1s - loss: 0.1852 - acc: 0.936 - ETA: 1s - loss: 0.1864 - acc: 0.936 - ETA: 1s - loss: 0.1854 - acc: 0.938 - ETA: 1s - loss: 0.1868 - acc: 0.937 - ETA: 1s - loss: 0.1858 - acc: 0.938 - ETA: 1s - loss: 0.1848 - acc: 0.939 - ETA: 0s - loss: 0.1850 - acc: 0.939 - ETA: 0s - loss: 0.1853 - acc: 0.939 - ETA: 0s - loss: 0.1833 - acc: 0.940 - ETA: 0s - loss: 0.1833 - acc: 0.940 - ETA: 0s - loss: 0.1828 - acc: 0.941 - ETA: 0s - loss: 0.1835 - acc: 0.941 - ETA: 0s - loss: 0.1840 - acc: 0.941 - ETA: 0s - loss: 0.1839 - acc: 0.941 - ETA: 0s - loss: 0.1843 - acc: 0.941 - ETA: 0s - loss: 0.1846 - acc: 0.940 - ETA: 0s - loss: 0.1836 - acc: 0.941 - ETA: 0s - loss: 0.1831 - acc: 0.942 - ETA: 0s - loss: 0.1837 - acc: 0.942 - ETA: 0s - loss: 0.1825 - acc: 0.943 - 2s 591us/step - loss: 0.1825 - acc: 0.9437 - val_loss: 0.2666 - val_acc: 0.9186\n",
      "Epoch 64/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2284 - acc: 0.906 - ETA: 2s - loss: 0.1818 - acc: 0.942 - ETA: 2s - loss: 0.2045 - acc: 0.928 - ETA: 2s - loss: 0.2231 - acc: 0.930 - ETA: 2s - loss: 0.2134 - acc: 0.932 - ETA: 1s - loss: 0.2015 - acc: 0.940 - ETA: 1s - loss: 0.1980 - acc: 0.942 - ETA: 1s - loss: 0.1946 - acc: 0.944 - ETA: 1s - loss: 0.1941 - acc: 0.944 - ETA: 1s - loss: 0.1994 - acc: 0.942 - ETA: 1s - loss: 0.2060 - acc: 0.937 - ETA: 1s - loss: 0.2096 - acc: 0.936 - ETA: 1s - loss: 0.2115 - acc: 0.936 - ETA: 1s - loss: 0.2172 - acc: 0.934 - ETA: 1s - loss: 0.2125 - acc: 0.937 - ETA: 1s - loss: 0.2122 - acc: 0.937 - ETA: 1s - loss: 0.2132 - acc: 0.936 - ETA: 1s - loss: 0.2121 - acc: 0.937 - ETA: 1s - loss: 0.2124 - acc: 0.936 - ETA: 1s - loss: 0.2133 - acc: 0.934 - ETA: 1s - loss: 0.2134 - acc: 0.935 - ETA: 1s - loss: 0.2120 - acc: 0.935 - ETA: 1s - loss: 0.2109 - acc: 0.935 - ETA: 1s - loss: 0.2111 - acc: 0.935 - ETA: 0s - loss: 0.2134 - acc: 0.935 - ETA: 0s - loss: 0.2159 - acc: 0.935 - ETA: 0s - loss: 0.2120 - acc: 0.936 - ETA: 0s - loss: 0.2163 - acc: 0.934 - ETA: 0s - loss: 0.2162 - acc: 0.934 - ETA: 0s - loss: 0.2128 - acc: 0.936 - ETA: 0s - loss: 0.2101 - acc: 0.938 - ETA: 0s - loss: 0.2103 - acc: 0.938 - ETA: 0s - loss: 0.2095 - acc: 0.938 - ETA: 0s - loss: 0.2086 - acc: 0.938 - ETA: 0s - loss: 0.2087 - acc: 0.938 - ETA: 0s - loss: 0.2083 - acc: 0.939 - ETA: 0s - loss: 0.2087 - acc: 0.939 - 3s 673us/step - loss: 0.2088 - acc: 0.9388 - val_loss: 0.2671 - val_acc: 0.9167\n",
      "Epoch 65/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1281 - acc: 0.984 - ETA: 2s - loss: 0.1201 - acc: 0.976 - ETA: 2s - loss: 0.1658 - acc: 0.960 - ETA: 2s - loss: 0.1630 - acc: 0.955 - ETA: 2s - loss: 0.1776 - acc: 0.955 - ETA: 2s - loss: 0.1761 - acc: 0.956 - ETA: 2s - loss: 0.1755 - acc: 0.954 - ETA: 2s - loss: 0.1743 - acc: 0.954 - ETA: 1s - loss: 0.1703 - acc: 0.956 - ETA: 1s - loss: 0.1678 - acc: 0.957 - ETA: 1s - loss: 0.1653 - acc: 0.958 - ETA: 1s - loss: 0.1683 - acc: 0.956 - ETA: 1s - loss: 0.1695 - acc: 0.957 - ETA: 1s - loss: 0.1705 - acc: 0.956 - ETA: 1s - loss: 0.1697 - acc: 0.957 - ETA: 1s - loss: 0.1729 - acc: 0.955 - ETA: 1s - loss: 0.1731 - acc: 0.955 - ETA: 1s - loss: 0.1723 - acc: 0.955 - ETA: 1s - loss: 0.1746 - acc: 0.954 - ETA: 1s - loss: 0.1739 - acc: 0.955 - ETA: 1s - loss: 0.1751 - acc: 0.954 - ETA: 0s - loss: 0.1731 - acc: 0.956 - ETA: 0s - loss: 0.1763 - acc: 0.955 - ETA: 0s - loss: 0.1755 - acc: 0.955 - ETA: 0s - loss: 0.1756 - acc: 0.955 - ETA: 0s - loss: 0.1775 - acc: 0.953 - ETA: 0s - loss: 0.1774 - acc: 0.953 - ETA: 0s - loss: 0.1776 - acc: 0.952 - ETA: 0s - loss: 0.1764 - acc: 0.953 - ETA: 0s - loss: 0.1785 - acc: 0.951 - ETA: 0s - loss: 0.1772 - acc: 0.951 - ETA: 0s - loss: 0.1774 - acc: 0.951 - ETA: 0s - loss: 0.1802 - acc: 0.950 - ETA: 0s - loss: 0.1793 - acc: 0.950 - ETA: 0s - loss: 0.1810 - acc: 0.949 - 3s 657us/step - loss: 0.1810 - acc: 0.9498 - val_loss: 0.2482 - val_acc: 0.9282\n",
      "Epoch 66/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2166 - acc: 0.906 - ETA: 2s - loss: 0.2383 - acc: 0.916 - ETA: 2s - loss: 0.2030 - acc: 0.940 - ETA: 2s - loss: 0.1898 - acc: 0.946 - ETA: 1s - loss: 0.1932 - acc: 0.941 - ETA: 1s - loss: 0.2050 - acc: 0.936 - ETA: 1s - loss: 0.2006 - acc: 0.937 - ETA: 1s - loss: 0.1901 - acc: 0.942 - ETA: 1s - loss: 0.1832 - acc: 0.944 - ETA: 1s - loss: 0.1822 - acc: 0.945 - ETA: 1s - loss: 0.1842 - acc: 0.944 - ETA: 1s - loss: 0.1851 - acc: 0.942 - ETA: 1s - loss: 0.1891 - acc: 0.941 - ETA: 1s - loss: 0.1855 - acc: 0.942 - ETA: 1s - loss: 0.1818 - acc: 0.944 - ETA: 1s - loss: 0.1792 - acc: 0.945 - ETA: 1s - loss: 0.1801 - acc: 0.944 - ETA: 1s - loss: 0.1821 - acc: 0.943 - ETA: 0s - loss: 0.1820 - acc: 0.943 - ETA: 0s - loss: 0.1833 - acc: 0.942 - ETA: 0s - loss: 0.1824 - acc: 0.942 - ETA: 0s - loss: 0.1851 - acc: 0.941 - ETA: 0s - loss: 0.1887 - acc: 0.938 - ETA: 0s - loss: 0.1873 - acc: 0.939 - ETA: 0s - loss: 0.1919 - acc: 0.937 - ETA: 0s - loss: 0.1913 - acc: 0.938 - ETA: 0s - loss: 0.1908 - acc: 0.938 - ETA: 0s - loss: 0.1896 - acc: 0.939 - ETA: 0s - loss: 0.1889 - acc: 0.939 - ETA: 0s - loss: 0.1911 - acc: 0.939 - ETA: 0s - loss: 0.1898 - acc: 0.939 - ETA: 0s - loss: 0.1890 - acc: 0.939 - 2s 601us/step - loss: 0.1886 - acc: 0.9398 - val_loss: 0.2800 - val_acc: 0.9103\n",
      "Epoch 67/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1947 - acc: 0.937 - ETA: 2s - loss: 0.1627 - acc: 0.953 - ETA: 2s - loss: 0.1392 - acc: 0.968 - ETA: 2s - loss: 0.1611 - acc: 0.959 - ETA: 1s - loss: 0.1555 - acc: 0.961 - ETA: 2s - loss: 0.1643 - acc: 0.957 - ETA: 2s - loss: 0.1772 - acc: 0.954 - ETA: 2s - loss: 0.1718 - acc: 0.955 - ETA: 1s - loss: 0.1834 - acc: 0.950 - ETA: 1s - loss: 0.1915 - acc: 0.947 - ETA: 1s - loss: 0.1975 - acc: 0.944 - ETA: 1s - loss: 0.1994 - acc: 0.943 - ETA: 1s - loss: 0.1967 - acc: 0.944 - ETA: 1s - loss: 0.1885 - acc: 0.948 - ETA: 1s - loss: 0.1867 - acc: 0.948 - ETA: 1s - loss: 0.1849 - acc: 0.949 - ETA: 1s - loss: 0.1839 - acc: 0.950 - ETA: 1s - loss: 0.1849 - acc: 0.949 - ETA: 1s - loss: 0.1839 - acc: 0.951 - ETA: 1s - loss: 0.1907 - acc: 0.948 - ETA: 1s - loss: 0.1881 - acc: 0.949 - ETA: 1s - loss: 0.1871 - acc: 0.948 - ETA: 1s - loss: 0.1885 - acc: 0.946 - ETA: 0s - loss: 0.1875 - acc: 0.946 - ETA: 0s - loss: 0.1865 - acc: 0.947 - ETA: 0s - loss: 0.1874 - acc: 0.946 - ETA: 0s - loss: 0.1883 - acc: 0.946 - ETA: 0s - loss: 0.1898 - acc: 0.945 - ETA: 0s - loss: 0.1889 - acc: 0.945 - ETA: 0s - loss: 0.1944 - acc: 0.942 - ETA: 0s - loss: 0.1952 - acc: 0.942 - ETA: 0s - loss: 0.1955 - acc: 0.942 - ETA: 0s - loss: 0.1958 - acc: 0.940 - ETA: 0s - loss: 0.1955 - acc: 0.940 - ETA: 0s - loss: 0.1968 - acc: 0.940 - 3s 644us/step - loss: 0.1972 - acc: 0.9398 - val_loss: 0.2734 - val_acc: 0.9115\n",
      "Epoch 68/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1796 - acc: 0.921 - ETA: 2s - loss: 0.1668 - acc: 0.942 - ETA: 2s - loss: 0.1963 - acc: 0.921 - ETA: 2s - loss: 0.1912 - acc: 0.926 - ETA: 1s - loss: 0.2023 - acc: 0.925 - ETA: 1s - loss: 0.1971 - acc: 0.930 - ETA: 1s - loss: 0.1930 - acc: 0.932 - ETA: 1s - loss: 0.1898 - acc: 0.935 - ETA: 1s - loss: 0.1831 - acc: 0.940 - ETA: 1s - loss: 0.1879 - acc: 0.938 - ETA: 1s - loss: 0.1884 - acc: 0.938 - ETA: 1s - loss: 0.1875 - acc: 0.938 - ETA: 1s - loss: 0.1899 - acc: 0.939 - ETA: 1s - loss: 0.1884 - acc: 0.939 - ETA: 1s - loss: 0.1856 - acc: 0.941 - ETA: 1s - loss: 0.1881 - acc: 0.940 - ETA: 1s - loss: 0.1892 - acc: 0.939 - ETA: 1s - loss: 0.1931 - acc: 0.938 - ETA: 0s - loss: 0.1923 - acc: 0.939 - ETA: 0s - loss: 0.1926 - acc: 0.939 - ETA: 0s - loss: 0.1904 - acc: 0.940 - ETA: 0s - loss: 0.1926 - acc: 0.939 - ETA: 0s - loss: 0.1910 - acc: 0.941 - ETA: 0s - loss: 0.1923 - acc: 0.940 - ETA: 0s - loss: 0.1901 - acc: 0.941 - ETA: 0s - loss: 0.1891 - acc: 0.940 - ETA: 0s - loss: 0.1884 - acc: 0.941 - ETA: 0s - loss: 0.1889 - acc: 0.941 - ETA: 0s - loss: 0.1860 - acc: 0.942 - ETA: 0s - loss: 0.1860 - acc: 0.942 - ETA: 0s - loss: 0.1859 - acc: 0.943 - ETA: 0s - loss: 0.1859 - acc: 0.943 - 2s 592us/step - loss: 0.1879 - acc: 0.9432 - val_loss: 0.2801 - val_acc: 0.9212\n",
      "Epoch 69/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1050 - acc: 0.953 - ETA: 2s - loss: 0.2426 - acc: 0.937 - ETA: 2s - loss: 0.2337 - acc: 0.921 - ETA: 2s - loss: 0.2137 - acc: 0.930 - ETA: 1s - loss: 0.2098 - acc: 0.932 - ETA: 1s - loss: 0.2059 - acc: 0.931 - ETA: 1s - loss: 0.2321 - acc: 0.930 - ETA: 1s - loss: 0.2502 - acc: 0.925 - ETA: 1s - loss: 0.2454 - acc: 0.923 - ETA: 1s - loss: 0.2480 - acc: 0.923 - ETA: 1s - loss: 0.2460 - acc: 0.924 - ETA: 1s - loss: 0.2502 - acc: 0.921 - ETA: 1s - loss: 0.2502 - acc: 0.921 - ETA: 1s - loss: 0.2479 - acc: 0.921 - ETA: 1s - loss: 0.2442 - acc: 0.924 - ETA: 1s - loss: 0.2427 - acc: 0.925 - ETA: 1s - loss: 0.2415 - acc: 0.925 - ETA: 1s - loss: 0.2426 - acc: 0.924 - ETA: 0s - loss: 0.2415 - acc: 0.924 - ETA: 0s - loss: 0.2417 - acc: 0.924 - ETA: 0s - loss: 0.2374 - acc: 0.927 - ETA: 0s - loss: 0.2354 - acc: 0.928 - ETA: 0s - loss: 0.2338 - acc: 0.929 - ETA: 0s - loss: 0.2311 - acc: 0.931 - ETA: 0s - loss: 0.2350 - acc: 0.929 - ETA: 0s - loss: 0.2339 - acc: 0.929 - ETA: 0s - loss: 0.2360 - acc: 0.927 - ETA: 0s - loss: 0.2369 - acc: 0.927 - ETA: 0s - loss: 0.2379 - acc: 0.926 - ETA: 0s - loss: 0.2376 - acc: 0.927 - ETA: 0s - loss: 0.2372 - acc: 0.927 - ETA: 0s - loss: 0.2344 - acc: 0.929 - 2s 594us/step - loss: 0.2346 - acc: 0.9289 - val_loss: 0.3076 - val_acc: 0.9083\n",
      "Epoch 70/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2601 - acc: 0.906 - ETA: 2s - loss: 0.2320 - acc: 0.937 - ETA: 2s - loss: 0.1918 - acc: 0.953 - ETA: 1s - loss: 0.2090 - acc: 0.937 - ETA: 1s - loss: 0.2130 - acc: 0.932 - ETA: 1s - loss: 0.2135 - acc: 0.931 - ETA: 1s - loss: 0.2134 - acc: 0.931 - ETA: 1s - loss: 0.2130 - acc: 0.931 - ETA: 1s - loss: 0.2149 - acc: 0.932 - ETA: 1s - loss: 0.2199 - acc: 0.929 - ETA: 1s - loss: 0.2200 - acc: 0.930 - ETA: 1s - loss: 0.2171 - acc: 0.932 - ETA: 1s - loss: 0.2221 - acc: 0.929 - ETA: 1s - loss: 0.2271 - acc: 0.925 - ETA: 1s - loss: 0.2251 - acc: 0.926 - ETA: 1s - loss: 0.2278 - acc: 0.923 - ETA: 1s - loss: 0.2274 - acc: 0.924 - ETA: 1s - loss: 0.2268 - acc: 0.924 - ETA: 0s - loss: 0.2250 - acc: 0.925 - ETA: 0s - loss: 0.2201 - acc: 0.928 - ETA: 0s - loss: 0.2209 - acc: 0.929 - ETA: 0s - loss: 0.2181 - acc: 0.930 - ETA: 0s - loss: 0.2184 - acc: 0.930 - ETA: 0s - loss: 0.2171 - acc: 0.931 - ETA: 0s - loss: 0.2144 - acc: 0.933 - ETA: 0s - loss: 0.2144 - acc: 0.932 - ETA: 0s - loss: 0.2138 - acc: 0.933 - ETA: 0s - loss: 0.2158 - acc: 0.932 - ETA: 0s - loss: 0.2158 - acc: 0.932 - ETA: 0s - loss: 0.2144 - acc: 0.933 - ETA: 0s - loss: 0.2137 - acc: 0.933 - ETA: 0s - loss: 0.2113 - acc: 0.934 - 2s 593us/step - loss: 0.2109 - acc: 0.9348 - val_loss: 0.2538 - val_acc: 0.9269\n",
      "Epoch 71/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1215 - acc: 0.984 - ETA: 2s - loss: 0.2007 - acc: 0.947 - ETA: 2s - loss: 0.1743 - acc: 0.959 - ETA: 2s - loss: 0.1757 - acc: 0.957 - ETA: 2s - loss: 0.1808 - acc: 0.949 - ETA: 1s - loss: 0.1690 - acc: 0.956 - ETA: 1s - loss: 0.1805 - acc: 0.949 - ETA: 1s - loss: 0.1747 - acc: 0.952 - ETA: 1s - loss: 0.1751 - acc: 0.953 - ETA: 1s - loss: 0.1722 - acc: 0.953 - ETA: 1s - loss: 0.1747 - acc: 0.952 - ETA: 1s - loss: 0.1783 - acc: 0.951 - ETA: 1s - loss: 0.1766 - acc: 0.952 - ETA: 1s - loss: 0.1730 - acc: 0.953 - ETA: 1s - loss: 0.1779 - acc: 0.951 - ETA: 1s - loss: 0.1773 - acc: 0.951 - ETA: 1s - loss: 0.1782 - acc: 0.950 - ETA: 1s - loss: 0.1779 - acc: 0.950 - ETA: 0s - loss: 0.1813 - acc: 0.949 - ETA: 0s - loss: 0.1808 - acc: 0.949 - ETA: 0s - loss: 0.1812 - acc: 0.948 - ETA: 0s - loss: 0.1835 - acc: 0.947 - ETA: 0s - loss: 0.1837 - acc: 0.947 - ETA: 0s - loss: 0.1850 - acc: 0.947 - ETA: 0s - loss: 0.1852 - acc: 0.947 - ETA: 0s - loss: 0.1848 - acc: 0.946 - ETA: 0s - loss: 0.1850 - acc: 0.946 - ETA: 0s - loss: 0.1848 - acc: 0.946 - ETA: 0s - loss: 0.1857 - acc: 0.944 - ETA: 0s - loss: 0.1844 - acc: 0.945 - ETA: 0s - loss: 0.1833 - acc: 0.946 - ETA: 0s - loss: 0.1827 - acc: 0.946 - 2s 601us/step - loss: 0.1831 - acc: 0.9462 - val_loss: 0.2498 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2506 - acc: 0.921 - ETA: 2s - loss: 0.1885 - acc: 0.947 - ETA: 2s - loss: 0.1901 - acc: 0.950 - ETA: 1s - loss: 0.1851 - acc: 0.948 - ETA: 1s - loss: 0.1793 - acc: 0.947 - ETA: 1s - loss: 0.1856 - acc: 0.946 - ETA: 1s - loss: 0.1803 - acc: 0.948 - ETA: 1s - loss: 0.1917 - acc: 0.942 - ETA: 1s - loss: 0.1981 - acc: 0.937 - ETA: 1s - loss: 0.1988 - acc: 0.935 - ETA: 1s - loss: 0.1998 - acc: 0.935 - ETA: 1s - loss: 0.1985 - acc: 0.934 - ETA: 1s - loss: 0.1982 - acc: 0.935 - ETA: 1s - loss: 0.1970 - acc: 0.936 - ETA: 1s - loss: 0.1984 - acc: 0.935 - ETA: 1s - loss: 0.1935 - acc: 0.939 - ETA: 1s - loss: 0.1935 - acc: 0.939 - ETA: 1s - loss: 0.1943 - acc: 0.938 - ETA: 0s - loss: 0.1956 - acc: 0.938 - ETA: 0s - loss: 0.1977 - acc: 0.938 - ETA: 0s - loss: 0.1967 - acc: 0.938 - ETA: 0s - loss: 0.1959 - acc: 0.938 - ETA: 0s - loss: 0.1950 - acc: 0.939 - ETA: 0s - loss: 0.1959 - acc: 0.938 - ETA: 0s - loss: 0.1937 - acc: 0.940 - ETA: 0s - loss: 0.1958 - acc: 0.938 - ETA: 0s - loss: 0.1980 - acc: 0.937 - ETA: 0s - loss: 0.1952 - acc: 0.938 - ETA: 0s - loss: 0.1944 - acc: 0.939 - ETA: 0s - loss: 0.1935 - acc: 0.940 - ETA: 0s - loss: 0.1917 - acc: 0.941 - ETA: 0s - loss: 0.1903 - acc: 0.942 - 2s 590us/step - loss: 0.1905 - acc: 0.9420 - val_loss: 0.2750 - val_acc: 0.9147\n",
      "Epoch 73/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1937 - acc: 0.953 - ETA: 2s - loss: 0.1659 - acc: 0.958 - ETA: 2s - loss: 0.1501 - acc: 0.962 - ETA: 2s - loss: 0.1789 - acc: 0.955 - ETA: 1s - loss: 0.1856 - acc: 0.949 - ETA: 1s - loss: 0.1813 - acc: 0.951 - ETA: 1s - loss: 0.1746 - acc: 0.954 - ETA: 1s - loss: 0.1702 - acc: 0.956 - ETA: 1s - loss: 0.1741 - acc: 0.952 - ETA: 1s - loss: 0.1746 - acc: 0.953 - ETA: 1s - loss: 0.1792 - acc: 0.949 - ETA: 1s - loss: 0.1786 - acc: 0.949 - ETA: 1s - loss: 0.1792 - acc: 0.948 - ETA: 1s - loss: 0.1794 - acc: 0.947 - ETA: 1s - loss: 0.1775 - acc: 0.948 - ETA: 1s - loss: 0.1772 - acc: 0.949 - ETA: 1s - loss: 0.1808 - acc: 0.949 - ETA: 1s - loss: 0.1853 - acc: 0.948 - ETA: 0s - loss: 0.1864 - acc: 0.947 - ETA: 0s - loss: 0.1847 - acc: 0.948 - ETA: 0s - loss: 0.1870 - acc: 0.946 - ETA: 0s - loss: 0.1904 - acc: 0.944 - ETA: 0s - loss: 0.1927 - acc: 0.942 - ETA: 0s - loss: 0.1918 - acc: 0.942 - ETA: 0s - loss: 0.1914 - acc: 0.942 - ETA: 0s - loss: 0.1915 - acc: 0.943 - ETA: 0s - loss: 0.1954 - acc: 0.941 - ETA: 0s - loss: 0.1954 - acc: 0.942 - ETA: 0s - loss: 0.1961 - acc: 0.941 - ETA: 0s - loss: 0.2044 - acc: 0.938 - ETA: 0s - loss: 0.2043 - acc: 0.939 - ETA: 0s - loss: 0.2044 - acc: 0.939 - 2s 600us/step - loss: 0.2049 - acc: 0.9390 - val_loss: 0.4187 - val_acc: 0.8712\n",
      "Epoch 74/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3131 - acc: 0.921 - ETA: 2s - loss: 0.2799 - acc: 0.916 - ETA: 2s - loss: 0.2476 - acc: 0.925 - ETA: 2s - loss: 0.2520 - acc: 0.919 - ETA: 2s - loss: 0.2468 - acc: 0.921 - ETA: 2s - loss: 0.2449 - acc: 0.927 - ETA: 2s - loss: 0.2371 - acc: 0.930 - ETA: 2s - loss: 0.2482 - acc: 0.923 - ETA: 2s - loss: 0.2489 - acc: 0.918 - ETA: 1s - loss: 0.2402 - acc: 0.921 - ETA: 1s - loss: 0.2417 - acc: 0.924 - ETA: 1s - loss: 0.2380 - acc: 0.925 - ETA: 1s - loss: 0.2410 - acc: 0.926 - ETA: 1s - loss: 0.2362 - acc: 0.928 - ETA: 1s - loss: 0.2305 - acc: 0.930 - ETA: 1s - loss: 0.2308 - acc: 0.929 - ETA: 1s - loss: 0.2237 - acc: 0.933 - ETA: 1s - loss: 0.2216 - acc: 0.934 - ETA: 1s - loss: 0.2227 - acc: 0.933 - ETA: 1s - loss: 0.2190 - acc: 0.933 - ETA: 0s - loss: 0.2158 - acc: 0.935 - ETA: 0s - loss: 0.2145 - acc: 0.935 - ETA: 0s - loss: 0.2105 - acc: 0.937 - ETA: 0s - loss: 0.2073 - acc: 0.938 - ETA: 0s - loss: 0.2073 - acc: 0.938 - ETA: 0s - loss: 0.2061 - acc: 0.938 - ETA: 0s - loss: 0.2047 - acc: 0.939 - ETA: 0s - loss: 0.2039 - acc: 0.939 - ETA: 0s - loss: 0.2019 - acc: 0.940 - ETA: 0s - loss: 0.2028 - acc: 0.940 - ETA: 0s - loss: 0.2011 - acc: 0.939 - ETA: 0s - loss: 0.2027 - acc: 0.939 - ETA: 0s - loss: 0.2017 - acc: 0.940 - 2s 614us/step - loss: 0.2016 - acc: 0.9403 - val_loss: 0.2824 - val_acc: 0.8987\n",
      "Epoch 75/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1769 - acc: 0.921 - ETA: 2s - loss: 0.2368 - acc: 0.916 - ETA: 2s - loss: 0.2190 - acc: 0.928 - ETA: 1s - loss: 0.2093 - acc: 0.933 - ETA: 1s - loss: 0.2022 - acc: 0.934 - ETA: 1s - loss: 0.1884 - acc: 0.941 - ETA: 1s - loss: 0.1841 - acc: 0.943 - ETA: 1s - loss: 0.1826 - acc: 0.944 - ETA: 1s - loss: 0.1804 - acc: 0.947 - ETA: 1s - loss: 0.1814 - acc: 0.946 - ETA: 1s - loss: 0.1807 - acc: 0.947 - ETA: 1s - loss: 0.1775 - acc: 0.948 - ETA: 1s - loss: 0.1812 - acc: 0.945 - ETA: 1s - loss: 0.1806 - acc: 0.946 - ETA: 1s - loss: 0.1777 - acc: 0.947 - ETA: 1s - loss: 0.1755 - acc: 0.949 - ETA: 1s - loss: 0.1830 - acc: 0.946 - ETA: 1s - loss: 0.1835 - acc: 0.947 - ETA: 0s - loss: 0.1851 - acc: 0.946 - ETA: 0s - loss: 0.1857 - acc: 0.946 - ETA: 0s - loss: 0.1854 - acc: 0.946 - ETA: 0s - loss: 0.1833 - acc: 0.948 - ETA: 0s - loss: 0.1841 - acc: 0.947 - ETA: 0s - loss: 0.1870 - acc: 0.945 - ETA: 0s - loss: 0.1867 - acc: 0.945 - ETA: 0s - loss: 0.1889 - acc: 0.943 - ETA: 0s - loss: 0.1880 - acc: 0.944 - ETA: 0s - loss: 0.1891 - acc: 0.943 - ETA: 0s - loss: 0.1901 - acc: 0.943 - ETA: 0s - loss: 0.1890 - acc: 0.943 - ETA: 0s - loss: 0.1902 - acc: 0.942 - ETA: 0s - loss: 0.1904 - acc: 0.943 - 2s 591us/step - loss: 0.1901 - acc: 0.9432 - val_loss: 0.2528 - val_acc: 0.9269\n",
      "Epoch 76/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3564 - acc: 0.921 - ETA: 2s - loss: 0.2908 - acc: 0.921 - ETA: 2s - loss: 0.2674 - acc: 0.925 - ETA: 2s - loss: 0.2603 - acc: 0.928 - ETA: 2s - loss: 0.2466 - acc: 0.932 - ETA: 1s - loss: 0.2446 - acc: 0.930 - ETA: 1s - loss: 0.2325 - acc: 0.931 - ETA: 1s - loss: 0.2186 - acc: 0.937 - ETA: 1s - loss: 0.2070 - acc: 0.941 - ETA: 1s - loss: 0.2103 - acc: 0.937 - ETA: 1s - loss: 0.2107 - acc: 0.936 - ETA: 1s - loss: 0.2082 - acc: 0.938 - ETA: 1s - loss: 0.2075 - acc: 0.937 - ETA: 1s - loss: 0.2034 - acc: 0.938 - ETA: 1s - loss: 0.2018 - acc: 0.938 - ETA: 1s - loss: 0.2035 - acc: 0.936 - ETA: 1s - loss: 0.1976 - acc: 0.938 - ETA: 1s - loss: 0.1964 - acc: 0.938 - ETA: 0s - loss: 0.1959 - acc: 0.939 - ETA: 0s - loss: 0.1936 - acc: 0.939 - ETA: 0s - loss: 0.1946 - acc: 0.940 - ETA: 0s - loss: 0.1978 - acc: 0.937 - ETA: 0s - loss: 0.1949 - acc: 0.939 - ETA: 0s - loss: 0.1945 - acc: 0.940 - ETA: 0s - loss: 0.1940 - acc: 0.941 - ETA: 0s - loss: 0.1929 - acc: 0.940 - ETA: 0s - loss: 0.1940 - acc: 0.940 - ETA: 0s - loss: 0.1930 - acc: 0.940 - ETA: 0s - loss: 0.1935 - acc: 0.940 - ETA: 0s - loss: 0.1916 - acc: 0.940 - ETA: 0s - loss: 0.1922 - acc: 0.940 - ETA: 0s - loss: 0.1909 - acc: 0.941 - 2s 599us/step - loss: 0.1906 - acc: 0.9415 - val_loss: 0.2595 - val_acc: 0.9224\n",
      "Epoch 77/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1674 - acc: 0.953 - ETA: 2s - loss: 0.2074 - acc: 0.942 - ETA: 2s - loss: 0.2569 - acc: 0.906 - ETA: 2s - loss: 0.2370 - acc: 0.916 - ETA: 2s - loss: 0.2252 - acc: 0.923 - ETA: 2s - loss: 0.2149 - acc: 0.929 - ETA: 1s - loss: 0.2231 - acc: 0.925 - ETA: 1s - loss: 0.2082 - acc: 0.930 - ETA: 1s - loss: 0.2182 - acc: 0.927 - ETA: 1s - loss: 0.2109 - acc: 0.930 - ETA: 1s - loss: 0.2136 - acc: 0.930 - ETA: 1s - loss: 0.2051 - acc: 0.934 - ETA: 1s - loss: 0.2059 - acc: 0.934 - ETA: 1s - loss: 0.2002 - acc: 0.938 - ETA: 1s - loss: 0.2047 - acc: 0.937 - ETA: 1s - loss: 0.2016 - acc: 0.939 - ETA: 1s - loss: 0.2020 - acc: 0.938 - ETA: 1s - loss: 0.1989 - acc: 0.940 - ETA: 1s - loss: 0.2025 - acc: 0.937 - ETA: 0s - loss: 0.2016 - acc: 0.939 - ETA: 0s - loss: 0.2003 - acc: 0.939 - ETA: 0s - loss: 0.1986 - acc: 0.939 - ETA: 0s - loss: 0.1960 - acc: 0.941 - ETA: 0s - loss: 0.1946 - acc: 0.940 - ETA: 0s - loss: 0.1944 - acc: 0.940 - ETA: 0s - loss: 0.1940 - acc: 0.940 - ETA: 0s - loss: 0.1946 - acc: 0.940 - ETA: 0s - loss: 0.1951 - acc: 0.940 - ETA: 0s - loss: 0.1942 - acc: 0.940 - ETA: 0s - loss: 0.1942 - acc: 0.939 - ETA: 0s - loss: 0.1962 - acc: 0.938 - ETA: 0s - loss: 0.1973 - acc: 0.938 - 3s 616us/step - loss: 0.1966 - acc: 0.9388 - val_loss: 0.2550 - val_acc: 0.9301\n",
      "Epoch 78/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1221 - acc: 0.968 - ETA: 2s - loss: 0.1977 - acc: 0.932 - ETA: 2s - loss: 0.2015 - acc: 0.931 - ETA: 2s - loss: 0.1879 - acc: 0.942 - ETA: 1s - loss: 0.1798 - acc: 0.947 - ETA: 1s - loss: 0.1817 - acc: 0.946 - ETA: 1s - loss: 0.1729 - acc: 0.950 - ETA: 1s - loss: 0.1693 - acc: 0.950 - ETA: 1s - loss: 0.1649 - acc: 0.952 - ETA: 1s - loss: 0.1685 - acc: 0.951 - ETA: 1s - loss: 0.1807 - acc: 0.943 - ETA: 1s - loss: 0.1875 - acc: 0.940 - ETA: 1s - loss: 0.1853 - acc: 0.942 - ETA: 1s - loss: 0.1907 - acc: 0.939 - ETA: 1s - loss: 0.1894 - acc: 0.940 - ETA: 1s - loss: 0.1891 - acc: 0.941 - ETA: 1s - loss: 0.1887 - acc: 0.940 - ETA: 1s - loss: 0.1896 - acc: 0.938 - ETA: 0s - loss: 0.1882 - acc: 0.940 - ETA: 0s - loss: 0.1858 - acc: 0.940 - ETA: 0s - loss: 0.1839 - acc: 0.942 - ETA: 0s - loss: 0.1834 - acc: 0.941 - ETA: 0s - loss: 0.1813 - acc: 0.943 - ETA: 0s - loss: 0.1798 - acc: 0.943 - ETA: 0s - loss: 0.1793 - acc: 0.943 - ETA: 0s - loss: 0.1796 - acc: 0.944 - ETA: 0s - loss: 0.1805 - acc: 0.944 - ETA: 0s - loss: 0.1803 - acc: 0.944 - ETA: 0s - loss: 0.1809 - acc: 0.944 - ETA: 0s - loss: 0.1799 - acc: 0.944 - ETA: 0s - loss: 0.1779 - acc: 0.945 - ETA: 0s - loss: 0.1776 - acc: 0.944 - 2s 589us/step - loss: 0.1767 - acc: 0.9452 - val_loss: 0.2531 - val_acc: 0.9244\n",
      "Epoch 79/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1485 - acc: 0.937 - ETA: 2s - loss: 0.1583 - acc: 0.953 - ETA: 2s - loss: 0.1534 - acc: 0.956 - ETA: 1s - loss: 0.1627 - acc: 0.953 - ETA: 1s - loss: 0.1815 - acc: 0.946 - ETA: 1s - loss: 0.1774 - acc: 0.947 - ETA: 1s - loss: 0.1816 - acc: 0.947 - ETA: 1s - loss: 0.1753 - acc: 0.949 - ETA: 1s - loss: 0.1698 - acc: 0.951 - ETA: 1s - loss: 0.1665 - acc: 0.951 - ETA: 1s - loss: 0.1609 - acc: 0.954 - ETA: 1s - loss: 0.1650 - acc: 0.953 - ETA: 1s - loss: 0.1685 - acc: 0.952 - ETA: 1s - loss: 0.1687 - acc: 0.953 - ETA: 1s - loss: 0.1688 - acc: 0.952 - ETA: 1s - loss: 0.1660 - acc: 0.954 - ETA: 1s - loss: 0.1708 - acc: 0.952 - ETA: 1s - loss: 0.1682 - acc: 0.954 - ETA: 0s - loss: 0.1698 - acc: 0.953 - ETA: 0s - loss: 0.1720 - acc: 0.953 - ETA: 0s - loss: 0.1737 - acc: 0.952 - ETA: 0s - loss: 0.1770 - acc: 0.950 - ETA: 0s - loss: 0.1783 - acc: 0.947 - ETA: 0s - loss: 0.1775 - acc: 0.948 - ETA: 0s - loss: 0.1791 - acc: 0.948 - ETA: 0s - loss: 0.1835 - acc: 0.945 - ETA: 0s - loss: 0.1842 - acc: 0.944 - ETA: 0s - loss: 0.1842 - acc: 0.944 - ETA: 0s - loss: 0.1841 - acc: 0.945 - ETA: 0s - loss: 0.1862 - acc: 0.944 - ETA: 0s - loss: 0.1868 - acc: 0.943 - ETA: 0s - loss: 0.1887 - acc: 0.943 - 2s 587us/step - loss: 0.1882 - acc: 0.9434 - val_loss: 0.3207 - val_acc: 0.9038\n",
      "Epoch 80/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1608 - acc: 0.937 - ETA: 2s - loss: 0.1541 - acc: 0.942 - ETA: 2s - loss: 0.1909 - acc: 0.934 - ETA: 2s - loss: 0.1873 - acc: 0.937 - ETA: 2s - loss: 0.1947 - acc: 0.932 - ETA: 1s - loss: 0.2020 - acc: 0.931 - ETA: 2s - loss: 0.2014 - acc: 0.932 - ETA: 2s - loss: 0.2029 - acc: 0.933 - ETA: 1s - loss: 0.1960 - acc: 0.940 - ETA: 1s - loss: 0.1929 - acc: 0.942 - ETA: 1s - loss: 0.1900 - acc: 0.943 - ETA: 1s - loss: 0.1884 - acc: 0.944 - ETA: 1s - loss: 0.1859 - acc: 0.945 - ETA: 1s - loss: 0.1893 - acc: 0.942 - ETA: 1s - loss: 0.1904 - acc: 0.942 - ETA: 1s - loss: 0.1886 - acc: 0.942 - ETA: 1s - loss: 0.1835 - acc: 0.945 - ETA: 1s - loss: 0.1892 - acc: 0.943 - ETA: 1s - loss: 0.1886 - acc: 0.943 - ETA: 1s - loss: 0.1938 - acc: 0.941 - ETA: 0s - loss: 0.1945 - acc: 0.940 - ETA: 0s - loss: 0.1964 - acc: 0.939 - ETA: 0s - loss: 0.1994 - acc: 0.938 - ETA: 0s - loss: 0.1974 - acc: 0.939 - ETA: 0s - loss: 0.1958 - acc: 0.940 - ETA: 0s - loss: 0.1983 - acc: 0.939 - ETA: 0s - loss: 0.1991 - acc: 0.938 - ETA: 0s - loss: 0.2027 - acc: 0.937 - ETA: 0s - loss: 0.2019 - acc: 0.937 - ETA: 0s - loss: 0.2013 - acc: 0.938 - ETA: 0s - loss: 0.2025 - acc: 0.939 - ETA: 0s - loss: 0.2055 - acc: 0.937 - ETA: 0s - loss: 0.2071 - acc: 0.936 - 3s 620us/step - loss: 0.2075 - acc: 0.9361 - val_loss: 0.2830 - val_acc: 0.9122\n",
      "Epoch 81/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2499 - acc: 0.937 - ETA: 2s - loss: 0.2792 - acc: 0.911 - ETA: 2s - loss: 0.2344 - acc: 0.928 - ETA: 2s - loss: 0.2272 - acc: 0.930 - ETA: 1s - loss: 0.2318 - acc: 0.925 - ETA: 1s - loss: 0.2242 - acc: 0.927 - ETA: 1s - loss: 0.2227 - acc: 0.926 - ETA: 1s - loss: 0.2228 - acc: 0.928 - ETA: 1s - loss: 0.2201 - acc: 0.932 - ETA: 1s - loss: 0.2208 - acc: 0.931 - ETA: 1s - loss: 0.2216 - acc: 0.931 - ETA: 1s - loss: 0.2256 - acc: 0.929 - ETA: 1s - loss: 0.2250 - acc: 0.929 - ETA: 1s - loss: 0.2196 - acc: 0.932 - ETA: 1s - loss: 0.2199 - acc: 0.933 - ETA: 1s - loss: 0.2207 - acc: 0.933 - ETA: 1s - loss: 0.2208 - acc: 0.932 - ETA: 1s - loss: 0.2187 - acc: 0.933 - ETA: 0s - loss: 0.2154 - acc: 0.934 - ETA: 0s - loss: 0.2135 - acc: 0.934 - ETA: 0s - loss: 0.2138 - acc: 0.935 - ETA: 0s - loss: 0.2127 - acc: 0.934 - ETA: 0s - loss: 0.2122 - acc: 0.934 - ETA: 0s - loss: 0.2130 - acc: 0.934 - ETA: 0s - loss: 0.2147 - acc: 0.932 - ETA: 0s - loss: 0.2145 - acc: 0.932 - ETA: 0s - loss: 0.2139 - acc: 0.932 - ETA: 0s - loss: 0.2125 - acc: 0.933 - ETA: 0s - loss: 0.2102 - acc: 0.934 - ETA: 0s - loss: 0.2084 - acc: 0.935 - ETA: 0s - loss: 0.2064 - acc: 0.937 - ETA: 0s - loss: 0.2052 - acc: 0.938 - 2s 597us/step - loss: 0.2053 - acc: 0.9378 - val_loss: 0.2709 - val_acc: 0.9026\n",
      "Epoch 82/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1708 - acc: 0.953 - ETA: 2s - loss: 0.2055 - acc: 0.937 - ETA: 2s - loss: 0.1958 - acc: 0.940 - ETA: 2s - loss: 0.1909 - acc: 0.933 - ETA: 1s - loss: 0.1846 - acc: 0.937 - ETA: 1s - loss: 0.1771 - acc: 0.941 - ETA: 1s - loss: 0.1708 - acc: 0.947 - ETA: 1s - loss: 0.1688 - acc: 0.950 - ETA: 1s - loss: 0.1752 - acc: 0.949 - ETA: 1s - loss: 0.1711 - acc: 0.951 - ETA: 1s - loss: 0.1695 - acc: 0.952 - ETA: 1s - loss: 0.1760 - acc: 0.949 - ETA: 1s - loss: 0.1767 - acc: 0.949 - ETA: 1s - loss: 0.1730 - acc: 0.952 - ETA: 1s - loss: 0.1776 - acc: 0.950 - ETA: 1s - loss: 0.1769 - acc: 0.951 - ETA: 1s - loss: 0.1760 - acc: 0.951 - ETA: 1s - loss: 0.1741 - acc: 0.952 - ETA: 0s - loss: 0.1736 - acc: 0.953 - ETA: 0s - loss: 0.1723 - acc: 0.953 - ETA: 0s - loss: 0.1713 - acc: 0.953 - ETA: 0s - loss: 0.1714 - acc: 0.953 - ETA: 0s - loss: 0.1724 - acc: 0.952 - ETA: 0s - loss: 0.1737 - acc: 0.952 - ETA: 0s - loss: 0.1714 - acc: 0.953 - ETA: 0s - loss: 0.1757 - acc: 0.950 - ETA: 0s - loss: 0.1780 - acc: 0.949 - ETA: 0s - loss: 0.1804 - acc: 0.948 - ETA: 0s - loss: 0.1804 - acc: 0.948 - ETA: 0s - loss: 0.1798 - acc: 0.949 - ETA: 0s - loss: 0.1814 - acc: 0.948 - ETA: 0s - loss: 0.1805 - acc: 0.949 - 2s 595us/step - loss: 0.1801 - acc: 0.9496 - val_loss: 0.2503 - val_acc: 0.9212\n",
      "Epoch 83/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1762 - acc: 0.937 - ETA: 2s - loss: 0.1960 - acc: 0.932 - ETA: 2s - loss: 0.1833 - acc: 0.946 - ETA: 2s - loss: 0.1648 - acc: 0.955 - ETA: 1s - loss: 0.1683 - acc: 0.951 - ETA: 1s - loss: 0.1799 - acc: 0.944 - ETA: 1s - loss: 0.1814 - acc: 0.942 - ETA: 1s - loss: 0.1755 - acc: 0.944 - ETA: 1s - loss: 0.1729 - acc: 0.945 - ETA: 1s - loss: 0.1714 - acc: 0.944 - ETA: 1s - loss: 0.1729 - acc: 0.944 - ETA: 1s - loss: 0.1757 - acc: 0.943 - ETA: 1s - loss: 0.1736 - acc: 0.944 - ETA: 1s - loss: 0.1750 - acc: 0.944 - ETA: 1s - loss: 0.1759 - acc: 0.942 - ETA: 1s - loss: 0.1747 - acc: 0.944 - ETA: 1s - loss: 0.1781 - acc: 0.942 - ETA: 1s - loss: 0.1828 - acc: 0.940 - ETA: 0s - loss: 0.1817 - acc: 0.940 - ETA: 0s - loss: 0.1800 - acc: 0.942 - ETA: 0s - loss: 0.1790 - acc: 0.943 - ETA: 0s - loss: 0.1799 - acc: 0.943 - ETA: 0s - loss: 0.1793 - acc: 0.943 - ETA: 0s - loss: 0.1803 - acc: 0.943 - ETA: 0s - loss: 0.1775 - acc: 0.945 - ETA: 0s - loss: 0.1760 - acc: 0.945 - ETA: 0s - loss: 0.1747 - acc: 0.946 - ETA: 0s - loss: 0.1768 - acc: 0.945 - ETA: 0s - loss: 0.1755 - acc: 0.946 - ETA: 0s - loss: 0.1740 - acc: 0.946 - ETA: 0s - loss: 0.1721 - acc: 0.946 - ETA: 0s - loss: 0.1719 - acc: 0.946 - 2s 597us/step - loss: 0.1727 - acc: 0.9466 - val_loss: 0.2498 - val_acc: 0.9115\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3628 - acc: 0.859 - ETA: 2s - loss: 0.2131 - acc: 0.937 - ETA: 2s - loss: 0.2239 - acc: 0.925 - ETA: 2s - loss: 0.2131 - acc: 0.924 - ETA: 1s - loss: 0.1990 - acc: 0.928 - ETA: 1s - loss: 0.2068 - acc: 0.923 - ETA: 1s - loss: 0.2121 - acc: 0.924 - ETA: 1s - loss: 0.2102 - acc: 0.926 - ETA: 1s - loss: 0.2031 - acc: 0.930 - ETA: 1s - loss: 0.2057 - acc: 0.930 - ETA: 1s - loss: 0.2079 - acc: 0.930 - ETA: 1s - loss: 0.2093 - acc: 0.928 - ETA: 1s - loss: 0.2079 - acc: 0.928 - ETA: 1s - loss: 0.2048 - acc: 0.929 - ETA: 1s - loss: 0.2035 - acc: 0.930 - ETA: 1s - loss: 0.1977 - acc: 0.933 - ETA: 1s - loss: 0.1993 - acc: 0.933 - ETA: 1s - loss: 0.2007 - acc: 0.933 - ETA: 0s - loss: 0.2026 - acc: 0.933 - ETA: 0s - loss: 0.2035 - acc: 0.932 - ETA: 0s - loss: 0.2000 - acc: 0.933 - ETA: 0s - loss: 0.1975 - acc: 0.935 - ETA: 0s - loss: 0.1986 - acc: 0.934 - ETA: 0s - loss: 0.1988 - acc: 0.935 - ETA: 0s - loss: 0.1962 - acc: 0.936 - ETA: 0s - loss: 0.1947 - acc: 0.937 - ETA: 0s - loss: 0.1938 - acc: 0.938 - ETA: 0s - loss: 0.1929 - acc: 0.938 - ETA: 0s - loss: 0.1920 - acc: 0.938 - ETA: 0s - loss: 0.1914 - acc: 0.939 - ETA: 0s - loss: 0.1904 - acc: 0.940 - ETA: 0s - loss: 0.1937 - acc: 0.939 - 3s 622us/step - loss: 0.1931 - acc: 0.9393 - val_loss: 0.2468 - val_acc: 0.9276\n",
      "Epoch 85/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1410 - acc: 0.968 - ETA: 2s - loss: 0.1880 - acc: 0.937 - ETA: 2s - loss: 0.1670 - acc: 0.940 - ETA: 2s - loss: 0.1838 - acc: 0.933 - ETA: 1s - loss: 0.1760 - acc: 0.942 - ETA: 1s - loss: 0.1678 - acc: 0.948 - ETA: 1s - loss: 0.1729 - acc: 0.947 - ETA: 1s - loss: 0.1767 - acc: 0.942 - ETA: 1s - loss: 0.1758 - acc: 0.943 - ETA: 1s - loss: 0.1800 - acc: 0.942 - ETA: 1s - loss: 0.1805 - acc: 0.944 - ETA: 1s - loss: 0.1853 - acc: 0.942 - ETA: 1s - loss: 0.1862 - acc: 0.942 - ETA: 1s - loss: 0.1857 - acc: 0.942 - ETA: 1s - loss: 0.1832 - acc: 0.943 - ETA: 1s - loss: 0.1845 - acc: 0.942 - ETA: 1s - loss: 0.1836 - acc: 0.942 - ETA: 1s - loss: 0.1793 - acc: 0.945 - ETA: 1s - loss: 0.1777 - acc: 0.946 - ETA: 0s - loss: 0.1801 - acc: 0.946 - ETA: 0s - loss: 0.1776 - acc: 0.947 - ETA: 0s - loss: 0.1773 - acc: 0.948 - ETA: 0s - loss: 0.1783 - acc: 0.947 - ETA: 0s - loss: 0.1783 - acc: 0.946 - ETA: 0s - loss: 0.1777 - acc: 0.946 - ETA: 0s - loss: 0.1760 - acc: 0.946 - ETA: 0s - loss: 0.1778 - acc: 0.945 - ETA: 0s - loss: 0.1750 - acc: 0.946 - ETA: 0s - loss: 0.1741 - acc: 0.947 - ETA: 0s - loss: 0.1740 - acc: 0.946 - ETA: 0s - loss: 0.1738 - acc: 0.947 - ETA: 0s - loss: 0.1752 - acc: 0.945 - 3s 620us/step - loss: 0.1754 - acc: 0.9459 - val_loss: 0.2638 - val_acc: 0.9250\n",
      "Epoch 86/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1034 - acc: 0.968 - ETA: 2s - loss: 0.0984 - acc: 0.989 - ETA: 2s - loss: 0.1175 - acc: 0.984 - ETA: 2s - loss: 0.1235 - acc: 0.975 - ETA: 1s - loss: 0.1274 - acc: 0.968 - ETA: 2s - loss: 0.1316 - acc: 0.965 - ETA: 2s - loss: 0.1278 - acc: 0.967 - ETA: 2s - loss: 0.1306 - acc: 0.963 - ETA: 2s - loss: 0.1332 - acc: 0.962 - ETA: 2s - loss: 0.1376 - acc: 0.960 - ETA: 1s - loss: 0.1391 - acc: 0.958 - ETA: 1s - loss: 0.1410 - acc: 0.959 - ETA: 1s - loss: 0.1460 - acc: 0.958 - ETA: 1s - loss: 0.1524 - acc: 0.954 - ETA: 1s - loss: 0.1547 - acc: 0.953 - ETA: 1s - loss: 0.1536 - acc: 0.954 - ETA: 1s - loss: 0.1535 - acc: 0.955 - ETA: 1s - loss: 0.1548 - acc: 0.955 - ETA: 1s - loss: 0.1602 - acc: 0.952 - ETA: 1s - loss: 0.1656 - acc: 0.948 - ETA: 1s - loss: 0.1627 - acc: 0.950 - ETA: 0s - loss: 0.1651 - acc: 0.948 - ETA: 0s - loss: 0.1655 - acc: 0.949 - ETA: 0s - loss: 0.1653 - acc: 0.949 - ETA: 0s - loss: 0.1655 - acc: 0.948 - ETA: 0s - loss: 0.1651 - acc: 0.948 - ETA: 0s - loss: 0.1680 - acc: 0.948 - ETA: 0s - loss: 0.1700 - acc: 0.947 - ETA: 0s - loss: 0.1677 - acc: 0.948 - ETA: 0s - loss: 0.1680 - acc: 0.948 - ETA: 0s - loss: 0.1678 - acc: 0.948 - ETA: 0s - loss: 0.1701 - acc: 0.947 - ETA: 0s - loss: 0.1686 - acc: 0.948 - ETA: 0s - loss: 0.1682 - acc: 0.948 - 3s 623us/step - loss: 0.1692 - acc: 0.9484 - val_loss: 0.3228 - val_acc: 0.9064\n",
      "Epoch 87/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1381 - acc: 0.953 - ETA: 2s - loss: 0.1497 - acc: 0.963 - ETA: 2s - loss: 0.1671 - acc: 0.959 - ETA: 2s - loss: 0.1596 - acc: 0.955 - ETA: 1s - loss: 0.1699 - acc: 0.947 - ETA: 1s - loss: 0.1728 - acc: 0.946 - ETA: 1s - loss: 0.1749 - acc: 0.947 - ETA: 1s - loss: 0.1792 - acc: 0.943 - ETA: 1s - loss: 0.1830 - acc: 0.943 - ETA: 1s - loss: 0.1826 - acc: 0.942 - ETA: 1s - loss: 0.1792 - acc: 0.944 - ETA: 1s - loss: 0.1745 - acc: 0.945 - ETA: 1s - loss: 0.1768 - acc: 0.945 - ETA: 1s - loss: 0.1733 - acc: 0.946 - ETA: 1s - loss: 0.1706 - acc: 0.947 - ETA: 1s - loss: 0.1694 - acc: 0.948 - ETA: 1s - loss: 0.1729 - acc: 0.947 - ETA: 1s - loss: 0.1721 - acc: 0.946 - ETA: 0s - loss: 0.1705 - acc: 0.947 - ETA: 0s - loss: 0.1727 - acc: 0.946 - ETA: 0s - loss: 0.1722 - acc: 0.947 - ETA: 0s - loss: 0.1738 - acc: 0.946 - ETA: 0s - loss: 0.1761 - acc: 0.946 - ETA: 0s - loss: 0.1744 - acc: 0.947 - ETA: 0s - loss: 0.1745 - acc: 0.946 - ETA: 0s - loss: 0.1752 - acc: 0.945 - ETA: 0s - loss: 0.1757 - acc: 0.945 - ETA: 0s - loss: 0.1784 - acc: 0.944 - ETA: 0s - loss: 0.1779 - acc: 0.945 - ETA: 0s - loss: 0.1779 - acc: 0.945 - ETA: 0s - loss: 0.1819 - acc: 0.943 - ETA: 0s - loss: 0.1822 - acc: 0.943 - 2s 600us/step - loss: 0.1833 - acc: 0.9434 - val_loss: 0.3104 - val_acc: 0.9013\n",
      "Epoch 88/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1441 - acc: 0.968 - ETA: 2s - loss: 0.3171 - acc: 0.890 - ETA: 2s - loss: 0.2802 - acc: 0.900 - ETA: 2s - loss: 0.2608 - acc: 0.906 - ETA: 1s - loss: 0.2355 - acc: 0.914 - ETA: 1s - loss: 0.2441 - acc: 0.917 - ETA: 1s - loss: 0.2314 - acc: 0.924 - ETA: 1s - loss: 0.2249 - acc: 0.927 - ETA: 1s - loss: 0.2219 - acc: 0.928 - ETA: 1s - loss: 0.2154 - acc: 0.930 - ETA: 1s - loss: 0.2138 - acc: 0.929 - ETA: 1s - loss: 0.2123 - acc: 0.930 - ETA: 1s - loss: 0.2103 - acc: 0.931 - ETA: 1s - loss: 0.2120 - acc: 0.931 - ETA: 1s - loss: 0.2106 - acc: 0.932 - ETA: 1s - loss: 0.2081 - acc: 0.932 - ETA: 1s - loss: 0.2058 - acc: 0.932 - ETA: 1s - loss: 0.2031 - acc: 0.934 - ETA: 0s - loss: 0.2014 - acc: 0.935 - ETA: 0s - loss: 0.1989 - acc: 0.936 - ETA: 0s - loss: 0.1986 - acc: 0.936 - ETA: 0s - loss: 0.1951 - acc: 0.937 - ETA: 0s - loss: 0.1939 - acc: 0.937 - ETA: 0s - loss: 0.1911 - acc: 0.938 - ETA: 0s - loss: 0.1907 - acc: 0.938 - ETA: 0s - loss: 0.1877 - acc: 0.940 - ETA: 0s - loss: 0.1882 - acc: 0.939 - ETA: 0s - loss: 0.1901 - acc: 0.939 - ETA: 0s - loss: 0.1910 - acc: 0.938 - ETA: 0s - loss: 0.1896 - acc: 0.939 - ETA: 0s - loss: 0.1885 - acc: 0.940 - ETA: 0s - loss: 0.1876 - acc: 0.941 - 2s 596us/step - loss: 0.1871 - acc: 0.9412 - val_loss: 0.2663 - val_acc: 0.9295\n",
      "Epoch 89/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1552 - acc: 0.937 - ETA: 2s - loss: 0.1365 - acc: 0.958 - ETA: 2s - loss: 0.1251 - acc: 0.965 - ETA: 2s - loss: 0.1624 - acc: 0.948 - ETA: 1s - loss: 0.1656 - acc: 0.949 - ETA: 1s - loss: 0.1704 - acc: 0.947 - ETA: 1s - loss: 0.1741 - acc: 0.948 - ETA: 1s - loss: 0.1827 - acc: 0.944 - ETA: 1s - loss: 0.1789 - acc: 0.947 - ETA: 1s - loss: 0.1828 - acc: 0.944 - ETA: 1s - loss: 0.1825 - acc: 0.945 - ETA: 1s - loss: 0.1768 - acc: 0.949 - ETA: 1s - loss: 0.1762 - acc: 0.949 - ETA: 1s - loss: 0.1788 - acc: 0.947 - ETA: 1s - loss: 0.1764 - acc: 0.948 - ETA: 1s - loss: 0.1816 - acc: 0.949 - ETA: 1s - loss: 0.1941 - acc: 0.947 - ETA: 1s - loss: 0.1934 - acc: 0.947 - ETA: 0s - loss: 0.1948 - acc: 0.946 - ETA: 0s - loss: 0.1944 - acc: 0.946 - ETA: 0s - loss: 0.1950 - acc: 0.946 - ETA: 0s - loss: 0.1968 - acc: 0.945 - ETA: 0s - loss: 0.1985 - acc: 0.945 - ETA: 0s - loss: 0.1988 - acc: 0.944 - ETA: 0s - loss: 0.1969 - acc: 0.944 - ETA: 0s - loss: 0.1969 - acc: 0.944 - ETA: 0s - loss: 0.1997 - acc: 0.943 - ETA: 0s - loss: 0.1980 - acc: 0.944 - ETA: 0s - loss: 0.1957 - acc: 0.944 - ETA: 0s - loss: 0.1977 - acc: 0.942 - ETA: 0s - loss: 0.1975 - acc: 0.942 - ETA: 0s - loss: 0.1962 - acc: 0.943 - 2s 602us/step - loss: 0.1977 - acc: 0.9430 - val_loss: 0.3390 - val_acc: 0.8814\n",
      "Epoch 90/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2451 - acc: 0.906 - ETA: 2s - loss: 0.2977 - acc: 0.906 - ETA: 2s - loss: 0.2841 - acc: 0.906 - ETA: 1s - loss: 0.2680 - acc: 0.906 - ETA: 1s - loss: 0.2427 - acc: 0.920 - ETA: 1s - loss: 0.2346 - acc: 0.924 - ETA: 1s - loss: 0.2223 - acc: 0.931 - ETA: 1s - loss: 0.2184 - acc: 0.935 - ETA: 1s - loss: 0.2194 - acc: 0.937 - ETA: 1s - loss: 0.2171 - acc: 0.938 - ETA: 1s - loss: 0.2241 - acc: 0.938 - ETA: 1s - loss: 0.2189 - acc: 0.940 - ETA: 1s - loss: 0.2260 - acc: 0.938 - ETA: 1s - loss: 0.2250 - acc: 0.938 - ETA: 1s - loss: 0.2200 - acc: 0.940 - ETA: 1s - loss: 0.2170 - acc: 0.941 - ETA: 1s - loss: 0.2189 - acc: 0.939 - ETA: 1s - loss: 0.2171 - acc: 0.940 - ETA: 0s - loss: 0.2194 - acc: 0.939 - ETA: 0s - loss: 0.2202 - acc: 0.938 - ETA: 0s - loss: 0.2198 - acc: 0.939 - ETA: 0s - loss: 0.2203 - acc: 0.939 - ETA: 0s - loss: 0.2193 - acc: 0.939 - ETA: 0s - loss: 0.2162 - acc: 0.940 - ETA: 0s - loss: 0.2146 - acc: 0.941 - ETA: 0s - loss: 0.2131 - acc: 0.942 - ETA: 0s - loss: 0.2124 - acc: 0.942 - ETA: 0s - loss: 0.2117 - acc: 0.942 - ETA: 0s - loss: 0.2127 - acc: 0.942 - ETA: 0s - loss: 0.2116 - acc: 0.942 - ETA: 0s - loss: 0.2114 - acc: 0.942 - ETA: 0s - loss: 0.2107 - acc: 0.943 - ETA: 0s - loss: 0.2115 - acc: 0.942 - ETA: 0s - loss: 0.2092 - acc: 0.943 - ETA: 0s - loss: 0.2094 - acc: 0.943 - 3s 635us/step - loss: 0.2089 - acc: 0.9434 - val_loss: 0.2510 - val_acc: 0.9327\n",
      "Epoch 91/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1385 - acc: 0.953 - ETA: 2s - loss: 0.1588 - acc: 0.942 - ETA: 2s - loss: 0.1872 - acc: 0.934 - ETA: 2s - loss: 0.2035 - acc: 0.930 - ETA: 2s - loss: 0.2026 - acc: 0.933 - ETA: 2s - loss: 0.1954 - acc: 0.937 - ETA: 1s - loss: 0.1833 - acc: 0.944 - ETA: 1s - loss: 0.1761 - acc: 0.947 - ETA: 1s - loss: 0.1778 - acc: 0.948 - ETA: 1s - loss: 0.1805 - acc: 0.946 - ETA: 1s - loss: 0.1769 - acc: 0.947 - ETA: 1s - loss: 0.1754 - acc: 0.948 - ETA: 1s - loss: 0.1731 - acc: 0.950 - ETA: 1s - loss: 0.1742 - acc: 0.950 - ETA: 1s - loss: 0.1735 - acc: 0.952 - ETA: 1s - loss: 0.1801 - acc: 0.950 - ETA: 1s - loss: 0.1802 - acc: 0.949 - ETA: 1s - loss: 0.1799 - acc: 0.949 - ETA: 1s - loss: 0.1776 - acc: 0.950 - ETA: 1s - loss: 0.1759 - acc: 0.950 - ETA: 0s - loss: 0.1744 - acc: 0.950 - ETA: 0s - loss: 0.1717 - acc: 0.952 - ETA: 0s - loss: 0.1716 - acc: 0.952 - ETA: 0s - loss: 0.1713 - acc: 0.952 - ETA: 0s - loss: 0.1736 - acc: 0.951 - ETA: 0s - loss: 0.1763 - acc: 0.949 - ETA: 0s - loss: 0.1781 - acc: 0.949 - ETA: 0s - loss: 0.1773 - acc: 0.949 - ETA: 0s - loss: 0.1763 - acc: 0.949 - ETA: 0s - loss: 0.1761 - acc: 0.949 - ETA: 0s - loss: 0.1795 - acc: 0.947 - ETA: 0s - loss: 0.1791 - acc: 0.947 - ETA: 0s - loss: 0.1793 - acc: 0.947 - ETA: 0s - loss: 0.1801 - acc: 0.947 - 3s 654us/step - loss: 0.1799 - acc: 0.9476 - val_loss: 0.2535 - val_acc: 0.9237\n",
      "Epoch 92/150\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1529 - acc: 0.968 - ETA: 2s - loss: 0.1654 - acc: 0.953 - ETA: 2s - loss: 0.1707 - acc: 0.943 - ETA: 2s - loss: 0.1727 - acc: 0.944 - ETA: 2s - loss: 0.1717 - acc: 0.942 - ETA: 2s - loss: 0.1688 - acc: 0.943 - ETA: 1s - loss: 0.1677 - acc: 0.944 - ETA: 1s - loss: 0.1679 - acc: 0.946 - ETA: 1s - loss: 0.1621 - acc: 0.949 - ETA: 1s - loss: 0.1628 - acc: 0.949 - ETA: 1s - loss: 0.1608 - acc: 0.950 - ETA: 1s - loss: 0.1619 - acc: 0.949 - ETA: 1s - loss: 0.1601 - acc: 0.950 - ETA: 1s - loss: 0.1609 - acc: 0.949 - ETA: 1s - loss: 0.1637 - acc: 0.948 - ETA: 1s - loss: 0.1643 - acc: 0.948 - ETA: 1s - loss: 0.1648 - acc: 0.947 - ETA: 1s - loss: 0.1672 - acc: 0.945 - ETA: 1s - loss: 0.1672 - acc: 0.945 - ETA: 1s - loss: 0.1672 - acc: 0.946 - ETA: 1s - loss: 0.1678 - acc: 0.946 - ETA: 1s - loss: 0.1679 - acc: 0.947 - ETA: 1s - loss: 0.1702 - acc: 0.946 - ETA: 0s - loss: 0.1704 - acc: 0.945 - ETA: 0s - loss: 0.1697 - acc: 0.945 - ETA: 0s - loss: 0.1723 - acc: 0.944 - ETA: 0s - loss: 0.1731 - acc: 0.945 - ETA: 0s - loss: 0.1737 - acc: 0.945 - ETA: 0s - loss: 0.1747 - acc: 0.944 - ETA: 0s - loss: 0.1745 - acc: 0.945 - ETA: 0s - loss: 0.1757 - acc: 0.944 - ETA: 0s - loss: 0.1742 - acc: 0.944 - ETA: 0s - loss: 0.1739 - acc: 0.945 - ETA: 0s - loss: 0.1727 - acc: 0.946 - ETA: 0s - loss: 0.1714 - acc: 0.947 - 3s 677us/step - loss: 0.1714 - acc: 0.9471 - val_loss: 0.2593 - val_acc: 0.9231\n",
      "Epoch 93/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1762 - acc: 0.953 - ETA: 2s - loss: 0.1525 - acc: 0.958 - ETA: 2s - loss: 0.1446 - acc: 0.959 - ETA: 1s - loss: 0.1688 - acc: 0.946 - ETA: 1s - loss: 0.1852 - acc: 0.941 - ETA: 1s - loss: 0.1884 - acc: 0.941 - ETA: 1s - loss: 0.1928 - acc: 0.941 - ETA: 1s - loss: 0.1879 - acc: 0.942 - ETA: 1s - loss: 0.1884 - acc: 0.942 - ETA: 1s - loss: 0.1867 - acc: 0.944 - ETA: 1s - loss: 0.1838 - acc: 0.945 - ETA: 1s - loss: 0.1862 - acc: 0.943 - ETA: 1s - loss: 0.1852 - acc: 0.943 - ETA: 1s - loss: 0.1845 - acc: 0.942 - ETA: 1s - loss: 0.1831 - acc: 0.941 - ETA: 1s - loss: 0.1842 - acc: 0.941 - ETA: 1s - loss: 0.1819 - acc: 0.942 - ETA: 1s - loss: 0.1839 - acc: 0.942 - ETA: 0s - loss: 0.1825 - acc: 0.943 - ETA: 0s - loss: 0.1788 - acc: 0.945 - ETA: 0s - loss: 0.1773 - acc: 0.946 - ETA: 0s - loss: 0.1745 - acc: 0.946 - ETA: 0s - loss: 0.1736 - acc: 0.947 - ETA: 0s - loss: 0.1757 - acc: 0.946 - ETA: 0s - loss: 0.1753 - acc: 0.947 - ETA: 0s - loss: 0.1748 - acc: 0.947 - ETA: 0s - loss: 0.1733 - acc: 0.948 - ETA: 0s - loss: 0.1730 - acc: 0.948 - ETA: 0s - loss: 0.1751 - acc: 0.947 - ETA: 0s - loss: 0.1746 - acc: 0.948 - ETA: 0s - loss: 0.1767 - acc: 0.948 - ETA: 0s - loss: 0.1766 - acc: 0.948 - 2s 595us/step - loss: 0.1763 - acc: 0.9484 - val_loss: 0.2504 - val_acc: 0.9192\n",
      "Epoch 94/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1844 - acc: 0.921 - ETA: 2s - loss: 0.1685 - acc: 0.953 - ETA: 2s - loss: 0.1592 - acc: 0.956 - ETA: 2s - loss: 0.1690 - acc: 0.955 - ETA: 1s - loss: 0.1664 - acc: 0.954 - ETA: 1s - loss: 0.1622 - acc: 0.958 - ETA: 1s - loss: 0.1706 - acc: 0.955 - ETA: 1s - loss: 0.1723 - acc: 0.955 - ETA: 1s - loss: 0.1703 - acc: 0.956 - ETA: 1s - loss: 0.1680 - acc: 0.958 - ETA: 1s - loss: 0.1705 - acc: 0.956 - ETA: 1s - loss: 0.1681 - acc: 0.955 - ETA: 1s - loss: 0.1708 - acc: 0.955 - ETA: 1s - loss: 0.1785 - acc: 0.952 - ETA: 1s - loss: 0.1749 - acc: 0.954 - ETA: 1s - loss: 0.1739 - acc: 0.953 - ETA: 1s - loss: 0.1739 - acc: 0.953 - ETA: 1s - loss: 0.1724 - acc: 0.953 - ETA: 0s - loss: 0.1753 - acc: 0.951 - ETA: 0s - loss: 0.1756 - acc: 0.951 - ETA: 0s - loss: 0.1744 - acc: 0.951 - ETA: 0s - loss: 0.1797 - acc: 0.949 - ETA: 0s - loss: 0.1810 - acc: 0.948 - ETA: 0s - loss: 0.1810 - acc: 0.947 - ETA: 0s - loss: 0.1821 - acc: 0.948 - ETA: 0s - loss: 0.1812 - acc: 0.948 - ETA: 0s - loss: 0.1821 - acc: 0.947 - ETA: 0s - loss: 0.1831 - acc: 0.946 - ETA: 0s - loss: 0.1870 - acc: 0.944 - ETA: 0s - loss: 0.1890 - acc: 0.943 - ETA: 0s - loss: 0.1902 - acc: 0.942 - ETA: 0s - loss: 0.1897 - acc: 0.943 - 2s 595us/step - loss: 0.1895 - acc: 0.9437 - val_loss: 0.2917 - val_acc: 0.9250\n",
      "Epoch 95/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1720 - acc: 0.937 - ETA: 2s - loss: 0.1982 - acc: 0.927 - ETA: 2s - loss: 0.1794 - acc: 0.943 - ETA: 2s - loss: 0.1854 - acc: 0.944 - ETA: 1s - loss: 0.1793 - acc: 0.946 - ETA: 1s - loss: 0.1657 - acc: 0.953 - ETA: 1s - loss: 0.1673 - acc: 0.951 - ETA: 1s - loss: 0.1734 - acc: 0.952 - ETA: 1s - loss: 0.1655 - acc: 0.956 - ETA: 1s - loss: 0.1653 - acc: 0.954 - ETA: 1s - loss: 0.1662 - acc: 0.954 - ETA: 1s - loss: 0.1682 - acc: 0.954 - ETA: 1s - loss: 0.1743 - acc: 0.950 - ETA: 1s - loss: 0.1760 - acc: 0.950 - ETA: 1s - loss: 0.1747 - acc: 0.952 - ETA: 1s - loss: 0.1770 - acc: 0.951 - ETA: 1s - loss: 0.1760 - acc: 0.952 - ETA: 1s - loss: 0.1817 - acc: 0.951 - ETA: 0s - loss: 0.1809 - acc: 0.951 - ETA: 0s - loss: 0.1813 - acc: 0.950 - ETA: 0s - loss: 0.1910 - acc: 0.948 - ETA: 0s - loss: 0.1976 - acc: 0.947 - ETA: 0s - loss: 0.1978 - acc: 0.947 - ETA: 0s - loss: 0.2000 - acc: 0.945 - ETA: 0s - loss: 0.1996 - acc: 0.945 - ETA: 0s - loss: 0.2014 - acc: 0.944 - ETA: 0s - loss: 0.2015 - acc: 0.944 - ETA: 0s - loss: 0.2026 - acc: 0.944 - ETA: 0s - loss: 0.2020 - acc: 0.944 - ETA: 0s - loss: 0.2058 - acc: 0.942 - ETA: 0s - loss: 0.2065 - acc: 0.941 - ETA: 0s - loss: 0.2067 - acc: 0.942 - 2s 596us/step - loss: 0.2071 - acc: 0.9422 - val_loss: 0.2570 - val_acc: 0.9250\n",
      "Epoch 96/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2131 - acc: 0.921 - ETA: 2s - loss: 0.2168 - acc: 0.932 - ETA: 2s - loss: 0.2224 - acc: 0.937 - ETA: 2s - loss: 0.2161 - acc: 0.935 - ETA: 1s - loss: 0.2072 - acc: 0.939 - ETA: 1s - loss: 0.2028 - acc: 0.943 - ETA: 1s - loss: 0.2117 - acc: 0.941 - ETA: 1s - loss: 0.2073 - acc: 0.940 - ETA: 1s - loss: 0.2044 - acc: 0.940 - ETA: 1s - loss: 0.2137 - acc: 0.937 - ETA: 1s - loss: 0.2223 - acc: 0.933 - ETA: 1s - loss: 0.2178 - acc: 0.934 - ETA: 1s - loss: 0.2178 - acc: 0.933 - ETA: 1s - loss: 0.2146 - acc: 0.934 - ETA: 1s - loss: 0.2113 - acc: 0.935 - ETA: 1s - loss: 0.2130 - acc: 0.935 - ETA: 1s - loss: 0.2107 - acc: 0.935 - ETA: 1s - loss: 0.2096 - acc: 0.936 - ETA: 0s - loss: 0.2130 - acc: 0.935 - ETA: 0s - loss: 0.2102 - acc: 0.937 - ETA: 0s - loss: 0.2072 - acc: 0.939 - ETA: 0s - loss: 0.2047 - acc: 0.940 - ETA: 0s - loss: 0.2082 - acc: 0.938 - ETA: 0s - loss: 0.2068 - acc: 0.939 - ETA: 0s - loss: 0.2065 - acc: 0.939 - ETA: 0s - loss: 0.2077 - acc: 0.938 - ETA: 0s - loss: 0.2060 - acc: 0.939 - ETA: 0s - loss: 0.2049 - acc: 0.938 - ETA: 0s - loss: 0.2057 - acc: 0.938 - ETA: 0s - loss: 0.2058 - acc: 0.938 - ETA: 0s - loss: 0.2026 - acc: 0.939 - ETA: 0s - loss: 0.2022 - acc: 0.940 - 2s 597us/step - loss: 0.2017 - acc: 0.9400 - val_loss: 0.3536 - val_acc: 0.8692\n",
      "Epoch 97/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3259 - acc: 0.890 - ETA: 2s - loss: 0.2713 - acc: 0.911 - ETA: 2s - loss: 0.2462 - acc: 0.915 - ETA: 2s - loss: 0.2260 - acc: 0.924 - ETA: 1s - loss: 0.2173 - acc: 0.930 - ETA: 1s - loss: 0.2193 - acc: 0.929 - ETA: 1s - loss: 0.2167 - acc: 0.931 - ETA: 1s - loss: 0.2169 - acc: 0.934 - ETA: 1s - loss: 0.2143 - acc: 0.937 - ETA: 1s - loss: 0.2074 - acc: 0.939 - ETA: 1s - loss: 0.2040 - acc: 0.941 - ETA: 1s - loss: 0.2070 - acc: 0.940 - ETA: 1s - loss: 0.2044 - acc: 0.940 - ETA: 1s - loss: 0.2091 - acc: 0.938 - ETA: 1s - loss: 0.2127 - acc: 0.936 - ETA: 1s - loss: 0.2109 - acc: 0.936 - ETA: 1s - loss: 0.2117 - acc: 0.935 - ETA: 1s - loss: 0.2102 - acc: 0.936 - ETA: 0s - loss: 0.2068 - acc: 0.937 - ETA: 0s - loss: 0.2112 - acc: 0.936 - ETA: 0s - loss: 0.2132 - acc: 0.935 - ETA: 0s - loss: 0.2113 - acc: 0.936 - ETA: 0s - loss: 0.2106 - acc: 0.936 - ETA: 0s - loss: 0.2088 - acc: 0.936 - ETA: 0s - loss: 0.2073 - acc: 0.937 - ETA: 0s - loss: 0.2081 - acc: 0.937 - ETA: 0s - loss: 0.2075 - acc: 0.937 - ETA: 0s - loss: 0.2080 - acc: 0.936 - ETA: 0s - loss: 0.2063 - acc: 0.937 - ETA: 0s - loss: 0.2070 - acc: 0.937 - ETA: 0s - loss: 0.2059 - acc: 0.938 - ETA: 0s - loss: 0.2044 - acc: 0.938 - 2s 598us/step - loss: 0.2042 - acc: 0.9385 - val_loss: 0.2599 - val_acc: 0.9269\n",
      "Epoch 98/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2076 - acc: 0.937 - ETA: 2s - loss: 0.1648 - acc: 0.953 - ETA: 2s - loss: 0.1588 - acc: 0.959 - ETA: 2s - loss: 0.1794 - acc: 0.948 - ETA: 2s - loss: 0.1825 - acc: 0.951 - ETA: 2s - loss: 0.1867 - acc: 0.947 - ETA: 2s - loss: 0.1783 - acc: 0.950 - ETA: 1s - loss: 0.1784 - acc: 0.949 - ETA: 1s - loss: 0.1827 - acc: 0.945 - ETA: 1s - loss: 0.1839 - acc: 0.944 - ETA: 1s - loss: 0.1764 - acc: 0.948 - ETA: 1s - loss: 0.1795 - acc: 0.947 - ETA: 1s - loss: 0.1816 - acc: 0.946 - ETA: 1s - loss: 0.1836 - acc: 0.945 - ETA: 1s - loss: 0.1828 - acc: 0.946 - ETA: 1s - loss: 0.1824 - acc: 0.946 - ETA: 1s - loss: 0.1820 - acc: 0.946 - ETA: 1s - loss: 0.1844 - acc: 0.947 - ETA: 1s - loss: 0.1821 - acc: 0.948 - ETA: 0s - loss: 0.1814 - acc: 0.948 - ETA: 0s - loss: 0.1812 - acc: 0.947 - ETA: 0s - loss: 0.1835 - acc: 0.946 - ETA: 0s - loss: 0.1832 - acc: 0.947 - ETA: 0s - loss: 0.1816 - acc: 0.947 - ETA: 0s - loss: 0.1809 - acc: 0.948 - ETA: 0s - loss: 0.1811 - acc: 0.947 - ETA: 0s - loss: 0.1820 - acc: 0.947 - ETA: 0s - loss: 0.1812 - acc: 0.947 - ETA: 0s - loss: 0.1831 - acc: 0.946 - ETA: 0s - loss: 0.1834 - acc: 0.946 - ETA: 0s - loss: 0.1841 - acc: 0.946 - ETA: 0s - loss: 0.1881 - acc: 0.944 - ETA: 0s - loss: 0.1872 - acc: 0.944 - ETA: 0s - loss: 0.1889 - acc: 0.943 - 3s 650us/step - loss: 0.1890 - acc: 0.9439 - val_loss: 0.2581 - val_acc: 0.9109\n",
      "Epoch 99/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1444 - acc: 0.968 - ETA: 2s - loss: 0.1388 - acc: 0.968 - ETA: 2s - loss: 0.1571 - acc: 0.962 - ETA: 2s - loss: 0.1823 - acc: 0.953 - ETA: 1s - loss: 0.1748 - acc: 0.951 - ETA: 1s - loss: 0.1828 - acc: 0.948 - ETA: 1s - loss: 0.1813 - acc: 0.947 - ETA: 1s - loss: 0.1754 - acc: 0.947 - ETA: 1s - loss: 0.1861 - acc: 0.943 - ETA: 1s - loss: 0.1905 - acc: 0.944 - ETA: 1s - loss: 0.1998 - acc: 0.940 - ETA: 1s - loss: 0.1960 - acc: 0.942 - ETA: 1s - loss: 0.1929 - acc: 0.942 - ETA: 1s - loss: 0.1923 - acc: 0.942 - ETA: 1s - loss: 0.1876 - acc: 0.945 - ETA: 1s - loss: 0.1914 - acc: 0.944 - ETA: 1s - loss: 0.1892 - acc: 0.944 - ETA: 1s - loss: 0.1924 - acc: 0.942 - ETA: 0s - loss: 0.1920 - acc: 0.943 - ETA: 0s - loss: 0.1898 - acc: 0.944 - ETA: 0s - loss: 0.1893 - acc: 0.943 - ETA: 0s - loss: 0.1906 - acc: 0.943 - ETA: 0s - loss: 0.1905 - acc: 0.942 - ETA: 0s - loss: 0.1937 - acc: 0.941 - ETA: 0s - loss: 0.1957 - acc: 0.940 - ETA: 0s - loss: 0.1967 - acc: 0.940 - ETA: 0s - loss: 0.1967 - acc: 0.939 - ETA: 0s - loss: 0.1937 - acc: 0.940 - ETA: 0s - loss: 0.1937 - acc: 0.940 - ETA: 0s - loss: 0.1928 - acc: 0.940 - ETA: 0s - loss: 0.1907 - acc: 0.942 - ETA: 0s - loss: 0.1891 - acc: 0.942 - 3s 615us/step - loss: 0.1891 - acc: 0.9417 - val_loss: 0.2658 - val_acc: 0.9128\n",
      "Epoch 100/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1110 - acc: 0.984 - ETA: 2s - loss: 0.1083 - acc: 0.974 - ETA: 2s - loss: 0.1257 - acc: 0.959 - ETA: 1s - loss: 0.1351 - acc: 0.955 - ETA: 1s - loss: 0.1696 - acc: 0.944 - ETA: 1s - loss: 0.1733 - acc: 0.944 - ETA: 1s - loss: 0.1673 - acc: 0.944 - ETA: 1s - loss: 0.1665 - acc: 0.947 - ETA: 1s - loss: 0.1724 - acc: 0.948 - ETA: 1s - loss: 0.1696 - acc: 0.950 - ETA: 1s - loss: 0.1689 - acc: 0.950 - ETA: 1s - loss: 0.1665 - acc: 0.950 - ETA: 1s - loss: 0.1735 - acc: 0.946 - ETA: 1s - loss: 0.1737 - acc: 0.947 - ETA: 1s - loss: 0.1756 - acc: 0.945 - ETA: 1s - loss: 0.1763 - acc: 0.945 - ETA: 1s - loss: 0.1748 - acc: 0.945 - ETA: 1s - loss: 0.1710 - acc: 0.947 - ETA: 0s - loss: 0.1714 - acc: 0.947 - ETA: 0s - loss: 0.1717 - acc: 0.947 - ETA: 0s - loss: 0.1711 - acc: 0.948 - ETA: 0s - loss: 0.1709 - acc: 0.948 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1692 - acc: 0.949 - ETA: 0s - loss: 0.1699 - acc: 0.948 - ETA: 0s - loss: 0.1709 - acc: 0.948 - ETA: 0s - loss: 0.1699 - acc: 0.948 - ETA: 0s - loss: 0.1698 - acc: 0.949 - ETA: 0s - loss: 0.1669 - acc: 0.950 - ETA: 0s - loss: 0.1680 - acc: 0.950 - ETA: 0s - loss: 0.1681 - acc: 0.950 - ETA: 0s - loss: 0.1692 - acc: 0.949 - 2s 594us/step - loss: 0.1695 - acc: 0.9493 - val_loss: 0.2633 - val_acc: 0.9154\n",
      "Epoch 101/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1683 - acc: 0.953 - ETA: 2s - loss: 0.1481 - acc: 0.953 - ETA: 2s - loss: 0.1559 - acc: 0.956 - ETA: 2s - loss: 0.1770 - acc: 0.944 - ETA: 2s - loss: 0.1728 - acc: 0.946 - ETA: 2s - loss: 0.1833 - acc: 0.944 - ETA: 1s - loss: 0.1771 - acc: 0.944 - ETA: 1s - loss: 0.1708 - acc: 0.949 - ETA: 1s - loss: 0.1764 - acc: 0.944 - ETA: 1s - loss: 0.1766 - acc: 0.947 - ETA: 1s - loss: 0.1702 - acc: 0.950 - ETA: 1s - loss: 0.1696 - acc: 0.951 - ETA: 1s - loss: 0.1701 - acc: 0.950 - ETA: 1s - loss: 0.1671 - acc: 0.951 - ETA: 1s - loss: 0.1704 - acc: 0.948 - ETA: 1s - loss: 0.1713 - acc: 0.948 - ETA: 1s - loss: 0.1675 - acc: 0.949 - ETA: 1s - loss: 0.1689 - acc: 0.949 - ETA: 0s - loss: 0.1677 - acc: 0.950 - ETA: 0s - loss: 0.1674 - acc: 0.949 - ETA: 0s - loss: 0.1696 - acc: 0.947 - ETA: 0s - loss: 0.1735 - acc: 0.945 - ETA: 0s - loss: 0.1758 - acc: 0.943 - ETA: 0s - loss: 0.1723 - acc: 0.945 - ETA: 0s - loss: 0.1720 - acc: 0.945 - ETA: 0s - loss: 0.1713 - acc: 0.946 - ETA: 0s - loss: 0.1715 - acc: 0.946 - ETA: 0s - loss: 0.1706 - acc: 0.946 - ETA: 0s - loss: 0.1724 - acc: 0.946 - ETA: 0s - loss: 0.1703 - acc: 0.948 - ETA: 0s - loss: 0.1706 - acc: 0.947 - ETA: 0s - loss: 0.1707 - acc: 0.947 - 2s 596us/step - loss: 0.1720 - acc: 0.9476 - val_loss: 0.2693 - val_acc: 0.9192\n",
      "Epoch 102/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2005 - acc: 0.937 - ETA: 2s - loss: 0.1966 - acc: 0.947 - ETA: 2s - loss: 0.1790 - acc: 0.946 - ETA: 2s - loss: 0.1747 - acc: 0.948 - ETA: 1s - loss: 0.1746 - acc: 0.946 - ETA: 1s - loss: 0.1768 - acc: 0.946 - ETA: 1s - loss: 0.1816 - acc: 0.945 - ETA: 1s - loss: 0.1755 - acc: 0.946 - ETA: 1s - loss: 0.1710 - acc: 0.947 - ETA: 1s - loss: 0.1687 - acc: 0.949 - ETA: 1s - loss: 0.1674 - acc: 0.949 - ETA: 1s - loss: 0.1701 - acc: 0.948 - ETA: 1s - loss: 0.1689 - acc: 0.949 - ETA: 1s - loss: 0.1717 - acc: 0.949 - ETA: 1s - loss: 0.1739 - acc: 0.949 - ETA: 1s - loss: 0.1721 - acc: 0.950 - ETA: 1s - loss: 0.1755 - acc: 0.948 - ETA: 1s - loss: 0.1733 - acc: 0.949 - ETA: 0s - loss: 0.1714 - acc: 0.949 - ETA: 0s - loss: 0.1700 - acc: 0.950 - ETA: 0s - loss: 0.1717 - acc: 0.949 - ETA: 0s - loss: 0.1714 - acc: 0.949 - ETA: 0s - loss: 0.1732 - acc: 0.949 - ETA: 0s - loss: 0.1712 - acc: 0.950 - ETA: 0s - loss: 0.1704 - acc: 0.950 - ETA: 0s - loss: 0.1682 - acc: 0.952 - ETA: 0s - loss: 0.1695 - acc: 0.951 - ETA: 0s - loss: 0.1718 - acc: 0.950 - ETA: 0s - loss: 0.1739 - acc: 0.949 - ETA: 0s - loss: 0.1733 - acc: 0.949 - ETA: 0s - loss: 0.1735 - acc: 0.950 - ETA: 0s - loss: 0.1734 - acc: 0.949 - 2s 590us/step - loss: 0.1727 - acc: 0.9501 - val_loss: 0.2533 - val_acc: 0.9308\n",
      "Epoch 103/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1511 - acc: 0.937 - ETA: 2s - loss: 0.1876 - acc: 0.932 - ETA: 2s - loss: 0.1880 - acc: 0.937 - ETA: 2s - loss: 0.1798 - acc: 0.946 - ETA: 1s - loss: 0.1666 - acc: 0.953 - ETA: 1s - loss: 0.1623 - acc: 0.957 - ETA: 1s - loss: 0.1577 - acc: 0.957 - ETA: 1s - loss: 0.1521 - acc: 0.959 - ETA: 1s - loss: 0.1476 - acc: 0.963 - ETA: 1s - loss: 0.1506 - acc: 0.960 - ETA: 1s - loss: 0.1488 - acc: 0.962 - ETA: 1s - loss: 0.1485 - acc: 0.962 - ETA: 1s - loss: 0.1493 - acc: 0.961 - ETA: 1s - loss: 0.1537 - acc: 0.958 - ETA: 1s - loss: 0.1591 - acc: 0.955 - ETA: 1s - loss: 0.1568 - acc: 0.956 - ETA: 1s - loss: 0.1553 - acc: 0.957 - ETA: 1s - loss: 0.1550 - acc: 0.957 - ETA: 0s - loss: 0.1557 - acc: 0.957 - ETA: 0s - loss: 0.1585 - acc: 0.955 - ETA: 0s - loss: 0.1585 - acc: 0.956 - ETA: 0s - loss: 0.1588 - acc: 0.955 - ETA: 0s - loss: 0.1599 - acc: 0.955 - ETA: 0s - loss: 0.1600 - acc: 0.954 - ETA: 0s - loss: 0.1581 - acc: 0.955 - ETA: 0s - loss: 0.1578 - acc: 0.955 - ETA: 0s - loss: 0.1578 - acc: 0.955 - ETA: 0s - loss: 0.1586 - acc: 0.954 - ETA: 0s - loss: 0.1619 - acc: 0.953 - ETA: 0s - loss: 0.1644 - acc: 0.953 - ETA: 0s - loss: 0.1650 - acc: 0.952 - ETA: 0s - loss: 0.1640 - acc: 0.953 - 2s 595us/step - loss: 0.1641 - acc: 0.9530 - val_loss: 0.2384 - val_acc: 0.9308\n",
      "Epoch 104/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1852 - acc: 0.937 - ETA: 2s - loss: 0.1734 - acc: 0.937 - ETA: 2s - loss: 0.1727 - acc: 0.943 - ETA: 2s - loss: 0.1823 - acc: 0.939 - ETA: 1s - loss: 0.1867 - acc: 0.935 - ETA: 1s - loss: 0.1829 - acc: 0.940 - ETA: 1s - loss: 0.1793 - acc: 0.942 - ETA: 1s - loss: 0.1757 - acc: 0.944 - ETA: 1s - loss: 0.1748 - acc: 0.944 - ETA: 1s - loss: 0.1757 - acc: 0.944 - ETA: 1s - loss: 0.1751 - acc: 0.945 - ETA: 1s - loss: 0.1721 - acc: 0.946 - ETA: 1s - loss: 0.1740 - acc: 0.945 - ETA: 1s - loss: 0.1743 - acc: 0.945 - ETA: 1s - loss: 0.1799 - acc: 0.942 - ETA: 1s - loss: 0.1814 - acc: 0.942 - ETA: 1s - loss: 0.1886 - acc: 0.938 - ETA: 1s - loss: 0.1862 - acc: 0.939 - ETA: 0s - loss: 0.1852 - acc: 0.939 - ETA: 0s - loss: 0.1852 - acc: 0.939 - ETA: 0s - loss: 0.1842 - acc: 0.939 - ETA: 0s - loss: 0.1827 - acc: 0.940 - ETA: 0s - loss: 0.1816 - acc: 0.941 - ETA: 0s - loss: 0.1786 - acc: 0.942 - ETA: 0s - loss: 0.1778 - acc: 0.943 - ETA: 0s - loss: 0.1762 - acc: 0.944 - ETA: 0s - loss: 0.1767 - acc: 0.944 - ETA: 0s - loss: 0.1757 - acc: 0.945 - ETA: 0s - loss: 0.1773 - acc: 0.944 - ETA: 0s - loss: 0.1762 - acc: 0.945 - ETA: 0s - loss: 0.1777 - acc: 0.944 - ETA: 0s - loss: 0.1757 - acc: 0.945 - 2s 601us/step - loss: 0.1750 - acc: 0.9457 - val_loss: 0.2731 - val_acc: 0.9115\n",
      "Epoch 105/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2369 - acc: 0.937 - ETA: 2s - loss: 0.1843 - acc: 0.947 - ETA: 2s - loss: 0.1915 - acc: 0.946 - ETA: 2s - loss: 0.1873 - acc: 0.950 - ETA: 1s - loss: 0.1819 - acc: 0.949 - ETA: 1s - loss: 0.1829 - acc: 0.951 - ETA: 1s - loss: 0.1777 - acc: 0.953 - ETA: 1s - loss: 0.1680 - acc: 0.956 - ETA: 1s - loss: 0.1684 - acc: 0.956 - ETA: 1s - loss: 0.1668 - acc: 0.954 - ETA: 1s - loss: 0.1677 - acc: 0.955 - ETA: 1s - loss: 0.1641 - acc: 0.955 - ETA: 1s - loss: 0.1636 - acc: 0.956 - ETA: 1s - loss: 0.1611 - acc: 0.956 - ETA: 1s - loss: 0.1637 - acc: 0.954 - ETA: 1s - loss: 0.1650 - acc: 0.953 - ETA: 1s - loss: 0.1644 - acc: 0.953 - ETA: 1s - loss: 0.1660 - acc: 0.951 - ETA: 0s - loss: 0.1674 - acc: 0.951 - ETA: 0s - loss: 0.1669 - acc: 0.951 - ETA: 0s - loss: 0.1674 - acc: 0.950 - ETA: 0s - loss: 0.1684 - acc: 0.950 - ETA: 0s - loss: 0.1684 - acc: 0.950 - ETA: 0s - loss: 0.1660 - acc: 0.951 - ETA: 0s - loss: 0.1661 - acc: 0.950 - ETA: 0s - loss: 0.1643 - acc: 0.951 - ETA: 0s - loss: 0.1652 - acc: 0.951 - ETA: 0s - loss: 0.1643 - acc: 0.951 - ETA: 0s - loss: 0.1652 - acc: 0.950 - ETA: 0s - loss: 0.1655 - acc: 0.949 - ETA: 0s - loss: 0.1660 - acc: 0.950 - ETA: 0s - loss: 0.1664 - acc: 0.949 - 2s 594us/step - loss: 0.1656 - acc: 0.9493 - val_loss: 0.2799 - val_acc: 0.9071\n",
      "Epoch 106/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1711 - acc: 0.953 - ETA: 2s - loss: 0.1500 - acc: 0.953 - ETA: 2s - loss: 0.1728 - acc: 0.940 - ETA: 2s - loss: 0.1506 - acc: 0.955 - ETA: 1s - loss: 0.1496 - acc: 0.954 - ETA: 1s - loss: 0.1538 - acc: 0.951 - ETA: 1s - loss: 0.1573 - acc: 0.950 - ETA: 1s - loss: 0.1503 - acc: 0.954 - ETA: 1s - loss: 0.1536 - acc: 0.952 - ETA: 1s - loss: 0.1611 - acc: 0.949 - ETA: 1s - loss: 0.1632 - acc: 0.948 - ETA: 1s - loss: 0.1617 - acc: 0.949 - ETA: 1s - loss: 0.1585 - acc: 0.951 - ETA: 1s - loss: 0.1643 - acc: 0.950 - ETA: 1s - loss: 0.1640 - acc: 0.951 - ETA: 1s - loss: 0.1626 - acc: 0.951 - ETA: 1s - loss: 0.1636 - acc: 0.952 - ETA: 1s - loss: 0.1688 - acc: 0.950 - ETA: 0s - loss: 0.1676 - acc: 0.949 - ETA: 0s - loss: 0.1655 - acc: 0.951 - ETA: 0s - loss: 0.1723 - acc: 0.948 - ETA: 0s - loss: 0.1719 - acc: 0.948 - ETA: 0s - loss: 0.1756 - acc: 0.947 - ETA: 0s - loss: 0.1790 - acc: 0.945 - ETA: 0s - loss: 0.1795 - acc: 0.945 - ETA: 0s - loss: 0.1787 - acc: 0.945 - ETA: 0s - loss: 0.1786 - acc: 0.946 - ETA: 0s - loss: 0.1779 - acc: 0.946 - ETA: 0s - loss: 0.1805 - acc: 0.945 - ETA: 0s - loss: 0.1799 - acc: 0.945 - ETA: 0s - loss: 0.1814 - acc: 0.944 - ETA: 0s - loss: 0.1833 - acc: 0.943 - 2s 594us/step - loss: 0.1830 - acc: 0.9442 - val_loss: 0.2643 - val_acc: 0.9250\n",
      "Epoch 107/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1421 - acc: 0.953 - ETA: 2s - loss: 0.1762 - acc: 0.947 - ETA: 2s - loss: 0.1745 - acc: 0.953 - ETA: 1s - loss: 0.1872 - acc: 0.942 - ETA: 1s - loss: 0.1713 - acc: 0.947 - ETA: 1s - loss: 0.1695 - acc: 0.947 - ETA: 1s - loss: 0.1650 - acc: 0.950 - ETA: 1s - loss: 0.1648 - acc: 0.952 - ETA: 1s - loss: 0.1614 - acc: 0.952 - ETA: 1s - loss: 0.1620 - acc: 0.953 - ETA: 1s - loss: 0.1642 - acc: 0.950 - ETA: 1s - loss: 0.1710 - acc: 0.948 - ETA: 1s - loss: 0.1718 - acc: 0.946 - ETA: 1s - loss: 0.1686 - acc: 0.949 - ETA: 1s - loss: 0.1695 - acc: 0.947 - ETA: 1s - loss: 0.1668 - acc: 0.949 - ETA: 1s - loss: 0.1657 - acc: 0.949 - ETA: 1s - loss: 0.1634 - acc: 0.951 - ETA: 0s - loss: 0.1663 - acc: 0.949 - ETA: 0s - loss: 0.1693 - acc: 0.948 - ETA: 0s - loss: 0.1719 - acc: 0.946 - ETA: 0s - loss: 0.1681 - acc: 0.948 - ETA: 0s - loss: 0.1737 - acc: 0.946 - ETA: 0s - loss: 0.1719 - acc: 0.947 - ETA: 0s - loss: 0.1706 - acc: 0.947 - ETA: 0s - loss: 0.1685 - acc: 0.948 - ETA: 0s - loss: 0.1706 - acc: 0.948 - ETA: 0s - loss: 0.1714 - acc: 0.947 - ETA: 0s - loss: 0.1708 - acc: 0.948 - ETA: 0s - loss: 0.1689 - acc: 0.949 - ETA: 0s - loss: 0.1679 - acc: 0.949 - ETA: 0s - loss: 0.1666 - acc: 0.950 - 2s 589us/step - loss: 0.1664 - acc: 0.9503 - val_loss: 0.2434 - val_acc: 0.9321\n",
      "Epoch 108/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2083 - acc: 0.921 - ETA: 2s - loss: 0.1792 - acc: 0.942 - ETA: 2s - loss: 0.1728 - acc: 0.953 - ETA: 1s - loss: 0.1708 - acc: 0.953 - ETA: 1s - loss: 0.1714 - acc: 0.951 - ETA: 1s - loss: 0.1599 - acc: 0.953 - ETA: 1s - loss: 0.1600 - acc: 0.951 - ETA: 1s - loss: 0.1679 - acc: 0.946 - ETA: 1s - loss: 0.1730 - acc: 0.945 - ETA: 1s - loss: 0.1738 - acc: 0.944 - ETA: 1s - loss: 0.1718 - acc: 0.945 - ETA: 1s - loss: 0.1687 - acc: 0.947 - ETA: 1s - loss: 0.1684 - acc: 0.947 - ETA: 1s - loss: 0.1699 - acc: 0.945 - ETA: 1s - loss: 0.1674 - acc: 0.946 - ETA: 1s - loss: 0.1688 - acc: 0.946 - ETA: 1s - loss: 0.1720 - acc: 0.944 - ETA: 1s - loss: 0.1676 - acc: 0.946 - ETA: 0s - loss: 0.1663 - acc: 0.946 - ETA: 0s - loss: 0.1692 - acc: 0.945 - ETA: 0s - loss: 0.1698 - acc: 0.945 - ETA: 0s - loss: 0.1686 - acc: 0.945 - ETA: 0s - loss: 0.1671 - acc: 0.946 - ETA: 0s - loss: 0.1695 - acc: 0.944 - ETA: 0s - loss: 0.1706 - acc: 0.944 - ETA: 0s - loss: 0.1715 - acc: 0.944 - ETA: 0s - loss: 0.1720 - acc: 0.944 - ETA: 0s - loss: 0.1723 - acc: 0.945 - ETA: 0s - loss: 0.1717 - acc: 0.946 - ETA: 0s - loss: 0.1710 - acc: 0.947 - ETA: 0s - loss: 0.1718 - acc: 0.946 - ETA: 0s - loss: 0.1700 - acc: 0.947 - 2s 594us/step - loss: 0.1694 - acc: 0.9474 - val_loss: 0.3201 - val_acc: 0.8821\n",
      "Epoch 109/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1091 - acc: 0.968 - ETA: 2s - loss: 0.1677 - acc: 0.927 - ETA: 2s - loss: 0.1576 - acc: 0.937 - ETA: 2s - loss: 0.1484 - acc: 0.946 - ETA: 1s - loss: 0.1633 - acc: 0.944 - ETA: 1s - loss: 0.1636 - acc: 0.944 - ETA: 1s - loss: 0.1617 - acc: 0.944 - ETA: 1s - loss: 0.1668 - acc: 0.944 - ETA: 1s - loss: 0.1626 - acc: 0.946 - ETA: 1s - loss: 0.1653 - acc: 0.946 - ETA: 1s - loss: 0.1647 - acc: 0.948 - ETA: 1s - loss: 0.1635 - acc: 0.949 - ETA: 1s - loss: 0.1593 - acc: 0.951 - ETA: 1s - loss: 0.1655 - acc: 0.948 - ETA: 1s - loss: 0.1692 - acc: 0.946 - ETA: 1s - loss: 0.1713 - acc: 0.944 - ETA: 1s - loss: 0.1672 - acc: 0.947 - ETA: 1s - loss: 0.1689 - acc: 0.946 - ETA: 0s - loss: 0.1680 - acc: 0.945 - ETA: 0s - loss: 0.1680 - acc: 0.945 - ETA: 0s - loss: 0.1657 - acc: 0.947 - ETA: 0s - loss: 0.1649 - acc: 0.948 - ETA: 0s - loss: 0.1645 - acc: 0.947 - ETA: 0s - loss: 0.1652 - acc: 0.947 - ETA: 0s - loss: 0.1657 - acc: 0.947 - ETA: 0s - loss: 0.1658 - acc: 0.947 - ETA: 0s - loss: 0.1661 - acc: 0.946 - ETA: 0s - loss: 0.1660 - acc: 0.946 - ETA: 0s - loss: 0.1646 - acc: 0.947 - ETA: 0s - loss: 0.1626 - acc: 0.949 - ETA: 0s - loss: 0.1614 - acc: 0.950 - ETA: 0s - loss: 0.1619 - acc: 0.949 - 2s 597us/step - loss: 0.1616 - acc: 0.9496 - val_loss: 0.2317 - val_acc: 0.9333\n",
      "Epoch 110/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2951 - acc: 0.890 - ETA: 2s - loss: 0.2058 - acc: 0.921 - ETA: 2s - loss: 0.1881 - acc: 0.928 - ETA: 2s - loss: 0.1950 - acc: 0.926 - ETA: 2s - loss: 0.1849 - acc: 0.932 - ETA: 1s - loss: 0.1737 - acc: 0.938 - ETA: 1s - loss: 0.1740 - acc: 0.941 - ETA: 1s - loss: 0.1682 - acc: 0.942 - ETA: 1s - loss: 0.1681 - acc: 0.942 - ETA: 1s - loss: 0.1764 - acc: 0.938 - ETA: 1s - loss: 0.1821 - acc: 0.936 - ETA: 1s - loss: 0.1872 - acc: 0.936 - ETA: 1s - loss: 0.1862 - acc: 0.938 - ETA: 1s - loss: 0.1845 - acc: 0.939 - ETA: 1s - loss: 0.1861 - acc: 0.939 - ETA: 1s - loss: 0.1922 - acc: 0.938 - ETA: 1s - loss: 0.1908 - acc: 0.939 - ETA: 1s - loss: 0.1873 - acc: 0.942 - ETA: 0s - loss: 0.1883 - acc: 0.943 - ETA: 0s - loss: 0.1852 - acc: 0.944 - ETA: 0s - loss: 0.1878 - acc: 0.943 - ETA: 0s - loss: 0.1865 - acc: 0.943 - ETA: 0s - loss: 0.1842 - acc: 0.945 - ETA: 0s - loss: 0.1831 - acc: 0.946 - ETA: 0s - loss: 0.1839 - acc: 0.946 - ETA: 0s - loss: 0.1814 - acc: 0.946 - ETA: 0s - loss: 0.1829 - acc: 0.946 - ETA: 0s - loss: 0.1830 - acc: 0.946 - ETA: 0s - loss: 0.1867 - acc: 0.945 - ETA: 0s - loss: 0.1877 - acc: 0.945 - ETA: 0s - loss: 0.1858 - acc: 0.946 - ETA: 0s - loss: 0.1867 - acc: 0.945 - 2s 604us/step - loss: 0.1865 - acc: 0.9457 - val_loss: 0.2824 - val_acc: 0.9038\n",
      "Epoch 111/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1941 - acc: 0.906 - ETA: 2s - loss: 0.2027 - acc: 0.927 - ETA: 2s - loss: 0.1996 - acc: 0.931 - ETA: 2s - loss: 0.2035 - acc: 0.935 - ETA: 2s - loss: 0.1950 - acc: 0.939 - ETA: 1s - loss: 0.2569 - acc: 0.929 - ETA: 1s - loss: 0.2675 - acc: 0.929 - ETA: 1s - loss: 0.2591 - acc: 0.930 - ETA: 1s - loss: 0.2747 - acc: 0.928 - ETA: 1s - loss: 0.2859 - acc: 0.926 - ETA: 1s - loss: 0.2824 - acc: 0.924 - ETA: 1s - loss: 0.2882 - acc: 0.923 - ETA: 1s - loss: 0.2902 - acc: 0.925 - ETA: 1s - loss: 0.2917 - acc: 0.924 - ETA: 1s - loss: 0.2920 - acc: 0.921 - ETA: 1s - loss: 0.2885 - acc: 0.922 - ETA: 1s - loss: 0.2859 - acc: 0.920 - ETA: 1s - loss: 0.2851 - acc: 0.922 - ETA: 0s - loss: 0.2858 - acc: 0.923 - ETA: 0s - loss: 0.2858 - acc: 0.925 - ETA: 0s - loss: 0.2823 - acc: 0.926 - ETA: 0s - loss: 0.2801 - acc: 0.928 - ETA: 0s - loss: 0.2764 - acc: 0.929 - ETA: 0s - loss: 0.2770 - acc: 0.929 - ETA: 0s - loss: 0.2776 - acc: 0.928 - ETA: 0s - loss: 0.2756 - acc: 0.928 - ETA: 0s - loss: 0.2726 - acc: 0.930 - ETA: 0s - loss: 0.2721 - acc: 0.931 - ETA: 0s - loss: 0.2679 - acc: 0.932 - ETA: 0s - loss: 0.2648 - acc: 0.933 - ETA: 0s - loss: 0.2656 - acc: 0.933 - ETA: 0s - loss: 0.2653 - acc: 0.932 - 2s 599us/step - loss: 0.2644 - acc: 0.9326 - val_loss: 0.2688 - val_acc: 0.9353\n",
      "Epoch 112/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1218 - acc: 1.000 - ETA: 2s - loss: 0.2330 - acc: 0.937 - ETA: 2s - loss: 0.2498 - acc: 0.921 - ETA: 2s - loss: 0.2476 - acc: 0.921 - ETA: 1s - loss: 0.2304 - acc: 0.925 - ETA: 1s - loss: 0.2214 - acc: 0.929 - ETA: 1s - loss: 0.2139 - acc: 0.933 - ETA: 1s - loss: 0.2039 - acc: 0.936 - ETA: 1s - loss: 0.1974 - acc: 0.940 - ETA: 1s - loss: 0.1915 - acc: 0.943 - ETA: 1s - loss: 0.1902 - acc: 0.942 - ETA: 1s - loss: 0.1876 - acc: 0.945 - ETA: 1s - loss: 0.1949 - acc: 0.942 - ETA: 1s - loss: 0.2023 - acc: 0.940 - ETA: 1s - loss: 0.2029 - acc: 0.939 - ETA: 1s - loss: 0.1998 - acc: 0.941 - ETA: 1s - loss: 0.2004 - acc: 0.941 - ETA: 1s - loss: 0.2073 - acc: 0.937 - ETA: 0s - loss: 0.2046 - acc: 0.938 - ETA: 0s - loss: 0.2028 - acc: 0.938 - ETA: 0s - loss: 0.2028 - acc: 0.938 - ETA: 0s - loss: 0.2024 - acc: 0.939 - ETA: 0s - loss: 0.2020 - acc: 0.939 - ETA: 0s - loss: 0.2015 - acc: 0.939 - ETA: 0s - loss: 0.2006 - acc: 0.940 - ETA: 0s - loss: 0.1989 - acc: 0.941 - ETA: 0s - loss: 0.1975 - acc: 0.942 - ETA: 0s - loss: 0.1957 - acc: 0.943 - ETA: 0s - loss: 0.1947 - acc: 0.943 - ETA: 0s - loss: 0.1938 - acc: 0.944 - ETA: 0s - loss: 0.1934 - acc: 0.944 - ETA: 0s - loss: 0.1930 - acc: 0.944 - 2s 598us/step - loss: 0.1932 - acc: 0.9442 - val_loss: 0.2416 - val_acc: 0.9397\n",
      "Epoch 113/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2112 - acc: 0.906 - ETA: 2s - loss: 0.1876 - acc: 0.927 - ETA: 2s - loss: 0.1646 - acc: 0.946 - ETA: 2s - loss: 0.1620 - acc: 0.953 - ETA: 1s - loss: 0.1559 - acc: 0.958 - ETA: 1s - loss: 0.1556 - acc: 0.960 - ETA: 1s - loss: 0.1520 - acc: 0.961 - ETA: 1s - loss: 0.1587 - acc: 0.957 - ETA: 1s - loss: 0.1684 - acc: 0.953 - ETA: 1s - loss: 0.1652 - acc: 0.953 - ETA: 1s - loss: 0.1760 - acc: 0.947 - ETA: 1s - loss: 0.1748 - acc: 0.948 - ETA: 1s - loss: 0.1776 - acc: 0.948 - ETA: 1s - loss: 0.1743 - acc: 0.949 - ETA: 1s - loss: 0.1716 - acc: 0.951 - ETA: 1s - loss: 0.1678 - acc: 0.953 - ETA: 1s - loss: 0.1659 - acc: 0.954 - ETA: 1s - loss: 0.1646 - acc: 0.955 - ETA: 0s - loss: 0.1648 - acc: 0.954 - ETA: 0s - loss: 0.1670 - acc: 0.954 - ETA: 0s - loss: 0.1648 - acc: 0.955 - ETA: 0s - loss: 0.1656 - acc: 0.955 - ETA: 0s - loss: 0.1658 - acc: 0.955 - ETA: 0s - loss: 0.1671 - acc: 0.953 - ETA: 0s - loss: 0.1705 - acc: 0.951 - ETA: 0s - loss: 0.1739 - acc: 0.951 - ETA: 0s - loss: 0.1754 - acc: 0.949 - ETA: 0s - loss: 0.1782 - acc: 0.948 - ETA: 0s - loss: 0.1776 - acc: 0.948 - ETA: 0s - loss: 0.1805 - acc: 0.947 - ETA: 0s - loss: 0.1807 - acc: 0.947 - ETA: 0s - loss: 0.1807 - acc: 0.946 - 2s 591us/step - loss: 0.1806 - acc: 0.9471 - val_loss: 0.2533 - val_acc: 0.9115\n",
      "Epoch 114/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1730 - acc: 0.953 - ETA: 2s - loss: 0.1815 - acc: 0.942 - ETA: 2s - loss: 0.1940 - acc: 0.943 - ETA: 2s - loss: 0.1892 - acc: 0.946 - ETA: 1s - loss: 0.1791 - acc: 0.951 - ETA: 1s - loss: 0.1713 - acc: 0.954 - ETA: 1s - loss: 0.1615 - acc: 0.959 - ETA: 1s - loss: 0.1577 - acc: 0.958 - ETA: 1s - loss: 0.1545 - acc: 0.958 - ETA: 1s - loss: 0.1535 - acc: 0.959 - ETA: 1s - loss: 0.1585 - acc: 0.959 - ETA: 1s - loss: 0.1622 - acc: 0.957 - ETA: 1s - loss: 0.1624 - acc: 0.956 - ETA: 1s - loss: 0.1610 - acc: 0.956 - ETA: 1s - loss: 0.1682 - acc: 0.951 - ETA: 1s - loss: 0.1724 - acc: 0.949 - ETA: 1s - loss: 0.1817 - acc: 0.946 - ETA: 1s - loss: 0.1804 - acc: 0.946 - ETA: 0s - loss: 0.1821 - acc: 0.945 - ETA: 0s - loss: 0.1822 - acc: 0.944 - ETA: 0s - loss: 0.1843 - acc: 0.943 - ETA: 0s - loss: 0.1863 - acc: 0.943 - ETA: 0s - loss: 0.1863 - acc: 0.942 - ETA: 0s - loss: 0.1834 - acc: 0.944 - ETA: 0s - loss: 0.1837 - acc: 0.944 - ETA: 0s - loss: 0.1814 - acc: 0.945 - ETA: 0s - loss: 0.1810 - acc: 0.945 - ETA: 0s - loss: 0.1799 - acc: 0.945 - ETA: 0s - loss: 0.1792 - acc: 0.946 - ETA: 0s - loss: 0.1789 - acc: 0.946 - ETA: 0s - loss: 0.1783 - acc: 0.946 - ETA: 0s - loss: 0.1790 - acc: 0.946 - 2s 595us/step - loss: 0.1783 - acc: 0.9469 - val_loss: 0.2731 - val_acc: 0.9077\n",
      "Epoch 115/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1982 - acc: 0.921 - ETA: 2s - loss: 0.2175 - acc: 0.921 - ETA: 2s - loss: 0.1786 - acc: 0.943 - ETA: 1s - loss: 0.1938 - acc: 0.935 - ETA: 1s - loss: 0.1881 - acc: 0.935 - ETA: 1s - loss: 0.1767 - acc: 0.940 - ETA: 1s - loss: 0.1808 - acc: 0.938 - ETA: 1s - loss: 0.1803 - acc: 0.939 - ETA: 1s - loss: 0.1839 - acc: 0.942 - ETA: 1s - loss: 0.1822 - acc: 0.942 - ETA: 1s - loss: 0.1850 - acc: 0.942 - ETA: 1s - loss: 0.1836 - acc: 0.942 - ETA: 1s - loss: 0.1794 - acc: 0.944 - ETA: 1s - loss: 0.1799 - acc: 0.944 - ETA: 1s - loss: 0.1801 - acc: 0.944 - ETA: 1s - loss: 0.1838 - acc: 0.943 - ETA: 1s - loss: 0.1830 - acc: 0.942 - ETA: 1s - loss: 0.1860 - acc: 0.940 - ETA: 0s - loss: 0.1846 - acc: 0.941 - ETA: 0s - loss: 0.1875 - acc: 0.939 - ETA: 0s - loss: 0.1891 - acc: 0.938 - ETA: 0s - loss: 0.1881 - acc: 0.939 - ETA: 0s - loss: 0.1912 - acc: 0.938 - ETA: 0s - loss: 0.1889 - acc: 0.940 - ETA: 0s - loss: 0.1893 - acc: 0.941 - ETA: 0s - loss: 0.1893 - acc: 0.940 - ETA: 0s - loss: 0.1880 - acc: 0.941 - ETA: 0s - loss: 0.1862 - acc: 0.941 - ETA: 0s - loss: 0.1870 - acc: 0.941 - ETA: 0s - loss: 0.1871 - acc: 0.941 - ETA: 0s - loss: 0.1864 - acc: 0.941 - ETA: 0s - loss: 0.1882 - acc: 0.941 - 2s 596us/step - loss: 0.1886 - acc: 0.9412 - val_loss: 0.2533 - val_acc: 0.9250\n",
      "Epoch 116/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1554 - acc: 0.968 - ETA: 2s - loss: 0.1285 - acc: 0.979 - ETA: 2s - loss: 0.1186 - acc: 0.984 - ETA: 2s - loss: 0.1315 - acc: 0.975 - ETA: 1s - loss: 0.1334 - acc: 0.974 - ETA: 1s - loss: 0.1400 - acc: 0.970 - ETA: 1s - loss: 0.1394 - acc: 0.970 - ETA: 1s - loss: 0.1413 - acc: 0.968 - ETA: 1s - loss: 0.1436 - acc: 0.963 - ETA: 1s - loss: 0.1500 - acc: 0.961 - ETA: 1s - loss: 0.1507 - acc: 0.960 - ETA: 1s - loss: 0.1478 - acc: 0.961 - ETA: 1s - loss: 0.1500 - acc: 0.960 - ETA: 1s - loss: 0.1516 - acc: 0.959 - ETA: 1s - loss: 0.1537 - acc: 0.958 - ETA: 1s - loss: 0.1543 - acc: 0.958 - ETA: 1s - loss: 0.1536 - acc: 0.958 - ETA: 1s - loss: 0.1530 - acc: 0.958 - ETA: 0s - loss: 0.1563 - acc: 0.956 - ETA: 0s - loss: 0.1552 - acc: 0.957 - ETA: 0s - loss: 0.1549 - acc: 0.957 - ETA: 0s - loss: 0.1540 - acc: 0.958 - ETA: 0s - loss: 0.1559 - acc: 0.957 - ETA: 0s - loss: 0.1569 - acc: 0.956 - ETA: 0s - loss: 0.1584 - acc: 0.955 - ETA: 0s - loss: 0.1592 - acc: 0.955 - ETA: 0s - loss: 0.1610 - acc: 0.954 - ETA: 0s - loss: 0.1611 - acc: 0.954 - ETA: 0s - loss: 0.1601 - acc: 0.955 - ETA: 0s - loss: 0.1624 - acc: 0.954 - ETA: 0s - loss: 0.1625 - acc: 0.954 - ETA: 0s - loss: 0.1645 - acc: 0.953 - 2s 598us/step - loss: 0.1632 - acc: 0.9538 - val_loss: 0.2563 - val_acc: 0.9295\n",
      "Epoch 117/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1451 - acc: 0.984 - ETA: 2s - loss: 0.1244 - acc: 0.984 - ETA: 2s - loss: 0.1255 - acc: 0.978 - ETA: 1s - loss: 0.1221 - acc: 0.975 - ETA: 1s - loss: 0.1320 - acc: 0.972 - ETA: 2s - loss: 0.1349 - acc: 0.968 - ETA: 2s - loss: 0.1373 - acc: 0.967 - ETA: 2s - loss: 0.1452 - acc: 0.963 - ETA: 2s - loss: 0.1555 - acc: 0.957 - ETA: 2s - loss: 0.1577 - acc: 0.953 - ETA: 2s - loss: 0.1587 - acc: 0.953 - ETA: 2s - loss: 0.1667 - acc: 0.949 - ETA: 2s - loss: 0.1715 - acc: 0.946 - ETA: 2s - loss: 0.1745 - acc: 0.944 - ETA: 1s - loss: 0.1727 - acc: 0.946 - ETA: 1s - loss: 0.1719 - acc: 0.949 - ETA: 1s - loss: 0.1757 - acc: 0.948 - ETA: 1s - loss: 0.1738 - acc: 0.949 - ETA: 1s - loss: 0.1718 - acc: 0.948 - ETA: 1s - loss: 0.1755 - acc: 0.947 - ETA: 1s - loss: 0.1776 - acc: 0.947 - ETA: 1s - loss: 0.1779 - acc: 0.947 - ETA: 1s - loss: 0.1771 - acc: 0.947 - ETA: 1s - loss: 0.1775 - acc: 0.947 - ETA: 1s - loss: 0.1811 - acc: 0.945 - ETA: 0s - loss: 0.1795 - acc: 0.946 - ETA: 0s - loss: 0.1779 - acc: 0.946 - ETA: 0s - loss: 0.1781 - acc: 0.946 - ETA: 0s - loss: 0.1778 - acc: 0.946 - ETA: 0s - loss: 0.1777 - acc: 0.946 - ETA: 0s - loss: 0.1779 - acc: 0.946 - ETA: 0s - loss: 0.1864 - acc: 0.946 - ETA: 0s - loss: 0.1895 - acc: 0.945 - ETA: 0s - loss: 0.1887 - acc: 0.945 - ETA: 0s - loss: 0.1881 - acc: 0.946 - ETA: 0s - loss: 0.1889 - acc: 0.945 - ETA: 0s - loss: 0.1936 - acc: 0.943 - ETA: 0s - loss: 0.1964 - acc: 0.943 - ETA: 0s - loss: 0.1979 - acc: 0.942 - 3s 696us/step - loss: 0.1992 - acc: 0.9420 - val_loss: 0.2673 - val_acc: 0.9000\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2096 - acc: 0.921 - ETA: 2s - loss: 0.1877 - acc: 0.937 - ETA: 2s - loss: 0.1597 - acc: 0.959 - ETA: 2s - loss: 0.1576 - acc: 0.953 - ETA: 1s - loss: 0.1619 - acc: 0.953 - ETA: 2s - loss: 0.1595 - acc: 0.954 - ETA: 1s - loss: 0.1677 - acc: 0.951 - ETA: 1s - loss: 0.1688 - acc: 0.949 - ETA: 1s - loss: 0.1642 - acc: 0.953 - ETA: 1s - loss: 0.1749 - acc: 0.949 - ETA: 1s - loss: 0.1709 - acc: 0.950 - ETA: 1s - loss: 0.1748 - acc: 0.949 - ETA: 1s - loss: 0.1771 - acc: 0.948 - ETA: 1s - loss: 0.1745 - acc: 0.949 - ETA: 1s - loss: 0.1771 - acc: 0.949 - ETA: 1s - loss: 0.1774 - acc: 0.949 - ETA: 1s - loss: 0.1760 - acc: 0.950 - ETA: 1s - loss: 0.1775 - acc: 0.948 - ETA: 1s - loss: 0.1769 - acc: 0.949 - ETA: 1s - loss: 0.1781 - acc: 0.948 - ETA: 1s - loss: 0.1764 - acc: 0.949 - ETA: 1s - loss: 0.1812 - acc: 0.947 - ETA: 0s - loss: 0.1819 - acc: 0.946 - ETA: 0s - loss: 0.1857 - acc: 0.945 - ETA: 0s - loss: 0.1862 - acc: 0.944 - ETA: 0s - loss: 0.1855 - acc: 0.944 - ETA: 0s - loss: 0.1844 - acc: 0.945 - ETA: 0s - loss: 0.1851 - acc: 0.944 - ETA: 0s - loss: 0.1846 - acc: 0.944 - ETA: 0s - loss: 0.1826 - acc: 0.945 - ETA: 0s - loss: 0.1839 - acc: 0.945 - ETA: 0s - loss: 0.1829 - acc: 0.945 - ETA: 0s - loss: 0.1827 - acc: 0.945 - ETA: 0s - loss: 0.1816 - acc: 0.945 - ETA: 0s - loss: 0.1816 - acc: 0.945 - ETA: 0s - loss: 0.1818 - acc: 0.945 - ETA: 0s - loss: 0.1834 - acc: 0.945 - 3s 667us/step - loss: 0.1825 - acc: 0.9457 - val_loss: 0.2480 - val_acc: 0.9308\n",
      "Epoch 119/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1886 - acc: 0.937 - ETA: 2s - loss: 0.1716 - acc: 0.937 - ETA: 2s - loss: 0.1965 - acc: 0.934 - ETA: 2s - loss: 0.1812 - acc: 0.939 - ETA: 2s - loss: 0.1717 - acc: 0.946 - ETA: 1s - loss: 0.1745 - acc: 0.946 - ETA: 1s - loss: 0.1733 - acc: 0.949 - ETA: 1s - loss: 0.1764 - acc: 0.947 - ETA: 1s - loss: 0.1710 - acc: 0.949 - ETA: 1s - loss: 0.1735 - acc: 0.946 - ETA: 1s - loss: 0.1759 - acc: 0.945 - ETA: 1s - loss: 0.1779 - acc: 0.945 - ETA: 1s - loss: 0.1744 - acc: 0.946 - ETA: 1s - loss: 0.1715 - acc: 0.949 - ETA: 1s - loss: 0.1731 - acc: 0.946 - ETA: 1s - loss: 0.1741 - acc: 0.947 - ETA: 1s - loss: 0.1729 - acc: 0.947 - ETA: 1s - loss: 0.1711 - acc: 0.949 - ETA: 0s - loss: 0.1698 - acc: 0.950 - ETA: 0s - loss: 0.1711 - acc: 0.949 - ETA: 0s - loss: 0.1719 - acc: 0.949 - ETA: 0s - loss: 0.1716 - acc: 0.949 - ETA: 0s - loss: 0.1717 - acc: 0.949 - ETA: 0s - loss: 0.1684 - acc: 0.950 - ETA: 0s - loss: 0.1669 - acc: 0.952 - ETA: 0s - loss: 0.1666 - acc: 0.952 - ETA: 0s - loss: 0.1683 - acc: 0.952 - ETA: 0s - loss: 0.1699 - acc: 0.952 - ETA: 0s - loss: 0.1680 - acc: 0.952 - ETA: 0s - loss: 0.1669 - acc: 0.953 - ETA: 0s - loss: 0.1685 - acc: 0.952 - ETA: 0s - loss: 0.1679 - acc: 0.952 - 2s 604us/step - loss: 0.1682 - acc: 0.9528 - val_loss: 0.2394 - val_acc: 0.9282\n",
      "Epoch 120/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1174 - acc: 0.984 - ETA: 2s - loss: 0.1498 - acc: 0.953 - ETA: 2s - loss: 0.1522 - acc: 0.956 - ETA: 2s - loss: 0.1594 - acc: 0.950 - ETA: 1s - loss: 0.1807 - acc: 0.941 - ETA: 1s - loss: 0.2042 - acc: 0.933 - ETA: 1s - loss: 0.1971 - acc: 0.938 - ETA: 1s - loss: 0.1939 - acc: 0.939 - ETA: 1s - loss: 0.1905 - acc: 0.941 - ETA: 1s - loss: 0.1900 - acc: 0.940 - ETA: 1s - loss: 0.1904 - acc: 0.942 - ETA: 1s - loss: 0.1855 - acc: 0.944 - ETA: 1s - loss: 0.1815 - acc: 0.947 - ETA: 1s - loss: 0.1794 - acc: 0.947 - ETA: 1s - loss: 0.1807 - acc: 0.946 - ETA: 1s - loss: 0.1836 - acc: 0.945 - ETA: 1s - loss: 0.1839 - acc: 0.945 - ETA: 1s - loss: 0.1830 - acc: 0.946 - ETA: 0s - loss: 0.1836 - acc: 0.945 - ETA: 0s - loss: 0.1811 - acc: 0.947 - ETA: 0s - loss: 0.1809 - acc: 0.946 - ETA: 0s - loss: 0.1780 - acc: 0.947 - ETA: 0s - loss: 0.1780 - acc: 0.947 - ETA: 0s - loss: 0.1788 - acc: 0.948 - ETA: 0s - loss: 0.1776 - acc: 0.949 - ETA: 0s - loss: 0.1755 - acc: 0.950 - ETA: 0s - loss: 0.1739 - acc: 0.950 - ETA: 0s - loss: 0.1763 - acc: 0.950 - ETA: 0s - loss: 0.1743 - acc: 0.950 - ETA: 0s - loss: 0.1755 - acc: 0.950 - ETA: 0s - loss: 0.1743 - acc: 0.951 - ETA: 0s - loss: 0.1757 - acc: 0.950 - 2s 594us/step - loss: 0.1752 - acc: 0.9508 - val_loss: 0.2290 - val_acc: 0.9308\n",
      "Epoch 121/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1325 - acc: 0.968 - ETA: 2s - loss: 0.1589 - acc: 0.958 - ETA: 2s - loss: 0.1768 - acc: 0.946 - ETA: 2s - loss: 0.1641 - acc: 0.950 - ETA: 1s - loss: 0.1845 - acc: 0.944 - ETA: 1s - loss: 0.1818 - acc: 0.947 - ETA: 1s - loss: 0.1736 - acc: 0.950 - ETA: 1s - loss: 0.1731 - acc: 0.951 - ETA: 1s - loss: 0.1700 - acc: 0.952 - ETA: 1s - loss: 0.1759 - acc: 0.951 - ETA: 1s - loss: 0.1756 - acc: 0.950 - ETA: 1s - loss: 0.1727 - acc: 0.950 - ETA: 1s - loss: 0.1710 - acc: 0.951 - ETA: 1s - loss: 0.1691 - acc: 0.950 - ETA: 1s - loss: 0.1698 - acc: 0.949 - ETA: 1s - loss: 0.1669 - acc: 0.952 - ETA: 1s - loss: 0.1668 - acc: 0.951 - ETA: 1s - loss: 0.1674 - acc: 0.951 - ETA: 0s - loss: 0.1677 - acc: 0.951 - ETA: 0s - loss: 0.1659 - acc: 0.952 - ETA: 0s - loss: 0.1668 - acc: 0.952 - ETA: 0s - loss: 0.1672 - acc: 0.951 - ETA: 0s - loss: 0.1665 - acc: 0.952 - ETA: 0s - loss: 0.1661 - acc: 0.953 - ETA: 0s - loss: 0.1658 - acc: 0.953 - ETA: 0s - loss: 0.1650 - acc: 0.953 - ETA: 0s - loss: 0.1637 - acc: 0.954 - ETA: 0s - loss: 0.1625 - acc: 0.955 - ETA: 0s - loss: 0.1626 - acc: 0.955 - ETA: 0s - loss: 0.1628 - acc: 0.954 - ETA: 0s - loss: 0.1622 - acc: 0.955 - ETA: 0s - loss: 0.1618 - acc: 0.954 - 2s 593us/step - loss: 0.1613 - acc: 0.9552 - val_loss: 0.2314 - val_acc: 0.9353\n",
      "Epoch 122/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1541 - acc: 0.953 - ETA: 2s - loss: 0.1337 - acc: 0.963 - ETA: 2s - loss: 0.1553 - acc: 0.962 - ETA: 2s - loss: 0.1625 - acc: 0.957 - ETA: 1s - loss: 0.1574 - acc: 0.958 - ETA: 1s - loss: 0.1700 - acc: 0.951 - ETA: 1s - loss: 0.1710 - acc: 0.950 - ETA: 1s - loss: 0.1688 - acc: 0.951 - ETA: 1s - loss: 0.1749 - acc: 0.950 - ETA: 1s - loss: 0.1775 - acc: 0.948 - ETA: 1s - loss: 0.1752 - acc: 0.950 - ETA: 1s - loss: 0.1731 - acc: 0.951 - ETA: 1s - loss: 0.1806 - acc: 0.948 - ETA: 1s - loss: 0.1771 - acc: 0.949 - ETA: 1s - loss: 0.1793 - acc: 0.948 - ETA: 1s - loss: 0.1771 - acc: 0.949 - ETA: 1s - loss: 0.1844 - acc: 0.947 - ETA: 1s - loss: 0.1870 - acc: 0.944 - ETA: 0s - loss: 0.1849 - acc: 0.945 - ETA: 0s - loss: 0.1825 - acc: 0.946 - ETA: 0s - loss: 0.1817 - acc: 0.947 - ETA: 0s - loss: 0.1807 - acc: 0.947 - ETA: 0s - loss: 0.1819 - acc: 0.946 - ETA: 0s - loss: 0.1800 - acc: 0.947 - ETA: 0s - loss: 0.1802 - acc: 0.947 - ETA: 0s - loss: 0.1803 - acc: 0.947 - ETA: 0s - loss: 0.1794 - acc: 0.947 - ETA: 0s - loss: 0.1775 - acc: 0.948 - ETA: 0s - loss: 0.1788 - acc: 0.948 - ETA: 0s - loss: 0.1801 - acc: 0.947 - ETA: 0s - loss: 0.1800 - acc: 0.948 - ETA: 0s - loss: 0.1784 - acc: 0.948 - 2s 594us/step - loss: 0.1780 - acc: 0.9489 - val_loss: 0.2415 - val_acc: 0.9314\n",
      "Epoch 123/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2454 - acc: 0.906 - ETA: 2s - loss: 0.2442 - acc: 0.895 - ETA: 2s - loss: 0.2175 - acc: 0.909 - ETA: 2s - loss: 0.2183 - acc: 0.912 - ETA: 1s - loss: 0.2006 - acc: 0.923 - ETA: 1s - loss: 0.1930 - acc: 0.926 - ETA: 1s - loss: 0.1873 - acc: 0.932 - ETA: 1s - loss: 0.1951 - acc: 0.933 - ETA: 1s - loss: 0.1910 - acc: 0.931 - ETA: 1s - loss: 0.1862 - acc: 0.935 - ETA: 1s - loss: 0.1813 - acc: 0.936 - ETA: 1s - loss: 0.1808 - acc: 0.937 - ETA: 1s - loss: 0.1863 - acc: 0.937 - ETA: 1s - loss: 0.1850 - acc: 0.939 - ETA: 1s - loss: 0.1839 - acc: 0.940 - ETA: 1s - loss: 0.1832 - acc: 0.941 - ETA: 1s - loss: 0.1822 - acc: 0.941 - ETA: 1s - loss: 0.1807 - acc: 0.942 - ETA: 0s - loss: 0.1805 - acc: 0.943 - ETA: 0s - loss: 0.1809 - acc: 0.944 - ETA: 0s - loss: 0.1805 - acc: 0.945 - ETA: 0s - loss: 0.1799 - acc: 0.945 - ETA: 0s - loss: 0.1804 - acc: 0.944 - ETA: 0s - loss: 0.1787 - acc: 0.946 - ETA: 0s - loss: 0.1798 - acc: 0.946 - ETA: 0s - loss: 0.1820 - acc: 0.945 - ETA: 0s - loss: 0.1810 - acc: 0.946 - ETA: 0s - loss: 0.1808 - acc: 0.946 - ETA: 0s - loss: 0.1788 - acc: 0.946 - ETA: 0s - loss: 0.1785 - acc: 0.947 - ETA: 0s - loss: 0.1763 - acc: 0.948 - ETA: 0s - loss: 0.1759 - acc: 0.948 - 2s 594us/step - loss: 0.1757 - acc: 0.9481 - val_loss: 0.2400 - val_acc: 0.9314\n",
      "Epoch 124/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.0904 - acc: 0.984 - ETA: 2s - loss: 0.1257 - acc: 0.963 - ETA: 2s - loss: 0.1550 - acc: 0.946 - ETA: 2s - loss: 0.1506 - acc: 0.946 - ETA: 1s - loss: 0.1638 - acc: 0.942 - ETA: 1s - loss: 0.1639 - acc: 0.943 - ETA: 1s - loss: 0.1611 - acc: 0.945 - ETA: 1s - loss: 0.1595 - acc: 0.949 - ETA: 1s - loss: 0.1651 - acc: 0.947 - ETA: 1s - loss: 0.1658 - acc: 0.949 - ETA: 1s - loss: 0.1645 - acc: 0.950 - ETA: 1s - loss: 0.1661 - acc: 0.951 - ETA: 1s - loss: 0.1636 - acc: 0.953 - ETA: 1s - loss: 0.1640 - acc: 0.953 - ETA: 1s - loss: 0.1646 - acc: 0.953 - ETA: 1s - loss: 0.1663 - acc: 0.951 - ETA: 1s - loss: 0.1667 - acc: 0.951 - ETA: 1s - loss: 0.1693 - acc: 0.949 - ETA: 0s - loss: 0.1671 - acc: 0.950 - ETA: 0s - loss: 0.1655 - acc: 0.949 - ETA: 0s - loss: 0.1637 - acc: 0.950 - ETA: 0s - loss: 0.1659 - acc: 0.948 - ETA: 0s - loss: 0.1685 - acc: 0.947 - ETA: 0s - loss: 0.1693 - acc: 0.947 - ETA: 0s - loss: 0.1709 - acc: 0.946 - ETA: 0s - loss: 0.1742 - acc: 0.943 - ETA: 0s - loss: 0.1735 - acc: 0.944 - ETA: 0s - loss: 0.1715 - acc: 0.945 - ETA: 0s - loss: 0.1717 - acc: 0.945 - ETA: 0s - loss: 0.1719 - acc: 0.945 - ETA: 0s - loss: 0.1705 - acc: 0.946 - ETA: 0s - loss: 0.1726 - acc: 0.945 - 2s 597us/step - loss: 0.1719 - acc: 0.9462 - val_loss: 0.2537 - val_acc: 0.9154\n",
      "Epoch 125/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1838 - acc: 0.953 - ETA: 2s - loss: 0.1857 - acc: 0.947 - ETA: 2s - loss: 0.1739 - acc: 0.953 - ETA: 2s - loss: 0.1745 - acc: 0.950 - ETA: 1s - loss: 0.1574 - acc: 0.960 - ETA: 1s - loss: 0.1566 - acc: 0.961 - ETA: 1s - loss: 0.1691 - acc: 0.951 - ETA: 1s - loss: 0.1712 - acc: 0.947 - ETA: 1s - loss: 0.1718 - acc: 0.949 - ETA: 1s - loss: 0.1723 - acc: 0.949 - ETA: 1s - loss: 0.1707 - acc: 0.951 - ETA: 1s - loss: 0.1634 - acc: 0.956 - ETA: 1s - loss: 0.1681 - acc: 0.953 - ETA: 1s - loss: 0.1660 - acc: 0.953 - ETA: 1s - loss: 0.1659 - acc: 0.952 - ETA: 1s - loss: 0.1673 - acc: 0.950 - ETA: 1s - loss: 0.1661 - acc: 0.951 - ETA: 1s - loss: 0.1636 - acc: 0.952 - ETA: 1s - loss: 0.1663 - acc: 0.951 - ETA: 0s - loss: 0.1653 - acc: 0.950 - ETA: 0s - loss: 0.1697 - acc: 0.949 - ETA: 0s - loss: 0.1688 - acc: 0.949 - ETA: 0s - loss: 0.1680 - acc: 0.949 - ETA: 0s - loss: 0.1693 - acc: 0.949 - ETA: 0s - loss: 0.1711 - acc: 0.948 - ETA: 0s - loss: 0.1725 - acc: 0.948 - ETA: 0s - loss: 0.1726 - acc: 0.948 - ETA: 0s - loss: 0.1731 - acc: 0.948 - ETA: 0s - loss: 0.1722 - acc: 0.948 - ETA: 0s - loss: 0.1738 - acc: 0.948 - ETA: 0s - loss: 0.1729 - acc: 0.948 - ETA: 0s - loss: 0.1746 - acc: 0.948 - 2s 595us/step - loss: 0.1738 - acc: 0.9484 - val_loss: 0.3426 - val_acc: 0.9000\n",
      "Epoch 126/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1604 - acc: 0.968 - ETA: 2s - loss: 0.1737 - acc: 0.958 - ETA: 2s - loss: 0.1664 - acc: 0.956 - ETA: 2s - loss: 0.1501 - acc: 0.962 - ETA: 1s - loss: 0.1637 - acc: 0.960 - ETA: 1s - loss: 0.1539 - acc: 0.964 - ETA: 1s - loss: 0.1720 - acc: 0.960 - ETA: 1s - loss: 0.1676 - acc: 0.959 - ETA: 1s - loss: 0.1690 - acc: 0.956 - ETA: 1s - loss: 0.1721 - acc: 0.954 - ETA: 1s - loss: 0.1707 - acc: 0.953 - ETA: 1s - loss: 0.1702 - acc: 0.953 - ETA: 1s - loss: 0.1681 - acc: 0.951 - ETA: 1s - loss: 0.1686 - acc: 0.950 - ETA: 1s - loss: 0.1710 - acc: 0.951 - ETA: 1s - loss: 0.1733 - acc: 0.950 - ETA: 1s - loss: 0.1741 - acc: 0.949 - ETA: 1s - loss: 0.1727 - acc: 0.950 - ETA: 0s - loss: 0.1714 - acc: 0.951 - ETA: 0s - loss: 0.1720 - acc: 0.950 - ETA: 0s - loss: 0.1711 - acc: 0.951 - ETA: 0s - loss: 0.1696 - acc: 0.952 - ETA: 0s - loss: 0.1711 - acc: 0.952 - ETA: 0s - loss: 0.1730 - acc: 0.951 - ETA: 0s - loss: 0.1696 - acc: 0.952 - ETA: 0s - loss: 0.1696 - acc: 0.952 - ETA: 0s - loss: 0.1697 - acc: 0.952 - ETA: 0s - loss: 0.1705 - acc: 0.952 - ETA: 0s - loss: 0.1695 - acc: 0.952 - ETA: 0s - loss: 0.1693 - acc: 0.952 - ETA: 0s - loss: 0.1684 - acc: 0.953 - ETA: 0s - loss: 0.1675 - acc: 0.953 - ETA: 0s - loss: 0.1674 - acc: 0.953 - 3s 620us/step - loss: 0.1678 - acc: 0.9528 - val_loss: 0.2340 - val_acc: 0.9327\n",
      "Epoch 127/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2169 - acc: 0.937 - ETA: 2s - loss: 0.1946 - acc: 0.947 - ETA: 2s - loss: 0.2112 - acc: 0.946 - ETA: 2s - loss: 0.2005 - acc: 0.944 - ETA: 1s - loss: 0.1777 - acc: 0.953 - ETA: 1s - loss: 0.1713 - acc: 0.957 - ETA: 1s - loss: 0.1765 - acc: 0.953 - ETA: 1s - loss: 0.1786 - acc: 0.951 - ETA: 1s - loss: 0.1800 - acc: 0.949 - ETA: 1s - loss: 0.1823 - acc: 0.947 - ETA: 1s - loss: 0.1798 - acc: 0.946 - ETA: 1s - loss: 0.1824 - acc: 0.945 - ETA: 1s - loss: 0.1804 - acc: 0.946 - ETA: 1s - loss: 0.1788 - acc: 0.947 - ETA: 1s - loss: 0.1807 - acc: 0.948 - ETA: 1s - loss: 0.1863 - acc: 0.946 - ETA: 1s - loss: 0.1841 - acc: 0.947 - ETA: 1s - loss: 0.1836 - acc: 0.946 - ETA: 0s - loss: 0.1826 - acc: 0.946 - ETA: 0s - loss: 0.1822 - acc: 0.946 - ETA: 0s - loss: 0.1821 - acc: 0.946 - ETA: 0s - loss: 0.1812 - acc: 0.946 - ETA: 0s - loss: 0.1811 - acc: 0.946 - ETA: 0s - loss: 0.1825 - acc: 0.946 - ETA: 0s - loss: 0.1826 - acc: 0.946 - ETA: 0s - loss: 0.1830 - acc: 0.946 - ETA: 0s - loss: 0.1829 - acc: 0.946 - ETA: 0s - loss: 0.1813 - acc: 0.947 - ETA: 0s - loss: 0.1793 - acc: 0.948 - ETA: 0s - loss: 0.1783 - acc: 0.948 - ETA: 0s - loss: 0.1770 - acc: 0.949 - ETA: 0s - loss: 0.1796 - acc: 0.948 - 2s 598us/step - loss: 0.1793 - acc: 0.9486 - val_loss: 0.2284 - val_acc: 0.9295\n",
      "Epoch 128/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2278 - acc: 0.921 - ETA: 2s - loss: 0.1812 - acc: 0.947 - ETA: 2s - loss: 0.2027 - acc: 0.946 - ETA: 2s - loss: 0.1894 - acc: 0.953 - ETA: 2s - loss: 0.1752 - acc: 0.956 - ETA: 1s - loss: 0.1808 - acc: 0.954 - ETA: 1s - loss: 0.1767 - acc: 0.954 - ETA: 1s - loss: 0.1712 - acc: 0.957 - ETA: 1s - loss: 0.1634 - acc: 0.960 - ETA: 1s - loss: 0.1662 - acc: 0.958 - ETA: 1s - loss: 0.1649 - acc: 0.958 - ETA: 1s - loss: 0.1660 - acc: 0.958 - ETA: 1s - loss: 0.1672 - acc: 0.958 - ETA: 1s - loss: 0.1697 - acc: 0.956 - ETA: 1s - loss: 0.1725 - acc: 0.955 - ETA: 1s - loss: 0.1718 - acc: 0.955 - ETA: 1s - loss: 0.1707 - acc: 0.955 - ETA: 1s - loss: 0.1735 - acc: 0.953 - ETA: 0s - loss: 0.1745 - acc: 0.953 - ETA: 0s - loss: 0.1744 - acc: 0.953 - ETA: 0s - loss: 0.1722 - acc: 0.954 - ETA: 0s - loss: 0.1735 - acc: 0.953 - ETA: 0s - loss: 0.1730 - acc: 0.953 - ETA: 0s - loss: 0.1727 - acc: 0.953 - ETA: 0s - loss: 0.1729 - acc: 0.953 - ETA: 0s - loss: 0.1747 - acc: 0.953 - ETA: 0s - loss: 0.1741 - acc: 0.953 - ETA: 0s - loss: 0.1743 - acc: 0.952 - ETA: 0s - loss: 0.1742 - acc: 0.952 - ETA: 0s - loss: 0.1742 - acc: 0.952 - ETA: 0s - loss: 0.1734 - acc: 0.952 - ETA: 0s - loss: 0.1720 - acc: 0.953 - 2s 605us/step - loss: 0.1715 - acc: 0.9535 - val_loss: 0.2447 - val_acc: 0.9256\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1112 - acc: 0.984 - ETA: 2s - loss: 0.1149 - acc: 0.979 - ETA: 2s - loss: 0.1277 - acc: 0.975 - ETA: 2s - loss: 0.1324 - acc: 0.971 - ETA: 1s - loss: 0.1344 - acc: 0.970 - ETA: 1s - loss: 0.1452 - acc: 0.964 - ETA: 1s - loss: 0.1571 - acc: 0.959 - ETA: 1s - loss: 0.1589 - acc: 0.956 - ETA: 1s - loss: 0.1640 - acc: 0.954 - ETA: 1s - loss: 0.1690 - acc: 0.949 - ETA: 1s - loss: 0.1705 - acc: 0.949 - ETA: 1s - loss: 0.1727 - acc: 0.948 - ETA: 1s - loss: 0.1728 - acc: 0.948 - ETA: 1s - loss: 0.1770 - acc: 0.946 - ETA: 1s - loss: 0.1778 - acc: 0.945 - ETA: 1s - loss: 0.1759 - acc: 0.946 - ETA: 1s - loss: 0.1771 - acc: 0.945 - ETA: 1s - loss: 0.1788 - acc: 0.945 - ETA: 0s - loss: 0.1791 - acc: 0.944 - ETA: 0s - loss: 0.1774 - acc: 0.945 - ETA: 0s - loss: 0.1774 - acc: 0.946 - ETA: 0s - loss: 0.1761 - acc: 0.947 - ETA: 0s - loss: 0.1730 - acc: 0.949 - ETA: 0s - loss: 0.1742 - acc: 0.948 - ETA: 0s - loss: 0.1730 - acc: 0.949 - ETA: 0s - loss: 0.1702 - acc: 0.950 - ETA: 0s - loss: 0.1710 - acc: 0.950 - ETA: 0s - loss: 0.1727 - acc: 0.950 - ETA: 0s - loss: 0.1705 - acc: 0.951 - ETA: 0s - loss: 0.1698 - acc: 0.951 - ETA: 0s - loss: 0.1691 - acc: 0.950 - ETA: 0s - loss: 0.1673 - acc: 0.951 - 2s 594us/step - loss: 0.1671 - acc: 0.9516 - val_loss: 0.2529 - val_acc: 0.9077\n",
      "Epoch 130/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1575 - acc: 0.921 - ETA: 2s - loss: 0.2229 - acc: 0.916 - ETA: 2s - loss: 0.1712 - acc: 0.950 - ETA: 2s - loss: 0.1832 - acc: 0.950 - ETA: 1s - loss: 0.1699 - acc: 0.954 - ETA: 1s - loss: 0.1770 - acc: 0.947 - ETA: 1s - loss: 0.1737 - acc: 0.948 - ETA: 1s - loss: 0.1755 - acc: 0.949 - ETA: 1s - loss: 0.1730 - acc: 0.950 - ETA: 1s - loss: 0.1706 - acc: 0.951 - ETA: 1s - loss: 0.1662 - acc: 0.953 - ETA: 1s - loss: 0.1638 - acc: 0.953 - ETA: 1s - loss: 0.1633 - acc: 0.951 - ETA: 1s - loss: 0.1597 - acc: 0.953 - ETA: 1s - loss: 0.1593 - acc: 0.954 - ETA: 1s - loss: 0.1588 - acc: 0.955 - ETA: 1s - loss: 0.1573 - acc: 0.956 - ETA: 1s - loss: 0.1595 - acc: 0.955 - ETA: 0s - loss: 0.1589 - acc: 0.955 - ETA: 0s - loss: 0.1582 - acc: 0.955 - ETA: 0s - loss: 0.1603 - acc: 0.953 - ETA: 0s - loss: 0.1581 - acc: 0.954 - ETA: 0s - loss: 0.1572 - acc: 0.954 - ETA: 0s - loss: 0.1567 - acc: 0.954 - ETA: 0s - loss: 0.1579 - acc: 0.954 - ETA: 0s - loss: 0.1582 - acc: 0.953 - ETA: 0s - loss: 0.1593 - acc: 0.953 - ETA: 0s - loss: 0.1577 - acc: 0.954 - ETA: 0s - loss: 0.1564 - acc: 0.954 - ETA: 0s - loss: 0.1562 - acc: 0.954 - ETA: 0s - loss: 0.1571 - acc: 0.954 - ETA: 0s - loss: 0.1574 - acc: 0.954 - 2s 592us/step - loss: 0.1580 - acc: 0.9538 - val_loss: 0.2599 - val_acc: 0.9199\n",
      "Epoch 131/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1582 - acc: 0.968 - ETA: 2s - loss: 0.1613 - acc: 0.942 - ETA: 2s - loss: 0.1384 - acc: 0.959 - ETA: 2s - loss: 0.1960 - acc: 0.935 - ETA: 1s - loss: 0.1948 - acc: 0.937 - ETA: 1s - loss: 0.1997 - acc: 0.936 - ETA: 1s - loss: 0.1926 - acc: 0.941 - ETA: 1s - loss: 0.1881 - acc: 0.942 - ETA: 1s - loss: 0.1932 - acc: 0.938 - ETA: 1s - loss: 0.1885 - acc: 0.942 - ETA: 1s - loss: 0.1856 - acc: 0.944 - ETA: 1s - loss: 0.1844 - acc: 0.945 - ETA: 1s - loss: 0.1806 - acc: 0.946 - ETA: 1s - loss: 0.1784 - acc: 0.947 - ETA: 1s - loss: 0.1781 - acc: 0.947 - ETA: 1s - loss: 0.1742 - acc: 0.948 - ETA: 1s - loss: 0.1730 - acc: 0.949 - ETA: 1s - loss: 0.1728 - acc: 0.949 - ETA: 0s - loss: 0.1747 - acc: 0.948 - ETA: 0s - loss: 0.1786 - acc: 0.946 - ETA: 0s - loss: 0.1771 - acc: 0.946 - ETA: 0s - loss: 0.1779 - acc: 0.946 - ETA: 0s - loss: 0.1755 - acc: 0.948 - ETA: 0s - loss: 0.1736 - acc: 0.949 - ETA: 0s - loss: 0.1736 - acc: 0.948 - ETA: 0s - loss: 0.1722 - acc: 0.948 - ETA: 0s - loss: 0.1719 - acc: 0.949 - ETA: 0s - loss: 0.1716 - acc: 0.949 - ETA: 0s - loss: 0.1703 - acc: 0.949 - ETA: 0s - loss: 0.1684 - acc: 0.950 - ETA: 0s - loss: 0.1673 - acc: 0.950 - ETA: 0s - loss: 0.1651 - acc: 0.951 - 2s 589us/step - loss: 0.1634 - acc: 0.9525 - val_loss: 0.2955 - val_acc: 0.9077\n",
      "Epoch 132/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.0984 - acc: 0.953 - ETA: 2s - loss: 0.1127 - acc: 0.963 - ETA: 2s - loss: 0.1343 - acc: 0.953 - ETA: 2s - loss: 0.1270 - acc: 0.957 - ETA: 2s - loss: 0.1368 - acc: 0.951 - ETA: 1s - loss: 0.1555 - acc: 0.941 - ETA: 1s - loss: 0.1505 - acc: 0.944 - ETA: 1s - loss: 0.1478 - acc: 0.946 - ETA: 1s - loss: 0.1504 - acc: 0.947 - ETA: 1s - loss: 0.1471 - acc: 0.949 - ETA: 1s - loss: 0.1441 - acc: 0.951 - ETA: 1s - loss: 0.1428 - acc: 0.953 - ETA: 1s - loss: 0.1394 - acc: 0.956 - ETA: 1s - loss: 0.1382 - acc: 0.957 - ETA: 1s - loss: 0.1410 - acc: 0.956 - ETA: 1s - loss: 0.1409 - acc: 0.957 - ETA: 1s - loss: 0.1394 - acc: 0.957 - ETA: 1s - loss: 0.1367 - acc: 0.959 - ETA: 0s - loss: 0.1394 - acc: 0.959 - ETA: 0s - loss: 0.1418 - acc: 0.958 - ETA: 0s - loss: 0.1419 - acc: 0.958 - ETA: 0s - loss: 0.1422 - acc: 0.958 - ETA: 0s - loss: 0.1444 - acc: 0.956 - ETA: 0s - loss: 0.1443 - acc: 0.957 - ETA: 0s - loss: 0.1459 - acc: 0.956 - ETA: 0s - loss: 0.1495 - acc: 0.954 - ETA: 0s - loss: 0.1488 - acc: 0.954 - ETA: 0s - loss: 0.1489 - acc: 0.954 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1480 - acc: 0.955 - ETA: 0s - loss: 0.1504 - acc: 0.954 - ETA: 0s - loss: 0.1505 - acc: 0.955 - 3s 619us/step - loss: 0.1513 - acc: 0.9545 - val_loss: 0.2973 - val_acc: 0.9026\n",
      "Epoch 133/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2172 - acc: 0.921 - ETA: 2s - loss: 0.1471 - acc: 0.958 - ETA: 2s - loss: 0.1416 - acc: 0.962 - ETA: 2s - loss: 0.1413 - acc: 0.959 - ETA: 1s - loss: 0.1534 - acc: 0.954 - ETA: 1s - loss: 0.1639 - acc: 0.946 - ETA: 1s - loss: 0.1614 - acc: 0.948 - ETA: 1s - loss: 0.1594 - acc: 0.950 - ETA: 1s - loss: 0.1629 - acc: 0.946 - ETA: 1s - loss: 0.1653 - acc: 0.948 - ETA: 1s - loss: 0.1650 - acc: 0.948 - ETA: 1s - loss: 0.1617 - acc: 0.950 - ETA: 1s - loss: 0.1589 - acc: 0.952 - ETA: 1s - loss: 0.1586 - acc: 0.953 - ETA: 1s - loss: 0.1580 - acc: 0.954 - ETA: 1s - loss: 0.1621 - acc: 0.953 - ETA: 1s - loss: 0.1644 - acc: 0.952 - ETA: 1s - loss: 0.1625 - acc: 0.953 - ETA: 0s - loss: 0.1682 - acc: 0.951 - ETA: 0s - loss: 0.1683 - acc: 0.950 - ETA: 0s - loss: 0.1680 - acc: 0.949 - ETA: 0s - loss: 0.1681 - acc: 0.949 - ETA: 0s - loss: 0.1700 - acc: 0.948 - ETA: 0s - loss: 0.1720 - acc: 0.947 - ETA: 0s - loss: 0.1707 - acc: 0.948 - ETA: 0s - loss: 0.1714 - acc: 0.947 - ETA: 0s - loss: 0.1719 - acc: 0.948 - ETA: 0s - loss: 0.1739 - acc: 0.948 - ETA: 0s - loss: 0.1734 - acc: 0.948 - ETA: 0s - loss: 0.1747 - acc: 0.948 - ETA: 0s - loss: 0.1759 - acc: 0.948 - ETA: 0s - loss: 0.1761 - acc: 0.947 - 2s 597us/step - loss: 0.1756 - acc: 0.9481 - val_loss: 0.2770 - val_acc: 0.9071\n",
      "Epoch 134/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2689 - acc: 0.890 - ETA: 2s - loss: 0.2541 - acc: 0.927 - ETA: 2s - loss: 0.1930 - acc: 0.943 - ETA: 2s - loss: 0.1819 - acc: 0.942 - ETA: 1s - loss: 0.1803 - acc: 0.941 - ETA: 1s - loss: 0.1735 - acc: 0.947 - ETA: 1s - loss: 0.1811 - acc: 0.943 - ETA: 1s - loss: 0.1945 - acc: 0.943 - ETA: 1s - loss: 0.1961 - acc: 0.945 - ETA: 1s - loss: 0.1941 - acc: 0.946 - ETA: 1s - loss: 0.1928 - acc: 0.947 - ETA: 1s - loss: 0.1917 - acc: 0.946 - ETA: 1s - loss: 0.1910 - acc: 0.944 - ETA: 1s - loss: 0.1904 - acc: 0.943 - ETA: 1s - loss: 0.1949 - acc: 0.941 - ETA: 1s - loss: 0.1970 - acc: 0.941 - ETA: 1s - loss: 0.1959 - acc: 0.941 - ETA: 1s - loss: 0.1967 - acc: 0.940 - ETA: 0s - loss: 0.1965 - acc: 0.940 - ETA: 0s - loss: 0.1958 - acc: 0.940 - ETA: 0s - loss: 0.1931 - acc: 0.942 - ETA: 0s - loss: 0.1928 - acc: 0.942 - ETA: 0s - loss: 0.1919 - acc: 0.942 - ETA: 0s - loss: 0.1978 - acc: 0.940 - ETA: 0s - loss: 0.1970 - acc: 0.940 - ETA: 0s - loss: 0.1956 - acc: 0.940 - ETA: 0s - loss: 0.1950 - acc: 0.940 - ETA: 0s - loss: 0.1938 - acc: 0.940 - ETA: 0s - loss: 0.1950 - acc: 0.940 - ETA: 0s - loss: 0.1955 - acc: 0.940 - ETA: 0s - loss: 0.1950 - acc: 0.940 - ETA: 0s - loss: 0.1943 - acc: 0.940 - 2s 602us/step - loss: 0.1947 - acc: 0.9407 - val_loss: 0.2334 - val_acc: 0.9301\n",
      "Epoch 135/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1630 - acc: 0.953 - ETA: 2s - loss: 0.1723 - acc: 0.947 - ETA: 2s - loss: 0.1825 - acc: 0.946 - ETA: 2s - loss: 0.1721 - acc: 0.948 - ETA: 1s - loss: 0.1676 - acc: 0.951 - ETA: 1s - loss: 0.1750 - acc: 0.953 - ETA: 1s - loss: 0.1760 - acc: 0.951 - ETA: 1s - loss: 0.1666 - acc: 0.955 - ETA: 1s - loss: 0.1643 - acc: 0.955 - ETA: 1s - loss: 0.1614 - acc: 0.957 - ETA: 1s - loss: 0.1590 - acc: 0.957 - ETA: 1s - loss: 0.1584 - acc: 0.957 - ETA: 1s - loss: 0.1597 - acc: 0.955 - ETA: 1s - loss: 0.1641 - acc: 0.954 - ETA: 1s - loss: 0.1614 - acc: 0.955 - ETA: 1s - loss: 0.1586 - acc: 0.957 - ETA: 1s - loss: 0.1597 - acc: 0.955 - ETA: 1s - loss: 0.1579 - acc: 0.956 - ETA: 0s - loss: 0.1596 - acc: 0.955 - ETA: 0s - loss: 0.1584 - acc: 0.956 - ETA: 0s - loss: 0.1590 - acc: 0.956 - ETA: 0s - loss: 0.1593 - acc: 0.955 - ETA: 0s - loss: 0.1616 - acc: 0.953 - ETA: 0s - loss: 0.1598 - acc: 0.954 - ETA: 0s - loss: 0.1580 - acc: 0.955 - ETA: 0s - loss: 0.1558 - acc: 0.956 - ETA: 0s - loss: 0.1574 - acc: 0.956 - ETA: 0s - loss: 0.1578 - acc: 0.956 - ETA: 0s - loss: 0.1589 - acc: 0.955 - ETA: 0s - loss: 0.1596 - acc: 0.955 - ETA: 0s - loss: 0.1584 - acc: 0.955 - ETA: 0s - loss: 0.1583 - acc: 0.955 - 2s 592us/step - loss: 0.1578 - acc: 0.9560 - val_loss: 0.2904 - val_acc: 0.8968\n",
      "Epoch 136/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1322 - acc: 0.968 - ETA: 2s - loss: 0.1048 - acc: 0.984 - ETA: 2s - loss: 0.1224 - acc: 0.971 - ETA: 1s - loss: 0.1193 - acc: 0.975 - ETA: 1s - loss: 0.1203 - acc: 0.977 - ETA: 1s - loss: 0.1229 - acc: 0.973 - ETA: 1s - loss: 0.1214 - acc: 0.973 - ETA: 1s - loss: 0.1220 - acc: 0.970 - ETA: 1s - loss: 0.1239 - acc: 0.967 - ETA: 1s - loss: 0.1328 - acc: 0.964 - ETA: 1s - loss: 0.1420 - acc: 0.961 - ETA: 1s - loss: 0.1427 - acc: 0.961 - ETA: 1s - loss: 0.1419 - acc: 0.962 - ETA: 1s - loss: 0.1405 - acc: 0.963 - ETA: 1s - loss: 0.1431 - acc: 0.962 - ETA: 1s - loss: 0.1399 - acc: 0.963 - ETA: 1s - loss: 0.1435 - acc: 0.961 - ETA: 1s - loss: 0.1467 - acc: 0.960 - ETA: 0s - loss: 0.1489 - acc: 0.958 - ETA: 0s - loss: 0.1486 - acc: 0.957 - ETA: 0s - loss: 0.1474 - acc: 0.958 - ETA: 0s - loss: 0.1469 - acc: 0.958 - ETA: 0s - loss: 0.1460 - acc: 0.958 - ETA: 0s - loss: 0.1472 - acc: 0.957 - ETA: 0s - loss: 0.1471 - acc: 0.957 - ETA: 0s - loss: 0.1457 - acc: 0.957 - ETA: 0s - loss: 0.1483 - acc: 0.956 - ETA: 0s - loss: 0.1497 - acc: 0.955 - ETA: 0s - loss: 0.1504 - acc: 0.955 - ETA: 0s - loss: 0.1524 - acc: 0.954 - ETA: 0s - loss: 0.1532 - acc: 0.954 - ETA: 0s - loss: 0.1562 - acc: 0.953 - 2s 589us/step - loss: 0.1570 - acc: 0.9528 - val_loss: 0.3215 - val_acc: 0.8955\n",
      "Epoch 137/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1185 - acc: 0.984 - ETA: 2s - loss: 0.1401 - acc: 0.968 - ETA: 2s - loss: 0.1424 - acc: 0.968 - ETA: 2s - loss: 0.1408 - acc: 0.966 - ETA: 1s - loss: 0.1554 - acc: 0.958 - ETA: 1s - loss: 0.1638 - acc: 0.953 - ETA: 1s - loss: 0.1639 - acc: 0.951 - ETA: 1s - loss: 0.1576 - acc: 0.955 - ETA: 1s - loss: 0.1635 - acc: 0.954 - ETA: 1s - loss: 0.1639 - acc: 0.953 - ETA: 1s - loss: 0.1657 - acc: 0.950 - ETA: 1s - loss: 0.1639 - acc: 0.951 - ETA: 1s - loss: 0.1678 - acc: 0.950 - ETA: 1s - loss: 0.1624 - acc: 0.953 - ETA: 1s - loss: 0.1641 - acc: 0.953 - ETA: 1s - loss: 0.1632 - acc: 0.952 - ETA: 1s - loss: 0.1611 - acc: 0.953 - ETA: 1s - loss: 0.1579 - acc: 0.954 - ETA: 0s - loss: 0.1586 - acc: 0.954 - ETA: 0s - loss: 0.1567 - acc: 0.954 - ETA: 0s - loss: 0.1565 - acc: 0.954 - ETA: 0s - loss: 0.1568 - acc: 0.953 - ETA: 0s - loss: 0.1560 - acc: 0.954 - ETA: 0s - loss: 0.1584 - acc: 0.953 - ETA: 0s - loss: 0.1583 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.952 - ETA: 0s - loss: 0.1586 - acc: 0.951 - ETA: 0s - loss: 0.1610 - acc: 0.950 - ETA: 0s - loss: 0.1634 - acc: 0.950 - ETA: 0s - loss: 0.1631 - acc: 0.950 - ETA: 0s - loss: 0.1621 - acc: 0.950 - ETA: 0s - loss: 0.1615 - acc: 0.951 - 2s 592us/step - loss: 0.1612 - acc: 0.9516 - val_loss: 0.3219 - val_acc: 0.9071\n",
      "Epoch 138/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1291 - acc: 0.968 - ETA: 2s - loss: 0.1586 - acc: 0.947 - ETA: 2s - loss: 0.1793 - acc: 0.934 - ETA: 2s - loss: 0.1699 - acc: 0.944 - ETA: 1s - loss: 0.1668 - acc: 0.951 - ETA: 1s - loss: 0.1575 - acc: 0.957 - ETA: 1s - loss: 0.1579 - acc: 0.955 - ETA: 1s - loss: 0.1610 - acc: 0.949 - ETA: 1s - loss: 0.1617 - acc: 0.948 - ETA: 1s - loss: 0.1647 - acc: 0.947 - ETA: 1s - loss: 0.1591 - acc: 0.949 - ETA: 1s - loss: 0.1560 - acc: 0.949 - ETA: 1s - loss: 0.1562 - acc: 0.950 - ETA: 1s - loss: 0.1595 - acc: 0.949 - ETA: 1s - loss: 0.1594 - acc: 0.949 - ETA: 1s - loss: 0.1607 - acc: 0.949 - ETA: 1s - loss: 0.1645 - acc: 0.947 - ETA: 1s - loss: 0.1631 - acc: 0.949 - ETA: 0s - loss: 0.1609 - acc: 0.951 - ETA: 0s - loss: 0.1606 - acc: 0.952 - ETA: 0s - loss: 0.1589 - acc: 0.953 - ETA: 0s - loss: 0.1599 - acc: 0.953 - ETA: 0s - loss: 0.1589 - acc: 0.954 - ETA: 0s - loss: 0.1593 - acc: 0.954 - ETA: 0s - loss: 0.1644 - acc: 0.952 - ETA: 0s - loss: 0.1640 - acc: 0.952 - ETA: 0s - loss: 0.1648 - acc: 0.951 - ETA: 0s - loss: 0.1728 - acc: 0.948 - ETA: 0s - loss: 0.1763 - acc: 0.948 - ETA: 0s - loss: 0.1762 - acc: 0.948 - ETA: 0s - loss: 0.1791 - acc: 0.947 - ETA: 0s - loss: 0.1784 - acc: 0.947 - 2s 610us/step - loss: 0.1794 - acc: 0.9476 - val_loss: 0.2757 - val_acc: 0.9013\n",
      "Epoch 139/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2588 - acc: 0.921 - ETA: 2s - loss: 0.2441 - acc: 0.927 - ETA: 2s - loss: 0.2160 - acc: 0.937 - ETA: 2s - loss: 0.2229 - acc: 0.937 - ETA: 1s - loss: 0.2323 - acc: 0.939 - ETA: 1s - loss: 0.2353 - acc: 0.940 - ETA: 1s - loss: 0.2331 - acc: 0.938 - ETA: 1s - loss: 0.2271 - acc: 0.940 - ETA: 1s - loss: 0.2209 - acc: 0.941 - ETA: 1s - loss: 0.2091 - acc: 0.945 - ETA: 1s - loss: 0.2029 - acc: 0.947 - ETA: 1s - loss: 0.2011 - acc: 0.948 - ETA: 1s - loss: 0.1967 - acc: 0.949 - ETA: 1s - loss: 0.1969 - acc: 0.949 - ETA: 1s - loss: 0.1938 - acc: 0.949 - ETA: 1s - loss: 0.2049 - acc: 0.946 - ETA: 1s - loss: 0.2017 - acc: 0.947 - ETA: 1s - loss: 0.2013 - acc: 0.948 - ETA: 0s - loss: 0.2014 - acc: 0.948 - ETA: 0s - loss: 0.2006 - acc: 0.947 - ETA: 0s - loss: 0.1993 - acc: 0.948 - ETA: 0s - loss: 0.2016 - acc: 0.947 - ETA: 0s - loss: 0.1990 - acc: 0.948 - ETA: 0s - loss: 0.2001 - acc: 0.947 - ETA: 0s - loss: 0.1980 - acc: 0.948 - ETA: 0s - loss: 0.2029 - acc: 0.947 - ETA: 0s - loss: 0.2018 - acc: 0.947 - ETA: 0s - loss: 0.2008 - acc: 0.947 - ETA: 0s - loss: 0.2015 - acc: 0.946 - ETA: 0s - loss: 0.2000 - acc: 0.947 - ETA: 0s - loss: 0.2053 - acc: 0.946 - ETA: 0s - loss: 0.2055 - acc: 0.945 - 2s 598us/step - loss: 0.2064 - acc: 0.9452 - val_loss: 0.3434 - val_acc: 0.8987\n",
      "Epoch 140/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1726 - acc: 0.953 - ETA: 2s - loss: 0.2486 - acc: 0.927 - ETA: 2s - loss: 0.2363 - acc: 0.921 - ETA: 2s - loss: 0.2096 - acc: 0.935 - ETA: 2s - loss: 0.2081 - acc: 0.935 - ETA: 1s - loss: 0.2024 - acc: 0.937 - ETA: 1s - loss: 0.1952 - acc: 0.939 - ETA: 1s - loss: 0.1927 - acc: 0.942 - ETA: 1s - loss: 0.1979 - acc: 0.942 - ETA: 1s - loss: 0.1972 - acc: 0.941 - ETA: 1s - loss: 0.2023 - acc: 0.938 - ETA: 1s - loss: 0.1970 - acc: 0.941 - ETA: 1s - loss: 0.1935 - acc: 0.942 - ETA: 1s - loss: 0.1966 - acc: 0.942 - ETA: 1s - loss: 0.1949 - acc: 0.942 - ETA: 1s - loss: 0.1910 - acc: 0.945 - ETA: 1s - loss: 0.1907 - acc: 0.945 - ETA: 1s - loss: 0.1894 - acc: 0.944 - ETA: 0s - loss: 0.1885 - acc: 0.944 - ETA: 0s - loss: 0.1847 - acc: 0.946 - ETA: 0s - loss: 0.1851 - acc: 0.947 - ETA: 0s - loss: 0.1848 - acc: 0.946 - ETA: 0s - loss: 0.1816 - acc: 0.948 - ETA: 0s - loss: 0.1816 - acc: 0.948 - ETA: 0s - loss: 0.1801 - acc: 0.949 - ETA: 0s - loss: 0.1816 - acc: 0.949 - ETA: 0s - loss: 0.1813 - acc: 0.950 - ETA: 0s - loss: 0.1830 - acc: 0.949 - ETA: 0s - loss: 0.1830 - acc: 0.948 - ETA: 0s - loss: 0.1804 - acc: 0.949 - ETA: 0s - loss: 0.1808 - acc: 0.949 - ETA: 0s - loss: 0.1823 - acc: 0.948 - 2s 612us/step - loss: 0.1828 - acc: 0.9486 - val_loss: 0.2581 - val_acc: 0.9237\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2600 - acc: 0.906 - ETA: 2s - loss: 0.1951 - acc: 0.945 - ETA: 3s - loss: 0.1959 - acc: 0.932 - ETA: 3s - loss: 0.1644 - acc: 0.950 - ETA: 2s - loss: 0.1548 - acc: 0.955 - ETA: 2s - loss: 0.1608 - acc: 0.954 - ETA: 2s - loss: 0.1699 - acc: 0.951 - ETA: 2s - loss: 0.1611 - acc: 0.956 - ETA: 1s - loss: 0.1615 - acc: 0.957 - ETA: 1s - loss: 0.1588 - acc: 0.956 - ETA: 1s - loss: 0.1512 - acc: 0.960 - ETA: 1s - loss: 0.1560 - acc: 0.959 - ETA: 1s - loss: 0.1578 - acc: 0.959 - ETA: 1s - loss: 0.1620 - acc: 0.957 - ETA: 1s - loss: 0.1615 - acc: 0.957 - ETA: 1s - loss: 0.1631 - acc: 0.956 - ETA: 1s - loss: 0.1636 - acc: 0.955 - ETA: 1s - loss: 0.1648 - acc: 0.955 - ETA: 1s - loss: 0.1663 - acc: 0.954 - ETA: 0s - loss: 0.1641 - acc: 0.955 - ETA: 0s - loss: 0.1671 - acc: 0.952 - ETA: 0s - loss: 0.1684 - acc: 0.951 - ETA: 0s - loss: 0.1737 - acc: 0.949 - ETA: 0s - loss: 0.1715 - acc: 0.950 - ETA: 0s - loss: 0.1709 - acc: 0.950 - ETA: 0s - loss: 0.1700 - acc: 0.950 - ETA: 0s - loss: 0.1685 - acc: 0.951 - ETA: 0s - loss: 0.1673 - acc: 0.951 - ETA: 0s - loss: 0.1684 - acc: 0.951 - ETA: 0s - loss: 0.1686 - acc: 0.951 - ETA: 0s - loss: 0.1684 - acc: 0.951 - ETA: 0s - loss: 0.1687 - acc: 0.951 - ETA: 0s - loss: 0.1688 - acc: 0.951 - 3s 629us/step - loss: 0.1693 - acc: 0.9508 - val_loss: 0.2384 - val_acc: 0.9346\n",
      "Epoch 142/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1500 - acc: 0.968 - ETA: 2s - loss: 0.1254 - acc: 0.974 - ETA: 2s - loss: 0.1537 - acc: 0.962 - ETA: 2s - loss: 0.1537 - acc: 0.962 - ETA: 1s - loss: 0.1651 - acc: 0.958 - ETA: 1s - loss: 0.1661 - acc: 0.957 - ETA: 1s - loss: 0.1676 - acc: 0.955 - ETA: 1s - loss: 0.1689 - acc: 0.953 - ETA: 1s - loss: 0.1614 - acc: 0.955 - ETA: 1s - loss: 0.1589 - acc: 0.957 - ETA: 1s - loss: 0.1622 - acc: 0.954 - ETA: 1s - loss: 0.1573 - acc: 0.956 - ETA: 1s - loss: 0.1558 - acc: 0.956 - ETA: 1s - loss: 0.1579 - acc: 0.955 - ETA: 1s - loss: 0.1551 - acc: 0.958 - ETA: 1s - loss: 0.1589 - acc: 0.957 - ETA: 1s - loss: 0.1565 - acc: 0.959 - ETA: 1s - loss: 0.1605 - acc: 0.957 - ETA: 0s - loss: 0.1627 - acc: 0.957 - ETA: 0s - loss: 0.1664 - acc: 0.955 - ETA: 0s - loss: 0.1657 - acc: 0.956 - ETA: 0s - loss: 0.1638 - acc: 0.956 - ETA: 0s - loss: 0.1642 - acc: 0.956 - ETA: 0s - loss: 0.1684 - acc: 0.956 - ETA: 0s - loss: 0.1687 - acc: 0.955 - ETA: 0s - loss: 0.1696 - acc: 0.955 - ETA: 0s - loss: 0.1718 - acc: 0.955 - ETA: 0s - loss: 0.1727 - acc: 0.955 - ETA: 0s - loss: 0.1764 - acc: 0.953 - ETA: 0s - loss: 0.1764 - acc: 0.953 - ETA: 0s - loss: 0.1766 - acc: 0.953 - ETA: 0s - loss: 0.1780 - acc: 0.952 - 2s 591us/step - loss: 0.1778 - acc: 0.9523 - val_loss: 0.2506 - val_acc: 0.9308\n",
      "Epoch 143/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1730 - acc: 0.953 - ETA: 2s - loss: 0.1933 - acc: 0.953 - ETA: 2s - loss: 0.1799 - acc: 0.950 - ETA: 2s - loss: 0.1893 - acc: 0.944 - ETA: 1s - loss: 0.1944 - acc: 0.939 - ETA: 1s - loss: 0.1936 - acc: 0.943 - ETA: 1s - loss: 0.1954 - acc: 0.942 - ETA: 1s - loss: 0.1892 - acc: 0.944 - ETA: 1s - loss: 0.1860 - acc: 0.945 - ETA: 1s - loss: 0.1899 - acc: 0.944 - ETA: 1s - loss: 0.1882 - acc: 0.944 - ETA: 1s - loss: 0.1874 - acc: 0.945 - ETA: 1s - loss: 0.1889 - acc: 0.944 - ETA: 1s - loss: 0.1855 - acc: 0.946 - ETA: 1s - loss: 0.1918 - acc: 0.944 - ETA: 1s - loss: 0.1955 - acc: 0.942 - ETA: 1s - loss: 0.1963 - acc: 0.941 - ETA: 1s - loss: 0.1959 - acc: 0.941 - ETA: 0s - loss: 0.1937 - acc: 0.941 - ETA: 0s - loss: 0.1899 - acc: 0.943 - ETA: 0s - loss: 0.1876 - acc: 0.944 - ETA: 0s - loss: 0.1863 - acc: 0.944 - ETA: 0s - loss: 0.1845 - acc: 0.944 - ETA: 0s - loss: 0.1823 - acc: 0.946 - ETA: 0s - loss: 0.1792 - acc: 0.947 - ETA: 0s - loss: 0.1794 - acc: 0.947 - ETA: 0s - loss: 0.1791 - acc: 0.947 - ETA: 0s - loss: 0.1780 - acc: 0.947 - ETA: 0s - loss: 0.1758 - acc: 0.948 - ETA: 0s - loss: 0.1757 - acc: 0.949 - ETA: 0s - loss: 0.1740 - acc: 0.949 - ETA: 0s - loss: 0.1734 - acc: 0.950 - ETA: 0s - loss: 0.1728 - acc: 0.950 - ETA: 0s - loss: 0.1706 - acc: 0.951 - 3s 636us/step - loss: 0.1709 - acc: 0.9513 - val_loss: 0.2691 - val_acc: 0.9186\n",
      "Epoch 144/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1408 - acc: 0.968 - ETA: 2s - loss: 0.1405 - acc: 0.953 - ETA: 2s - loss: 0.1486 - acc: 0.943 - ETA: 2s - loss: 0.1655 - acc: 0.942 - ETA: 2s - loss: 0.1514 - acc: 0.949 - ETA: 2s - loss: 0.1444 - acc: 0.953 - ETA: 2s - loss: 0.1440 - acc: 0.954 - ETA: 1s - loss: 0.1481 - acc: 0.952 - ETA: 1s - loss: 0.1439 - acc: 0.955 - ETA: 1s - loss: 0.1437 - acc: 0.958 - ETA: 1s - loss: 0.1441 - acc: 0.957 - ETA: 1s - loss: 0.1465 - acc: 0.956 - ETA: 1s - loss: 0.1463 - acc: 0.957 - ETA: 1s - loss: 0.1493 - acc: 0.954 - ETA: 1s - loss: 0.1457 - acc: 0.957 - ETA: 1s - loss: 0.1445 - acc: 0.958 - ETA: 1s - loss: 0.1430 - acc: 0.959 - ETA: 1s - loss: 0.1422 - acc: 0.959 - ETA: 1s - loss: 0.1447 - acc: 0.959 - ETA: 1s - loss: 0.1429 - acc: 0.960 - ETA: 1s - loss: 0.1445 - acc: 0.959 - ETA: 0s - loss: 0.1468 - acc: 0.958 - ETA: 0s - loss: 0.1466 - acc: 0.958 - ETA: 0s - loss: 0.1519 - acc: 0.958 - ETA: 0s - loss: 0.1514 - acc: 0.958 - ETA: 0s - loss: 0.1523 - acc: 0.958 - ETA: 0s - loss: 0.1515 - acc: 0.958 - ETA: 0s - loss: 0.1536 - acc: 0.958 - ETA: 0s - loss: 0.1556 - acc: 0.957 - ETA: 0s - loss: 0.1547 - acc: 0.958 - ETA: 0s - loss: 0.1544 - acc: 0.958 - ETA: 0s - loss: 0.1553 - acc: 0.958 - ETA: 0s - loss: 0.1550 - acc: 0.958 - ETA: 0s - loss: 0.1540 - acc: 0.958 - ETA: 0s - loss: 0.1542 - acc: 0.958 - 3s 648us/step - loss: 0.1557 - acc: 0.9582 - val_loss: 0.2622 - val_acc: 0.9244\n",
      "Epoch 145/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1340 - acc: 0.968 - ETA: 2s - loss: 0.1666 - acc: 0.958 - ETA: 2s - loss: 0.1699 - acc: 0.946 - ETA: 2s - loss: 0.1516 - acc: 0.955 - ETA: 2s - loss: 0.1554 - acc: 0.956 - ETA: 2s - loss: 0.1654 - acc: 0.953 - ETA: 1s - loss: 0.1599 - acc: 0.954 - ETA: 1s - loss: 0.1580 - acc: 0.956 - ETA: 1s - loss: 0.1568 - acc: 0.956 - ETA: 1s - loss: 0.1617 - acc: 0.953 - ETA: 1s - loss: 0.1597 - acc: 0.955 - ETA: 1s - loss: 0.1603 - acc: 0.954 - ETA: 1s - loss: 0.1610 - acc: 0.954 - ETA: 1s - loss: 0.1591 - acc: 0.954 - ETA: 1s - loss: 0.1576 - acc: 0.956 - ETA: 1s - loss: 0.1531 - acc: 0.959 - ETA: 1s - loss: 0.1499 - acc: 0.960 - ETA: 1s - loss: 0.1489 - acc: 0.960 - ETA: 1s - loss: 0.1497 - acc: 0.958 - ETA: 1s - loss: 0.1507 - acc: 0.958 - ETA: 0s - loss: 0.1512 - acc: 0.959 - ETA: 0s - loss: 0.1504 - acc: 0.959 - ETA: 0s - loss: 0.1500 - acc: 0.959 - ETA: 0s - loss: 0.1486 - acc: 0.960 - ETA: 0s - loss: 0.1505 - acc: 0.959 - ETA: 0s - loss: 0.1522 - acc: 0.958 - ETA: 0s - loss: 0.1507 - acc: 0.959 - ETA: 0s - loss: 0.1514 - acc: 0.958 - ETA: 0s - loss: 0.1531 - acc: 0.958 - ETA: 0s - loss: 0.1534 - acc: 0.958 - ETA: 0s - loss: 0.1519 - acc: 0.958 - ETA: 0s - loss: 0.1518 - acc: 0.958 - ETA: 0s - loss: 0.1515 - acc: 0.958 - 3s 623us/step - loss: 0.1533 - acc: 0.9582 - val_loss: 0.2577 - val_acc: 0.9205\n",
      "Epoch 146/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1101 - acc: 0.984 - ETA: 2s - loss: 0.1604 - acc: 0.974 - ETA: 2s - loss: 0.1466 - acc: 0.968 - ETA: 2s - loss: 0.1469 - acc: 0.962 - ETA: 1s - loss: 0.1520 - acc: 0.960 - ETA: 1s - loss: 0.1547 - acc: 0.961 - ETA: 1s - loss: 0.1493 - acc: 0.963 - ETA: 1s - loss: 0.1530 - acc: 0.961 - ETA: 1s - loss: 0.1529 - acc: 0.961 - ETA: 1s - loss: 0.1521 - acc: 0.962 - ETA: 1s - loss: 0.1581 - acc: 0.959 - ETA: 1s - loss: 0.1614 - acc: 0.957 - ETA: 1s - loss: 0.1640 - acc: 0.957 - ETA: 1s - loss: 0.1641 - acc: 0.957 - ETA: 1s - loss: 0.1630 - acc: 0.956 - ETA: 1s - loss: 0.1600 - acc: 0.956 - ETA: 1s - loss: 0.1590 - acc: 0.956 - ETA: 1s - loss: 0.1605 - acc: 0.955 - ETA: 0s - loss: 0.1618 - acc: 0.954 - ETA: 0s - loss: 0.1621 - acc: 0.953 - ETA: 0s - loss: 0.1618 - acc: 0.953 - ETA: 0s - loss: 0.1643 - acc: 0.952 - ETA: 0s - loss: 0.1640 - acc: 0.952 - ETA: 0s - loss: 0.1657 - acc: 0.952 - ETA: 0s - loss: 0.1636 - acc: 0.953 - ETA: 0s - loss: 0.1639 - acc: 0.952 - ETA: 0s - loss: 0.1628 - acc: 0.953 - ETA: 0s - loss: 0.1614 - acc: 0.953 - ETA: 0s - loss: 0.1600 - acc: 0.954 - ETA: 0s - loss: 0.1611 - acc: 0.953 - ETA: 0s - loss: 0.1610 - acc: 0.954 - ETA: 0s - loss: 0.1583 - acc: 0.955 - 2s 600us/step - loss: 0.1579 - acc: 0.9552 - val_loss: 0.3829 - val_acc: 0.8769\n",
      "Epoch 147/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2050 - acc: 0.906 - ETA: 2s - loss: 0.2132 - acc: 0.921 - ETA: 2s - loss: 0.1821 - acc: 0.937 - ETA: 2s - loss: 0.1849 - acc: 0.942 - ETA: 1s - loss: 0.1901 - acc: 0.935 - ETA: 1s - loss: 0.1809 - acc: 0.938 - ETA: 1s - loss: 0.1695 - acc: 0.942 - ETA: 1s - loss: 0.1688 - acc: 0.944 - ETA: 1s - loss: 0.1596 - acc: 0.949 - ETA: 1s - loss: 0.1597 - acc: 0.951 - ETA: 1s - loss: 0.1561 - acc: 0.953 - ETA: 1s - loss: 0.1562 - acc: 0.953 - ETA: 1s - loss: 0.1522 - acc: 0.953 - ETA: 1s - loss: 0.1527 - acc: 0.952 - ETA: 1s - loss: 0.1526 - acc: 0.952 - ETA: 1s - loss: 0.1555 - acc: 0.950 - ETA: 1s - loss: 0.1539 - acc: 0.950 - ETA: 1s - loss: 0.1541 - acc: 0.950 - ETA: 0s - loss: 0.1556 - acc: 0.949 - ETA: 0s - loss: 0.1548 - acc: 0.950 - ETA: 0s - loss: 0.1566 - acc: 0.951 - ETA: 0s - loss: 0.1573 - acc: 0.951 - ETA: 0s - loss: 0.1561 - acc: 0.952 - ETA: 0s - loss: 0.1598 - acc: 0.951 - ETA: 0s - loss: 0.1593 - acc: 0.952 - ETA: 0s - loss: 0.1581 - acc: 0.953 - ETA: 0s - loss: 0.1596 - acc: 0.953 - ETA: 0s - loss: 0.1632 - acc: 0.951 - ETA: 0s - loss: 0.1645 - acc: 0.951 - ETA: 0s - loss: 0.1721 - acc: 0.949 - ETA: 0s - loss: 0.1723 - acc: 0.950 - ETA: 0s - loss: 0.1746 - acc: 0.948 - 2s 591us/step - loss: 0.1745 - acc: 0.9489 - val_loss: 0.2497 - val_acc: 0.9256\n",
      "Epoch 148/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1011 - acc: 0.984 - ETA: 2s - loss: 0.1508 - acc: 0.953 - ETA: 2s - loss: 0.1731 - acc: 0.943 - ETA: 2s - loss: 0.1683 - acc: 0.946 - ETA: 1s - loss: 0.1774 - acc: 0.944 - ETA: 1s - loss: 0.1740 - acc: 0.943 - ETA: 1s - loss: 0.1776 - acc: 0.941 - ETA: 1s - loss: 0.1770 - acc: 0.941 - ETA: 1s - loss: 0.1806 - acc: 0.940 - ETA: 1s - loss: 0.1763 - acc: 0.940 - ETA: 1s - loss: 0.1763 - acc: 0.942 - ETA: 1s - loss: 0.1729 - acc: 0.943 - ETA: 1s - loss: 0.1773 - acc: 0.941 - ETA: 1s - loss: 0.1881 - acc: 0.941 - ETA: 1s - loss: 0.1903 - acc: 0.941 - ETA: 1s - loss: 0.1913 - acc: 0.941 - ETA: 1s - loss: 0.1875 - acc: 0.943 - ETA: 1s - loss: 0.1852 - acc: 0.945 - ETA: 0s - loss: 0.1855 - acc: 0.945 - ETA: 0s - loss: 0.1835 - acc: 0.946 - ETA: 0s - loss: 0.1825 - acc: 0.947 - ETA: 0s - loss: 0.1797 - acc: 0.948 - ETA: 0s - loss: 0.1786 - acc: 0.949 - ETA: 0s - loss: 0.1757 - acc: 0.950 - ETA: 0s - loss: 0.1743 - acc: 0.950 - ETA: 0s - loss: 0.1749 - acc: 0.949 - ETA: 0s - loss: 0.1739 - acc: 0.950 - ETA: 0s - loss: 0.1730 - acc: 0.949 - ETA: 0s - loss: 0.1752 - acc: 0.948 - ETA: 0s - loss: 0.1737 - acc: 0.949 - ETA: 0s - loss: 0.1738 - acc: 0.949 - ETA: 0s - loss: 0.1739 - acc: 0.948 - 2s 591us/step - loss: 0.1745 - acc: 0.9486 - val_loss: 0.2648 - val_acc: 0.9192\n",
      "Epoch 149/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1322 - acc: 0.953 - ETA: 2s - loss: 0.1562 - acc: 0.947 - ETA: 2s - loss: 0.1807 - acc: 0.940 - ETA: 1s - loss: 0.1736 - acc: 0.942 - ETA: 1s - loss: 0.1793 - acc: 0.939 - ETA: 1s - loss: 0.1741 - acc: 0.943 - ETA: 1s - loss: 0.1735 - acc: 0.948 - ETA: 1s - loss: 0.1842 - acc: 0.942 - ETA: 1s - loss: 0.1796 - acc: 0.944 - ETA: 1s - loss: 0.1731 - acc: 0.948 - ETA: 1s - loss: 0.1781 - acc: 0.944 - ETA: 1s - loss: 0.1757 - acc: 0.945 - ETA: 1s - loss: 0.1775 - acc: 0.945 - ETA: 1s - loss: 0.1805 - acc: 0.946 - ETA: 1s - loss: 0.1782 - acc: 0.947 - ETA: 1s - loss: 0.1807 - acc: 0.946 - ETA: 1s - loss: 0.1810 - acc: 0.946 - ETA: 1s - loss: 0.1819 - acc: 0.945 - ETA: 0s - loss: 0.1821 - acc: 0.945 - ETA: 0s - loss: 0.1801 - acc: 0.945 - ETA: 0s - loss: 0.1793 - acc: 0.947 - ETA: 0s - loss: 0.1759 - acc: 0.948 - ETA: 0s - loss: 0.1750 - acc: 0.949 - ETA: 0s - loss: 0.1784 - acc: 0.947 - ETA: 0s - loss: 0.1766 - acc: 0.949 - ETA: 0s - loss: 0.1756 - acc: 0.949 - ETA: 0s - loss: 0.1742 - acc: 0.950 - ETA: 0s - loss: 0.1726 - acc: 0.951 - ETA: 0s - loss: 0.1719 - acc: 0.951 - ETA: 0s - loss: 0.1723 - acc: 0.950 - ETA: 0s - loss: 0.1727 - acc: 0.950 - ETA: 0s - loss: 0.1731 - acc: 0.950 - 2s 600us/step - loss: 0.1734 - acc: 0.9503 - val_loss: 0.2794 - val_acc: 0.8994\n",
      "Epoch 150/150\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3002 - acc: 0.890 - ETA: 2s - loss: 0.2414 - acc: 0.916 - ETA: 2s - loss: 0.2280 - acc: 0.912 - ETA: 2s - loss: 0.1988 - acc: 0.930 - ETA: 1s - loss: 0.1957 - acc: 0.935 - ETA: 1s - loss: 0.1927 - acc: 0.940 - ETA: 1s - loss: 0.1878 - acc: 0.943 - ETA: 1s - loss: 0.1827 - acc: 0.946 - ETA: 1s - loss: 0.1831 - acc: 0.945 - ETA: 1s - loss: 0.1845 - acc: 0.944 - ETA: 1s - loss: 0.1810 - acc: 0.946 - ETA: 1s - loss: 0.1786 - acc: 0.946 - ETA: 1s - loss: 0.1805 - acc: 0.944 - ETA: 1s - loss: 0.1791 - acc: 0.944 - ETA: 1s - loss: 0.1828 - acc: 0.942 - ETA: 1s - loss: 0.1800 - acc: 0.944 - ETA: 1s - loss: 0.1855 - acc: 0.942 - ETA: 1s - loss: 0.1852 - acc: 0.941 - ETA: 1s - loss: 0.1921 - acc: 0.939 - ETA: 1s - loss: 0.1886 - acc: 0.941 - ETA: 0s - loss: 0.1879 - acc: 0.942 - ETA: 0s - loss: 0.1877 - acc: 0.942 - ETA: 0s - loss: 0.1886 - acc: 0.941 - ETA: 0s - loss: 0.1906 - acc: 0.941 - ETA: 0s - loss: 0.1908 - acc: 0.941 - ETA: 0s - loss: 0.1903 - acc: 0.942 - ETA: 0s - loss: 0.1909 - acc: 0.942 - ETA: 0s - loss: 0.1918 - acc: 0.942 - ETA: 0s - loss: 0.1907 - acc: 0.943 - ETA: 0s - loss: 0.1892 - acc: 0.944 - ETA: 0s - loss: 0.1881 - acc: 0.945 - ETA: 0s - loss: 0.1872 - acc: 0.946 - ETA: 0s - loss: 0.1862 - acc: 0.946 - 3s 624us/step - loss: 0.1857 - acc: 0.9469 - val_loss: 0.2779 - val_acc: 0.9237\n"
     ]
    }
   ],
   "source": [
    "runtime_param['nb_epoch'] = 150\n",
    "runtime_best_model,result = keras_fmin_fnct(runtime_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(result.history['loss'],label='Train loss')\n",
    "plt.plot(result.history['val_loss'],label = 'Validation Loss')\n",
    "plt.xlabel('epoch no')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(result.history['loss'],label='Train loss')\n",
    "plt.plot(result.history['val_loss'],label = 'Validation Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('epoch no')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(result.history['acc'],label='Train acc')\n",
    "plt.plot(result.history['val_acc'],label = 'Validation acc')\n",
    "plt.xlabel('epoch no')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(result.history['acc'],label='Train acc')\n",
    "plt.plot(result.history['val_acc'],label = 'Validation acc')\n",
    "plt.xlabel('epoch no')\n",
    "plt.ylabel('acc')\n",
    "plt.ylim(0.90,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "around 15-20 score is giving good accuracy wit less overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 124, 16)           1552      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 124, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 41, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                42048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 44,691\n",
      "Trainable params: 44,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/59\n",
      "4067/4067 [==============================] - ETA: 57s - loss: 26.5661 - acc: 0.265 - ETA: 20s - loss: 25.5138 - acc: 0.484 - ETA: 12s - loss: 24.9786 - acc: 0.540 - ETA: 9s - loss: 24.3368 - acc: 0.582 - ETA: 7s - loss: 23.7448 - acc: 0.61 - ETA: 6s - loss: 23.1270 - acc: 0.65 - ETA: 5s - loss: 22.5990 - acc: 0.66 - ETA: 4s - loss: 22.0628 - acc: 0.68 - ETA: 4s - loss: 21.5278 - acc: 0.70 - ETA: 3s - loss: 21.0187 - acc: 0.71 - ETA: 3s - loss: 20.5310 - acc: 0.72 - ETA: 3s - loss: 20.0598 - acc: 0.73 - ETA: 2s - loss: 19.6059 - acc: 0.73 - ETA: 2s - loss: 19.1584 - acc: 0.74 - ETA: 2s - loss: 18.7239 - acc: 0.74 - ETA: 2s - loss: 18.3009 - acc: 0.75 - ETA: 1s - loss: 17.8967 - acc: 0.76 - ETA: 1s - loss: 17.5006 - acc: 0.76 - ETA: 1s - loss: 17.1220 - acc: 0.77 - ETA: 1s - loss: 16.7541 - acc: 0.77 - ETA: 1s - loss: 16.4009 - acc: 0.77 - ETA: 1s - loss: 16.0553 - acc: 0.78 - ETA: 1s - loss: 15.7180 - acc: 0.78 - ETA: 0s - loss: 15.3976 - acc: 0.79 - ETA: 0s - loss: 15.0778 - acc: 0.79 - ETA: 0s - loss: 14.7695 - acc: 0.80 - ETA: 0s - loss: 14.4704 - acc: 0.80 - ETA: 0s - loss: 14.1850 - acc: 0.80 - ETA: 0s - loss: 13.9067 - acc: 0.81 - ETA: 0s - loss: 13.6332 - acc: 0.81 - ETA: 0s - loss: 13.3717 - acc: 0.81 - ETA: 0s - loss: 13.1165 - acc: 0.82 - 4s 862us/step - loss: 13.0489 - acc: 0.8208 - val_loss: 5.1375 - val_acc: 0.8955\n",
      "Epoch 2/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 4.9802 - acc: 0.921 - ETA: 2s - loss: 4.8662 - acc: 0.947 - ETA: 2s - loss: 4.7774 - acc: 0.921 - ETA: 2s - loss: 4.6663 - acc: 0.919 - ETA: 2s - loss: 4.5902 - acc: 0.901 - ETA: 2s - loss: 4.5055 - acc: 0.899 - ETA: 2s - loss: 4.4508 - acc: 0.902 - ETA: 2s - loss: 4.4049 - acc: 0.903 - ETA: 2s - loss: 4.3069 - acc: 0.906 - ETA: 1s - loss: 4.2139 - acc: 0.905 - ETA: 1s - loss: 4.1181 - acc: 0.907 - ETA: 1s - loss: 4.0361 - acc: 0.904 - ETA: 1s - loss: 3.9573 - acc: 0.899 - ETA: 1s - loss: 3.8725 - acc: 0.900 - ETA: 1s - loss: 3.7922 - acc: 0.903 - ETA: 1s - loss: 3.7505 - acc: 0.905 - ETA: 1s - loss: 3.7127 - acc: 0.904 - ETA: 1s - loss: 3.6770 - acc: 0.904 - ETA: 1s - loss: 3.6058 - acc: 0.903 - ETA: 1s - loss: 3.5372 - acc: 0.904 - ETA: 1s - loss: 3.4708 - acc: 0.904 - ETA: 1s - loss: 3.4050 - acc: 0.906 - ETA: 0s - loss: 3.3426 - acc: 0.907 - ETA: 0s - loss: 3.2802 - acc: 0.907 - ETA: 0s - loss: 3.2208 - acc: 0.908 - ETA: 0s - loss: 3.1676 - acc: 0.905 - ETA: 0s - loss: 3.1135 - acc: 0.905 - ETA: 0s - loss: 3.0611 - acc: 0.905 - ETA: 0s - loss: 3.0087 - acc: 0.905 - ETA: 0s - loss: 2.9588 - acc: 0.906 - ETA: 0s - loss: 2.9120 - acc: 0.906 - ETA: 0s - loss: 2.8672 - acc: 0.906 - ETA: 0s - loss: 2.8238 - acc: 0.906 - ETA: 0s - loss: 2.7838 - acc: 0.906 - 3s 661us/step - loss: 2.7506 - acc: 0.9063 - val_loss: 1.5117 - val_acc: 0.8795\n",
      "Epoch 3/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 1.3581 - acc: 0.953 - ETA: 2s - loss: 1.3648 - acc: 0.911 - ETA: 2s - loss: 1.3012 - acc: 0.934 - ETA: 2s - loss: 1.3000 - acc: 0.921 - ETA: 1s - loss: 1.2743 - acc: 0.927 - ETA: 1s - loss: 1.2699 - acc: 0.916 - ETA: 1s - loss: 1.2427 - acc: 0.925 - ETA: 1s - loss: 1.2318 - acc: 0.921 - ETA: 1s - loss: 1.2076 - acc: 0.922 - ETA: 1s - loss: 1.1962 - acc: 0.920 - ETA: 1s - loss: 1.1814 - acc: 0.919 - ETA: 1s - loss: 1.1730 - acc: 0.914 - ETA: 1s - loss: 1.1562 - acc: 0.915 - ETA: 1s - loss: 1.1496 - acc: 0.914 - ETA: 1s - loss: 1.1417 - acc: 0.914 - ETA: 1s - loss: 1.1328 - acc: 0.914 - ETA: 1s - loss: 1.1289 - acc: 0.912 - ETA: 1s - loss: 1.1162 - acc: 0.913 - ETA: 1s - loss: 1.1042 - acc: 0.912 - ETA: 1s - loss: 1.0907 - acc: 0.911 - ETA: 1s - loss: 1.0811 - acc: 0.911 - ETA: 0s - loss: 1.0689 - acc: 0.911 - ETA: 0s - loss: 1.0564 - acc: 0.912 - ETA: 0s - loss: 1.0454 - acc: 0.912 - ETA: 0s - loss: 1.0325 - acc: 0.914 - ETA: 0s - loss: 1.0199 - acc: 0.914 - ETA: 0s - loss: 1.0109 - acc: 0.913 - ETA: 0s - loss: 0.9990 - acc: 0.913 - ETA: 0s - loss: 0.9889 - acc: 0.913 - ETA: 0s - loss: 0.9790 - acc: 0.912 - ETA: 0s - loss: 0.9688 - acc: 0.912 - ETA: 0s - loss: 0.9596 - acc: 0.912 - ETA: 0s - loss: 0.9511 - acc: 0.912 - ETA: 0s - loss: 0.9422 - acc: 0.912 - 3s 633us/step - loss: 0.9403 - acc: 0.9117 - val_loss: 0.7600 - val_acc: 0.8686\n",
      "Epoch 4/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.6401 - acc: 0.859 - ETA: 2s - loss: 0.6214 - acc: 0.895 - ETA: 2s - loss: 0.6097 - acc: 0.900 - ETA: 2s - loss: 0.6049 - acc: 0.899 - ETA: 2s - loss: 0.6062 - acc: 0.902 - ETA: 2s - loss: 0.5953 - acc: 0.910 - ETA: 2s - loss: 0.5889 - acc: 0.909 - ETA: 1s - loss: 0.5835 - acc: 0.909 - ETA: 1s - loss: 0.5779 - acc: 0.909 - ETA: 1s - loss: 0.5825 - acc: 0.905 - ETA: 1s - loss: 0.5753 - acc: 0.905 - ETA: 1s - loss: 0.5711 - acc: 0.907 - ETA: 1s - loss: 0.5650 - acc: 0.909 - ETA: 1s - loss: 0.5555 - acc: 0.912 - ETA: 1s - loss: 0.5507 - acc: 0.914 - ETA: 1s - loss: 0.5448 - acc: 0.914 - ETA: 1s - loss: 0.5389 - acc: 0.914 - ETA: 1s - loss: 0.5300 - acc: 0.916 - ETA: 1s - loss: 0.5280 - acc: 0.915 - ETA: 0s - loss: 0.5273 - acc: 0.913 - ETA: 0s - loss: 0.5212 - acc: 0.914 - ETA: 0s - loss: 0.5216 - acc: 0.912 - ETA: 0s - loss: 0.5195 - acc: 0.913 - ETA: 0s - loss: 0.5172 - acc: 0.913 - ETA: 0s - loss: 0.5159 - acc: 0.912 - ETA: 0s - loss: 0.5127 - acc: 0.911 - ETA: 0s - loss: 0.5123 - acc: 0.909 - ETA: 0s - loss: 0.5123 - acc: 0.908 - ETA: 0s - loss: 0.5101 - acc: 0.907 - ETA: 0s - loss: 0.5069 - acc: 0.907 - ETA: 0s - loss: 0.5032 - acc: 0.908 - ETA: 0s - loss: 0.5006 - acc: 0.908 - 3s 627us/step - loss: 0.4996 - acc: 0.9083 - val_loss: 0.5055 - val_acc: 0.8750\n",
      "Epoch 5/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3464 - acc: 0.953 - ETA: 2s - loss: 0.4146 - acc: 0.906 - ETA: 2s - loss: 0.4002 - acc: 0.918 - ETA: 2s - loss: 0.3989 - acc: 0.912 - ETA: 2s - loss: 0.3825 - acc: 0.918 - ETA: 1s - loss: 0.3764 - acc: 0.917 - ETA: 1s - loss: 0.3817 - acc: 0.914 - ETA: 1s - loss: 0.3758 - acc: 0.913 - ETA: 1s - loss: 0.3720 - acc: 0.917 - ETA: 1s - loss: 0.3790 - acc: 0.913 - ETA: 1s - loss: 0.3776 - acc: 0.912 - ETA: 1s - loss: 0.3821 - acc: 0.910 - ETA: 1s - loss: 0.3757 - acc: 0.913 - ETA: 1s - loss: 0.3785 - acc: 0.912 - ETA: 1s - loss: 0.3806 - acc: 0.910 - ETA: 1s - loss: 0.3768 - acc: 0.911 - ETA: 1s - loss: 0.3817 - acc: 0.908 - ETA: 1s - loss: 0.3803 - acc: 0.908 - ETA: 1s - loss: 0.3739 - acc: 0.910 - ETA: 0s - loss: 0.3793 - acc: 0.906 - ETA: 0s - loss: 0.3758 - acc: 0.907 - ETA: 0s - loss: 0.3744 - acc: 0.908 - ETA: 0s - loss: 0.3705 - acc: 0.910 - ETA: 0s - loss: 0.3714 - acc: 0.908 - ETA: 0s - loss: 0.3704 - acc: 0.909 - ETA: 0s - loss: 0.3692 - acc: 0.909 - ETA: 0s - loss: 0.3682 - acc: 0.908 - ETA: 0s - loss: 0.3655 - acc: 0.908 - ETA: 0s - loss: 0.3651 - acc: 0.907 - ETA: 0s - loss: 0.3649 - acc: 0.907 - ETA: 0s - loss: 0.3623 - acc: 0.908 - ETA: 0s - loss: 0.3615 - acc: 0.908 - 3s 624us/step - loss: 0.3604 - acc: 0.9090 - val_loss: 0.4308 - val_acc: 0.8686\n",
      "Epoch 6/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3686 - acc: 0.828 - ETA: 2s - loss: 0.3249 - acc: 0.890 - ETA: 2s - loss: 0.3056 - acc: 0.906 - ETA: 2s - loss: 0.2998 - acc: 0.911 - ETA: 2s - loss: 0.2964 - acc: 0.912 - ETA: 2s - loss: 0.2976 - acc: 0.913 - ETA: 2s - loss: 0.2905 - acc: 0.919 - ETA: 2s - loss: 0.2882 - acc: 0.917 - ETA: 2s - loss: 0.2857 - acc: 0.918 - ETA: 2s - loss: 0.2922 - acc: 0.913 - ETA: 1s - loss: 0.2928 - acc: 0.915 - ETA: 1s - loss: 0.2919 - acc: 0.915 - ETA: 1s - loss: 0.2936 - acc: 0.914 - ETA: 1s - loss: 0.2960 - acc: 0.912 - ETA: 1s - loss: 0.2953 - acc: 0.913 - ETA: 1s - loss: 0.2950 - acc: 0.912 - ETA: 1s - loss: 0.2906 - acc: 0.914 - ETA: 1s - loss: 0.2882 - acc: 0.915 - ETA: 1s - loss: 0.2832 - acc: 0.917 - ETA: 1s - loss: 0.2837 - acc: 0.916 - ETA: 1s - loss: 0.2861 - acc: 0.913 - ETA: 0s - loss: 0.2855 - acc: 0.914 - ETA: 0s - loss: 0.2833 - acc: 0.915 - ETA: 0s - loss: 0.2807 - acc: 0.917 - ETA: 0s - loss: 0.2806 - acc: 0.917 - ETA: 0s - loss: 0.2783 - acc: 0.919 - ETA: 0s - loss: 0.2787 - acc: 0.919 - ETA: 0s - loss: 0.2786 - acc: 0.919 - ETA: 0s - loss: 0.2780 - acc: 0.919 - ETA: 0s - loss: 0.2799 - acc: 0.918 - ETA: 0s - loss: 0.2774 - acc: 0.919 - ETA: 0s - loss: 0.2784 - acc: 0.918 - ETA: 0s - loss: 0.2775 - acc: 0.919 - 3s 662us/step - loss: 0.2767 - acc: 0.9196 - val_loss: 0.4197 - val_acc: 0.8692\n",
      "Epoch 7/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2527 - acc: 0.937 - ETA: 2s - loss: 0.2684 - acc: 0.921 - ETA: 2s - loss: 0.2857 - acc: 0.912 - ETA: 2s - loss: 0.2640 - acc: 0.924 - ETA: 2s - loss: 0.2591 - acc: 0.927 - ETA: 1s - loss: 0.2567 - acc: 0.927 - ETA: 1s - loss: 0.2587 - acc: 0.924 - ETA: 1s - loss: 0.2623 - acc: 0.922 - ETA: 1s - loss: 0.2579 - acc: 0.922 - ETA: 1s - loss: 0.2600 - acc: 0.918 - ETA: 1s - loss: 0.2607 - acc: 0.920 - ETA: 1s - loss: 0.2588 - acc: 0.922 - ETA: 1s - loss: 0.2557 - acc: 0.923 - ETA: 1s - loss: 0.2565 - acc: 0.920 - ETA: 1s - loss: 0.2574 - acc: 0.920 - ETA: 1s - loss: 0.2570 - acc: 0.919 - ETA: 1s - loss: 0.2584 - acc: 0.918 - ETA: 1s - loss: 0.2605 - acc: 0.917 - ETA: 1s - loss: 0.2649 - acc: 0.914 - ETA: 0s - loss: 0.2688 - acc: 0.913 - ETA: 0s - loss: 0.2636 - acc: 0.916 - ETA: 0s - loss: 0.2681 - acc: 0.913 - ETA: 0s - loss: 0.2695 - acc: 0.911 - ETA: 0s - loss: 0.2707 - acc: 0.911 - ETA: 0s - loss: 0.2696 - acc: 0.912 - ETA: 0s - loss: 0.2695 - acc: 0.912 - ETA: 0s - loss: 0.2701 - acc: 0.912 - ETA: 0s - loss: 0.2706 - acc: 0.912 - ETA: 0s - loss: 0.2683 - acc: 0.912 - ETA: 0s - loss: 0.2670 - acc: 0.913 - ETA: 0s - loss: 0.2670 - acc: 0.912 - ETA: 0s - loss: 0.2646 - acc: 0.914 - ETA: 0s - loss: 0.2634 - acc: 0.915 - ETA: 0s - loss: 0.2633 - acc: 0.915 - 3s 641us/step - loss: 0.2634 - acc: 0.9149 - val_loss: 0.3678 - val_acc: 0.8667\n",
      "Epoch 8/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2594 - acc: 0.921 - ETA: 1s - loss: 0.3391 - acc: 0.875 - ETA: 1s - loss: 0.2978 - acc: 0.900 - ETA: 1s - loss: 0.2946 - acc: 0.904 - ETA: 1s - loss: 0.2778 - acc: 0.908 - ETA: 1s - loss: 0.2767 - acc: 0.904 - ETA: 1s - loss: 0.2800 - acc: 0.905 - ETA: 1s - loss: 0.2701 - acc: 0.908 - ETA: 1s - loss: 0.2665 - acc: 0.909 - ETA: 1s - loss: 0.2746 - acc: 0.907 - ETA: 1s - loss: 0.2816 - acc: 0.904 - ETA: 1s - loss: 0.2816 - acc: 0.904 - ETA: 1s - loss: 0.2789 - acc: 0.905 - ETA: 1s - loss: 0.2820 - acc: 0.903 - ETA: 1s - loss: 0.2842 - acc: 0.900 - ETA: 1s - loss: 0.2853 - acc: 0.900 - ETA: 1s - loss: 0.2873 - acc: 0.901 - ETA: 0s - loss: 0.2853 - acc: 0.902 - ETA: 0s - loss: 0.2849 - acc: 0.902 - ETA: 0s - loss: 0.2855 - acc: 0.901 - ETA: 0s - loss: 0.2875 - acc: 0.899 - ETA: 0s - loss: 0.2854 - acc: 0.901 - ETA: 0s - loss: 0.2869 - acc: 0.899 - ETA: 0s - loss: 0.2883 - acc: 0.899 - ETA: 0s - loss: 0.2876 - acc: 0.900 - ETA: 0s - loss: 0.2860 - acc: 0.901 - ETA: 0s - loss: 0.2873 - acc: 0.900 - ETA: 0s - loss: 0.2871 - acc: 0.900 - ETA: 0s - loss: 0.2865 - acc: 0.901 - ETA: 0s - loss: 0.2871 - acc: 0.900 - ETA: 0s - loss: 0.2862 - acc: 0.901 - ETA: 0s - loss: 0.2858 - acc: 0.901 - 2s 548us/step - loss: 0.2863 - acc: 0.9016 - val_loss: 0.3700 - val_acc: 0.8795\n",
      "Epoch 9/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2512 - acc: 0.906 - ETA: 2s - loss: 0.2320 - acc: 0.921 - ETA: 1s - loss: 0.2751 - acc: 0.906 - ETA: 1s - loss: 0.2821 - acc: 0.899 - ETA: 1s - loss: 0.2793 - acc: 0.901 - ETA: 1s - loss: 0.2804 - acc: 0.902 - ETA: 1s - loss: 0.2742 - acc: 0.906 - ETA: 1s - loss: 0.2816 - acc: 0.904 - ETA: 1s - loss: 0.2769 - acc: 0.907 - ETA: 1s - loss: 0.2783 - acc: 0.905 - ETA: 1s - loss: 0.2730 - acc: 0.907 - ETA: 1s - loss: 0.2807 - acc: 0.905 - ETA: 1s - loss: 0.2771 - acc: 0.906 - ETA: 1s - loss: 0.2733 - acc: 0.909 - ETA: 1s - loss: 0.2716 - acc: 0.911 - ETA: 1s - loss: 0.2695 - acc: 0.913 - ETA: 1s - loss: 0.2715 - acc: 0.913 - ETA: 0s - loss: 0.2695 - acc: 0.915 - ETA: 0s - loss: 0.2670 - acc: 0.916 - ETA: 0s - loss: 0.2695 - acc: 0.915 - ETA: 0s - loss: 0.2700 - acc: 0.913 - ETA: 0s - loss: 0.2676 - acc: 0.914 - ETA: 0s - loss: 0.2708 - acc: 0.913 - ETA: 0s - loss: 0.2684 - acc: 0.914 - ETA: 0s - loss: 0.2653 - acc: 0.916 - ETA: 0s - loss: 0.2667 - acc: 0.916 - ETA: 0s - loss: 0.2664 - acc: 0.915 - ETA: 0s - loss: 0.2631 - acc: 0.917 - ETA: 0s - loss: 0.2613 - acc: 0.918 - ETA: 0s - loss: 0.2590 - acc: 0.919 - ETA: 0s - loss: 0.2585 - acc: 0.918 - ETA: 0s - loss: 0.2574 - acc: 0.919 - 2s 557us/step - loss: 0.2568 - acc: 0.9198 - val_loss: 0.3542 - val_acc: 0.8801\n",
      "Epoch 10/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2697 - acc: 0.906 - ETA: 1s - loss: 0.2099 - acc: 0.937 - ETA: 1s - loss: 0.2319 - acc: 0.928 - ETA: 1s - loss: 0.2327 - acc: 0.926 - ETA: 1s - loss: 0.2437 - acc: 0.920 - ETA: 1s - loss: 0.2444 - acc: 0.916 - ETA: 1s - loss: 0.2483 - acc: 0.912 - ETA: 1s - loss: 0.2394 - acc: 0.915 - ETA: 1s - loss: 0.2392 - acc: 0.912 - ETA: 1s - loss: 0.2341 - acc: 0.916 - ETA: 1s - loss: 0.2281 - acc: 0.919 - ETA: 1s - loss: 0.2288 - acc: 0.919 - ETA: 1s - loss: 0.2255 - acc: 0.920 - ETA: 1s - loss: 0.2197 - acc: 0.923 - ETA: 1s - loss: 0.2231 - acc: 0.921 - ETA: 1s - loss: 0.2238 - acc: 0.921 - ETA: 1s - loss: 0.2265 - acc: 0.920 - ETA: 0s - loss: 0.2267 - acc: 0.921 - ETA: 0s - loss: 0.2283 - acc: 0.921 - ETA: 0s - loss: 0.2274 - acc: 0.922 - ETA: 0s - loss: 0.2290 - acc: 0.922 - ETA: 0s - loss: 0.2323 - acc: 0.921 - ETA: 0s - loss: 0.2342 - acc: 0.920 - ETA: 0s - loss: 0.2353 - acc: 0.919 - ETA: 0s - loss: 0.2353 - acc: 0.920 - ETA: 0s - loss: 0.2377 - acc: 0.918 - ETA: 0s - loss: 0.2386 - acc: 0.917 - ETA: 0s - loss: 0.2394 - acc: 0.916 - ETA: 0s - loss: 0.2422 - acc: 0.915 - ETA: 0s - loss: 0.2433 - acc: 0.916 - ETA: 0s - loss: 0.2442 - acc: 0.915 - ETA: 0s - loss: 0.2440 - acc: 0.915 - 2s 554us/step - loss: 0.2435 - acc: 0.9162 - val_loss: 0.3395 - val_acc: 0.8712\n",
      "Epoch 11/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2161 - acc: 0.937 - ETA: 1s - loss: 0.2159 - acc: 0.947 - ETA: 1s - loss: 0.2454 - acc: 0.921 - ETA: 1s - loss: 0.2557 - acc: 0.919 - ETA: 1s - loss: 0.2543 - acc: 0.918 - ETA: 1s - loss: 0.2636 - acc: 0.911 - ETA: 1s - loss: 0.2536 - acc: 0.914 - ETA: 1s - loss: 0.2524 - acc: 0.919 - ETA: 1s - loss: 0.2522 - acc: 0.920 - ETA: 1s - loss: 0.2538 - acc: 0.916 - ETA: 1s - loss: 0.2491 - acc: 0.915 - ETA: 1s - loss: 0.2478 - acc: 0.914 - ETA: 1s - loss: 0.2492 - acc: 0.913 - ETA: 1s - loss: 0.2476 - acc: 0.913 - ETA: 1s - loss: 0.2429 - acc: 0.915 - ETA: 1s - loss: 0.2431 - acc: 0.914 - ETA: 1s - loss: 0.2443 - acc: 0.912 - ETA: 0s - loss: 0.2479 - acc: 0.909 - ETA: 0s - loss: 0.2466 - acc: 0.910 - ETA: 0s - loss: 0.2533 - acc: 0.909 - ETA: 0s - loss: 0.2579 - acc: 0.910 - ETA: 0s - loss: 0.2592 - acc: 0.911 - ETA: 0s - loss: 0.2596 - acc: 0.911 - ETA: 0s - loss: 0.2582 - acc: 0.913 - ETA: 0s - loss: 0.2587 - acc: 0.912 - ETA: 0s - loss: 0.2646 - acc: 0.910 - ETA: 0s - loss: 0.2654 - acc: 0.909 - ETA: 0s - loss: 0.2633 - acc: 0.910 - ETA: 0s - loss: 0.2638 - acc: 0.910 - ETA: 0s - loss: 0.2656 - acc: 0.910 - ETA: 0s - loss: 0.2665 - acc: 0.910 - ETA: 0s - loss: 0.2667 - acc: 0.909 - 2s 565us/step - loss: 0.2642 - acc: 0.9110 - val_loss: 0.3493 - val_acc: 0.8776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2231 - acc: 0.921 - ETA: 1s - loss: 0.2330 - acc: 0.927 - ETA: 1s - loss: 0.2323 - acc: 0.928 - ETA: 1s - loss: 0.2516 - acc: 0.915 - ETA: 1s - loss: 0.2440 - acc: 0.920 - ETA: 1s - loss: 0.2430 - acc: 0.921 - ETA: 1s - loss: 0.2392 - acc: 0.923 - ETA: 1s - loss: 0.2491 - acc: 0.917 - ETA: 1s - loss: 0.2520 - acc: 0.915 - ETA: 1s - loss: 0.2467 - acc: 0.917 - ETA: 1s - loss: 0.2447 - acc: 0.919 - ETA: 1s - loss: 0.2502 - acc: 0.917 - ETA: 1s - loss: 0.2487 - acc: 0.919 - ETA: 1s - loss: 0.2481 - acc: 0.917 - ETA: 1s - loss: 0.2494 - acc: 0.917 - ETA: 1s - loss: 0.2483 - acc: 0.918 - ETA: 0s - loss: 0.2512 - acc: 0.918 - ETA: 0s - loss: 0.2511 - acc: 0.918 - ETA: 0s - loss: 0.2547 - acc: 0.916 - ETA: 0s - loss: 0.2587 - acc: 0.914 - ETA: 0s - loss: 0.2626 - acc: 0.911 - ETA: 0s - loss: 0.2618 - acc: 0.911 - ETA: 0s - loss: 0.2595 - acc: 0.912 - ETA: 0s - loss: 0.2615 - acc: 0.911 - ETA: 0s - loss: 0.2606 - acc: 0.912 - ETA: 0s - loss: 0.2590 - acc: 0.911 - ETA: 0s - loss: 0.2579 - acc: 0.911 - ETA: 0s - loss: 0.2573 - acc: 0.911 - ETA: 0s - loss: 0.2576 - acc: 0.911 - ETA: 0s - loss: 0.2571 - acc: 0.912 - ETA: 0s - loss: 0.2563 - acc: 0.912 - ETA: 0s - loss: 0.2566 - acc: 0.912 - 2s 541us/step - loss: 0.2569 - acc: 0.9125 - val_loss: 0.3219 - val_acc: 0.8853\n",
      "Epoch 13/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1246 - acc: 1.000 - ETA: 1s - loss: 0.2198 - acc: 0.937 - ETA: 1s - loss: 0.2320 - acc: 0.928 - ETA: 1s - loss: 0.2350 - acc: 0.930 - ETA: 1s - loss: 0.2245 - acc: 0.934 - ETA: 1s - loss: 0.2282 - acc: 0.931 - ETA: 1s - loss: 0.2280 - acc: 0.930 - ETA: 1s - loss: 0.2298 - acc: 0.926 - ETA: 1s - loss: 0.2293 - acc: 0.925 - ETA: 1s - loss: 0.2342 - acc: 0.921 - ETA: 1s - loss: 0.2381 - acc: 0.919 - ETA: 1s - loss: 0.2419 - acc: 0.918 - ETA: 1s - loss: 0.2373 - acc: 0.918 - ETA: 1s - loss: 0.2361 - acc: 0.920 - ETA: 1s - loss: 0.2323 - acc: 0.921 - ETA: 1s - loss: 0.2354 - acc: 0.918 - ETA: 1s - loss: 0.2403 - acc: 0.917 - ETA: 0s - loss: 0.2395 - acc: 0.917 - ETA: 0s - loss: 0.2385 - acc: 0.916 - ETA: 0s - loss: 0.2368 - acc: 0.917 - ETA: 0s - loss: 0.2421 - acc: 0.913 - ETA: 0s - loss: 0.2412 - acc: 0.915 - ETA: 0s - loss: 0.2380 - acc: 0.916 - ETA: 0s - loss: 0.2382 - acc: 0.916 - ETA: 0s - loss: 0.2405 - acc: 0.916 - ETA: 0s - loss: 0.2388 - acc: 0.917 - ETA: 0s - loss: 0.2386 - acc: 0.917 - ETA: 0s - loss: 0.2389 - acc: 0.917 - ETA: 0s - loss: 0.2378 - acc: 0.918 - ETA: 0s - loss: 0.2346 - acc: 0.920 - ETA: 0s - loss: 0.2335 - acc: 0.921 - ETA: 0s - loss: 0.2344 - acc: 0.920 - 2s 548us/step - loss: 0.2340 - acc: 0.9208 - val_loss: 0.3321 - val_acc: 0.8821\n",
      "Epoch 14/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2356 - acc: 0.890 - ETA: 1s - loss: 0.1969 - acc: 0.927 - ETA: 1s - loss: 0.1998 - acc: 0.934 - ETA: 1s - loss: 0.2087 - acc: 0.928 - ETA: 1s - loss: 0.2176 - acc: 0.923 - ETA: 1s - loss: 0.2188 - acc: 0.921 - ETA: 1s - loss: 0.2119 - acc: 0.929 - ETA: 1s - loss: 0.2189 - acc: 0.924 - ETA: 1s - loss: 0.2280 - acc: 0.920 - ETA: 1s - loss: 0.2370 - acc: 0.916 - ETA: 1s - loss: 0.2303 - acc: 0.920 - ETA: 1s - loss: 0.2295 - acc: 0.919 - ETA: 1s - loss: 0.2283 - acc: 0.921 - ETA: 1s - loss: 0.2302 - acc: 0.921 - ETA: 1s - loss: 0.2322 - acc: 0.921 - ETA: 1s - loss: 0.2286 - acc: 0.923 - ETA: 1s - loss: 0.2291 - acc: 0.923 - ETA: 0s - loss: 0.2307 - acc: 0.924 - ETA: 0s - loss: 0.2356 - acc: 0.920 - ETA: 0s - loss: 0.2356 - acc: 0.921 - ETA: 0s - loss: 0.2350 - acc: 0.922 - ETA: 0s - loss: 0.2384 - acc: 0.921 - ETA: 0s - loss: 0.2406 - acc: 0.920 - ETA: 0s - loss: 0.2410 - acc: 0.918 - ETA: 0s - loss: 0.2409 - acc: 0.918 - ETA: 0s - loss: 0.2416 - acc: 0.918 - ETA: 0s - loss: 0.2414 - acc: 0.918 - ETA: 0s - loss: 0.2403 - acc: 0.919 - ETA: 0s - loss: 0.2442 - acc: 0.916 - ETA: 0s - loss: 0.2432 - acc: 0.917 - ETA: 0s - loss: 0.2431 - acc: 0.917 - ETA: 0s - loss: 0.2445 - acc: 0.916 - 2s 546us/step - loss: 0.2448 - acc: 0.9166 - val_loss: 0.3188 - val_acc: 0.8699\n",
      "Epoch 15/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1776 - acc: 0.937 - ETA: 1s - loss: 0.1767 - acc: 0.953 - ETA: 1s - loss: 0.1821 - acc: 0.953 - ETA: 1s - loss: 0.2194 - acc: 0.928 - ETA: 1s - loss: 0.2345 - acc: 0.921 - ETA: 1s - loss: 0.2324 - acc: 0.919 - ETA: 1s - loss: 0.2426 - acc: 0.919 - ETA: 1s - loss: 0.2446 - acc: 0.916 - ETA: 1s - loss: 0.2375 - acc: 0.921 - ETA: 1s - loss: 0.2422 - acc: 0.918 - ETA: 1s - loss: 0.2391 - acc: 0.921 - ETA: 1s - loss: 0.2402 - acc: 0.920 - ETA: 1s - loss: 0.2414 - acc: 0.922 - ETA: 1s - loss: 0.2392 - acc: 0.923 - ETA: 1s - loss: 0.2359 - acc: 0.924 - ETA: 1s - loss: 0.2367 - acc: 0.924 - ETA: 1s - loss: 0.2417 - acc: 0.922 - ETA: 0s - loss: 0.2458 - acc: 0.920 - ETA: 0s - loss: 0.2469 - acc: 0.918 - ETA: 0s - loss: 0.2531 - acc: 0.914 - ETA: 0s - loss: 0.2511 - acc: 0.916 - ETA: 0s - loss: 0.2492 - acc: 0.917 - ETA: 0s - loss: 0.2477 - acc: 0.917 - ETA: 0s - loss: 0.2485 - acc: 0.917 - ETA: 0s - loss: 0.2473 - acc: 0.918 - ETA: 0s - loss: 0.2475 - acc: 0.919 - ETA: 0s - loss: 0.2488 - acc: 0.918 - ETA: 0s - loss: 0.2461 - acc: 0.920 - ETA: 0s - loss: 0.2453 - acc: 0.920 - ETA: 0s - loss: 0.2452 - acc: 0.921 - ETA: 0s - loss: 0.2454 - acc: 0.920 - ETA: 0s - loss: 0.2459 - acc: 0.919 - 2s 547us/step - loss: 0.2450 - acc: 0.9203 - val_loss: 0.3215 - val_acc: 0.8808\n",
      "Epoch 16/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1986 - acc: 0.921 - ETA: 1s - loss: 0.2378 - acc: 0.921 - ETA: 1s - loss: 0.2205 - acc: 0.931 - ETA: 1s - loss: 0.2092 - acc: 0.939 - ETA: 1s - loss: 0.2090 - acc: 0.939 - ETA: 1s - loss: 0.2066 - acc: 0.937 - ETA: 1s - loss: 0.2049 - acc: 0.939 - ETA: 1s - loss: 0.2039 - acc: 0.939 - ETA: 1s - loss: 0.2203 - acc: 0.930 - ETA: 1s - loss: 0.2186 - acc: 0.930 - ETA: 1s - loss: 0.2206 - acc: 0.930 - ETA: 1s - loss: 0.2265 - acc: 0.926 - ETA: 1s - loss: 0.2292 - acc: 0.924 - ETA: 1s - loss: 0.2305 - acc: 0.923 - ETA: 1s - loss: 0.2326 - acc: 0.923 - ETA: 1s - loss: 0.2320 - acc: 0.924 - ETA: 0s - loss: 0.2349 - acc: 0.922 - ETA: 0s - loss: 0.2380 - acc: 0.922 - ETA: 0s - loss: 0.2381 - acc: 0.922 - ETA: 0s - loss: 0.2413 - acc: 0.922 - ETA: 0s - loss: 0.2389 - acc: 0.923 - ETA: 0s - loss: 0.2403 - acc: 0.921 - ETA: 0s - loss: 0.2382 - acc: 0.921 - ETA: 0s - loss: 0.2400 - acc: 0.920 - ETA: 0s - loss: 0.2420 - acc: 0.920 - ETA: 0s - loss: 0.2405 - acc: 0.921 - ETA: 0s - loss: 0.2417 - acc: 0.920 - ETA: 0s - loss: 0.2444 - acc: 0.919 - ETA: 0s - loss: 0.2462 - acc: 0.917 - ETA: 0s - loss: 0.2473 - acc: 0.917 - ETA: 0s - loss: 0.2515 - acc: 0.915 - ETA: 0s - loss: 0.2521 - acc: 0.915 - 2s 545us/step - loss: 0.2521 - acc: 0.9144 - val_loss: 0.3546 - val_acc: 0.8583\n",
      "Epoch 17/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.3114 - acc: 0.859 - ETA: 1s - loss: 0.3430 - acc: 0.864 - ETA: 1s - loss: 0.3145 - acc: 0.884 - ETA: 1s - loss: 0.2955 - acc: 0.895 - ETA: 1s - loss: 0.2863 - acc: 0.899 - ETA: 1s - loss: 0.2825 - acc: 0.899 - ETA: 1s - loss: 0.2793 - acc: 0.897 - ETA: 1s - loss: 0.2765 - acc: 0.896 - ETA: 1s - loss: 0.2747 - acc: 0.901 - ETA: 1s - loss: 0.2754 - acc: 0.902 - ETA: 1s - loss: 0.2715 - acc: 0.906 - ETA: 1s - loss: 0.2678 - acc: 0.907 - ETA: 1s - loss: 0.2775 - acc: 0.906 - ETA: 1s - loss: 0.2777 - acc: 0.906 - ETA: 1s - loss: 0.2781 - acc: 0.903 - ETA: 1s - loss: 0.2783 - acc: 0.903 - ETA: 1s - loss: 0.2763 - acc: 0.903 - ETA: 0s - loss: 0.2766 - acc: 0.903 - ETA: 0s - loss: 0.2770 - acc: 0.904 - ETA: 0s - loss: 0.2779 - acc: 0.904 - ETA: 0s - loss: 0.2830 - acc: 0.903 - ETA: 0s - loss: 0.2809 - acc: 0.904 - ETA: 0s - loss: 0.2864 - acc: 0.903 - ETA: 0s - loss: 0.2847 - acc: 0.905 - ETA: 0s - loss: 0.2827 - acc: 0.906 - ETA: 0s - loss: 0.2831 - acc: 0.906 - ETA: 0s - loss: 0.2817 - acc: 0.906 - ETA: 0s - loss: 0.2828 - acc: 0.905 - ETA: 0s - loss: 0.2815 - acc: 0.906 - ETA: 0s - loss: 0.2799 - acc: 0.907 - ETA: 0s - loss: 0.2791 - acc: 0.907 - ETA: 0s - loss: 0.2770 - acc: 0.908 - 2s 549us/step - loss: 0.2756 - acc: 0.9095 - val_loss: 0.3355 - val_acc: 0.8776\n",
      "Epoch 18/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1892 - acc: 0.937 - ETA: 1s - loss: 0.1973 - acc: 0.937 - ETA: 1s - loss: 0.2335 - acc: 0.915 - ETA: 1s - loss: 0.2426 - acc: 0.917 - ETA: 1s - loss: 0.2361 - acc: 0.921 - ETA: 1s - loss: 0.2443 - acc: 0.916 - ETA: 1s - loss: 0.2513 - acc: 0.908 - ETA: 1s - loss: 0.2651 - acc: 0.904 - ETA: 1s - loss: 0.2632 - acc: 0.906 - ETA: 1s - loss: 0.2587 - acc: 0.908 - ETA: 1s - loss: 0.2544 - acc: 0.911 - ETA: 1s - loss: 0.2529 - acc: 0.913 - ETA: 1s - loss: 0.2548 - acc: 0.911 - ETA: 1s - loss: 0.2585 - acc: 0.909 - ETA: 1s - loss: 0.2690 - acc: 0.904 - ETA: 1s - loss: 0.2652 - acc: 0.907 - ETA: 1s - loss: 0.2631 - acc: 0.908 - ETA: 0s - loss: 0.2646 - acc: 0.908 - ETA: 0s - loss: 0.2637 - acc: 0.908 - ETA: 0s - loss: 0.2617 - acc: 0.909 - ETA: 0s - loss: 0.2618 - acc: 0.908 - ETA: 0s - loss: 0.2626 - acc: 0.907 - ETA: 0s - loss: 0.2605 - acc: 0.908 - ETA: 0s - loss: 0.2601 - acc: 0.908 - ETA: 0s - loss: 0.2596 - acc: 0.908 - ETA: 0s - loss: 0.2590 - acc: 0.909 - ETA: 0s - loss: 0.2582 - acc: 0.911 - ETA: 0s - loss: 0.2573 - acc: 0.911 - ETA: 0s - loss: 0.2563 - acc: 0.912 - ETA: 0s - loss: 0.2549 - acc: 0.913 - ETA: 0s - loss: 0.2555 - acc: 0.913 - ETA: 0s - loss: 0.2541 - acc: 0.913 - 2s 547us/step - loss: 0.2535 - acc: 0.9142 - val_loss: 0.3182 - val_acc: 0.8840\n",
      "Epoch 19/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.3247 - acc: 0.890 - ETA: 1s - loss: 0.2430 - acc: 0.906 - ETA: 1s - loss: 0.2223 - acc: 0.931 - ETA: 1s - loss: 0.2268 - acc: 0.926 - ETA: 1s - loss: 0.2278 - acc: 0.921 - ETA: 1s - loss: 0.2265 - acc: 0.924 - ETA: 1s - loss: 0.2292 - acc: 0.925 - ETA: 1s - loss: 0.2269 - acc: 0.925 - ETA: 1s - loss: 0.2218 - acc: 0.926 - ETA: 1s - loss: 0.2173 - acc: 0.930 - ETA: 1s - loss: 0.2140 - acc: 0.930 - ETA: 1s - loss: 0.2085 - acc: 0.932 - ETA: 1s - loss: 0.2123 - acc: 0.928 - ETA: 1s - loss: 0.2189 - acc: 0.925 - ETA: 1s - loss: 0.2258 - acc: 0.921 - ETA: 1s - loss: 0.2279 - acc: 0.920 - ETA: 1s - loss: 0.2297 - acc: 0.920 - ETA: 0s - loss: 0.2323 - acc: 0.920 - ETA: 0s - loss: 0.2349 - acc: 0.919 - ETA: 0s - loss: 0.2354 - acc: 0.919 - ETA: 0s - loss: 0.2362 - acc: 0.919 - ETA: 0s - loss: 0.2346 - acc: 0.920 - ETA: 0s - loss: 0.2345 - acc: 0.921 - ETA: 0s - loss: 0.2324 - acc: 0.923 - ETA: 0s - loss: 0.2310 - acc: 0.923 - ETA: 0s - loss: 0.2290 - acc: 0.923 - ETA: 0s - loss: 0.2281 - acc: 0.924 - ETA: 0s - loss: 0.2283 - acc: 0.924 - ETA: 0s - loss: 0.2279 - acc: 0.924 - ETA: 0s - loss: 0.2280 - acc: 0.924 - ETA: 0s - loss: 0.2292 - acc: 0.923 - ETA: 0s - loss: 0.2281 - acc: 0.924 - 2s 578us/step - loss: 0.2274 - acc: 0.9243 - val_loss: 0.3222 - val_acc: 0.8801\n",
      "Epoch 20/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1876 - acc: 0.937 - ETA: 2s - loss: 0.1932 - acc: 0.947 - ETA: 2s - loss: 0.2205 - acc: 0.934 - ETA: 1s - loss: 0.2205 - acc: 0.933 - ETA: 1s - loss: 0.2185 - acc: 0.934 - ETA: 1s - loss: 0.2179 - acc: 0.933 - ETA: 1s - loss: 0.2224 - acc: 0.930 - ETA: 1s - loss: 0.2150 - acc: 0.933 - ETA: 1s - loss: 0.2155 - acc: 0.932 - ETA: 1s - loss: 0.2190 - acc: 0.933 - ETA: 1s - loss: 0.2243 - acc: 0.932 - ETA: 1s - loss: 0.2219 - acc: 0.933 - ETA: 1s - loss: 0.2216 - acc: 0.932 - ETA: 1s - loss: 0.2227 - acc: 0.929 - ETA: 1s - loss: 0.2295 - acc: 0.927 - ETA: 1s - loss: 0.2293 - acc: 0.927 - ETA: 1s - loss: 0.2238 - acc: 0.929 - ETA: 1s - loss: 0.2264 - acc: 0.928 - ETA: 1s - loss: 0.2278 - acc: 0.927 - ETA: 1s - loss: 0.2331 - acc: 0.922 - ETA: 0s - loss: 0.2316 - acc: 0.924 - ETA: 0s - loss: 0.2306 - acc: 0.925 - ETA: 0s - loss: 0.2310 - acc: 0.924 - ETA: 0s - loss: 0.2281 - acc: 0.925 - ETA: 0s - loss: 0.2277 - acc: 0.924 - ETA: 0s - loss: 0.2263 - acc: 0.924 - ETA: 0s - loss: 0.2251 - acc: 0.924 - ETA: 0s - loss: 0.2276 - acc: 0.921 - ETA: 0s - loss: 0.2254 - acc: 0.923 - ETA: 0s - loss: 0.2275 - acc: 0.922 - ETA: 0s - loss: 0.2273 - acc: 0.922 - ETA: 0s - loss: 0.2281 - acc: 0.922 - ETA: 0s - loss: 0.2285 - acc: 0.922 - ETA: 0s - loss: 0.2263 - acc: 0.923 - 2s 612us/step - loss: 0.2256 - acc: 0.9235 - val_loss: 0.3200 - val_acc: 0.8660\n",
      "Epoch 21/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2164 - acc: 0.921 - ETA: 1s - loss: 0.2231 - acc: 0.927 - ETA: 1s - loss: 0.2190 - acc: 0.934 - ETA: 1s - loss: 0.2258 - acc: 0.937 - ETA: 2s - loss: 0.2144 - acc: 0.943 - ETA: 2s - loss: 0.2101 - acc: 0.941 - ETA: 2s - loss: 0.2191 - acc: 0.934 - ETA: 2s - loss: 0.2144 - acc: 0.936 - ETA: 2s - loss: 0.2073 - acc: 0.938 - ETA: 2s - loss: 0.2040 - acc: 0.937 - ETA: 1s - loss: 0.1990 - acc: 0.938 - ETA: 1s - loss: 0.1984 - acc: 0.936 - ETA: 1s - loss: 0.2026 - acc: 0.935 - ETA: 1s - loss: 0.2068 - acc: 0.933 - ETA: 1s - loss: 0.2060 - acc: 0.934 - ETA: 1s - loss: 0.2153 - acc: 0.929 - ETA: 1s - loss: 0.2210 - acc: 0.925 - ETA: 1s - loss: 0.2242 - acc: 0.924 - ETA: 1s - loss: 0.2252 - acc: 0.923 - ETA: 1s - loss: 0.2243 - acc: 0.924 - ETA: 1s - loss: 0.2247 - acc: 0.924 - ETA: 0s - loss: 0.2286 - acc: 0.923 - ETA: 0s - loss: 0.2272 - acc: 0.923 - ETA: 0s - loss: 0.2250 - acc: 0.924 - ETA: 0s - loss: 0.2251 - acc: 0.924 - ETA: 0s - loss: 0.2249 - acc: 0.923 - ETA: 0s - loss: 0.2224 - acc: 0.924 - ETA: 0s - loss: 0.2225 - acc: 0.924 - ETA: 0s - loss: 0.2252 - acc: 0.922 - ETA: 0s - loss: 0.2263 - acc: 0.922 - ETA: 0s - loss: 0.2280 - acc: 0.921 - ETA: 0s - loss: 0.2258 - acc: 0.922 - ETA: 0s - loss: 0.2237 - acc: 0.923 - ETA: 0s - loss: 0.2239 - acc: 0.923 - 2s 602us/step - loss: 0.2231 - acc: 0.9238 - val_loss: 0.3454 - val_acc: 0.8519\n",
      "Epoch 22/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.3133 - acc: 0.875 - ETA: 1s - loss: 0.2268 - acc: 0.927 - ETA: 1s - loss: 0.2535 - acc: 0.921 - ETA: 1s - loss: 0.2459 - acc: 0.924 - ETA: 1s - loss: 0.2385 - acc: 0.927 - ETA: 1s - loss: 0.2387 - acc: 0.926 - ETA: 1s - loss: 0.2349 - acc: 0.923 - ETA: 1s - loss: 0.2397 - acc: 0.917 - ETA: 1s - loss: 0.2345 - acc: 0.920 - ETA: 1s - loss: 0.2321 - acc: 0.921 - ETA: 1s - loss: 0.2270 - acc: 0.923 - ETA: 1s - loss: 0.2224 - acc: 0.925 - ETA: 1s - loss: 0.2254 - acc: 0.923 - ETA: 1s - loss: 0.2300 - acc: 0.922 - ETA: 1s - loss: 0.2298 - acc: 0.923 - ETA: 1s - loss: 0.2365 - acc: 0.923 - ETA: 1s - loss: 0.2363 - acc: 0.922 - ETA: 0s - loss: 0.2407 - acc: 0.921 - ETA: 0s - loss: 0.2454 - acc: 0.919 - ETA: 0s - loss: 0.2459 - acc: 0.919 - ETA: 0s - loss: 0.2492 - acc: 0.920 - ETA: 0s - loss: 0.2492 - acc: 0.920 - ETA: 0s - loss: 0.2499 - acc: 0.919 - ETA: 0s - loss: 0.2518 - acc: 0.919 - ETA: 0s - loss: 0.2496 - acc: 0.920 - ETA: 0s - loss: 0.2495 - acc: 0.919 - ETA: 0s - loss: 0.2490 - acc: 0.918 - ETA: 0s - loss: 0.2495 - acc: 0.918 - ETA: 0s - loss: 0.2502 - acc: 0.918 - ETA: 0s - loss: 0.2505 - acc: 0.918 - ETA: 0s - loss: 0.2482 - acc: 0.919 - ETA: 0s - loss: 0.2483 - acc: 0.918 - 2s 546us/step - loss: 0.2493 - acc: 0.9181 - val_loss: 0.3117 - val_acc: 0.8859\n",
      "Epoch 23/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2285 - acc: 0.937 - ETA: 2s - loss: 0.2660 - acc: 0.921 - ETA: 1s - loss: 0.2835 - acc: 0.909 - ETA: 1s - loss: 0.2804 - acc: 0.906 - ETA: 1s - loss: 0.2625 - acc: 0.916 - ETA: 1s - loss: 0.2787 - acc: 0.909 - ETA: 1s - loss: 0.2712 - acc: 0.908 - ETA: 1s - loss: 0.2700 - acc: 0.910 - ETA: 1s - loss: 0.2839 - acc: 0.903 - ETA: 1s - loss: 0.2834 - acc: 0.904 - ETA: 1s - loss: 0.2797 - acc: 0.904 - ETA: 1s - loss: 0.2680 - acc: 0.909 - ETA: 1s - loss: 0.2646 - acc: 0.910 - ETA: 1s - loss: 0.2588 - acc: 0.913 - ETA: 1s - loss: 0.2597 - acc: 0.912 - ETA: 1s - loss: 0.2601 - acc: 0.911 - ETA: 1s - loss: 0.2577 - acc: 0.911 - ETA: 0s - loss: 0.2575 - acc: 0.911 - ETA: 0s - loss: 0.2543 - acc: 0.912 - ETA: 0s - loss: 0.2513 - acc: 0.913 - ETA: 0s - loss: 0.2497 - acc: 0.913 - ETA: 0s - loss: 0.2480 - acc: 0.915 - ETA: 0s - loss: 0.2470 - acc: 0.914 - ETA: 0s - loss: 0.2437 - acc: 0.916 - ETA: 0s - loss: 0.2445 - acc: 0.917 - ETA: 0s - loss: 0.2432 - acc: 0.917 - ETA: 0s - loss: 0.2387 - acc: 0.920 - ETA: 0s - loss: 0.2379 - acc: 0.920 - ETA: 0s - loss: 0.2380 - acc: 0.920 - ETA: 0s - loss: 0.2376 - acc: 0.919 - ETA: 0s - loss: 0.2394 - acc: 0.919 - ETA: 0s - loss: 0.2385 - acc: 0.920 - 2s 551us/step - loss: 0.2384 - acc: 0.9206 - val_loss: 0.3037 - val_acc: 0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2287 - acc: 0.906 - ETA: 1s - loss: 0.2346 - acc: 0.921 - ETA: 1s - loss: 0.2302 - acc: 0.921 - ETA: 1s - loss: 0.2409 - acc: 0.912 - ETA: 1s - loss: 0.2403 - acc: 0.916 - ETA: 1s - loss: 0.2269 - acc: 0.924 - ETA: 1s - loss: 0.2207 - acc: 0.925 - ETA: 1s - loss: 0.2256 - acc: 0.927 - ETA: 1s - loss: 0.2179 - acc: 0.929 - ETA: 1s - loss: 0.2213 - acc: 0.927 - ETA: 1s - loss: 0.2223 - acc: 0.927 - ETA: 1s - loss: 0.2201 - acc: 0.929 - ETA: 1s - loss: 0.2163 - acc: 0.930 - ETA: 1s - loss: 0.2187 - acc: 0.929 - ETA: 1s - loss: 0.2199 - acc: 0.928 - ETA: 1s - loss: 0.2220 - acc: 0.926 - ETA: 1s - loss: 0.2273 - acc: 0.924 - ETA: 0s - loss: 0.2291 - acc: 0.924 - ETA: 0s - loss: 0.2274 - acc: 0.924 - ETA: 0s - loss: 0.2245 - acc: 0.925 - ETA: 0s - loss: 0.2239 - acc: 0.926 - ETA: 0s - loss: 0.2235 - acc: 0.926 - ETA: 0s - loss: 0.2264 - acc: 0.925 - ETA: 0s - loss: 0.2261 - acc: 0.925 - ETA: 0s - loss: 0.2248 - acc: 0.927 - ETA: 0s - loss: 0.2241 - acc: 0.926 - ETA: 0s - loss: 0.2255 - acc: 0.925 - ETA: 0s - loss: 0.2261 - acc: 0.925 - ETA: 0s - loss: 0.2258 - acc: 0.925 - ETA: 0s - loss: 0.2273 - acc: 0.924 - ETA: 0s - loss: 0.2277 - acc: 0.924 - ETA: 0s - loss: 0.2291 - acc: 0.924 - 2s 544us/step - loss: 0.2295 - acc: 0.9248 - val_loss: 0.3061 - val_acc: 0.8936\n",
      "Epoch 25/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1517 - acc: 0.953 - ETA: 1s - loss: 0.1859 - acc: 0.942 - ETA: 1s - loss: 0.1991 - acc: 0.946 - ETA: 1s - loss: 0.1963 - acc: 0.948 - ETA: 1s - loss: 0.1898 - acc: 0.947 - ETA: 1s - loss: 0.2054 - acc: 0.937 - ETA: 1s - loss: 0.2080 - acc: 0.936 - ETA: 1s - loss: 0.2120 - acc: 0.934 - ETA: 1s - loss: 0.2101 - acc: 0.933 - ETA: 1s - loss: 0.2148 - acc: 0.930 - ETA: 1s - loss: 0.2214 - acc: 0.927 - ETA: 1s - loss: 0.2244 - acc: 0.925 - ETA: 1s - loss: 0.2231 - acc: 0.926 - ETA: 1s - loss: 0.2265 - acc: 0.924 - ETA: 1s - loss: 0.2223 - acc: 0.926 - ETA: 1s - loss: 0.2239 - acc: 0.922 - ETA: 1s - loss: 0.2266 - acc: 0.921 - ETA: 0s - loss: 0.2260 - acc: 0.921 - ETA: 0s - loss: 0.2329 - acc: 0.918 - ETA: 0s - loss: 0.2319 - acc: 0.919 - ETA: 0s - loss: 0.2315 - acc: 0.918 - ETA: 0s - loss: 0.2335 - acc: 0.916 - ETA: 0s - loss: 0.2318 - acc: 0.916 - ETA: 0s - loss: 0.2323 - acc: 0.916 - ETA: 0s - loss: 0.2337 - acc: 0.916 - ETA: 0s - loss: 0.2314 - acc: 0.917 - ETA: 0s - loss: 0.2294 - acc: 0.919 - ETA: 0s - loss: 0.2297 - acc: 0.918 - ETA: 0s - loss: 0.2270 - acc: 0.919 - ETA: 0s - loss: 0.2262 - acc: 0.919 - ETA: 0s - loss: 0.2274 - acc: 0.918 - ETA: 0s - loss: 0.2263 - acc: 0.919 - 2s 572us/step - loss: 0.2273 - acc: 0.9189 - val_loss: 0.2969 - val_acc: 0.8942\n",
      "Epoch 26/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2520 - acc: 0.890 - ETA: 2s - loss: 0.2027 - acc: 0.921 - ETA: 2s - loss: 0.2157 - acc: 0.921 - ETA: 2s - loss: 0.2029 - acc: 0.930 - ETA: 2s - loss: 0.2051 - acc: 0.932 - ETA: 1s - loss: 0.2227 - acc: 0.921 - ETA: 1s - loss: 0.2289 - acc: 0.918 - ETA: 1s - loss: 0.2291 - acc: 0.916 - ETA: 1s - loss: 0.2249 - acc: 0.919 - ETA: 1s - loss: 0.2259 - acc: 0.916 - ETA: 1s - loss: 0.2282 - acc: 0.917 - ETA: 1s - loss: 0.2330 - acc: 0.917 - ETA: 1s - loss: 0.2251 - acc: 0.922 - ETA: 1s - loss: 0.2211 - acc: 0.925 - ETA: 1s - loss: 0.2255 - acc: 0.922 - ETA: 1s - loss: 0.2232 - acc: 0.923 - ETA: 1s - loss: 0.2237 - acc: 0.923 - ETA: 0s - loss: 0.2273 - acc: 0.923 - ETA: 0s - loss: 0.2242 - acc: 0.924 - ETA: 0s - loss: 0.2290 - acc: 0.923 - ETA: 0s - loss: 0.2310 - acc: 0.921 - ETA: 0s - loss: 0.2292 - acc: 0.922 - ETA: 0s - loss: 0.2340 - acc: 0.920 - ETA: 0s - loss: 0.2369 - acc: 0.919 - ETA: 0s - loss: 0.2353 - acc: 0.920 - ETA: 0s - loss: 0.2385 - acc: 0.919 - ETA: 0s - loss: 0.2381 - acc: 0.918 - ETA: 0s - loss: 0.2388 - acc: 0.918 - ETA: 0s - loss: 0.2380 - acc: 0.918 - ETA: 0s - loss: 0.2371 - acc: 0.918 - ETA: 0s - loss: 0.2375 - acc: 0.919 - ETA: 0s - loss: 0.2376 - acc: 0.920 - 2s 557us/step - loss: 0.2374 - acc: 0.9206 - val_loss: 0.3015 - val_acc: 0.8974\n",
      "Epoch 27/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2445 - acc: 0.890 - ETA: 1s - loss: 0.2797 - acc: 0.885 - ETA: 1s - loss: 0.2442 - acc: 0.909 - ETA: 1s - loss: 0.2362 - acc: 0.915 - ETA: 1s - loss: 0.2294 - acc: 0.920 - ETA: 1s - loss: 0.2224 - acc: 0.927 - ETA: 1s - loss: 0.2211 - acc: 0.927 - ETA: 1s - loss: 0.2143 - acc: 0.932 - ETA: 1s - loss: 0.2188 - acc: 0.928 - ETA: 1s - loss: 0.2160 - acc: 0.928 - ETA: 1s - loss: 0.2130 - acc: 0.929 - ETA: 1s - loss: 0.2139 - acc: 0.928 - ETA: 1s - loss: 0.2140 - acc: 0.928 - ETA: 1s - loss: 0.2122 - acc: 0.929 - ETA: 1s - loss: 0.2118 - acc: 0.928 - ETA: 1s - loss: 0.2090 - acc: 0.930 - ETA: 1s - loss: 0.2091 - acc: 0.931 - ETA: 0s - loss: 0.2118 - acc: 0.929 - ETA: 0s - loss: 0.2136 - acc: 0.928 - ETA: 0s - loss: 0.2126 - acc: 0.930 - ETA: 0s - loss: 0.2146 - acc: 0.929 - ETA: 0s - loss: 0.2156 - acc: 0.930 - ETA: 0s - loss: 0.2164 - acc: 0.928 - ETA: 0s - loss: 0.2184 - acc: 0.927 - ETA: 0s - loss: 0.2178 - acc: 0.928 - ETA: 0s - loss: 0.2174 - acc: 0.928 - ETA: 0s - loss: 0.2164 - acc: 0.929 - ETA: 0s - loss: 0.2172 - acc: 0.928 - ETA: 0s - loss: 0.2198 - acc: 0.926 - ETA: 0s - loss: 0.2221 - acc: 0.925 - ETA: 0s - loss: 0.2226 - acc: 0.925 - ETA: 0s - loss: 0.2224 - acc: 0.925 - 2s 550us/step - loss: 0.2220 - acc: 0.9250 - val_loss: 0.3004 - val_acc: 0.8891\n",
      "Epoch 28/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1969 - acc: 0.937 - ETA: 1s - loss: 0.2455 - acc: 0.916 - ETA: 1s - loss: 0.2390 - acc: 0.918 - ETA: 1s - loss: 0.2205 - acc: 0.926 - ETA: 1s - loss: 0.2320 - acc: 0.925 - ETA: 1s - loss: 0.2254 - acc: 0.927 - ETA: 1s - loss: 0.2140 - acc: 0.935 - ETA: 1s - loss: 0.2136 - acc: 0.932 - ETA: 1s - loss: 0.2074 - acc: 0.934 - ETA: 1s - loss: 0.2041 - acc: 0.937 - ETA: 1s - loss: 0.1990 - acc: 0.940 - ETA: 1s - loss: 0.2001 - acc: 0.940 - ETA: 1s - loss: 0.2011 - acc: 0.939 - ETA: 1s - loss: 0.2006 - acc: 0.938 - ETA: 1s - loss: 0.2004 - acc: 0.939 - ETA: 1s - loss: 0.2009 - acc: 0.939 - ETA: 1s - loss: 0.2092 - acc: 0.935 - ETA: 0s - loss: 0.2110 - acc: 0.932 - ETA: 0s - loss: 0.2135 - acc: 0.930 - ETA: 0s - loss: 0.2118 - acc: 0.929 - ETA: 0s - loss: 0.2136 - acc: 0.929 - ETA: 0s - loss: 0.2137 - acc: 0.928 - ETA: 0s - loss: 0.2144 - acc: 0.928 - ETA: 0s - loss: 0.2147 - acc: 0.928 - ETA: 0s - loss: 0.2143 - acc: 0.928 - ETA: 0s - loss: 0.2191 - acc: 0.926 - ETA: 0s - loss: 0.2173 - acc: 0.927 - ETA: 0s - loss: 0.2183 - acc: 0.927 - ETA: 0s - loss: 0.2186 - acc: 0.927 - ETA: 0s - loss: 0.2202 - acc: 0.926 - ETA: 0s - loss: 0.2189 - acc: 0.926 - ETA: 0s - loss: 0.2190 - acc: 0.926 - 2s 562us/step - loss: 0.2184 - acc: 0.9267 - val_loss: 0.2916 - val_acc: 0.9109\n",
      "Epoch 29/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2330 - acc: 0.937 - ETA: 2s - loss: 0.2545 - acc: 0.921 - ETA: 2s - loss: 0.2340 - acc: 0.931 - ETA: 1s - loss: 0.2164 - acc: 0.937 - ETA: 1s - loss: 0.2315 - acc: 0.925 - ETA: 1s - loss: 0.2229 - acc: 0.930 - ETA: 1s - loss: 0.2182 - acc: 0.932 - ETA: 1s - loss: 0.2159 - acc: 0.932 - ETA: 1s - loss: 0.2163 - acc: 0.930 - ETA: 1s - loss: 0.2167 - acc: 0.931 - ETA: 1s - loss: 0.2173 - acc: 0.930 - ETA: 1s - loss: 0.2168 - acc: 0.930 - ETA: 1s - loss: 0.2147 - acc: 0.932 - ETA: 1s - loss: 0.2126 - acc: 0.934 - ETA: 1s - loss: 0.2105 - acc: 0.935 - ETA: 1s - loss: 0.2150 - acc: 0.934 - ETA: 1s - loss: 0.2184 - acc: 0.931 - ETA: 0s - loss: 0.2225 - acc: 0.929 - ETA: 0s - loss: 0.2229 - acc: 0.929 - ETA: 0s - loss: 0.2301 - acc: 0.926 - ETA: 0s - loss: 0.2294 - acc: 0.927 - ETA: 0s - loss: 0.2308 - acc: 0.926 - ETA: 0s - loss: 0.2336 - acc: 0.924 - ETA: 0s - loss: 0.2346 - acc: 0.923 - ETA: 0s - loss: 0.2348 - acc: 0.923 - ETA: 0s - loss: 0.2358 - acc: 0.922 - ETA: 0s - loss: 0.2328 - acc: 0.924 - ETA: 0s - loss: 0.2334 - acc: 0.924 - ETA: 0s - loss: 0.2306 - acc: 0.925 - ETA: 0s - loss: 0.2300 - acc: 0.925 - ETA: 0s - loss: 0.2310 - acc: 0.925 - ETA: 0s - loss: 0.2324 - acc: 0.924 - 2s 566us/step - loss: 0.2327 - acc: 0.9240 - val_loss: 0.3035 - val_acc: 0.9122\n",
      "Epoch 30/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.4300 - acc: 0.875 - ETA: 1s - loss: 0.2753 - acc: 0.921 - ETA: 1s - loss: 0.2478 - acc: 0.915 - ETA: 1s - loss: 0.2435 - acc: 0.917 - ETA: 1s - loss: 0.2296 - acc: 0.925 - ETA: 1s - loss: 0.2295 - acc: 0.926 - ETA: 1s - loss: 0.2373 - acc: 0.923 - ETA: 1s - loss: 0.2294 - acc: 0.927 - ETA: 1s - loss: 0.2326 - acc: 0.924 - ETA: 1s - loss: 0.2336 - acc: 0.923 - ETA: 1s - loss: 0.2333 - acc: 0.922 - ETA: 1s - loss: 0.2303 - acc: 0.923 - ETA: 1s - loss: 0.2275 - acc: 0.926 - ETA: 1s - loss: 0.2243 - acc: 0.928 - ETA: 1s - loss: 0.2289 - acc: 0.927 - ETA: 1s - loss: 0.2332 - acc: 0.926 - ETA: 1s - loss: 0.2359 - acc: 0.926 - ETA: 0s - loss: 0.2341 - acc: 0.926 - ETA: 0s - loss: 0.2319 - acc: 0.927 - ETA: 0s - loss: 0.2316 - acc: 0.928 - ETA: 0s - loss: 0.2332 - acc: 0.926 - ETA: 0s - loss: 0.2301 - acc: 0.927 - ETA: 0s - loss: 0.2280 - acc: 0.928 - ETA: 0s - loss: 0.2272 - acc: 0.928 - ETA: 0s - loss: 0.2257 - acc: 0.929 - ETA: 0s - loss: 0.2238 - acc: 0.930 - ETA: 0s - loss: 0.2234 - acc: 0.930 - ETA: 0s - loss: 0.2240 - acc: 0.929 - ETA: 0s - loss: 0.2233 - acc: 0.929 - ETA: 0s - loss: 0.2210 - acc: 0.930 - ETA: 0s - loss: 0.2192 - acc: 0.931 - ETA: 0s - loss: 0.2202 - acc: 0.930 - 2s 547us/step - loss: 0.2203 - acc: 0.9299 - val_loss: 0.2773 - val_acc: 0.9045\n",
      "Epoch 31/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1250 - acc: 0.984 - ETA: 2s - loss: 0.2079 - acc: 0.937 - ETA: 1s - loss: 0.2014 - acc: 0.946 - ETA: 1s - loss: 0.2026 - acc: 0.942 - ETA: 1s - loss: 0.2109 - acc: 0.935 - ETA: 1s - loss: 0.2035 - acc: 0.940 - ETA: 1s - loss: 0.2020 - acc: 0.942 - ETA: 1s - loss: 0.2011 - acc: 0.942 - ETA: 1s - loss: 0.2046 - acc: 0.936 - ETA: 1s - loss: 0.2028 - acc: 0.936 - ETA: 1s - loss: 0.2013 - acc: 0.936 - ETA: 1s - loss: 0.2088 - acc: 0.933 - ETA: 1s - loss: 0.2079 - acc: 0.933 - ETA: 1s - loss: 0.2091 - acc: 0.931 - ETA: 1s - loss: 0.2086 - acc: 0.932 - ETA: 1s - loss: 0.2112 - acc: 0.932 - ETA: 0s - loss: 0.2091 - acc: 0.933 - ETA: 0s - loss: 0.2105 - acc: 0.933 - ETA: 0s - loss: 0.2127 - acc: 0.932 - ETA: 0s - loss: 0.2125 - acc: 0.933 - ETA: 0s - loss: 0.2125 - acc: 0.934 - ETA: 0s - loss: 0.2119 - acc: 0.935 - ETA: 0s - loss: 0.2096 - acc: 0.935 - ETA: 0s - loss: 0.2087 - acc: 0.935 - ETA: 0s - loss: 0.2059 - acc: 0.937 - ETA: 0s - loss: 0.2061 - acc: 0.937 - ETA: 0s - loss: 0.2069 - acc: 0.936 - ETA: 0s - loss: 0.2084 - acc: 0.935 - ETA: 0s - loss: 0.2094 - acc: 0.934 - ETA: 0s - loss: 0.2083 - acc: 0.934 - ETA: 0s - loss: 0.2111 - acc: 0.933 - ETA: 0s - loss: 0.2102 - acc: 0.933 - 2s 544us/step - loss: 0.2110 - acc: 0.9336 - val_loss: 0.2738 - val_acc: 0.9083\n",
      "Epoch 32/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2153 - acc: 0.921 - ETA: 1s - loss: 0.1840 - acc: 0.937 - ETA: 1s - loss: 0.1804 - acc: 0.946 - ETA: 1s - loss: 0.2203 - acc: 0.930 - ETA: 1s - loss: 0.2267 - acc: 0.927 - ETA: 1s - loss: 0.2209 - acc: 0.926 - ETA: 1s - loss: 0.2155 - acc: 0.931 - ETA: 1s - loss: 0.2176 - acc: 0.929 - ETA: 1s - loss: 0.2165 - acc: 0.930 - ETA: 1s - loss: 0.2121 - acc: 0.931 - ETA: 1s - loss: 0.2081 - acc: 0.933 - ETA: 1s - loss: 0.2056 - acc: 0.935 - ETA: 1s - loss: 0.2051 - acc: 0.933 - ETA: 1s - loss: 0.2029 - acc: 0.934 - ETA: 1s - loss: 0.2010 - acc: 0.935 - ETA: 1s - loss: 0.2007 - acc: 0.937 - ETA: 1s - loss: 0.1992 - acc: 0.938 - ETA: 0s - loss: 0.1982 - acc: 0.937 - ETA: 0s - loss: 0.1978 - acc: 0.938 - ETA: 0s - loss: 0.2012 - acc: 0.937 - ETA: 0s - loss: 0.2026 - acc: 0.937 - ETA: 0s - loss: 0.2064 - acc: 0.935 - ETA: 0s - loss: 0.2083 - acc: 0.935 - ETA: 0s - loss: 0.2084 - acc: 0.934 - ETA: 0s - loss: 0.2084 - acc: 0.934 - ETA: 0s - loss: 0.2080 - acc: 0.934 - ETA: 0s - loss: 0.2081 - acc: 0.933 - ETA: 0s - loss: 0.2087 - acc: 0.933 - ETA: 0s - loss: 0.2103 - acc: 0.932 - ETA: 0s - loss: 0.2117 - acc: 0.932 - ETA: 0s - loss: 0.2123 - acc: 0.932 - ETA: 0s - loss: 0.2154 - acc: 0.930 - 2s 547us/step - loss: 0.2144 - acc: 0.9312 - val_loss: 0.2920 - val_acc: 0.8833\n",
      "Epoch 33/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1374 - acc: 0.968 - ETA: 1s - loss: 0.1709 - acc: 0.958 - ETA: 1s - loss: 0.1775 - acc: 0.950 - ETA: 1s - loss: 0.1762 - acc: 0.953 - ETA: 1s - loss: 0.1804 - acc: 0.946 - ETA: 1s - loss: 0.1855 - acc: 0.941 - ETA: 1s - loss: 0.1884 - acc: 0.938 - ETA: 1s - loss: 0.1953 - acc: 0.938 - ETA: 1s - loss: 0.2008 - acc: 0.937 - ETA: 1s - loss: 0.2013 - acc: 0.936 - ETA: 1s - loss: 0.2000 - acc: 0.939 - ETA: 1s - loss: 0.1969 - acc: 0.940 - ETA: 1s - loss: 0.1951 - acc: 0.942 - ETA: 1s - loss: 0.1928 - acc: 0.943 - ETA: 1s - loss: 0.1923 - acc: 0.942 - ETA: 1s - loss: 0.1947 - acc: 0.941 - ETA: 0s - loss: 0.2018 - acc: 0.937 - ETA: 0s - loss: 0.1991 - acc: 0.938 - ETA: 0s - loss: 0.1993 - acc: 0.939 - ETA: 0s - loss: 0.1988 - acc: 0.939 - ETA: 0s - loss: 0.1996 - acc: 0.938 - ETA: 0s - loss: 0.2011 - acc: 0.939 - ETA: 0s - loss: 0.2017 - acc: 0.938 - ETA: 0s - loss: 0.2016 - acc: 0.939 - ETA: 0s - loss: 0.2025 - acc: 0.938 - ETA: 0s - loss: 0.2020 - acc: 0.939 - ETA: 0s - loss: 0.2006 - acc: 0.940 - ETA: 0s - loss: 0.2013 - acc: 0.938 - ETA: 0s - loss: 0.2005 - acc: 0.938 - ETA: 0s - loss: 0.2000 - acc: 0.938 - ETA: 0s - loss: 0.2022 - acc: 0.938 - ETA: 0s - loss: 0.2033 - acc: 0.937 - 2s 546us/step - loss: 0.2029 - acc: 0.9380 - val_loss: 0.2979 - val_acc: 0.8910\n",
      "Epoch 34/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2954 - acc: 0.875 - ETA: 1s - loss: 0.2450 - acc: 0.906 - ETA: 1s - loss: 0.2285 - acc: 0.909 - ETA: 1s - loss: 0.2284 - acc: 0.910 - ETA: 1s - loss: 0.2284 - acc: 0.911 - ETA: 1s - loss: 0.2269 - acc: 0.916 - ETA: 1s - loss: 0.2336 - acc: 0.909 - ETA: 1s - loss: 0.2245 - acc: 0.916 - ETA: 1s - loss: 0.2446 - acc: 0.913 - ETA: 1s - loss: 0.2423 - acc: 0.916 - ETA: 1s - loss: 0.2402 - acc: 0.916 - ETA: 1s - loss: 0.2388 - acc: 0.919 - ETA: 1s - loss: 0.2472 - acc: 0.917 - ETA: 1s - loss: 0.2502 - acc: 0.915 - ETA: 1s - loss: 0.2501 - acc: 0.916 - ETA: 1s - loss: 0.2463 - acc: 0.918 - ETA: 1s - loss: 0.2418 - acc: 0.920 - ETA: 0s - loss: 0.2421 - acc: 0.920 - ETA: 0s - loss: 0.2400 - acc: 0.921 - ETA: 0s - loss: 0.2358 - acc: 0.924 - ETA: 0s - loss: 0.2368 - acc: 0.924 - ETA: 0s - loss: 0.2372 - acc: 0.923 - ETA: 0s - loss: 0.2380 - acc: 0.924 - ETA: 0s - loss: 0.2356 - acc: 0.924 - ETA: 0s - loss: 0.2341 - acc: 0.925 - ETA: 0s - loss: 0.2331 - acc: 0.925 - ETA: 0s - loss: 0.2319 - acc: 0.925 - ETA: 0s - loss: 0.2294 - acc: 0.927 - ETA: 0s - loss: 0.2314 - acc: 0.926 - ETA: 0s - loss: 0.2313 - acc: 0.925 - ETA: 0s - loss: 0.2332 - acc: 0.924 - ETA: 0s - loss: 0.2336 - acc: 0.924 - 2s 551us/step - loss: 0.2326 - acc: 0.9253 - val_loss: 0.2856 - val_acc: 0.9083\n",
      "Epoch 35/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.3646 - acc: 0.843 - ETA: 1s - loss: 0.2749 - acc: 0.901 - ETA: 1s - loss: 0.2770 - acc: 0.890 - ETA: 1s - loss: 0.2759 - acc: 0.910 - ETA: 1s - loss: 0.2806 - acc: 0.908 - ETA: 1s - loss: 0.2655 - acc: 0.911 - ETA: 1s - loss: 0.2673 - acc: 0.914 - ETA: 1s - loss: 0.2646 - acc: 0.916 - ETA: 1s - loss: 0.2650 - acc: 0.919 - ETA: 1s - loss: 0.2918 - acc: 0.916 - ETA: 1s - loss: 0.2828 - acc: 0.918 - ETA: 1s - loss: 0.2844 - acc: 0.917 - ETA: 1s - loss: 0.2860 - acc: 0.920 - ETA: 1s - loss: 0.2779 - acc: 0.924 - ETA: 1s - loss: 0.2763 - acc: 0.924 - ETA: 1s - loss: 0.2777 - acc: 0.923 - ETA: 1s - loss: 0.2744 - acc: 0.923 - ETA: 0s - loss: 0.2706 - acc: 0.925 - ETA: 0s - loss: 0.2690 - acc: 0.924 - ETA: 0s - loss: 0.2674 - acc: 0.925 - ETA: 0s - loss: 0.2729 - acc: 0.923 - ETA: 0s - loss: 0.2687 - acc: 0.925 - ETA: 0s - loss: 0.2673 - acc: 0.926 - ETA: 0s - loss: 0.2678 - acc: 0.926 - ETA: 0s - loss: 0.2673 - acc: 0.925 - ETA: 0s - loss: 0.2643 - acc: 0.926 - ETA: 0s - loss: 0.2605 - acc: 0.927 - ETA: 0s - loss: 0.2579 - acc: 0.927 - ETA: 0s - loss: 0.2587 - acc: 0.927 - ETA: 0s - loss: 0.2577 - acc: 0.927 - ETA: 0s - loss: 0.2574 - acc: 0.926 - ETA: 0s - loss: 0.2545 - acc: 0.927 - 2s 546us/step - loss: 0.2536 - acc: 0.9277 - val_loss: 0.2874 - val_acc: 0.9019\n",
      "Epoch 36/59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2837 - acc: 0.937 - ETA: 1s - loss: 0.2547 - acc: 0.916 - ETA: 1s - loss: 0.2350 - acc: 0.918 - ETA: 1s - loss: 0.2170 - acc: 0.924 - ETA: 1s - loss: 0.2063 - acc: 0.930 - ETA: 1s - loss: 0.2040 - acc: 0.931 - ETA: 1s - loss: 0.2116 - acc: 0.929 - ETA: 1s - loss: 0.2074 - acc: 0.930 - ETA: 1s - loss: 0.2125 - acc: 0.929 - ETA: 1s - loss: 0.2150 - acc: 0.928 - ETA: 1s - loss: 0.2102 - acc: 0.930 - ETA: 1s - loss: 0.2131 - acc: 0.926 - ETA: 1s - loss: 0.2143 - acc: 0.926 - ETA: 1s - loss: 0.2126 - acc: 0.928 - ETA: 1s - loss: 0.2115 - acc: 0.928 - ETA: 1s - loss: 0.2079 - acc: 0.930 - ETA: 0s - loss: 0.2084 - acc: 0.930 - ETA: 0s - loss: 0.2146 - acc: 0.929 - ETA: 0s - loss: 0.2125 - acc: 0.930 - ETA: 0s - loss: 0.2129 - acc: 0.930 - ETA: 0s - loss: 0.2111 - acc: 0.931 - ETA: 0s - loss: 0.2118 - acc: 0.930 - ETA: 0s - loss: 0.2111 - acc: 0.931 - ETA: 0s - loss: 0.2129 - acc: 0.930 - ETA: 0s - loss: 0.2108 - acc: 0.930 - ETA: 0s - loss: 0.2103 - acc: 0.931 - ETA: 0s - loss: 0.2088 - acc: 0.931 - ETA: 0s - loss: 0.2083 - acc: 0.932 - ETA: 0s - loss: 0.2067 - acc: 0.932 - ETA: 0s - loss: 0.2093 - acc: 0.932 - ETA: 0s - loss: 0.2084 - acc: 0.933 - ETA: 0s - loss: 0.2107 - acc: 0.933 - 2s 543us/step - loss: 0.2097 - acc: 0.9336 - val_loss: 0.2820 - val_acc: 0.9090\n",
      "Epoch 37/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2444 - acc: 0.921 - ETA: 1s - loss: 0.2583 - acc: 0.906 - ETA: 1s - loss: 0.2402 - acc: 0.925 - ETA: 1s - loss: 0.2168 - acc: 0.935 - ETA: 1s - loss: 0.2136 - acc: 0.939 - ETA: 1s - loss: 0.2097 - acc: 0.934 - ETA: 1s - loss: 0.2081 - acc: 0.936 - ETA: 1s - loss: 0.2029 - acc: 0.939 - ETA: 1s - loss: 0.2110 - acc: 0.931 - ETA: 1s - loss: 0.2120 - acc: 0.930 - ETA: 1s - loss: 0.2125 - acc: 0.930 - ETA: 1s - loss: 0.2084 - acc: 0.930 - ETA: 1s - loss: 0.2052 - acc: 0.931 - ETA: 1s - loss: 0.2071 - acc: 0.930 - ETA: 1s - loss: 0.2077 - acc: 0.931 - ETA: 1s - loss: 0.2083 - acc: 0.932 - ETA: 0s - loss: 0.2058 - acc: 0.933 - ETA: 0s - loss: 0.2037 - acc: 0.934 - ETA: 0s - loss: 0.2010 - acc: 0.936 - ETA: 0s - loss: 0.2020 - acc: 0.934 - ETA: 0s - loss: 0.2026 - acc: 0.933 - ETA: 0s - loss: 0.2023 - acc: 0.933 - ETA: 0s - loss: 0.2066 - acc: 0.933 - ETA: 0s - loss: 0.2063 - acc: 0.933 - ETA: 0s - loss: 0.2132 - acc: 0.930 - ETA: 0s - loss: 0.2139 - acc: 0.932 - ETA: 0s - loss: 0.2189 - acc: 0.930 - ETA: 0s - loss: 0.2233 - acc: 0.929 - ETA: 0s - loss: 0.2212 - acc: 0.930 - ETA: 0s - loss: 0.2200 - acc: 0.931 - ETA: 0s - loss: 0.2229 - acc: 0.931 - ETA: 0s - loss: 0.2245 - acc: 0.930 - 2s 545us/step - loss: 0.2253 - acc: 0.9299 - val_loss: 0.2933 - val_acc: 0.9071\n",
      "Epoch 38/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2631 - acc: 0.875 - ETA: 1s - loss: 0.2316 - acc: 0.916 - ETA: 1s - loss: 0.2220 - acc: 0.925 - ETA: 1s - loss: 0.2556 - acc: 0.910 - ETA: 1s - loss: 0.2507 - acc: 0.914 - ETA: 1s - loss: 0.2557 - acc: 0.916 - ETA: 1s - loss: 0.2613 - acc: 0.915 - ETA: 1s - loss: 0.2535 - acc: 0.920 - ETA: 1s - loss: 0.2576 - acc: 0.917 - ETA: 1s - loss: 0.2523 - acc: 0.919 - ETA: 1s - loss: 0.2575 - acc: 0.916 - ETA: 1s - loss: 0.2564 - acc: 0.917 - ETA: 1s - loss: 0.2629 - acc: 0.915 - ETA: 1s - loss: 0.2571 - acc: 0.918 - ETA: 1s - loss: 0.2511 - acc: 0.921 - ETA: 1s - loss: 0.2497 - acc: 0.921 - ETA: 0s - loss: 0.2438 - acc: 0.924 - ETA: 0s - loss: 0.2452 - acc: 0.924 - ETA: 0s - loss: 0.2400 - acc: 0.925 - ETA: 0s - loss: 0.2400 - acc: 0.926 - ETA: 0s - loss: 0.2408 - acc: 0.926 - ETA: 0s - loss: 0.2401 - acc: 0.925 - ETA: 0s - loss: 0.2408 - acc: 0.924 - ETA: 0s - loss: 0.2374 - acc: 0.925 - ETA: 0s - loss: 0.2361 - acc: 0.926 - ETA: 0s - loss: 0.2374 - acc: 0.927 - ETA: 0s - loss: 0.2372 - acc: 0.926 - ETA: 0s - loss: 0.2362 - acc: 0.926 - ETA: 0s - loss: 0.2352 - acc: 0.926 - ETA: 0s - loss: 0.2359 - acc: 0.925 - ETA: 0s - loss: 0.2359 - acc: 0.925 - ETA: 0s - loss: 0.2344 - acc: 0.926 - 2s 543us/step - loss: 0.2343 - acc: 0.9262 - val_loss: 0.2975 - val_acc: 0.8923\n",
      "Epoch 39/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2374 - acc: 0.906 - ETA: 1s - loss: 0.2546 - acc: 0.895 - ETA: 1s - loss: 0.2298 - acc: 0.912 - ETA: 2s - loss: 0.2115 - acc: 0.921 - ETA: 2s - loss: 0.2089 - acc: 0.931 - ETA: 2s - loss: 0.2075 - acc: 0.935 - ETA: 2s - loss: 0.1999 - acc: 0.940 - ETA: 2s - loss: 0.1939 - acc: 0.944 - ETA: 1s - loss: 0.1957 - acc: 0.942 - ETA: 1s - loss: 0.1950 - acc: 0.941 - ETA: 1s - loss: 0.1927 - acc: 0.940 - ETA: 1s - loss: 0.1927 - acc: 0.942 - ETA: 1s - loss: 0.1953 - acc: 0.941 - ETA: 1s - loss: 0.1940 - acc: 0.941 - ETA: 1s - loss: 0.1927 - acc: 0.942 - ETA: 1s - loss: 0.1921 - acc: 0.940 - ETA: 1s - loss: 0.1937 - acc: 0.940 - ETA: 1s - loss: 0.1974 - acc: 0.939 - ETA: 1s - loss: 0.1953 - acc: 0.940 - ETA: 0s - loss: 0.2000 - acc: 0.937 - ETA: 0s - loss: 0.2022 - acc: 0.936 - ETA: 0s - loss: 0.2010 - acc: 0.937 - ETA: 0s - loss: 0.2033 - acc: 0.936 - ETA: 0s - loss: 0.2071 - acc: 0.935 - ETA: 0s - loss: 0.2038 - acc: 0.937 - ETA: 0s - loss: 0.2049 - acc: 0.937 - ETA: 0s - loss: 0.2037 - acc: 0.939 - ETA: 0s - loss: 0.2036 - acc: 0.938 - ETA: 0s - loss: 0.2048 - acc: 0.938 - ETA: 0s - loss: 0.2050 - acc: 0.938 - ETA: 0s - loss: 0.2032 - acc: 0.939 - ETA: 0s - loss: 0.2039 - acc: 0.938 - ETA: 0s - loss: 0.2042 - acc: 0.938 - 2s 565us/step - loss: 0.2046 - acc: 0.9383 - val_loss: 0.2884 - val_acc: 0.9103\n",
      "Epoch 40/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1663 - acc: 0.968 - ETA: 1s - loss: 0.1708 - acc: 0.953 - ETA: 1s - loss: 0.1731 - acc: 0.950 - ETA: 1s - loss: 0.1854 - acc: 0.948 - ETA: 1s - loss: 0.1929 - acc: 0.946 - ETA: 1s - loss: 0.1958 - acc: 0.943 - ETA: 1s - loss: 0.2043 - acc: 0.937 - ETA: 1s - loss: 0.2054 - acc: 0.936 - ETA: 1s - loss: 0.2057 - acc: 0.938 - ETA: 1s - loss: 0.2083 - acc: 0.935 - ETA: 1s - loss: 0.2081 - acc: 0.936 - ETA: 1s - loss: 0.2036 - acc: 0.936 - ETA: 1s - loss: 0.2044 - acc: 0.935 - ETA: 1s - loss: 0.2025 - acc: 0.936 - ETA: 1s - loss: 0.2038 - acc: 0.934 - ETA: 1s - loss: 0.2030 - acc: 0.935 - ETA: 0s - loss: 0.2024 - acc: 0.936 - ETA: 0s - loss: 0.2057 - acc: 0.934 - ETA: 0s - loss: 0.2070 - acc: 0.935 - ETA: 0s - loss: 0.2092 - acc: 0.935 - ETA: 0s - loss: 0.2127 - acc: 0.935 - ETA: 0s - loss: 0.2160 - acc: 0.934 - ETA: 0s - loss: 0.2149 - acc: 0.935 - ETA: 0s - loss: 0.2130 - acc: 0.935 - ETA: 0s - loss: 0.2108 - acc: 0.936 - ETA: 0s - loss: 0.2113 - acc: 0.936 - ETA: 0s - loss: 0.2118 - acc: 0.935 - ETA: 0s - loss: 0.2145 - acc: 0.934 - ETA: 0s - loss: 0.2156 - acc: 0.933 - ETA: 0s - loss: 0.2162 - acc: 0.933 - ETA: 0s - loss: 0.2170 - acc: 0.933 - ETA: 0s - loss: 0.2199 - acc: 0.932 - 2s 544us/step - loss: 0.2201 - acc: 0.9326 - val_loss: 0.2937 - val_acc: 0.9071\n",
      "Epoch 41/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1682 - acc: 0.968 - ETA: 1s - loss: 0.1874 - acc: 0.958 - ETA: 1s - loss: 0.1733 - acc: 0.959 - ETA: 1s - loss: 0.1801 - acc: 0.950 - ETA: 1s - loss: 0.1705 - acc: 0.956 - ETA: 1s - loss: 0.1730 - acc: 0.954 - ETA: 1s - loss: 0.1800 - acc: 0.950 - ETA: 1s - loss: 0.1809 - acc: 0.949 - ETA: 1s - loss: 0.1880 - acc: 0.948 - ETA: 1s - loss: 0.1830 - acc: 0.949 - ETA: 1s - loss: 0.1860 - acc: 0.946 - ETA: 1s - loss: 0.1922 - acc: 0.942 - ETA: 1s - loss: 0.1888 - acc: 0.943 - ETA: 1s - loss: 0.1858 - acc: 0.945 - ETA: 1s - loss: 0.1857 - acc: 0.945 - ETA: 1s - loss: 0.1848 - acc: 0.947 - ETA: 0s - loss: 0.1841 - acc: 0.947 - ETA: 0s - loss: 0.1856 - acc: 0.947 - ETA: 0s - loss: 0.1894 - acc: 0.945 - ETA: 0s - loss: 0.1944 - acc: 0.942 - ETA: 0s - loss: 0.1933 - acc: 0.942 - ETA: 0s - loss: 0.1922 - acc: 0.944 - ETA: 0s - loss: 0.1962 - acc: 0.942 - ETA: 0s - loss: 0.1961 - acc: 0.941 - ETA: 0s - loss: 0.1981 - acc: 0.940 - ETA: 0s - loss: 0.1997 - acc: 0.939 - ETA: 0s - loss: 0.1992 - acc: 0.939 - ETA: 0s - loss: 0.1995 - acc: 0.938 - ETA: 0s - loss: 0.2004 - acc: 0.938 - ETA: 0s - loss: 0.1995 - acc: 0.938 - ETA: 0s - loss: 0.2031 - acc: 0.936 - ETA: 0s - loss: 0.2024 - acc: 0.936 - 2s 545us/step - loss: 0.2018 - acc: 0.9366 - val_loss: 0.2669 - val_acc: 0.9154\n",
      "Epoch 42/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1996 - acc: 0.953 - ETA: 1s - loss: 0.2181 - acc: 0.937 - ETA: 1s - loss: 0.2145 - acc: 0.943 - ETA: 1s - loss: 0.2084 - acc: 0.946 - ETA: 1s - loss: 0.1961 - acc: 0.947 - ETA: 1s - loss: 0.2023 - acc: 0.940 - ETA: 1s - loss: 0.1987 - acc: 0.942 - ETA: 1s - loss: 0.2092 - acc: 0.937 - ETA: 1s - loss: 0.2041 - acc: 0.940 - ETA: 1s - loss: 0.2017 - acc: 0.941 - ETA: 1s - loss: 0.2024 - acc: 0.939 - ETA: 1s - loss: 0.2082 - acc: 0.935 - ETA: 1s - loss: 0.2120 - acc: 0.931 - ETA: 1s - loss: 0.2122 - acc: 0.931 - ETA: 1s - loss: 0.2140 - acc: 0.931 - ETA: 1s - loss: 0.2108 - acc: 0.933 - ETA: 0s - loss: 0.2092 - acc: 0.934 - ETA: 0s - loss: 0.2078 - acc: 0.934 - ETA: 0s - loss: 0.2045 - acc: 0.935 - ETA: 0s - loss: 0.2056 - acc: 0.934 - ETA: 0s - loss: 0.2068 - acc: 0.933 - ETA: 0s - loss: 0.2067 - acc: 0.933 - ETA: 0s - loss: 0.2064 - acc: 0.934 - ETA: 0s - loss: 0.2052 - acc: 0.934 - ETA: 0s - loss: 0.2048 - acc: 0.933 - ETA: 0s - loss: 0.2052 - acc: 0.933 - ETA: 0s - loss: 0.2062 - acc: 0.934 - ETA: 0s - loss: 0.2047 - acc: 0.934 - ETA: 0s - loss: 0.2055 - acc: 0.934 - ETA: 0s - loss: 0.2028 - acc: 0.934 - ETA: 0s - loss: 0.2012 - acc: 0.934 - ETA: 0s - loss: 0.2015 - acc: 0.934 - 2s 541us/step - loss: 0.2010 - acc: 0.9351 - val_loss: 0.2588 - val_acc: 0.9231\n",
      "Epoch 43/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1077 - acc: 0.984 - ETA: 1s - loss: 0.1462 - acc: 0.979 - ETA: 1s - loss: 0.1710 - acc: 0.953 - ETA: 1s - loss: 0.1795 - acc: 0.946 - ETA: 1s - loss: 0.1776 - acc: 0.946 - ETA: 1s - loss: 0.1761 - acc: 0.946 - ETA: 1s - loss: 0.1702 - acc: 0.949 - ETA: 1s - loss: 0.1762 - acc: 0.944 - ETA: 1s - loss: 0.1730 - acc: 0.945 - ETA: 1s - loss: 0.1785 - acc: 0.944 - ETA: 1s - loss: 0.1772 - acc: 0.945 - ETA: 1s - loss: 0.1812 - acc: 0.943 - ETA: 1s - loss: 0.1820 - acc: 0.943 - ETA: 1s - loss: 0.1831 - acc: 0.941 - ETA: 1s - loss: 0.1865 - acc: 0.939 - ETA: 1s - loss: 0.1895 - acc: 0.939 - ETA: 1s - loss: 0.1893 - acc: 0.938 - ETA: 0s - loss: 0.1907 - acc: 0.937 - ETA: 0s - loss: 0.1893 - acc: 0.940 - ETA: 0s - loss: 0.1891 - acc: 0.939 - ETA: 0s - loss: 0.1880 - acc: 0.940 - ETA: 0s - loss: 0.1881 - acc: 0.940 - ETA: 0s - loss: 0.1883 - acc: 0.938 - ETA: 0s - loss: 0.1897 - acc: 0.938 - ETA: 0s - loss: 0.1942 - acc: 0.935 - ETA: 0s - loss: 0.1948 - acc: 0.936 - ETA: 0s - loss: 0.1940 - acc: 0.936 - ETA: 0s - loss: 0.1967 - acc: 0.936 - ETA: 0s - loss: 0.1968 - acc: 0.935 - ETA: 0s - loss: 0.1967 - acc: 0.934 - ETA: 0s - loss: 0.1960 - acc: 0.935 - ETA: 0s - loss: 0.1955 - acc: 0.935 - 2s 548us/step - loss: 0.1966 - acc: 0.9348 - val_loss: 0.2645 - val_acc: 0.9147\n",
      "Epoch 44/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2445 - acc: 0.906 - ETA: 1s - loss: 0.2510 - acc: 0.901 - ETA: 1s - loss: 0.2267 - acc: 0.925 - ETA: 1s - loss: 0.2285 - acc: 0.921 - ETA: 1s - loss: 0.2190 - acc: 0.927 - ETA: 1s - loss: 0.2199 - acc: 0.924 - ETA: 1s - loss: 0.2146 - acc: 0.926 - ETA: 1s - loss: 0.2109 - acc: 0.927 - ETA: 1s - loss: 0.2125 - acc: 0.928 - ETA: 1s - loss: 0.2077 - acc: 0.931 - ETA: 1s - loss: 0.2071 - acc: 0.930 - ETA: 1s - loss: 0.2041 - acc: 0.932 - ETA: 1s - loss: 0.2151 - acc: 0.928 - ETA: 1s - loss: 0.2134 - acc: 0.927 - ETA: 1s - loss: 0.2148 - acc: 0.926 - ETA: 1s - loss: 0.2137 - acc: 0.926 - ETA: 1s - loss: 0.2160 - acc: 0.926 - ETA: 0s - loss: 0.2126 - acc: 0.929 - ETA: 0s - loss: 0.2117 - acc: 0.929 - ETA: 0s - loss: 0.2081 - acc: 0.932 - ETA: 0s - loss: 0.2092 - acc: 0.932 - ETA: 0s - loss: 0.2063 - acc: 0.933 - ETA: 0s - loss: 0.2074 - acc: 0.933 - ETA: 0s - loss: 0.2066 - acc: 0.934 - ETA: 0s - loss: 0.2053 - acc: 0.934 - ETA: 0s - loss: 0.2041 - acc: 0.934 - ETA: 0s - loss: 0.2027 - acc: 0.935 - ETA: 0s - loss: 0.1998 - acc: 0.936 - ETA: 0s - loss: 0.1992 - acc: 0.936 - ETA: 0s - loss: 0.1989 - acc: 0.937 - ETA: 0s - loss: 0.1982 - acc: 0.937 - ETA: 0s - loss: 0.1992 - acc: 0.937 - 2s 548us/step - loss: 0.1990 - acc: 0.9375 - val_loss: 0.2600 - val_acc: 0.9090\n",
      "Epoch 45/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1524 - acc: 0.968 - ETA: 1s - loss: 0.2168 - acc: 0.927 - ETA: 1s - loss: 0.2122 - acc: 0.928 - ETA: 1s - loss: 0.1910 - acc: 0.935 - ETA: 1s - loss: 0.2014 - acc: 0.932 - ETA: 1s - loss: 0.2032 - acc: 0.931 - ETA: 1s - loss: 0.2063 - acc: 0.933 - ETA: 1s - loss: 0.2072 - acc: 0.932 - ETA: 1s - loss: 0.2048 - acc: 0.935 - ETA: 1s - loss: 0.1967 - acc: 0.939 - ETA: 1s - loss: 0.1996 - acc: 0.938 - ETA: 1s - loss: 0.1987 - acc: 0.938 - ETA: 1s - loss: 0.1983 - acc: 0.938 - ETA: 1s - loss: 0.1930 - acc: 0.942 - ETA: 1s - loss: 0.1923 - acc: 0.941 - ETA: 1s - loss: 0.1940 - acc: 0.942 - ETA: 0s - loss: 0.1941 - acc: 0.942 - ETA: 0s - loss: 0.1983 - acc: 0.940 - ETA: 0s - loss: 0.1991 - acc: 0.940 - ETA: 0s - loss: 0.2046 - acc: 0.937 - ETA: 0s - loss: 0.2026 - acc: 0.938 - ETA: 0s - loss: 0.2006 - acc: 0.939 - ETA: 0s - loss: 0.2028 - acc: 0.937 - ETA: 0s - loss: 0.2018 - acc: 0.937 - ETA: 0s - loss: 0.2024 - acc: 0.937 - ETA: 0s - loss: 0.2016 - acc: 0.937 - ETA: 0s - loss: 0.2019 - acc: 0.936 - ETA: 0s - loss: 0.2041 - acc: 0.934 - ETA: 0s - loss: 0.2073 - acc: 0.933 - ETA: 0s - loss: 0.2059 - acc: 0.934 - ETA: 0s - loss: 0.2037 - acc: 0.935 - ETA: 0s - loss: 0.2025 - acc: 0.936 - ETA: 0s - loss: 0.2021 - acc: 0.936 - ETA: 0s - loss: 0.2007 - acc: 0.936 - 2s 585us/step - loss: 0.2002 - acc: 0.9371 - val_loss: 0.2554 - val_acc: 0.9090\n",
      "Epoch 46/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.3840 - acc: 0.828 - ETA: 1s - loss: 0.2111 - acc: 0.927 - ETA: 1s - loss: 0.1965 - acc: 0.934 - ETA: 1s - loss: 0.2340 - acc: 0.919 - ETA: 1s - loss: 0.2275 - acc: 0.927 - ETA: 1s - loss: 0.2448 - acc: 0.920 - ETA: 1s - loss: 0.2377 - acc: 0.921 - ETA: 1s - loss: 0.2338 - acc: 0.922 - ETA: 1s - loss: 0.2265 - acc: 0.923 - ETA: 1s - loss: 0.2258 - acc: 0.926 - ETA: 1s - loss: 0.2292 - acc: 0.922 - ETA: 1s - loss: 0.2303 - acc: 0.920 - ETA: 1s - loss: 0.2255 - acc: 0.921 - ETA: 1s - loss: 0.2304 - acc: 0.919 - ETA: 1s - loss: 0.2327 - acc: 0.917 - ETA: 1s - loss: 0.2313 - acc: 0.917 - ETA: 1s - loss: 0.2320 - acc: 0.917 - ETA: 1s - loss: 0.2321 - acc: 0.918 - ETA: 1s - loss: 0.2288 - acc: 0.921 - ETA: 0s - loss: 0.2300 - acc: 0.920 - ETA: 0s - loss: 0.2300 - acc: 0.920 - ETA: 0s - loss: 0.2279 - acc: 0.921 - ETA: 0s - loss: 0.2280 - acc: 0.922 - ETA: 0s - loss: 0.2277 - acc: 0.922 - ETA: 0s - loss: 0.2239 - acc: 0.924 - ETA: 0s - loss: 0.2241 - acc: 0.925 - ETA: 0s - loss: 0.2227 - acc: 0.925 - ETA: 0s - loss: 0.2219 - acc: 0.926 - ETA: 0s - loss: 0.2194 - acc: 0.928 - ETA: 0s - loss: 0.2186 - acc: 0.929 - ETA: 0s - loss: 0.2178 - acc: 0.930 - ETA: 0s - loss: 0.2174 - acc: 0.930 - ETA: 0s - loss: 0.2194 - acc: 0.929 - 2s 569us/step - loss: 0.2213 - acc: 0.9284 - val_loss: 0.3003 - val_acc: 0.9064\n",
      "Epoch 47/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2659 - acc: 0.906 - ETA: 2s - loss: 0.2008 - acc: 0.953 - ETA: 1s - loss: 0.2363 - acc: 0.934 - ETA: 1s - loss: 0.2227 - acc: 0.937 - ETA: 1s - loss: 0.2268 - acc: 0.934 - ETA: 1s - loss: 0.2327 - acc: 0.929 - ETA: 1s - loss: 0.2429 - acc: 0.924 - ETA: 1s - loss: 0.2525 - acc: 0.919 - ETA: 1s - loss: 0.2644 - acc: 0.914 - ETA: 1s - loss: 0.2636 - acc: 0.915 - ETA: 1s - loss: 0.2536 - acc: 0.919 - ETA: 1s - loss: 0.2549 - acc: 0.917 - ETA: 1s - loss: 0.2543 - acc: 0.917 - ETA: 1s - loss: 0.2535 - acc: 0.919 - ETA: 1s - loss: 0.2548 - acc: 0.918 - ETA: 1s - loss: 0.2534 - acc: 0.918 - ETA: 1s - loss: 0.2511 - acc: 0.919 - ETA: 0s - loss: 0.2506 - acc: 0.919 - ETA: 0s - loss: 0.2478 - acc: 0.921 - ETA: 0s - loss: 0.2447 - acc: 0.923 - ETA: 0s - loss: 0.2417 - acc: 0.924 - ETA: 0s - loss: 0.2406 - acc: 0.924 - ETA: 0s - loss: 0.2428 - acc: 0.922 - ETA: 0s - loss: 0.2416 - acc: 0.923 - ETA: 0s - loss: 0.2447 - acc: 0.923 - ETA: 0s - loss: 0.2417 - acc: 0.924 - ETA: 0s - loss: 0.2428 - acc: 0.923 - ETA: 0s - loss: 0.2399 - acc: 0.925 - ETA: 0s - loss: 0.2399 - acc: 0.924 - ETA: 0s - loss: 0.2395 - acc: 0.924 - ETA: 0s - loss: 0.2377 - acc: 0.926 - ETA: 0s - loss: 0.2400 - acc: 0.925 - 2s 550us/step - loss: 0.2413 - acc: 0.9250 - val_loss: 0.3431 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2008 - acc: 0.968 - ETA: 1s - loss: 0.3463 - acc: 0.927 - ETA: 1s - loss: 0.3126 - acc: 0.921 - ETA: 1s - loss: 0.3005 - acc: 0.924 - ETA: 1s - loss: 0.2807 - acc: 0.930 - ETA: 1s - loss: 0.2765 - acc: 0.931 - ETA: 1s - loss: 0.2623 - acc: 0.935 - ETA: 1s - loss: 0.2630 - acc: 0.931 - ETA: 1s - loss: 0.2569 - acc: 0.933 - ETA: 1s - loss: 0.2518 - acc: 0.935 - ETA: 1s - loss: 0.2473 - acc: 0.934 - ETA: 1s - loss: 0.2484 - acc: 0.934 - ETA: 1s - loss: 0.2526 - acc: 0.932 - ETA: 1s - loss: 0.2476 - acc: 0.934 - ETA: 1s - loss: 0.2462 - acc: 0.934 - ETA: 1s - loss: 0.2440 - acc: 0.935 - ETA: 1s - loss: 0.2451 - acc: 0.933 - ETA: 0s - loss: 0.2439 - acc: 0.933 - ETA: 0s - loss: 0.2430 - acc: 0.933 - ETA: 0s - loss: 0.2437 - acc: 0.932 - ETA: 0s - loss: 0.2436 - acc: 0.931 - ETA: 0s - loss: 0.2427 - acc: 0.931 - ETA: 0s - loss: 0.2435 - acc: 0.930 - ETA: 0s - loss: 0.2461 - acc: 0.928 - ETA: 0s - loss: 0.2439 - acc: 0.929 - ETA: 0s - loss: 0.2438 - acc: 0.928 - ETA: 0s - loss: 0.2434 - acc: 0.928 - ETA: 0s - loss: 0.2415 - acc: 0.928 - ETA: 0s - loss: 0.2404 - acc: 0.929 - ETA: 0s - loss: 0.2417 - acc: 0.928 - ETA: 0s - loss: 0.2404 - acc: 0.929 - ETA: 0s - loss: 0.2396 - acc: 0.929 - ETA: 0s - loss: 0.2391 - acc: 0.930 - 2s 585us/step - loss: 0.2383 - acc: 0.9302 - val_loss: 0.2706 - val_acc: 0.9038\n",
      "Epoch 49/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.0905 - acc: 1.000 - ETA: 1s - loss: 0.2467 - acc: 0.927 - ETA: 1s - loss: 0.2291 - acc: 0.928 - ETA: 1s - loss: 0.2335 - acc: 0.928 - ETA: 1s - loss: 0.2452 - acc: 0.923 - ETA: 1s - loss: 0.2372 - acc: 0.927 - ETA: 1s - loss: 0.2382 - acc: 0.926 - ETA: 1s - loss: 0.2308 - acc: 0.929 - ETA: 1s - loss: 0.2231 - acc: 0.932 - ETA: 1s - loss: 0.2178 - acc: 0.934 - ETA: 1s - loss: 0.2227 - acc: 0.932 - ETA: 1s - loss: 0.2199 - acc: 0.934 - ETA: 1s - loss: 0.2173 - acc: 0.936 - ETA: 1s - loss: 0.2151 - acc: 0.938 - ETA: 1s - loss: 0.2123 - acc: 0.937 - ETA: 1s - loss: 0.2140 - acc: 0.935 - ETA: 1s - loss: 0.2112 - acc: 0.936 - ETA: 1s - loss: 0.2098 - acc: 0.937 - ETA: 1s - loss: 0.2098 - acc: 0.937 - ETA: 0s - loss: 0.2080 - acc: 0.938 - ETA: 0s - loss: 0.2104 - acc: 0.937 - ETA: 0s - loss: 0.2094 - acc: 0.938 - ETA: 0s - loss: 0.2084 - acc: 0.937 - ETA: 0s - loss: 0.2096 - acc: 0.937 - ETA: 0s - loss: 0.2098 - acc: 0.937 - ETA: 0s - loss: 0.2091 - acc: 0.937 - ETA: 0s - loss: 0.2112 - acc: 0.935 - ETA: 0s - loss: 0.2095 - acc: 0.936 - ETA: 0s - loss: 0.2092 - acc: 0.935 - ETA: 0s - loss: 0.2073 - acc: 0.936 - ETA: 0s - loss: 0.2067 - acc: 0.936 - ETA: 0s - loss: 0.2054 - acc: 0.937 - ETA: 0s - loss: 0.2056 - acc: 0.936 - 2s 588us/step - loss: 0.2043 - acc: 0.9373 - val_loss: 0.2597 - val_acc: 0.9173\n",
      "Epoch 50/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2751 - acc: 0.906 - ETA: 2s - loss: 0.2092 - acc: 0.932 - ETA: 2s - loss: 0.2191 - acc: 0.925 - ETA: 2s - loss: 0.2083 - acc: 0.933 - ETA: 1s - loss: 0.1975 - acc: 0.939 - ETA: 1s - loss: 0.1995 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.939 - ETA: 1s - loss: 0.1917 - acc: 0.942 - ETA: 1s - loss: 0.1958 - acc: 0.939 - ETA: 1s - loss: 0.1910 - acc: 0.942 - ETA: 1s - loss: 0.1924 - acc: 0.941 - ETA: 1s - loss: 0.1899 - acc: 0.942 - ETA: 1s - loss: 0.1876 - acc: 0.943 - ETA: 1s - loss: 0.1858 - acc: 0.944 - ETA: 1s - loss: 0.1898 - acc: 0.942 - ETA: 1s - loss: 0.1893 - acc: 0.943 - ETA: 1s - loss: 0.1926 - acc: 0.940 - ETA: 0s - loss: 0.1908 - acc: 0.941 - ETA: 0s - loss: 0.1910 - acc: 0.941 - ETA: 0s - loss: 0.1917 - acc: 0.940 - ETA: 0s - loss: 0.1909 - acc: 0.940 - ETA: 0s - loss: 0.1906 - acc: 0.940 - ETA: 0s - loss: 0.1910 - acc: 0.940 - ETA: 0s - loss: 0.1902 - acc: 0.940 - ETA: 0s - loss: 0.1888 - acc: 0.941 - ETA: 0s - loss: 0.1891 - acc: 0.941 - ETA: 0s - loss: 0.1885 - acc: 0.941 - ETA: 0s - loss: 0.1904 - acc: 0.940 - ETA: 0s - loss: 0.1910 - acc: 0.940 - ETA: 0s - loss: 0.1917 - acc: 0.940 - ETA: 0s - loss: 0.1909 - acc: 0.941 - ETA: 0s - loss: 0.1919 - acc: 0.940 - 2s 571us/step - loss: 0.1928 - acc: 0.9398 - val_loss: 0.2509 - val_acc: 0.9186\n",
      "Epoch 51/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1383 - acc: 0.968 - ETA: 1s - loss: 0.2081 - acc: 0.921 - ETA: 1s - loss: 0.1815 - acc: 0.943 - ETA: 1s - loss: 0.2014 - acc: 0.933 - ETA: 1s - loss: 0.1864 - acc: 0.942 - ETA: 1s - loss: 0.1895 - acc: 0.941 - ETA: 1s - loss: 0.1900 - acc: 0.944 - ETA: 1s - loss: 0.1890 - acc: 0.946 - ETA: 1s - loss: 0.1836 - acc: 0.950 - ETA: 1s - loss: 0.1852 - acc: 0.949 - ETA: 1s - loss: 0.1906 - acc: 0.949 - ETA: 1s - loss: 0.2035 - acc: 0.945 - ETA: 1s - loss: 0.2070 - acc: 0.942 - ETA: 1s - loss: 0.2054 - acc: 0.943 - ETA: 1s - loss: 0.2100 - acc: 0.941 - ETA: 1s - loss: 0.2163 - acc: 0.938 - ETA: 1s - loss: 0.2128 - acc: 0.938 - ETA: 0s - loss: 0.2125 - acc: 0.937 - ETA: 0s - loss: 0.2119 - acc: 0.937 - ETA: 0s - loss: 0.2104 - acc: 0.937 - ETA: 0s - loss: 0.2139 - acc: 0.937 - ETA: 0s - loss: 0.2156 - acc: 0.937 - ETA: 0s - loss: 0.2143 - acc: 0.936 - ETA: 0s - loss: 0.2181 - acc: 0.933 - ETA: 0s - loss: 0.2193 - acc: 0.933 - ETA: 0s - loss: 0.2185 - acc: 0.933 - ETA: 0s - loss: 0.2189 - acc: 0.933 - ETA: 0s - loss: 0.2190 - acc: 0.933 - ETA: 0s - loss: 0.2204 - acc: 0.932 - ETA: 0s - loss: 0.2185 - acc: 0.933 - ETA: 0s - loss: 0.2153 - acc: 0.934 - ETA: 0s - loss: 0.2136 - acc: 0.935 - 2s 547us/step - loss: 0.2135 - acc: 0.9356 - val_loss: 0.2537 - val_acc: 0.9263\n",
      "Epoch 52/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1614 - acc: 0.953 - ETA: 1s - loss: 0.1848 - acc: 0.953 - ETA: 1s - loss: 0.1884 - acc: 0.950 - ETA: 1s - loss: 0.1945 - acc: 0.948 - ETA: 1s - loss: 0.1971 - acc: 0.942 - ETA: 1s - loss: 0.2196 - acc: 0.938 - ETA: 1s - loss: 0.2109 - acc: 0.941 - ETA: 1s - loss: 0.2143 - acc: 0.938 - ETA: 1s - loss: 0.2077 - acc: 0.941 - ETA: 1s - loss: 0.2037 - acc: 0.940 - ETA: 1s - loss: 0.2029 - acc: 0.939 - ETA: 1s - loss: 0.2066 - acc: 0.937 - ETA: 1s - loss: 0.2039 - acc: 0.937 - ETA: 1s - loss: 0.2027 - acc: 0.938 - ETA: 1s - loss: 0.2050 - acc: 0.936 - ETA: 1s - loss: 0.2111 - acc: 0.931 - ETA: 1s - loss: 0.2081 - acc: 0.933 - ETA: 0s - loss: 0.2062 - acc: 0.934 - ETA: 0s - loss: 0.2071 - acc: 0.934 - ETA: 0s - loss: 0.2071 - acc: 0.933 - ETA: 0s - loss: 0.2072 - acc: 0.933 - ETA: 0s - loss: 0.2065 - acc: 0.933 - ETA: 0s - loss: 0.2088 - acc: 0.932 - ETA: 0s - loss: 0.2090 - acc: 0.932 - ETA: 0s - loss: 0.2097 - acc: 0.931 - ETA: 0s - loss: 0.2084 - acc: 0.932 - ETA: 0s - loss: 0.2066 - acc: 0.933 - ETA: 0s - loss: 0.2069 - acc: 0.933 - ETA: 0s - loss: 0.2054 - acc: 0.933 - ETA: 0s - loss: 0.2046 - acc: 0.934 - ETA: 0s - loss: 0.2022 - acc: 0.934 - ETA: 0s - loss: 0.2023 - acc: 0.934 - 2s 547us/step - loss: 0.2023 - acc: 0.9343 - val_loss: 0.2475 - val_acc: 0.9276\n",
      "Epoch 53/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1210 - acc: 0.968 - ETA: 1s - loss: 0.1754 - acc: 0.942 - ETA: 1s - loss: 0.1925 - acc: 0.940 - ETA: 1s - loss: 0.1866 - acc: 0.937 - ETA: 1s - loss: 0.1973 - acc: 0.928 - ETA: 1s - loss: 0.1944 - acc: 0.930 - ETA: 1s - loss: 0.1847 - acc: 0.936 - ETA: 1s - loss: 0.1762 - acc: 0.940 - ETA: 1s - loss: 0.1848 - acc: 0.934 - ETA: 1s - loss: 0.1898 - acc: 0.933 - ETA: 1s - loss: 0.1896 - acc: 0.933 - ETA: 1s - loss: 0.1893 - acc: 0.934 - ETA: 1s - loss: 0.1900 - acc: 0.935 - ETA: 1s - loss: 0.1928 - acc: 0.935 - ETA: 1s - loss: 0.1947 - acc: 0.934 - ETA: 1s - loss: 0.1910 - acc: 0.936 - ETA: 1s - loss: 0.1944 - acc: 0.935 - ETA: 0s - loss: 0.1951 - acc: 0.934 - ETA: 0s - loss: 0.1957 - acc: 0.934 - ETA: 0s - loss: 0.1955 - acc: 0.933 - ETA: 0s - loss: 0.1960 - acc: 0.933 - ETA: 0s - loss: 0.1957 - acc: 0.933 - ETA: 0s - loss: 0.1967 - acc: 0.933 - ETA: 0s - loss: 0.1950 - acc: 0.934 - ETA: 0s - loss: 0.1947 - acc: 0.934 - ETA: 0s - loss: 0.1928 - acc: 0.936 - ETA: 0s - loss: 0.1906 - acc: 0.937 - ETA: 0s - loss: 0.1912 - acc: 0.937 - ETA: 0s - loss: 0.1907 - acc: 0.938 - ETA: 0s - loss: 0.1889 - acc: 0.939 - ETA: 0s - loss: 0.1874 - acc: 0.940 - ETA: 0s - loss: 0.1886 - acc: 0.939 - 2s 549us/step - loss: 0.1896 - acc: 0.9393 - val_loss: 0.2463 - val_acc: 0.9218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1035 - acc: 0.984 - ETA: 2s - loss: 0.1526 - acc: 0.958 - ETA: 1s - loss: 0.1607 - acc: 0.953 - ETA: 1s - loss: 0.1619 - acc: 0.955 - ETA: 1s - loss: 0.1906 - acc: 0.946 - ETA: 1s - loss: 0.1917 - acc: 0.946 - ETA: 1s - loss: 0.1993 - acc: 0.941 - ETA: 1s - loss: 0.1973 - acc: 0.940 - ETA: 1s - loss: 0.1958 - acc: 0.942 - ETA: 1s - loss: 0.2024 - acc: 0.939 - ETA: 1s - loss: 0.2046 - acc: 0.938 - ETA: 1s - loss: 0.2032 - acc: 0.938 - ETA: 1s - loss: 0.2022 - acc: 0.938 - ETA: 1s - loss: 0.2061 - acc: 0.937 - ETA: 1s - loss: 0.2087 - acc: 0.937 - ETA: 1s - loss: 0.2044 - acc: 0.940 - ETA: 1s - loss: 0.2039 - acc: 0.940 - ETA: 0s - loss: 0.2055 - acc: 0.938 - ETA: 0s - loss: 0.2036 - acc: 0.939 - ETA: 0s - loss: 0.2018 - acc: 0.940 - ETA: 0s - loss: 0.2001 - acc: 0.940 - ETA: 0s - loss: 0.2014 - acc: 0.938 - ETA: 0s - loss: 0.2020 - acc: 0.937 - ETA: 0s - loss: 0.2015 - acc: 0.937 - ETA: 0s - loss: 0.2007 - acc: 0.936 - ETA: 0s - loss: 0.1999 - acc: 0.937 - ETA: 0s - loss: 0.1976 - acc: 0.938 - ETA: 0s - loss: 0.1993 - acc: 0.937 - ETA: 0s - loss: 0.1990 - acc: 0.938 - ETA: 0s - loss: 0.1984 - acc: 0.938 - ETA: 0s - loss: 0.1977 - acc: 0.938 - ETA: 0s - loss: 0.1968 - acc: 0.939 - 2s 553us/step - loss: 0.1963 - acc: 0.9395 - val_loss: 0.3178 - val_acc: 0.8776\n",
      "Epoch 55/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2233 - acc: 0.921 - ETA: 1s - loss: 0.2403 - acc: 0.916 - ETA: 1s - loss: 0.1902 - acc: 0.940 - ETA: 1s - loss: 0.1932 - acc: 0.939 - ETA: 1s - loss: 0.2038 - acc: 0.928 - ETA: 1s - loss: 0.2042 - acc: 0.929 - ETA: 1s - loss: 0.2076 - acc: 0.930 - ETA: 1s - loss: 0.2051 - acc: 0.933 - ETA: 1s - loss: 0.2034 - acc: 0.933 - ETA: 1s - loss: 0.1987 - acc: 0.936 - ETA: 1s - loss: 0.1952 - acc: 0.939 - ETA: 1s - loss: 0.1979 - acc: 0.937 - ETA: 1s - loss: 0.1949 - acc: 0.939 - ETA: 1s - loss: 0.1933 - acc: 0.940 - ETA: 1s - loss: 0.1916 - acc: 0.940 - ETA: 1s - loss: 0.1925 - acc: 0.940 - ETA: 1s - loss: 0.1901 - acc: 0.940 - ETA: 0s - loss: 0.1921 - acc: 0.938 - ETA: 0s - loss: 0.1948 - acc: 0.937 - ETA: 0s - loss: 0.1934 - acc: 0.937 - ETA: 0s - loss: 0.1954 - acc: 0.936 - ETA: 0s - loss: 0.1958 - acc: 0.935 - ETA: 0s - loss: 0.2000 - acc: 0.934 - ETA: 0s - loss: 0.1980 - acc: 0.935 - ETA: 0s - loss: 0.2042 - acc: 0.934 - ETA: 0s - loss: 0.2047 - acc: 0.934 - ETA: 0s - loss: 0.2060 - acc: 0.934 - ETA: 0s - loss: 0.2087 - acc: 0.934 - ETA: 0s - loss: 0.2093 - acc: 0.934 - ETA: 0s - loss: 0.2104 - acc: 0.934 - ETA: 0s - loss: 0.2104 - acc: 0.934 - ETA: 0s - loss: 0.2118 - acc: 0.934 - 2s 553us/step - loss: 0.2116 - acc: 0.9343 - val_loss: 0.2638 - val_acc: 0.9090\n",
      "Epoch 56/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.2077 - acc: 0.937 - ETA: 2s - loss: 0.1800 - acc: 0.953 - ETA: 1s - loss: 0.2200 - acc: 0.931 - ETA: 1s - loss: 0.2195 - acc: 0.933 - ETA: 1s - loss: 0.2107 - acc: 0.932 - ETA: 1s - loss: 0.2182 - acc: 0.931 - ETA: 1s - loss: 0.2197 - acc: 0.929 - ETA: 1s - loss: 0.2113 - acc: 0.935 - ETA: 1s - loss: 0.2067 - acc: 0.937 - ETA: 1s - loss: 0.2058 - acc: 0.935 - ETA: 1s - loss: 0.2015 - acc: 0.938 - ETA: 1s - loss: 0.2005 - acc: 0.938 - ETA: 1s - loss: 0.2041 - acc: 0.938 - ETA: 1s - loss: 0.1987 - acc: 0.940 - ETA: 1s - loss: 0.1951 - acc: 0.942 - ETA: 1s - loss: 0.1959 - acc: 0.941 - ETA: 1s - loss: 0.1953 - acc: 0.941 - ETA: 0s - loss: 0.1924 - acc: 0.942 - ETA: 0s - loss: 0.1921 - acc: 0.941 - ETA: 0s - loss: 0.1914 - acc: 0.942 - ETA: 0s - loss: 0.1947 - acc: 0.941 - ETA: 0s - loss: 0.1965 - acc: 0.940 - ETA: 0s - loss: 0.1966 - acc: 0.939 - ETA: 0s - loss: 0.1948 - acc: 0.939 - ETA: 0s - loss: 0.1967 - acc: 0.938 - ETA: 0s - loss: 0.1947 - acc: 0.938 - ETA: 0s - loss: 0.1944 - acc: 0.939 - ETA: 0s - loss: 0.1948 - acc: 0.938 - ETA: 0s - loss: 0.1933 - acc: 0.938 - ETA: 0s - loss: 0.1932 - acc: 0.938 - ETA: 0s - loss: 0.1957 - acc: 0.938 - ETA: 0s - loss: 0.1991 - acc: 0.937 - 2s 552us/step - loss: 0.2012 - acc: 0.9371 - val_loss: 0.2743 - val_acc: 0.9256\n",
      "Epoch 57/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.2566 - acc: 0.921 - ETA: 1s - loss: 0.2040 - acc: 0.932 - ETA: 1s - loss: 0.2039 - acc: 0.940 - ETA: 1s - loss: 0.2330 - acc: 0.930 - ETA: 1s - loss: 0.2220 - acc: 0.935 - ETA: 1s - loss: 0.2095 - acc: 0.940 - ETA: 1s - loss: 0.2076 - acc: 0.939 - ETA: 1s - loss: 0.2106 - acc: 0.938 - ETA: 1s - loss: 0.2089 - acc: 0.936 - ETA: 1s - loss: 0.2081 - acc: 0.937 - ETA: 1s - loss: 0.2096 - acc: 0.936 - ETA: 1s - loss: 0.2071 - acc: 0.938 - ETA: 1s - loss: 0.2061 - acc: 0.939 - ETA: 1s - loss: 0.2042 - acc: 0.940 - ETA: 1s - loss: 0.2013 - acc: 0.940 - ETA: 1s - loss: 0.2035 - acc: 0.940 - ETA: 0s - loss: 0.2044 - acc: 0.938 - ETA: 0s - loss: 0.2015 - acc: 0.940 - ETA: 0s - loss: 0.2009 - acc: 0.939 - ETA: 0s - loss: 0.2017 - acc: 0.939 - ETA: 0s - loss: 0.2001 - acc: 0.940 - ETA: 0s - loss: 0.1999 - acc: 0.940 - ETA: 0s - loss: 0.1989 - acc: 0.940 - ETA: 0s - loss: 0.1975 - acc: 0.941 - ETA: 0s - loss: 0.1975 - acc: 0.940 - ETA: 0s - loss: 0.1966 - acc: 0.939 - ETA: 0s - loss: 0.1957 - acc: 0.939 - ETA: 0s - loss: 0.1949 - acc: 0.939 - ETA: 0s - loss: 0.1927 - acc: 0.940 - ETA: 0s - loss: 0.1950 - acc: 0.939 - ETA: 0s - loss: 0.1945 - acc: 0.940 - ETA: 0s - loss: 0.1949 - acc: 0.940 - 2s 546us/step - loss: 0.1970 - acc: 0.9390 - val_loss: 0.2551 - val_acc: 0.9263\n",
      "Epoch 58/59\n",
      "4067/4067 [==============================] - ETA: 2s - loss: 0.1819 - acc: 0.937 - ETA: 2s - loss: 0.1401 - acc: 0.974 - ETA: 1s - loss: 0.1529 - acc: 0.956 - ETA: 1s - loss: 0.1680 - acc: 0.953 - ETA: 1s - loss: 0.1788 - acc: 0.944 - ETA: 1s - loss: 0.1786 - acc: 0.946 - ETA: 1s - loss: 0.1832 - acc: 0.950 - ETA: 1s - loss: 0.1857 - acc: 0.949 - ETA: 1s - loss: 0.1840 - acc: 0.948 - ETA: 1s - loss: 0.1830 - acc: 0.948 - ETA: 1s - loss: 0.1877 - acc: 0.946 - ETA: 1s - loss: 0.1888 - acc: 0.947 - ETA: 1s - loss: 0.1933 - acc: 0.945 - ETA: 1s - loss: 0.1936 - acc: 0.945 - ETA: 1s - loss: 0.1965 - acc: 0.944 - ETA: 1s - loss: 0.1938 - acc: 0.945 - ETA: 1s - loss: 0.1928 - acc: 0.945 - ETA: 0s - loss: 0.1969 - acc: 0.942 - ETA: 0s - loss: 0.1977 - acc: 0.941 - ETA: 0s - loss: 0.1964 - acc: 0.942 - ETA: 0s - loss: 0.1991 - acc: 0.941 - ETA: 0s - loss: 0.2023 - acc: 0.941 - ETA: 0s - loss: 0.2017 - acc: 0.941 - ETA: 0s - loss: 0.2008 - acc: 0.940 - ETA: 0s - loss: 0.2006 - acc: 0.940 - ETA: 0s - loss: 0.1983 - acc: 0.942 - ETA: 0s - loss: 0.1960 - acc: 0.943 - ETA: 0s - loss: 0.1963 - acc: 0.942 - ETA: 0s - loss: 0.1936 - acc: 0.943 - ETA: 0s - loss: 0.1911 - acc: 0.944 - ETA: 0s - loss: 0.1906 - acc: 0.944 - ETA: 0s - loss: 0.1903 - acc: 0.943 - 2s 549us/step - loss: 0.1907 - acc: 0.9439 - val_loss: 0.2423 - val_acc: 0.9263\n",
      "Epoch 59/59\n",
      "4067/4067 [==============================] - ETA: 1s - loss: 0.1002 - acc: 0.984 - ETA: 1s - loss: 0.1318 - acc: 0.974 - ETA: 1s - loss: 0.1633 - acc: 0.956 - ETA: 1s - loss: 0.1853 - acc: 0.948 - ETA: 1s - loss: 0.1801 - acc: 0.947 - ETA: 1s - loss: 0.1721 - acc: 0.948 - ETA: 1s - loss: 0.1772 - acc: 0.947 - ETA: 1s - loss: 0.1786 - acc: 0.945 - ETA: 1s - loss: 0.1714 - acc: 0.951 - ETA: 1s - loss: 0.1817 - acc: 0.944 - ETA: 1s - loss: 0.1777 - acc: 0.946 - ETA: 1s - loss: 0.1746 - acc: 0.945 - ETA: 1s - loss: 0.1746 - acc: 0.945 - ETA: 1s - loss: 0.1747 - acc: 0.945 - ETA: 1s - loss: 0.1770 - acc: 0.944 - ETA: 1s - loss: 0.1757 - acc: 0.946 - ETA: 1s - loss: 0.1750 - acc: 0.946 - ETA: 0s - loss: 0.1761 - acc: 0.946 - ETA: 0s - loss: 0.1755 - acc: 0.945 - ETA: 0s - loss: 0.1767 - acc: 0.944 - ETA: 0s - loss: 0.1771 - acc: 0.943 - ETA: 0s - loss: 0.1765 - acc: 0.944 - ETA: 0s - loss: 0.1787 - acc: 0.942 - ETA: 0s - loss: 0.1784 - acc: 0.943 - ETA: 0s - loss: 0.1798 - acc: 0.942 - ETA: 0s - loss: 0.1780 - acc: 0.942 - ETA: 0s - loss: 0.1829 - acc: 0.940 - ETA: 0s - loss: 0.1811 - acc: 0.941 - ETA: 0s - loss: 0.1836 - acc: 0.940 - ETA: 0s - loss: 0.1866 - acc: 0.939 - ETA: 0s - loss: 0.1858 - acc: 0.939 - ETA: 0s - loss: 0.1892 - acc: 0.937 - 2s 549us/step - loss: 0.1894 - acc: 0.9373 - val_loss: 0.2496 - val_acc: 0.9288\n"
     ]
    }
   ],
   "source": [
    "runtime_param['nb_epoch'] = 59\n",
    "best_model,result = keras_fmin_fnct(runtime_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 0.9338578805015982 test_accuracy 0.9288461538461539\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = best_model.evaluate(X_val_s,Y_val_s,verbose=0)\n",
    "_,acc_train = best_model.evaluate(X_train_s,Y_train_s,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0]\n",
      " [  0 439  52]\n",
      " [  0  59 473]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "# Activities are the class labels\n",
    "# It is a 3 class classification\n",
    "from sklearn import metrics\n",
    "ACTIVITIES = {\n",
    "    0: 'SITTING',\n",
    "    1: 'STANDING',\n",
    "    2: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix_cnn(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    #return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "    return metrics.confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix_cnn(Y_val_s, best_model.predict(X_val_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "cm = confusion_matrix_cnn(Y_val_s, best_model.predict(X_val_s))\n",
    "plot_confusion_matrix(cm, classes=['SITTING','STANDING','LAYING'], normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Greens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving model\n",
    "best_model.save('final_model_static.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Dynamic activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data preparation\n",
    "def data_scaled_dynamic():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Data directory\n",
    "    DATADIR = 'UCI_HAR_Dataset'\n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "        ]\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        def __init__(self):\n",
    "            self.scale = None\n",
    "\n",
    "        def transform(self, X):\n",
    "            temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "            temp_X1 = self.scale.transform(temp_X1)\n",
    "            return temp_X1.reshape(X.shape)\n",
    "\n",
    "        def fit(self, X):\n",
    "            # remove overlaping\n",
    "            remove = int(X.shape[1] / 2)\n",
    "            temp_X = X[:, -remove:, :]\n",
    "            # flatten data\n",
    "            temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
    "            scale = StandardScaler()\n",
    "            scale.fit(temp_X)\n",
    "            pickle.dump(scale,open('Scale_dynamic.p','wb'))\n",
    "            self.scale = scale\n",
    "            return self\n",
    "        \n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Utility function to load the load\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "            signals_data.append( _read_csv(filename).as_matrix()) \n",
    "\n",
    "        # Transpose is used to change the dimensionality of the output,\n",
    "        # aggregating the signals by combination of sample/timestep.\n",
    "        # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        y_subset = y<=3\n",
    "        y = y[y_subset]\n",
    "        return pd.get_dummies(y).as_matrix(),y_subset\n",
    "    \n",
    "    Y_train_d,y_train_sub = load_y('train')\n",
    "    Y_val_d,y_test_sub = load_y('test')\n",
    "    X_train_d, X_val_d = load_signals('train'), load_signals('test')\n",
    "    X_train_d = X_train_d[y_train_sub]\n",
    "    X_val_d = X_val_d[y_test_sub]\n",
    "    \n",
    "    ###Scling data\n",
    "    Scale = scaling_tseries_data()\n",
    "    Scale.fit(X_train_d)\n",
    "    X_train_d = Scale.transform(X_train_d)\n",
    "    X_val_d = Scale.transform(X_val_d)\n",
    "\n",
    "    return X_train_d, Y_train_d, X_val_d,  Y_val_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape (3285, 128, 9) Test X shape (1387, 128, 9)\n",
      "Train Y shape (3285, 3) Test Y shape (1387, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_d, Y_train_d, X_val_d,  Y_val_d = data_scaled_dynamic()\n",
    "print('Train X shape',X_train_d.shape,'Test X shape',X_val_d.shape)\n",
    "print('Train Y shape',Y_train_d.shape,'Test Y shape',Y_val_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 122, 64)           4096      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 120, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 30)                38430     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 48,795\n",
      "Trainable params: 48,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/100\n",
      "4067/4067 [==============================] - ETA: 3:54 - loss: 2.0608 - acc: 0.375 - ETA: 1:02 - loss: 2.1721 - acc: 0.484 - ETA: 43s - loss: 1.5969 - acc: 0.572 - ETA: 34s - loss: 1.4281 - acc: 0.60 - ETA: 28s - loss: 1.2191 - acc: 0.65 - ETA: 22s - loss: 1.3602 - acc: 0.66 - ETA: 20s - loss: 1.4137 - acc: 0.66 - ETA: 18s - loss: 1.3443 - acc: 0.66 - ETA: 16s - loss: 1.2184 - acc: 0.68 - ETA: 14s - loss: 1.1831 - acc: 0.70 - ETA: 13s - loss: 1.1511 - acc: 0.71 - ETA: 13s - loss: 1.1674 - acc: 0.71 - ETA: 12s - loss: 1.1011 - acc: 0.72 - ETA: 11s - loss: 1.1216 - acc: 0.72 - ETA: 10s - loss: 1.0856 - acc: 0.73 - ETA: 10s - loss: 1.0339 - acc: 0.74 - ETA: 9s - loss: 1.0068 - acc: 0.7574 - ETA: 9s - loss: 0.9771 - acc: 0.765 - ETA: 8s - loss: 0.9542 - acc: 0.769 - ETA: 8s - loss: 0.9426 - acc: 0.776 - ETA: 8s - loss: 0.9306 - acc: 0.778 - ETA: 8s - loss: 0.9117 - acc: 0.782 - ETA: 7s - loss: 0.8713 - acc: 0.790 - ETA: 7s - loss: 0.8728 - acc: 0.792 - ETA: 7s - loss: 0.8905 - acc: 0.793 - ETA: 7s - loss: 0.8621 - acc: 0.797 - ETA: 6s - loss: 0.8612 - acc: 0.800 - ETA: 6s - loss: 0.8531 - acc: 0.803 - ETA: 6s - loss: 0.8362 - acc: 0.805 - ETA: 6s - loss: 0.8219 - acc: 0.810 - ETA: 6s - loss: 0.8114 - acc: 0.812 - ETA: 6s - loss: 0.7992 - acc: 0.814 - ETA: 6s - loss: 0.7821 - acc: 0.817 - ETA: 5s - loss: 0.7920 - acc: 0.817 - ETA: 5s - loss: 0.7817 - acc: 0.821 - ETA: 5s - loss: 0.7643 - acc: 0.822 - ETA: 5s - loss: 0.7669 - acc: 0.821 - ETA: 5s - loss: 0.7503 - acc: 0.825 - ETA: 5s - loss: 0.7569 - acc: 0.824 - ETA: 5s - loss: 0.7456 - acc: 0.827 - ETA: 4s - loss: 0.7309 - acc: 0.829 - ETA: 4s - loss: 0.7221 - acc: 0.830 - ETA: 4s - loss: 0.7109 - acc: 0.833 - ETA: 4s - loss: 0.7054 - acc: 0.833 - ETA: 4s - loss: 0.7007 - acc: 0.833 - ETA: 4s - loss: 0.6947 - acc: 0.835 - ETA: 4s - loss: 0.6812 - acc: 0.836 - ETA: 4s - loss: 0.6730 - acc: 0.837 - ETA: 4s - loss: 0.6660 - acc: 0.838 - ETA: 4s - loss: 0.6574 - acc: 0.839 - ETA: 3s - loss: 0.6636 - acc: 0.838 - ETA: 3s - loss: 0.6571 - acc: 0.839 - ETA: 3s - loss: 0.6567 - acc: 0.840 - ETA: 3s - loss: 0.6577 - acc: 0.840 - ETA: 3s - loss: 0.6505 - acc: 0.841 - ETA: 3s - loss: 0.6498 - acc: 0.843 - ETA: 3s - loss: 0.6443 - acc: 0.845 - ETA: 3s - loss: 0.6429 - acc: 0.845 - ETA: 3s - loss: 0.6389 - acc: 0.845 - ETA: 3s - loss: 0.6330 - acc: 0.845 - ETA: 3s - loss: 0.6353 - acc: 0.845 - ETA: 2s - loss: 0.6434 - acc: 0.844 - ETA: 2s - loss: 0.6551 - acc: 0.843 - ETA: 2s - loss: 0.6632 - acc: 0.843 - ETA: 2s - loss: 0.6592 - acc: 0.843 - ETA: 2s - loss: 0.6539 - acc: 0.843 - ETA: 2s - loss: 0.6551 - acc: 0.844 - ETA: 2s - loss: 0.6551 - acc: 0.845 - ETA: 2s - loss: 0.6513 - acc: 0.846 - ETA: 2s - loss: 0.6520 - acc: 0.847 - ETA: 2s - loss: 0.6479 - acc: 0.846 - ETA: 2s - loss: 0.6495 - acc: 0.846 - ETA: 2s - loss: 0.6489 - acc: 0.846 - ETA: 2s - loss: 0.6424 - acc: 0.846 - ETA: 2s - loss: 0.6367 - acc: 0.847 - ETA: 2s - loss: 0.6356 - acc: 0.847 - ETA: 2s - loss: 0.6316 - acc: 0.847 - ETA: 1s - loss: 0.6261 - acc: 0.847 - ETA: 1s - loss: 0.6187 - acc: 0.849 - ETA: 1s - loss: 0.6162 - acc: 0.849 - ETA: 1s - loss: 0.6088 - acc: 0.851 - ETA: 1s - loss: 0.6074 - acc: 0.852 - ETA: 1s - loss: 0.6016 - acc: 0.852 - ETA: 1s - loss: 0.6016 - acc: 0.851 - ETA: 1s - loss: 0.6008 - acc: 0.852 - ETA: 1s - loss: 0.6027 - acc: 0.853 - ETA: 1s - loss: 0.5994 - acc: 0.852 - ETA: 1s - loss: 0.5971 - acc: 0.852 - ETA: 1s - loss: 0.5978 - acc: 0.852 - ETA: 0s - loss: 0.5972 - acc: 0.852 - ETA: 0s - loss: 0.5943 - acc: 0.852 - ETA: 0s - loss: 0.5932 - acc: 0.852 - ETA: 0s - loss: 0.5934 - acc: 0.852 - ETA: 0s - loss: 0.5881 - acc: 0.853 - ETA: 0s - loss: 0.5843 - acc: 0.854 - ETA: 0s - loss: 0.5841 - acc: 0.855 - ETA: 0s - loss: 0.5874 - acc: 0.854 - ETA: 0s - loss: 0.5875 - acc: 0.853 - ETA: 0s - loss: 0.5833 - acc: 0.854 - ETA: 0s - loss: 0.5786 - acc: 0.854 - ETA: 0s - loss: 0.5753 - acc: 0.853 - ETA: 0s - loss: 0.5732 - acc: 0.853 - ETA: 0s - loss: 0.5753 - acc: 0.852 - 8s 2ms/step - loss: 0.5730 - acc: 0.8532 - val_loss: 0.3949 - val_acc: 0.8795\n",
      "Epoch 2/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.3663 - acc: 0.875 - ETA: 6s - loss: 0.1649 - acc: 0.937 - ETA: 6s - loss: 0.1641 - acc: 0.937 - ETA: 6s - loss: 0.3284 - acc: 0.928 - ETA: 6s - loss: 0.2454 - acc: 0.950 - ETA: 5s - loss: 0.2127 - acc: 0.947 - ETA: 5s - loss: 0.2244 - acc: 0.937 - ETA: 5s - loss: 0.2216 - acc: 0.933 - ETA: 6s - loss: 0.2101 - acc: 0.934 - ETA: 6s - loss: 0.2439 - acc: 0.928 - ETA: 5s - loss: 0.2738 - acc: 0.927 - ETA: 5s - loss: 0.2554 - acc: 0.932 - ETA: 5s - loss: 0.3187 - acc: 0.926 - ETA: 5s - loss: 0.3147 - acc: 0.920 - ETA: 5s - loss: 0.3593 - acc: 0.907 - ETA: 5s - loss: 0.3491 - acc: 0.904 - ETA: 5s - loss: 0.3440 - acc: 0.905 - ETA: 5s - loss: 0.3344 - acc: 0.909 - ETA: 4s - loss: 0.3358 - acc: 0.904 - ETA: 4s - loss: 0.3298 - acc: 0.905 - ETA: 4s - loss: 0.3219 - acc: 0.908 - ETA: 4s - loss: 0.3133 - acc: 0.910 - ETA: 4s - loss: 0.3098 - acc: 0.906 - ETA: 4s - loss: 0.2998 - acc: 0.910 - ETA: 4s - loss: 0.2924 - acc: 0.910 - ETA: 4s - loss: 0.2879 - acc: 0.911 - ETA: 4s - loss: 0.2873 - acc: 0.911 - ETA: 4s - loss: 0.2891 - acc: 0.911 - ETA: 4s - loss: 0.2811 - acc: 0.913 - ETA: 4s - loss: 0.2778 - acc: 0.914 - ETA: 4s - loss: 0.2745 - acc: 0.915 - ETA: 3s - loss: 0.2717 - acc: 0.916 - ETA: 3s - loss: 0.2747 - acc: 0.915 - ETA: 3s - loss: 0.2819 - acc: 0.915 - ETA: 3s - loss: 0.2808 - acc: 0.913 - ETA: 3s - loss: 0.2810 - acc: 0.912 - ETA: 3s - loss: 0.2766 - acc: 0.912 - ETA: 3s - loss: 0.2770 - acc: 0.912 - ETA: 3s - loss: 0.2727 - acc: 0.913 - ETA: 3s - loss: 0.2716 - acc: 0.913 - ETA: 3s - loss: 0.2696 - acc: 0.914 - ETA: 3s - loss: 0.2691 - acc: 0.913 - ETA: 3s - loss: 0.2660 - acc: 0.914 - ETA: 3s - loss: 0.2631 - acc: 0.914 - ETA: 2s - loss: 0.2768 - acc: 0.913 - ETA: 2s - loss: 0.2898 - acc: 0.912 - ETA: 2s - loss: 0.3131 - acc: 0.910 - ETA: 2s - loss: 0.3217 - acc: 0.907 - ETA: 2s - loss: 0.3319 - acc: 0.907 - ETA: 2s - loss: 0.3431 - acc: 0.907 - ETA: 2s - loss: 0.3419 - acc: 0.907 - ETA: 2s - loss: 0.3387 - acc: 0.907 - ETA: 2s - loss: 0.3362 - acc: 0.908 - ETA: 2s - loss: 0.3419 - acc: 0.907 - ETA: 2s - loss: 0.3407 - acc: 0.908 - ETA: 2s - loss: 0.3386 - acc: 0.908 - ETA: 2s - loss: 0.3411 - acc: 0.908 - ETA: 2s - loss: 0.3403 - acc: 0.906 - ETA: 2s - loss: 0.3380 - acc: 0.907 - ETA: 2s - loss: 0.3349 - acc: 0.908 - ETA: 1s - loss: 0.3314 - acc: 0.908 - ETA: 1s - loss: 0.3281 - acc: 0.909 - ETA: 1s - loss: 0.3311 - acc: 0.907 - ETA: 1s - loss: 0.3287 - acc: 0.907 - ETA: 1s - loss: 0.3249 - acc: 0.908 - ETA: 1s - loss: 0.3254 - acc: 0.907 - ETA: 1s - loss: 0.3233 - acc: 0.906 - ETA: 1s - loss: 0.3219 - acc: 0.906 - ETA: 1s - loss: 0.3205 - acc: 0.906 - ETA: 1s - loss: 0.3195 - acc: 0.906 - ETA: 1s - loss: 0.3184 - acc: 0.906 - ETA: 1s - loss: 0.3217 - acc: 0.906 - ETA: 1s - loss: 0.3196 - acc: 0.907 - ETA: 1s - loss: 0.3196 - acc: 0.907 - ETA: 1s - loss: 0.3176 - acc: 0.907 - ETA: 1s - loss: 0.3156 - acc: 0.907 - ETA: 1s - loss: 0.3128 - acc: 0.907 - ETA: 0s - loss: 0.3113 - acc: 0.908 - ETA: 0s - loss: 0.3101 - acc: 0.908 - ETA: 0s - loss: 0.3080 - acc: 0.908 - ETA: 0s - loss: 0.3150 - acc: 0.908 - ETA: 0s - loss: 0.3136 - acc: 0.908 - ETA: 0s - loss: 0.3136 - acc: 0.907 - ETA: 0s - loss: 0.3160 - acc: 0.906 - ETA: 0s - loss: 0.3163 - acc: 0.905 - ETA: 0s - loss: 0.3142 - acc: 0.906 - ETA: 0s - loss: 0.3133 - acc: 0.906 - ETA: 0s - loss: 0.3123 - acc: 0.906 - ETA: 0s - loss: 0.3115 - acc: 0.906 - ETA: 0s - loss: 0.3098 - acc: 0.906 - ETA: 0s - loss: 0.3086 - acc: 0.907 - ETA: 0s - loss: 0.3084 - acc: 0.906 - ETA: 0s - loss: 0.3093 - acc: 0.906 - ETA: 0s - loss: 0.3088 - acc: 0.906 - ETA: 0s - loss: 0.3074 - acc: 0.906 - ETA: 0s - loss: 0.3067 - acc: 0.905 - 6s 1ms/step - loss: 0.3066 - acc: 0.9058 - val_loss: 0.3005 - val_acc: 0.8878\n",
      "Epoch 3/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1726 - acc: 0.875 - ETA: 5s - loss: 0.2146 - acc: 0.921 - ETA: 5s - loss: 0.2335 - acc: 0.892 - ETA: 5s - loss: 0.2138 - acc: 0.906 - ETA: 5s - loss: 0.1960 - acc: 0.913 - ETA: 4s - loss: 0.1752 - acc: 0.918 - ETA: 4s - loss: 0.1601 - acc: 0.927 - ETA: 4s - loss: 0.1532 - acc: 0.931 - ETA: 4s - loss: 0.1834 - acc: 0.922 - ETA: 4s - loss: 0.1807 - acc: 0.924 - ETA: 4s - loss: 0.1729 - acc: 0.925 - ETA: 4s - loss: 0.1703 - acc: 0.926 - ETA: 4s - loss: 0.1645 - acc: 0.930 - ETA: 4s - loss: 0.1611 - acc: 0.932 - ETA: 4s - loss: 0.1872 - acc: 0.928 - ETA: 4s - loss: 0.1826 - acc: 0.930 - ETA: 4s - loss: 0.1829 - acc: 0.929 - ETA: 4s - loss: 0.1820 - acc: 0.928 - ETA: 4s - loss: 0.1989 - acc: 0.928 - ETA: 4s - loss: 0.1980 - acc: 0.928 - ETA: 4s - loss: 0.1957 - acc: 0.928 - ETA: 3s - loss: 0.1929 - acc: 0.928 - ETA: 3s - loss: 0.1887 - acc: 0.930 - ETA: 3s - loss: 0.1920 - acc: 0.928 - ETA: 3s - loss: 0.1901 - acc: 0.927 - ETA: 3s - loss: 0.1874 - acc: 0.928 - ETA: 3s - loss: 0.1864 - acc: 0.928 - ETA: 3s - loss: 0.1865 - acc: 0.929 - ETA: 3s - loss: 0.1918 - acc: 0.927 - ETA: 3s - loss: 0.1979 - acc: 0.926 - ETA: 3s - loss: 0.1975 - acc: 0.923 - ETA: 3s - loss: 0.2017 - acc: 0.920 - ETA: 3s - loss: 0.1975 - acc: 0.923 - ETA: 3s - loss: 0.1959 - acc: 0.923 - ETA: 3s - loss: 0.1933 - acc: 0.924 - ETA: 3s - loss: 0.1950 - acc: 0.922 - ETA: 3s - loss: 0.1943 - acc: 0.921 - ETA: 2s - loss: 0.1925 - acc: 0.921 - ETA: 2s - loss: 0.1902 - acc: 0.922 - ETA: 2s - loss: 0.1890 - acc: 0.923 - ETA: 2s - loss: 0.1930 - acc: 0.921 - ETA: 2s - loss: 0.1921 - acc: 0.921 - ETA: 2s - loss: 0.1896 - acc: 0.923 - ETA: 2s - loss: 0.1888 - acc: 0.924 - ETA: 2s - loss: 0.1912 - acc: 0.923 - ETA: 2s - loss: 0.1895 - acc: 0.924 - ETA: 2s - loss: 0.1891 - acc: 0.923 - ETA: 2s - loss: 0.1899 - acc: 0.923 - ETA: 2s - loss: 0.1916 - acc: 0.923 - ETA: 2s - loss: 0.1904 - acc: 0.923 - ETA: 2s - loss: 0.1904 - acc: 0.922 - ETA: 2s - loss: 0.1915 - acc: 0.922 - ETA: 2s - loss: 0.1905 - acc: 0.922 - ETA: 2s - loss: 0.1912 - acc: 0.922 - ETA: 2s - loss: 0.1915 - acc: 0.922 - ETA: 1s - loss: 0.1910 - acc: 0.922 - ETA: 1s - loss: 0.1896 - acc: 0.922 - ETA: 1s - loss: 0.1898 - acc: 0.922 - ETA: 1s - loss: 0.1897 - acc: 0.922 - ETA: 1s - loss: 0.1905 - acc: 0.922 - ETA: 1s - loss: 0.1888 - acc: 0.922 - ETA: 1s - loss: 0.1891 - acc: 0.922 - ETA: 1s - loss: 0.1878 - acc: 0.922 - ETA: 1s - loss: 0.1860 - acc: 0.923 - ETA: 1s - loss: 0.1872 - acc: 0.922 - ETA: 1s - loss: 0.1866 - acc: 0.923 - ETA: 1s - loss: 0.1871 - acc: 0.922 - ETA: 1s - loss: 0.1870 - acc: 0.923 - ETA: 1s - loss: 0.1871 - acc: 0.923 - ETA: 1s - loss: 0.1865 - acc: 0.923 - ETA: 1s - loss: 0.1875 - acc: 0.922 - ETA: 1s - loss: 0.1877 - acc: 0.922 - ETA: 0s - loss: 0.1889 - acc: 0.922 - ETA: 0s - loss: 0.1911 - acc: 0.921 - ETA: 0s - loss: 0.1901 - acc: 0.922 - ETA: 0s - loss: 0.1917 - acc: 0.921 - ETA: 0s - loss: 0.1902 - acc: 0.922 - ETA: 0s - loss: 0.1894 - acc: 0.923 - ETA: 0s - loss: 0.1906 - acc: 0.922 - ETA: 0s - loss: 0.1899 - acc: 0.923 - ETA: 0s - loss: 0.1890 - acc: 0.923 - ETA: 0s - loss: 0.1897 - acc: 0.923 - ETA: 0s - loss: 0.1910 - acc: 0.922 - ETA: 0s - loss: 0.1905 - acc: 0.922 - ETA: 0s - loss: 0.1914 - acc: 0.922 - ETA: 0s - loss: 0.1922 - acc: 0.922 - ETA: 0s - loss: 0.1917 - acc: 0.922 - ETA: 0s - loss: 0.1918 - acc: 0.922 - 6s 1ms/step - loss: 0.1927 - acc: 0.9221 - val_loss: 0.2805 - val_acc: 0.8994\n",
      "Epoch 4/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0820 - acc: 1.000 - ETA: 4s - loss: 0.1479 - acc: 0.937 - ETA: 4s - loss: 0.1113 - acc: 0.955 - ETA: 4s - loss: 0.1438 - acc: 0.943 - ETA: 4s - loss: 0.1671 - acc: 0.927 - ETA: 4s - loss: 0.1849 - acc: 0.918 - ETA: 4s - loss: 0.1708 - acc: 0.921 - ETA: 4s - loss: 0.1605 - acc: 0.926 - ETA: 4s - loss: 0.1631 - acc: 0.925 - ETA: 4s - loss: 0.1621 - acc: 0.921 - ETA: 4s - loss: 0.1638 - acc: 0.925 - ETA: 4s - loss: 0.1605 - acc: 0.926 - ETA: 4s - loss: 0.1599 - acc: 0.927 - ETA: 4s - loss: 0.1528 - acc: 0.931 - ETA: 4s - loss: 0.1480 - acc: 0.931 - ETA: 4s - loss: 0.1434 - acc: 0.934 - ETA: 4s - loss: 0.1404 - acc: 0.936 - ETA: 3s - loss: 0.1438 - acc: 0.933 - ETA: 3s - loss: 0.1469 - acc: 0.933 - ETA: 3s - loss: 0.1437 - acc: 0.934 - ETA: 3s - loss: 0.1433 - acc: 0.935 - ETA: 3s - loss: 0.1451 - acc: 0.933 - ETA: 3s - loss: 0.1462 - acc: 0.932 - ETA: 3s - loss: 0.1554 - acc: 0.930 - ETA: 3s - loss: 0.1592 - acc: 0.931 - ETA: 3s - loss: 0.1641 - acc: 0.928 - ETA: 3s - loss: 0.1630 - acc: 0.929 - ETA: 3s - loss: 0.1676 - acc: 0.926 - ETA: 3s - loss: 0.1696 - acc: 0.926 - ETA: 3s - loss: 0.1715 - acc: 0.925 - ETA: 3s - loss: 0.1747 - acc: 0.924 - ETA: 3s - loss: 0.1721 - acc: 0.926 - ETA: 3s - loss: 0.1717 - acc: 0.926 - ETA: 3s - loss: 0.1750 - acc: 0.925 - ETA: 2s - loss: 0.1763 - acc: 0.924 - ETA: 2s - loss: 0.1732 - acc: 0.926 - ETA: 2s - loss: 0.1720 - acc: 0.927 - ETA: 2s - loss: 0.1717 - acc: 0.927 - ETA: 2s - loss: 0.1717 - acc: 0.927 - ETA: 2s - loss: 0.1693 - acc: 0.929 - ETA: 2s - loss: 0.1692 - acc: 0.928 - ETA: 2s - loss: 0.1692 - acc: 0.927 - ETA: 2s - loss: 0.1665 - acc: 0.928 - ETA: 2s - loss: 0.1668 - acc: 0.928 - ETA: 2s - loss: 0.1660 - acc: 0.928 - ETA: 2s - loss: 0.1654 - acc: 0.927 - ETA: 2s - loss: 0.1643 - acc: 0.928 - ETA: 2s - loss: 0.1669 - acc: 0.928 - ETA: 2s - loss: 0.1670 - acc: 0.927 - ETA: 2s - loss: 0.1704 - acc: 0.926 - ETA: 2s - loss: 0.1696 - acc: 0.928 - ETA: 1s - loss: 0.1687 - acc: 0.927 - ETA: 1s - loss: 0.1713 - acc: 0.928 - ETA: 1s - loss: 0.1710 - acc: 0.929 - ETA: 1s - loss: 0.1723 - acc: 0.929 - ETA: 1s - loss: 0.1741 - acc: 0.928 - ETA: 1s - loss: 0.1752 - acc: 0.927 - ETA: 1s - loss: 0.1806 - acc: 0.927 - ETA: 1s - loss: 0.1803 - acc: 0.927 - ETA: 1s - loss: 0.1805 - acc: 0.927 - ETA: 1s - loss: 0.1808 - acc: 0.927 - ETA: 1s - loss: 0.1815 - acc: 0.926 - ETA: 1s - loss: 0.1818 - acc: 0.927 - ETA: 1s - loss: 0.1834 - acc: 0.926 - ETA: 1s - loss: 0.1840 - acc: 0.926 - ETA: 1s - loss: 0.1834 - acc: 0.926 - ETA: 1s - loss: 0.1819 - acc: 0.927 - ETA: 1s - loss: 0.1820 - acc: 0.927 - ETA: 0s - loss: 0.1821 - acc: 0.926 - ETA: 0s - loss: 0.1837 - acc: 0.925 - ETA: 0s - loss: 0.1845 - acc: 0.924 - ETA: 0s - loss: 0.1845 - acc: 0.924 - ETA: 0s - loss: 0.1871 - acc: 0.923 - ETA: 0s - loss: 0.1879 - acc: 0.923 - ETA: 0s - loss: 0.1871 - acc: 0.923 - ETA: 0s - loss: 0.1897 - acc: 0.923 - ETA: 0s - loss: 0.1894 - acc: 0.923 - ETA: 0s - loss: 0.1896 - acc: 0.923 - ETA: 0s - loss: 0.1909 - acc: 0.922 - ETA: 0s - loss: 0.1907 - acc: 0.923 - ETA: 0s - loss: 0.1960 - acc: 0.921 - ETA: 0s - loss: 0.1960 - acc: 0.921 - ETA: 0s - loss: 0.1955 - acc: 0.922 - ETA: 0s - loss: 0.1958 - acc: 0.922 - ETA: 0s - loss: 0.1953 - acc: 0.922 - 5s 1ms/step - loss: 0.1948 - acc: 0.9225 - val_loss: 0.2599 - val_acc: 0.8949\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0882 - acc: 0.937 - ETA: 4s - loss: 0.1177 - acc: 0.953 - ETA: 4s - loss: 0.1730 - acc: 0.928 - ETA: 5s - loss: 0.1703 - acc: 0.930 - ETA: 5s - loss: 0.2781 - acc: 0.916 - ETA: 5s - loss: 0.3048 - acc: 0.916 - ETA: 4s - loss: 0.2912 - acc: 0.916 - ETA: 4s - loss: 0.2670 - acc: 0.922 - ETA: 4s - loss: 0.2562 - acc: 0.919 - ETA: 4s - loss: 0.2412 - acc: 0.921 - ETA: 4s - loss: 0.2787 - acc: 0.920 - ETA: 4s - loss: 0.2690 - acc: 0.920 - ETA: 4s - loss: 0.2697 - acc: 0.921 - ETA: 4s - loss: 0.2709 - acc: 0.921 - ETA: 4s - loss: 0.2819 - acc: 0.924 - ETA: 4s - loss: 0.3044 - acc: 0.919 - ETA: 4s - loss: 0.2912 - acc: 0.921 - ETA: 4s - loss: 0.3072 - acc: 0.919 - ETA: 4s - loss: 0.3081 - acc: 0.917 - ETA: 4s - loss: 0.3023 - acc: 0.920 - ETA: 3s - loss: 0.2970 - acc: 0.917 - ETA: 3s - loss: 0.2887 - acc: 0.919 - ETA: 3s - loss: 0.2879 - acc: 0.917 - ETA: 3s - loss: 0.2885 - acc: 0.913 - ETA: 3s - loss: 0.2832 - acc: 0.915 - ETA: 3s - loss: 0.2825 - acc: 0.913 - ETA: 3s - loss: 0.2793 - acc: 0.912 - ETA: 3s - loss: 0.2784 - acc: 0.912 - ETA: 3s - loss: 0.2772 - acc: 0.912 - ETA: 3s - loss: 0.2772 - acc: 0.910 - ETA: 3s - loss: 0.2910 - acc: 0.909 - ETA: 3s - loss: 0.2853 - acc: 0.910 - ETA: 3s - loss: 0.2807 - acc: 0.911 - ETA: 3s - loss: 0.2764 - acc: 0.911 - ETA: 3s - loss: 0.2715 - acc: 0.913 - ETA: 3s - loss: 0.2712 - acc: 0.912 - ETA: 2s - loss: 0.2677 - acc: 0.912 - ETA: 2s - loss: 0.2659 - acc: 0.911 - ETA: 2s - loss: 0.2651 - acc: 0.910 - ETA: 2s - loss: 0.2716 - acc: 0.910 - ETA: 2s - loss: 0.2692 - acc: 0.910 - ETA: 2s - loss: 0.2698 - acc: 0.911 - ETA: 2s - loss: 0.2665 - acc: 0.912 - ETA: 2s - loss: 0.2652 - acc: 0.911 - ETA: 2s - loss: 0.2624 - acc: 0.912 - ETA: 2s - loss: 0.2603 - acc: 0.912 - ETA: 2s - loss: 0.2599 - acc: 0.912 - ETA: 2s - loss: 0.2576 - acc: 0.912 - ETA: 2s - loss: 0.2556 - acc: 0.913 - ETA: 2s - loss: 0.2655 - acc: 0.912 - ETA: 2s - loss: 0.2631 - acc: 0.912 - ETA: 2s - loss: 0.2594 - acc: 0.913 - ETA: 2s - loss: 0.2601 - acc: 0.913 - ETA: 2s - loss: 0.2597 - acc: 0.913 - ETA: 2s - loss: 0.2581 - acc: 0.914 - ETA: 2s - loss: 0.2628 - acc: 0.912 - ETA: 1s - loss: 0.2603 - acc: 0.913 - ETA: 1s - loss: 0.2589 - acc: 0.913 - ETA: 1s - loss: 0.2570 - acc: 0.914 - ETA: 1s - loss: 0.2569 - acc: 0.914 - ETA: 1s - loss: 0.2561 - acc: 0.913 - ETA: 1s - loss: 0.2546 - acc: 0.914 - ETA: 1s - loss: 0.2710 - acc: 0.912 - ETA: 1s - loss: 0.2688 - acc: 0.913 - ETA: 1s - loss: 0.2652 - acc: 0.914 - ETA: 1s - loss: 0.2647 - acc: 0.914 - ETA: 1s - loss: 0.2670 - acc: 0.913 - ETA: 1s - loss: 0.2693 - acc: 0.911 - ETA: 1s - loss: 0.2666 - acc: 0.912 - ETA: 1s - loss: 0.2657 - acc: 0.912 - ETA: 1s - loss: 0.2667 - acc: 0.912 - ETA: 1s - loss: 0.2674 - acc: 0.911 - ETA: 1s - loss: 0.2722 - acc: 0.910 - ETA: 1s - loss: 0.2761 - acc: 0.910 - ETA: 1s - loss: 0.2761 - acc: 0.909 - ETA: 0s - loss: 0.2755 - acc: 0.909 - ETA: 0s - loss: 0.2751 - acc: 0.909 - ETA: 0s - loss: 0.2768 - acc: 0.908 - ETA: 0s - loss: 0.2746 - acc: 0.909 - ETA: 0s - loss: 0.2729 - acc: 0.909 - ETA: 0s - loss: 0.2743 - acc: 0.908 - ETA: 0s - loss: 0.2739 - acc: 0.908 - ETA: 0s - loss: 0.2738 - acc: 0.908 - ETA: 0s - loss: 0.2737 - acc: 0.907 - ETA: 0s - loss: 0.2749 - acc: 0.906 - ETA: 0s - loss: 0.2737 - acc: 0.907 - ETA: 0s - loss: 0.2742 - acc: 0.907 - ETA: 0s - loss: 0.2732 - acc: 0.907 - ETA: 0s - loss: 0.2720 - acc: 0.908 - ETA: 0s - loss: 0.2705 - acc: 0.908 - ETA: 0s - loss: 0.2681 - acc: 0.909 - ETA: 0s - loss: 0.2681 - acc: 0.909 - ETA: 0s - loss: 0.2714 - acc: 0.908 - 6s 1ms/step - loss: 0.2703 - acc: 0.9090 - val_loss: 0.3529 - val_acc: 0.8987\n",
      "Epoch 6/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.5667 - acc: 0.875 - ETA: 5s - loss: 0.3011 - acc: 0.937 - ETA: 5s - loss: 0.3400 - acc: 0.919 - ETA: 5s - loss: 0.4297 - acc: 0.909 - ETA: 6s - loss: 0.4001 - acc: 0.912 - ETA: 6s - loss: 0.3729 - acc: 0.906 - ETA: 6s - loss: 0.3541 - acc: 0.900 - ETA: 5s - loss: 0.3225 - acc: 0.909 - ETA: 5s - loss: 0.2954 - acc: 0.919 - ETA: 5s - loss: 0.2914 - acc: 0.916 - ETA: 5s - loss: 0.2743 - acc: 0.923 - ETA: 5s - loss: 0.2686 - acc: 0.920 - ETA: 5s - loss: 0.2647 - acc: 0.921 - ETA: 4s - loss: 0.2800 - acc: 0.923 - ETA: 4s - loss: 0.2715 - acc: 0.925 - ETA: 4s - loss: 0.2670 - acc: 0.924 - ETA: 4s - loss: 0.2560 - acc: 0.928 - ETA: 4s - loss: 0.2508 - acc: 0.927 - ETA: 4s - loss: 0.2435 - acc: 0.929 - ETA: 4s - loss: 0.2396 - acc: 0.928 - ETA: 4s - loss: 0.2574 - acc: 0.925 - ETA: 4s - loss: 0.2629 - acc: 0.924 - ETA: 4s - loss: 0.2562 - acc: 0.924 - ETA: 4s - loss: 0.2531 - acc: 0.926 - ETA: 4s - loss: 0.2518 - acc: 0.925 - ETA: 4s - loss: 0.2493 - acc: 0.925 - ETA: 4s - loss: 0.2473 - acc: 0.926 - ETA: 4s - loss: 0.2503 - acc: 0.922 - ETA: 4s - loss: 0.2502 - acc: 0.921 - ETA: 4s - loss: 0.2479 - acc: 0.922 - ETA: 3s - loss: 0.2578 - acc: 0.922 - ETA: 3s - loss: 0.2552 - acc: 0.922 - ETA: 3s - loss: 0.2505 - acc: 0.922 - ETA: 3s - loss: 0.2582 - acc: 0.921 - ETA: 3s - loss: 0.2553 - acc: 0.921 - ETA: 3s - loss: 0.2648 - acc: 0.921 - ETA: 3s - loss: 0.2615 - acc: 0.921 - ETA: 3s - loss: 0.2601 - acc: 0.921 - ETA: 3s - loss: 0.2658 - acc: 0.921 - ETA: 3s - loss: 0.2602 - acc: 0.923 - ETA: 3s - loss: 0.2709 - acc: 0.921 - ETA: 3s - loss: 0.2669 - acc: 0.923 - ETA: 3s - loss: 0.2700 - acc: 0.924 - ETA: 2s - loss: 0.2831 - acc: 0.924 - ETA: 2s - loss: 0.2815 - acc: 0.923 - ETA: 2s - loss: 0.2773 - acc: 0.924 - ETA: 2s - loss: 0.2834 - acc: 0.924 - ETA: 2s - loss: 0.2803 - acc: 0.925 - ETA: 2s - loss: 0.2762 - acc: 0.925 - ETA: 2s - loss: 0.2732 - acc: 0.926 - ETA: 2s - loss: 0.2718 - acc: 0.925 - ETA: 2s - loss: 0.2796 - acc: 0.924 - ETA: 2s - loss: 0.2808 - acc: 0.924 - ETA: 2s - loss: 0.2893 - acc: 0.922 - ETA: 2s - loss: 0.2894 - acc: 0.922 - ETA: 2s - loss: 0.2895 - acc: 0.921 - ETA: 2s - loss: 0.2947 - acc: 0.920 - ETA: 1s - loss: 0.2917 - acc: 0.921 - ETA: 1s - loss: 0.2887 - acc: 0.921 - ETA: 1s - loss: 0.2864 - acc: 0.922 - ETA: 1s - loss: 0.2891 - acc: 0.922 - ETA: 1s - loss: 0.2872 - acc: 0.922 - ETA: 1s - loss: 0.2840 - acc: 0.923 - ETA: 1s - loss: 0.2826 - acc: 0.923 - ETA: 1s - loss: 0.2815 - acc: 0.923 - ETA: 1s - loss: 0.2792 - acc: 0.924 - ETA: 1s - loss: 0.2857 - acc: 0.924 - ETA: 1s - loss: 0.2833 - acc: 0.924 - ETA: 1s - loss: 0.2832 - acc: 0.924 - ETA: 1s - loss: 0.2873 - acc: 0.924 - ETA: 1s - loss: 0.2866 - acc: 0.924 - ETA: 1s - loss: 0.2900 - acc: 0.923 - ETA: 1s - loss: 0.2873 - acc: 0.924 - ETA: 1s - loss: 0.2854 - acc: 0.925 - ETA: 1s - loss: 0.2859 - acc: 0.924 - ETA: 0s - loss: 0.2849 - acc: 0.923 - ETA: 0s - loss: 0.2834 - acc: 0.923 - ETA: 0s - loss: 0.2810 - acc: 0.924 - ETA: 0s - loss: 0.2809 - acc: 0.924 - ETA: 0s - loss: 0.2792 - acc: 0.924 - ETA: 0s - loss: 0.2784 - acc: 0.924 - ETA: 0s - loss: 0.2763 - acc: 0.924 - ETA: 0s - loss: 0.2742 - acc: 0.924 - ETA: 0s - loss: 0.2719 - acc: 0.924 - ETA: 0s - loss: 0.2729 - acc: 0.924 - ETA: 0s - loss: 0.2741 - acc: 0.923 - ETA: 0s - loss: 0.2765 - acc: 0.923 - ETA: 0s - loss: 0.2741 - acc: 0.923 - ETA: 0s - loss: 0.2770 - acc: 0.923 - ETA: 0s - loss: 0.2769 - acc: 0.922 - 6s 1ms/step - loss: 0.2757 - acc: 0.9230 - val_loss: 0.2442 - val_acc: 0.9212\n",
      "Epoch 7/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.1000 - acc: 0.937 - ETA: 4s - loss: 0.4048 - acc: 0.937 - ETA: 4s - loss: 0.2803 - acc: 0.937 - ETA: 4s - loss: 0.2565 - acc: 0.931 - ETA: 4s - loss: 0.2476 - acc: 0.923 - ETA: 4s - loss: 0.3021 - acc: 0.918 - ETA: 4s - loss: 0.2898 - acc: 0.917 - ETA: 4s - loss: 0.2842 - acc: 0.917 - ETA: 4s - loss: 0.2700 - acc: 0.915 - ETA: 4s - loss: 0.2652 - acc: 0.912 - ETA: 4s - loss: 0.2511 - acc: 0.917 - ETA: 4s - loss: 0.2361 - acc: 0.924 - ETA: 4s - loss: 0.2299 - acc: 0.925 - ETA: 4s - loss: 0.2186 - acc: 0.928 - ETA: 4s - loss: 0.2090 - acc: 0.931 - ETA: 4s - loss: 0.2049 - acc: 0.930 - ETA: 4s - loss: 0.2064 - acc: 0.928 - ETA: 4s - loss: 0.2032 - acc: 0.927 - ETA: 3s - loss: 0.2231 - acc: 0.926 - ETA: 3s - loss: 0.2371 - acc: 0.922 - ETA: 3s - loss: 0.2366 - acc: 0.922 - ETA: 3s - loss: 0.2354 - acc: 0.920 - ETA: 3s - loss: 0.2345 - acc: 0.920 - ETA: 3s - loss: 0.2329 - acc: 0.921 - ETA: 3s - loss: 0.2309 - acc: 0.920 - ETA: 3s - loss: 0.2304 - acc: 0.920 - ETA: 3s - loss: 0.2283 - acc: 0.920 - ETA: 3s - loss: 0.2280 - acc: 0.919 - ETA: 3s - loss: 0.2382 - acc: 0.919 - ETA: 3s - loss: 0.2380 - acc: 0.920 - ETA: 3s - loss: 0.2375 - acc: 0.918 - ETA: 3s - loss: 0.2387 - acc: 0.917 - ETA: 3s - loss: 0.2455 - acc: 0.917 - ETA: 3s - loss: 0.2425 - acc: 0.918 - ETA: 3s - loss: 0.2421 - acc: 0.918 - ETA: 3s - loss: 0.2415 - acc: 0.918 - ETA: 3s - loss: 0.2419 - acc: 0.917 - ETA: 3s - loss: 0.2489 - acc: 0.917 - ETA: 3s - loss: 0.2471 - acc: 0.918 - ETA: 2s - loss: 0.2477 - acc: 0.916 - ETA: 2s - loss: 0.2466 - acc: 0.916 - ETA: 2s - loss: 0.2431 - acc: 0.917 - ETA: 2s - loss: 0.2478 - acc: 0.918 - ETA: 2s - loss: 0.2473 - acc: 0.917 - ETA: 2s - loss: 0.2497 - acc: 0.915 - ETA: 2s - loss: 0.2502 - acc: 0.913 - ETA: 2s - loss: 0.2487 - acc: 0.914 - ETA: 2s - loss: 0.2490 - acc: 0.913 - ETA: 2s - loss: 0.2464 - acc: 0.914 - ETA: 2s - loss: 0.2436 - acc: 0.915 - ETA: 2s - loss: 0.2434 - acc: 0.914 - ETA: 2s - loss: 0.2461 - acc: 0.913 - ETA: 2s - loss: 0.2437 - acc: 0.914 - ETA: 2s - loss: 0.2448 - acc: 0.914 - ETA: 1s - loss: 0.2458 - acc: 0.914 - ETA: 1s - loss: 0.2452 - acc: 0.914 - ETA: 1s - loss: 0.2429 - acc: 0.915 - ETA: 1s - loss: 0.2432 - acc: 0.915 - ETA: 1s - loss: 0.2467 - acc: 0.915 - ETA: 1s - loss: 0.2486 - acc: 0.915 - ETA: 1s - loss: 0.2471 - acc: 0.915 - ETA: 1s - loss: 0.2480 - acc: 0.915 - ETA: 1s - loss: 0.2537 - acc: 0.914 - ETA: 1s - loss: 0.2526 - acc: 0.915 - ETA: 1s - loss: 0.2509 - acc: 0.915 - ETA: 1s - loss: 0.2491 - acc: 0.916 - ETA: 1s - loss: 0.2458 - acc: 0.917 - ETA: 1s - loss: 0.2510 - acc: 0.917 - ETA: 1s - loss: 0.2514 - acc: 0.917 - ETA: 1s - loss: 0.2500 - acc: 0.917 - ETA: 1s - loss: 0.2501 - acc: 0.916 - ETA: 0s - loss: 0.2485 - acc: 0.916 - ETA: 0s - loss: 0.2471 - acc: 0.917 - ETA: 0s - loss: 0.2471 - acc: 0.917 - ETA: 0s - loss: 0.2449 - acc: 0.918 - ETA: 0s - loss: 0.2434 - acc: 0.918 - ETA: 0s - loss: 0.2434 - acc: 0.918 - ETA: 0s - loss: 0.2470 - acc: 0.917 - ETA: 0s - loss: 0.2463 - acc: 0.917 - ETA: 0s - loss: 0.2439 - acc: 0.918 - ETA: 0s - loss: 0.2417 - acc: 0.919 - ETA: 0s - loss: 0.2405 - acc: 0.919 - ETA: 0s - loss: 0.2387 - acc: 0.919 - ETA: 0s - loss: 0.2414 - acc: 0.920 - ETA: 0s - loss: 0.2440 - acc: 0.920 - ETA: 0s - loss: 0.2484 - acc: 0.919 - ETA: 0s - loss: 0.2507 - acc: 0.919 - 5s 1ms/step - loss: 0.2499 - acc: 0.9198 - val_loss: 0.2519 - val_acc: 0.9135\n",
      "Epoch 8/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0722 - acc: 1.000 - ETA: 4s - loss: 0.1435 - acc: 0.953 - ETA: 4s - loss: 0.1766 - acc: 0.928 - ETA: 4s - loss: 0.1979 - acc: 0.906 - ETA: 4s - loss: 0.2085 - acc: 0.908 - ETA: 4s - loss: 0.1922 - acc: 0.918 - ETA: 4s - loss: 0.1821 - acc: 0.921 - ETA: 4s - loss: 0.2152 - acc: 0.929 - ETA: 4s - loss: 0.2069 - acc: 0.932 - ETA: 4s - loss: 0.2039 - acc: 0.928 - ETA: 4s - loss: 0.1979 - acc: 0.929 - ETA: 4s - loss: 0.1918 - acc: 0.930 - ETA: 4s - loss: 0.1930 - acc: 0.929 - ETA: 4s - loss: 0.1902 - acc: 0.929 - ETA: 4s - loss: 0.1822 - acc: 0.931 - ETA: 4s - loss: 0.1821 - acc: 0.932 - ETA: 4s - loss: 0.1917 - acc: 0.927 - ETA: 3s - loss: 0.1850 - acc: 0.930 - ETA: 3s - loss: 0.1812 - acc: 0.930 - ETA: 3s - loss: 0.1762 - acc: 0.933 - ETA: 3s - loss: 0.1732 - acc: 0.934 - ETA: 3s - loss: 0.1711 - acc: 0.934 - ETA: 3s - loss: 0.1758 - acc: 0.933 - ETA: 3s - loss: 0.1731 - acc: 0.933 - ETA: 3s - loss: 0.1879 - acc: 0.933 - ETA: 3s - loss: 0.1905 - acc: 0.932 - ETA: 3s - loss: 0.1932 - acc: 0.932 - ETA: 3s - loss: 0.2047 - acc: 0.931 - ETA: 3s - loss: 0.2151 - acc: 0.931 - ETA: 3s - loss: 0.2305 - acc: 0.928 - ETA: 3s - loss: 0.2413 - acc: 0.926 - ETA: 3s - loss: 0.2544 - acc: 0.922 - ETA: 3s - loss: 0.2520 - acc: 0.921 - ETA: 3s - loss: 0.2479 - acc: 0.922 - ETA: 3s - loss: 0.2636 - acc: 0.922 - ETA: 3s - loss: 0.2607 - acc: 0.923 - ETA: 3s - loss: 0.2812 - acc: 0.921 - ETA: 3s - loss: 0.3000 - acc: 0.921 - ETA: 3s - loss: 0.3008 - acc: 0.919 - ETA: 2s - loss: 0.3156 - acc: 0.919 - ETA: 2s - loss: 0.3135 - acc: 0.918 - ETA: 2s - loss: 0.3090 - acc: 0.919 - ETA: 2s - loss: 0.3140 - acc: 0.918 - ETA: 2s - loss: 0.3117 - acc: 0.918 - ETA: 2s - loss: 0.3202 - acc: 0.919 - ETA: 2s - loss: 0.3236 - acc: 0.918 - ETA: 2s - loss: 0.3248 - acc: 0.918 - ETA: 2s - loss: 0.3211 - acc: 0.918 - ETA: 2s - loss: 0.3166 - acc: 0.918 - ETA: 2s - loss: 0.3289 - acc: 0.917 - ETA: 2s - loss: 0.3293 - acc: 0.916 - ETA: 2s - loss: 0.3316 - acc: 0.916 - ETA: 2s - loss: 0.3538 - acc: 0.915 - ETA: 2s - loss: 0.3689 - acc: 0.915 - ETA: 1s - loss: 0.4053 - acc: 0.912 - ETA: 1s - loss: 0.4414 - acc: 0.910 - ETA: 1s - loss: 0.4364 - acc: 0.911 - ETA: 1s - loss: 0.4323 - acc: 0.912 - ETA: 1s - loss: 0.4337 - acc: 0.911 - ETA: 1s - loss: 0.4477 - acc: 0.908 - ETA: 1s - loss: 0.4501 - acc: 0.907 - ETA: 1s - loss: 0.4506 - acc: 0.907 - ETA: 1s - loss: 0.4480 - acc: 0.907 - ETA: 1s - loss: 0.4485 - acc: 0.906 - ETA: 1s - loss: 0.4428 - acc: 0.907 - ETA: 1s - loss: 0.4473 - acc: 0.908 - ETA: 1s - loss: 0.4446 - acc: 0.908 - ETA: 1s - loss: 0.4416 - acc: 0.908 - ETA: 1s - loss: 0.4414 - acc: 0.909 - ETA: 0s - loss: 0.4481 - acc: 0.908 - ETA: 0s - loss: 0.4488 - acc: 0.908 - ETA: 0s - loss: 0.4547 - acc: 0.908 - ETA: 0s - loss: 0.4632 - acc: 0.908 - ETA: 0s - loss: 0.4683 - acc: 0.908 - ETA: 0s - loss: 0.4661 - acc: 0.907 - ETA: 0s - loss: 0.4688 - acc: 0.907 - ETA: 0s - loss: 0.4653 - acc: 0.907 - ETA: 0s - loss: 0.4641 - acc: 0.908 - ETA: 0s - loss: 0.4623 - acc: 0.907 - ETA: 0s - loss: 0.4627 - acc: 0.907 - ETA: 0s - loss: 0.4588 - acc: 0.908 - ETA: 0s - loss: 0.4651 - acc: 0.906 - ETA: 0s - loss: 0.4656 - acc: 0.906 - ETA: 0s - loss: 0.4777 - acc: 0.906 - ETA: 0s - loss: 0.4818 - acc: 0.906 - 5s 1ms/step - loss: 0.4832 - acc: 0.9068 - val_loss: 0.3702 - val_acc: 0.8846\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1672 - acc: 0.875 - ETA: 5s - loss: 0.7712 - acc: 0.875 - ETA: 4s - loss: 0.7803 - acc: 0.883 - ETA: 4s - loss: 0.8787 - acc: 0.887 - ETA: 4s - loss: 0.7089 - acc: 0.899 - ETA: 4s - loss: 0.6846 - acc: 0.898 - ETA: 4s - loss: 0.7277 - acc: 0.884 - ETA: 4s - loss: 0.7521 - acc: 0.875 - ETA: 4s - loss: 0.7244 - acc: 0.880 - ETA: 4s - loss: 0.6676 - acc: 0.883 - ETA: 4s - loss: 0.6861 - acc: 0.885 - ETA: 4s - loss: 0.6710 - acc: 0.886 - ETA: 4s - loss: 0.6855 - acc: 0.885 - ETA: 4s - loss: 0.6725 - acc: 0.884 - ETA: 4s - loss: 0.6856 - acc: 0.883 - ETA: 4s - loss: 0.6690 - acc: 0.888 - ETA: 4s - loss: 0.6600 - acc: 0.887 - ETA: 3s - loss: 0.6505 - acc: 0.888 - ETA: 3s - loss: 0.6230 - acc: 0.890 - ETA: 3s - loss: 0.6319 - acc: 0.891 - ETA: 3s - loss: 0.6506 - acc: 0.889 - ETA: 3s - loss: 0.6424 - acc: 0.891 - ETA: 3s - loss: 0.6167 - acc: 0.895 - ETA: 3s - loss: 0.6136 - acc: 0.894 - ETA: 3s - loss: 0.6283 - acc: 0.892 - ETA: 3s - loss: 0.6124 - acc: 0.892 - ETA: 3s - loss: 0.5955 - acc: 0.893 - ETA: 3s - loss: 0.5771 - acc: 0.897 - ETA: 3s - loss: 0.5724 - acc: 0.899 - ETA: 3s - loss: 0.5840 - acc: 0.898 - ETA: 3s - loss: 0.5742 - acc: 0.899 - ETA: 3s - loss: 0.5633 - acc: 0.900 - ETA: 3s - loss: 0.5600 - acc: 0.900 - ETA: 3s - loss: 0.5455 - acc: 0.901 - ETA: 3s - loss: 0.5446 - acc: 0.901 - ETA: 3s - loss: 0.5347 - acc: 0.902 - ETA: 2s - loss: 0.5323 - acc: 0.903 - ETA: 2s - loss: 0.5213 - acc: 0.904 - ETA: 2s - loss: 0.5233 - acc: 0.904 - ETA: 2s - loss: 0.5181 - acc: 0.905 - ETA: 2s - loss: 0.5107 - acc: 0.906 - ETA: 2s - loss: 0.5134 - acc: 0.906 - ETA: 2s - loss: 0.5028 - acc: 0.908 - ETA: 2s - loss: 0.5032 - acc: 0.908 - ETA: 2s - loss: 0.4997 - acc: 0.907 - ETA: 2s - loss: 0.5075 - acc: 0.907 - ETA: 2s - loss: 0.5085 - acc: 0.906 - ETA: 2s - loss: 0.5006 - acc: 0.907 - ETA: 2s - loss: 0.4926 - acc: 0.908 - ETA: 2s - loss: 0.4845 - acc: 0.909 - ETA: 2s - loss: 0.5013 - acc: 0.908 - ETA: 2s - loss: 0.5073 - acc: 0.907 - ETA: 2s - loss: 0.5092 - acc: 0.907 - ETA: 2s - loss: 0.5103 - acc: 0.907 - ETA: 2s - loss: 0.5074 - acc: 0.906 - ETA: 2s - loss: 0.5077 - acc: 0.907 - ETA: 2s - loss: 0.5025 - acc: 0.907 - ETA: 2s - loss: 0.5031 - acc: 0.908 - ETA: 1s - loss: 0.5051 - acc: 0.908 - ETA: 1s - loss: 0.5041 - acc: 0.907 - ETA: 1s - loss: 0.4987 - acc: 0.908 - ETA: 1s - loss: 0.4934 - acc: 0.908 - ETA: 1s - loss: 0.4903 - acc: 0.908 - ETA: 1s - loss: 0.4860 - acc: 0.909 - ETA: 1s - loss: 0.4934 - acc: 0.908 - ETA: 1s - loss: 0.4915 - acc: 0.907 - ETA: 1s - loss: 0.4875 - acc: 0.908 - ETA: 1s - loss: 0.4833 - acc: 0.908 - ETA: 1s - loss: 0.4780 - acc: 0.908 - ETA: 1s - loss: 0.4828 - acc: 0.909 - ETA: 1s - loss: 0.4765 - acc: 0.910 - ETA: 1s - loss: 0.4770 - acc: 0.910 - ETA: 1s - loss: 0.4752 - acc: 0.910 - ETA: 1s - loss: 0.4733 - acc: 0.910 - ETA: 1s - loss: 0.4704 - acc: 0.910 - ETA: 1s - loss: 0.4778 - acc: 0.910 - ETA: 0s - loss: 0.4794 - acc: 0.910 - ETA: 0s - loss: 0.4771 - acc: 0.911 - ETA: 0s - loss: 0.4735 - acc: 0.912 - ETA: 0s - loss: 0.4800 - acc: 0.912 - ETA: 0s - loss: 0.4809 - acc: 0.911 - ETA: 0s - loss: 0.4785 - acc: 0.911 - ETA: 0s - loss: 0.4755 - acc: 0.911 - ETA: 0s - loss: 0.4745 - acc: 0.911 - ETA: 0s - loss: 0.4755 - acc: 0.911 - ETA: 0s - loss: 0.4727 - acc: 0.911 - ETA: 0s - loss: 0.4737 - acc: 0.912 - ETA: 0s - loss: 0.4703 - acc: 0.912 - ETA: 0s - loss: 0.4677 - acc: 0.912 - ETA: 0s - loss: 0.4640 - acc: 0.913 - ETA: 0s - loss: 0.4654 - acc: 0.913 - ETA: 0s - loss: 0.4626 - acc: 0.913 - ETA: 0s - loss: 0.4659 - acc: 0.913 - ETA: 0s - loss: 0.4632 - acc: 0.913 - ETA: 0s - loss: 0.4599 - acc: 0.913 - ETA: 0s - loss: 0.4661 - acc: 0.913 - ETA: 0s - loss: 0.4631 - acc: 0.914 - 6s 2ms/step - loss: 0.4616 - acc: 0.9142 - val_loss: 0.3087 - val_acc: 0.8917\n",
      "Epoch 10/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 1.2396 - acc: 0.812 - ETA: 6s - loss: 0.5022 - acc: 0.916 - ETA: 6s - loss: 0.3964 - acc: 0.925 - ETA: 6s - loss: 0.4455 - acc: 0.937 - ETA: 6s - loss: 0.3892 - acc: 0.937 - ETA: 6s - loss: 0.3061 - acc: 0.947 - ETA: 5s - loss: 0.3445 - acc: 0.950 - ETA: 5s - loss: 0.3199 - acc: 0.944 - ETA: 5s - loss: 0.3221 - acc: 0.940 - ETA: 5s - loss: 0.3011 - acc: 0.937 - ETA: 5s - loss: 0.3193 - acc: 0.935 - ETA: 5s - loss: 0.3182 - acc: 0.927 - ETA: 5s - loss: 0.3429 - acc: 0.921 - ETA: 5s - loss: 0.3551 - acc: 0.923 - ETA: 5s - loss: 0.4090 - acc: 0.917 - ETA: 5s - loss: 0.3993 - acc: 0.915 - ETA: 5s - loss: 0.3946 - acc: 0.911 - ETA: 5s - loss: 0.4023 - acc: 0.911 - ETA: 5s - loss: 0.4103 - acc: 0.910 - ETA: 4s - loss: 0.3963 - acc: 0.911 - ETA: 4s - loss: 0.3787 - acc: 0.915 - ETA: 4s - loss: 0.3885 - acc: 0.911 - ETA: 4s - loss: 0.4054 - acc: 0.913 - ETA: 4s - loss: 0.4425 - acc: 0.912 - ETA: 4s - loss: 0.4291 - acc: 0.912 - ETA: 4s - loss: 0.4236 - acc: 0.913 - ETA: 4s - loss: 0.4322 - acc: 0.912 - ETA: 4s - loss: 0.4244 - acc: 0.914 - ETA: 4s - loss: 0.4240 - acc: 0.915 - ETA: 4s - loss: 0.4222 - acc: 0.916 - ETA: 4s - loss: 0.4266 - acc: 0.917 - ETA: 4s - loss: 0.4203 - acc: 0.918 - ETA: 4s - loss: 0.4192 - acc: 0.919 - ETA: 4s - loss: 0.4192 - acc: 0.919 - ETA: 4s - loss: 0.4217 - acc: 0.917 - ETA: 3s - loss: 0.4466 - acc: 0.915 - ETA: 3s - loss: 0.4465 - acc: 0.917 - ETA: 3s - loss: 0.4466 - acc: 0.917 - ETA: 3s - loss: 0.4372 - acc: 0.918 - ETA: 3s - loss: 0.4369 - acc: 0.919 - ETA: 3s - loss: 0.4295 - acc: 0.918 - ETA: 3s - loss: 0.4305 - acc: 0.918 - ETA: 3s - loss: 0.4364 - acc: 0.918 - ETA: 3s - loss: 0.4289 - acc: 0.917 - ETA: 3s - loss: 0.4295 - acc: 0.918 - ETA: 3s - loss: 0.4252 - acc: 0.917 - ETA: 3s - loss: 0.4179 - acc: 0.918 - ETA: 3s - loss: 0.4198 - acc: 0.918 - ETA: 3s - loss: 0.4153 - acc: 0.918 - ETA: 3s - loss: 0.4113 - acc: 0.918 - ETA: 2s - loss: 0.4066 - acc: 0.919 - ETA: 2s - loss: 0.4112 - acc: 0.918 - ETA: 2s - loss: 0.4143 - acc: 0.919 - ETA: 2s - loss: 0.4174 - acc: 0.919 - ETA: 2s - loss: 0.4134 - acc: 0.920 - ETA: 2s - loss: 0.4080 - acc: 0.920 - ETA: 2s - loss: 0.4069 - acc: 0.920 - ETA: 2s - loss: 0.4014 - acc: 0.920 - ETA: 2s - loss: 0.3978 - acc: 0.920 - ETA: 2s - loss: 0.4000 - acc: 0.920 - ETA: 2s - loss: 0.4069 - acc: 0.920 - ETA: 2s - loss: 0.4013 - acc: 0.921 - ETA: 2s - loss: 0.4037 - acc: 0.921 - ETA: 2s - loss: 0.4061 - acc: 0.921 - ETA: 2s - loss: 0.4111 - acc: 0.922 - ETA: 1s - loss: 0.4123 - acc: 0.920 - ETA: 1s - loss: 0.4176 - acc: 0.919 - ETA: 1s - loss: 0.4122 - acc: 0.920 - ETA: 1s - loss: 0.4144 - acc: 0.919 - ETA: 1s - loss: 0.4113 - acc: 0.920 - ETA: 1s - loss: 0.4126 - acc: 0.920 - ETA: 1s - loss: 0.4208 - acc: 0.918 - ETA: 1s - loss: 0.4227 - acc: 0.918 - ETA: 1s - loss: 0.4247 - acc: 0.918 - ETA: 1s - loss: 0.4251 - acc: 0.918 - ETA: 1s - loss: 0.4314 - acc: 0.917 - ETA: 1s - loss: 0.4257 - acc: 0.919 - ETA: 1s - loss: 0.4247 - acc: 0.918 - ETA: 1s - loss: 0.4267 - acc: 0.918 - ETA: 1s - loss: 0.4275 - acc: 0.918 - ETA: 0s - loss: 0.4283 - acc: 0.917 - ETA: 0s - loss: 0.4293 - acc: 0.917 - ETA: 0s - loss: 0.4264 - acc: 0.917 - ETA: 0s - loss: 0.4236 - acc: 0.917 - ETA: 0s - loss: 0.4258 - acc: 0.916 - ETA: 0s - loss: 0.4269 - acc: 0.916 - ETA: 0s - loss: 0.4237 - acc: 0.915 - ETA: 0s - loss: 0.4210 - acc: 0.916 - ETA: 0s - loss: 0.4173 - acc: 0.916 - ETA: 0s - loss: 0.4216 - acc: 0.916 - ETA: 0s - loss: 0.4199 - acc: 0.916 - ETA: 0s - loss: 0.4170 - acc: 0.915 - ETA: 0s - loss: 0.4146 - acc: 0.916 - ETA: 0s - loss: 0.4160 - acc: 0.916 - ETA: 0s - loss: 0.4134 - acc: 0.916 - 6s 2ms/step - loss: 0.4117 - acc: 0.9169 - val_loss: 0.2600 - val_acc: 0.9212\n",
      "Epoch 11/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0877 - acc: 0.875 - ETA: 5s - loss: 0.4498 - acc: 0.906 - ETA: 5s - loss: 0.6690 - acc: 0.892 - ETA: 5s - loss: 0.4890 - acc: 0.918 - ETA: 5s - loss: 0.4262 - acc: 0.923 - ETA: 5s - loss: 0.4032 - acc: 0.906 - ETA: 5s - loss: 0.4827 - acc: 0.904 - ETA: 5s - loss: 0.5986 - acc: 0.898 - ETA: 5s - loss: 0.5623 - acc: 0.902 - ETA: 5s - loss: 0.5430 - acc: 0.902 - ETA: 5s - loss: 0.5162 - acc: 0.900 - ETA: 5s - loss: 0.5466 - acc: 0.900 - ETA: 5s - loss: 0.5141 - acc: 0.901 - ETA: 4s - loss: 0.4920 - acc: 0.899 - ETA: 4s - loss: 0.4975 - acc: 0.901 - ETA: 4s - loss: 0.4729 - acc: 0.905 - ETA: 4s - loss: 0.4959 - acc: 0.904 - ETA: 4s - loss: 0.4866 - acc: 0.909 - ETA: 4s - loss: 0.4828 - acc: 0.911 - ETA: 4s - loss: 0.4602 - acc: 0.912 - ETA: 4s - loss: 0.4515 - acc: 0.912 - ETA: 4s - loss: 0.4385 - acc: 0.912 - ETA: 4s - loss: 0.4363 - acc: 0.914 - ETA: 4s - loss: 0.4373 - acc: 0.915 - ETA: 4s - loss: 0.4572 - acc: 0.913 - ETA: 4s - loss: 0.4442 - acc: 0.912 - ETA: 4s - loss: 0.4317 - acc: 0.913 - ETA: 3s - loss: 0.4233 - acc: 0.913 - ETA: 3s - loss: 0.4260 - acc: 0.912 - ETA: 3s - loss: 0.4187 - acc: 0.914 - ETA: 3s - loss: 0.4099 - acc: 0.916 - ETA: 3s - loss: 0.4162 - acc: 0.918 - ETA: 3s - loss: 0.4097 - acc: 0.918 - ETA: 3s - loss: 0.3985 - acc: 0.920 - ETA: 3s - loss: 0.3971 - acc: 0.915 - ETA: 3s - loss: 0.3932 - acc: 0.916 - ETA: 3s - loss: 0.3915 - acc: 0.915 - ETA: 3s - loss: 0.3886 - acc: 0.915 - ETA: 3s - loss: 0.3831 - acc: 0.917 - ETA: 3s - loss: 0.3846 - acc: 0.918 - ETA: 3s - loss: 0.3779 - acc: 0.918 - ETA: 3s - loss: 0.3700 - acc: 0.920 - ETA: 2s - loss: 0.3656 - acc: 0.920 - ETA: 2s - loss: 0.3909 - acc: 0.917 - ETA: 2s - loss: 0.3844 - acc: 0.917 - ETA: 2s - loss: 0.3871 - acc: 0.917 - ETA: 2s - loss: 0.3952 - acc: 0.917 - ETA: 2s - loss: 0.4085 - acc: 0.917 - ETA: 2s - loss: 0.4101 - acc: 0.916 - ETA: 2s - loss: 0.4046 - acc: 0.917 - ETA: 2s - loss: 0.4000 - acc: 0.916 - ETA: 2s - loss: 0.4079 - acc: 0.916 - ETA: 2s - loss: 0.4019 - acc: 0.917 - ETA: 2s - loss: 0.3985 - acc: 0.916 - ETA: 2s - loss: 0.3923 - acc: 0.917 - ETA: 2s - loss: 0.3929 - acc: 0.917 - ETA: 2s - loss: 0.3896 - acc: 0.918 - ETA: 2s - loss: 0.3858 - acc: 0.919 - ETA: 1s - loss: 0.3817 - acc: 0.920 - ETA: 1s - loss: 0.3782 - acc: 0.920 - ETA: 1s - loss: 0.3728 - acc: 0.921 - ETA: 1s - loss: 0.3698 - acc: 0.921 - ETA: 1s - loss: 0.3708 - acc: 0.921 - ETA: 1s - loss: 0.3722 - acc: 0.921 - ETA: 1s - loss: 0.3692 - acc: 0.922 - ETA: 1s - loss: 0.3709 - acc: 0.922 - ETA: 1s - loss: 0.3663 - acc: 0.924 - ETA: 1s - loss: 0.3741 - acc: 0.923 - ETA: 1s - loss: 0.3803 - acc: 0.923 - ETA: 1s - loss: 0.3875 - acc: 0.923 - ETA: 1s - loss: 0.3859 - acc: 0.922 - ETA: 1s - loss: 0.3844 - acc: 0.922 - ETA: 1s - loss: 0.3827 - acc: 0.922 - ETA: 1s - loss: 0.3853 - acc: 0.921 - ETA: 1s - loss: 0.3885 - acc: 0.921 - ETA: 1s - loss: 0.3852 - acc: 0.921 - ETA: 0s - loss: 0.3880 - acc: 0.921 - ETA: 0s - loss: 0.3888 - acc: 0.921 - ETA: 0s - loss: 0.3850 - acc: 0.921 - ETA: 0s - loss: 0.3858 - acc: 0.922 - ETA: 0s - loss: 0.3832 - acc: 0.921 - ETA: 0s - loss: 0.3904 - acc: 0.921 - ETA: 0s - loss: 0.3869 - acc: 0.922 - ETA: 0s - loss: 0.3851 - acc: 0.922 - ETA: 0s - loss: 0.3840 - acc: 0.922 - ETA: 0s - loss: 0.3908 - acc: 0.922 - ETA: 0s - loss: 0.3936 - acc: 0.922 - ETA: 0s - loss: 0.3920 - acc: 0.922 - ETA: 0s - loss: 0.3897 - acc: 0.922 - ETA: 0s - loss: 0.3930 - acc: 0.922 - ETA: 0s - loss: 0.3954 - acc: 0.922 - ETA: 0s - loss: 0.3979 - acc: 0.922 - ETA: 0s - loss: 0.3955 - acc: 0.922 - ETA: 0s - loss: 0.3988 - acc: 0.922 - 6s 2ms/step - loss: 0.4024 - acc: 0.9223 - val_loss: 0.3377 - val_acc: 0.8910\n",
      "Epoch 12/100\n",
      "4067/4067 [==============================] - ETA: 7s - loss: 0.0345 - acc: 1.000 - ETA: 6s - loss: 0.1758 - acc: 0.953 - ETA: 6s - loss: 0.1734 - acc: 0.958 - ETA: 6s - loss: 0.2702 - acc: 0.937 - ETA: 6s - loss: 0.2438 - acc: 0.937 - ETA: 6s - loss: 0.2062 - acc: 0.937 - ETA: 6s - loss: 0.1999 - acc: 0.937 - ETA: 5s - loss: 0.4182 - acc: 0.913 - ETA: 5s - loss: 0.4307 - acc: 0.910 - ETA: 5s - loss: 0.4921 - acc: 0.901 - ETA: 5s - loss: 0.4857 - acc: 0.899 - ETA: 6s - loss: 0.4946 - acc: 0.899 - ETA: 5s - loss: 0.4620 - acc: 0.901 - ETA: 5s - loss: 0.4280 - acc: 0.908 - ETA: 5s - loss: 0.4381 - acc: 0.905 - ETA: 5s - loss: 0.4192 - acc: 0.908 - ETA: 5s - loss: 0.4042 - acc: 0.911 - ETA: 5s - loss: 0.3903 - acc: 0.911 - ETA: 5s - loss: 0.3989 - acc: 0.913 - ETA: 5s - loss: 0.4227 - acc: 0.913 - ETA: 5s - loss: 0.4362 - acc: 0.913 - ETA: 4s - loss: 0.4228 - acc: 0.914 - ETA: 4s - loss: 0.4137 - acc: 0.914 - ETA: 4s - loss: 0.4197 - acc: 0.913 - ETA: 4s - loss: 0.4108 - acc: 0.913 - ETA: 4s - loss: 0.3998 - acc: 0.911 - ETA: 4s - loss: 0.4017 - acc: 0.912 - ETA: 4s - loss: 0.3910 - acc: 0.915 - ETA: 4s - loss: 0.3781 - acc: 0.916 - ETA: 4s - loss: 0.3819 - acc: 0.916 - ETA: 3s - loss: 0.3918 - acc: 0.916 - ETA: 3s - loss: 0.3828 - acc: 0.916 - ETA: 3s - loss: 0.3908 - acc: 0.914 - ETA: 3s - loss: 0.3932 - acc: 0.916 - ETA: 3s - loss: 0.4081 - acc: 0.914 - ETA: 3s - loss: 0.4147 - acc: 0.914 - ETA: 3s - loss: 0.4071 - acc: 0.915 - ETA: 3s - loss: 0.4104 - acc: 0.913 - ETA: 3s - loss: 0.4065 - acc: 0.913 - ETA: 3s - loss: 0.3996 - acc: 0.913 - ETA: 3s - loss: 0.3903 - acc: 0.916 - ETA: 3s - loss: 0.3971 - acc: 0.916 - ETA: 3s - loss: 0.4031 - acc: 0.916 - ETA: 3s - loss: 0.3971 - acc: 0.917 - ETA: 2s - loss: 0.3997 - acc: 0.917 - ETA: 2s - loss: 0.3925 - acc: 0.918 - ETA: 2s - loss: 0.3873 - acc: 0.918 - ETA: 2s - loss: 0.3919 - acc: 0.918 - ETA: 2s - loss: 0.3944 - acc: 0.917 - ETA: 2s - loss: 0.3967 - acc: 0.915 - ETA: 2s - loss: 0.3981 - acc: 0.916 - ETA: 2s - loss: 0.3929 - acc: 0.917 - ETA: 2s - loss: 0.3909 - acc: 0.917 - ETA: 2s - loss: 0.3897 - acc: 0.917 - ETA: 2s - loss: 0.3886 - acc: 0.916 - ETA: 2s - loss: 0.3919 - acc: 0.916 - ETA: 2s - loss: 0.3876 - acc: 0.916 - ETA: 2s - loss: 0.3840 - acc: 0.917 - ETA: 2s - loss: 0.3886 - acc: 0.915 - ETA: 2s - loss: 0.3903 - acc: 0.914 - ETA: 1s - loss: 0.3941 - acc: 0.913 - ETA: 1s - loss: 0.3892 - acc: 0.914 - ETA: 1s - loss: 0.3901 - acc: 0.914 - ETA: 1s - loss: 0.3896 - acc: 0.915 - ETA: 1s - loss: 0.3847 - acc: 0.916 - ETA: 1s - loss: 0.3809 - acc: 0.916 - ETA: 1s - loss: 0.3792 - acc: 0.915 - ETA: 1s - loss: 0.3756 - acc: 0.916 - ETA: 1s - loss: 0.3782 - acc: 0.915 - ETA: 1s - loss: 0.3743 - acc: 0.916 - ETA: 1s - loss: 0.3713 - acc: 0.916 - ETA: 1s - loss: 0.3735 - acc: 0.916 - ETA: 1s - loss: 0.3699 - acc: 0.916 - ETA: 1s - loss: 0.3665 - acc: 0.917 - ETA: 1s - loss: 0.3730 - acc: 0.917 - ETA: 0s - loss: 0.3740 - acc: 0.917 - ETA: 0s - loss: 0.3751 - acc: 0.918 - ETA: 0s - loss: 0.3714 - acc: 0.918 - ETA: 0s - loss: 0.3688 - acc: 0.918 - ETA: 0s - loss: 0.3643 - acc: 0.919 - ETA: 0s - loss: 0.3672 - acc: 0.919 - ETA: 0s - loss: 0.3723 - acc: 0.919 - ETA: 0s - loss: 0.3703 - acc: 0.919 - ETA: 0s - loss: 0.3754 - acc: 0.919 - ETA: 0s - loss: 0.3737 - acc: 0.920 - ETA: 0s - loss: 0.3707 - acc: 0.920 - ETA: 0s - loss: 0.3718 - acc: 0.920 - ETA: 0s - loss: 0.3786 - acc: 0.920 - ETA: 0s - loss: 0.3759 - acc: 0.920 - ETA: 0s - loss: 0.3728 - acc: 0.921 - 6s 1ms/step - loss: 0.3726 - acc: 0.9213 - val_loss: 0.2617 - val_acc: 0.9096\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1395 - acc: 0.937 - ETA: 4s - loss: 0.0695 - acc: 0.984 - ETA: 4s - loss: 0.1068 - acc: 0.964 - ETA: 4s - loss: 0.4177 - acc: 0.937 - ETA: 4s - loss: 0.3412 - acc: 0.942 - ETA: 4s - loss: 0.3635 - acc: 0.933 - ETA: 4s - loss: 0.3756 - acc: 0.930 - ETA: 4s - loss: 0.3385 - acc: 0.937 - ETA: 4s - loss: 0.3617 - acc: 0.937 - ETA: 4s - loss: 0.3304 - acc: 0.942 - ETA: 4s - loss: 0.3036 - acc: 0.947 - ETA: 4s - loss: 0.2894 - acc: 0.944 - ETA: 4s - loss: 0.2993 - acc: 0.944 - ETA: 4s - loss: 0.3199 - acc: 0.937 - ETA: 4s - loss: 0.3328 - acc: 0.937 - ETA: 4s - loss: 0.3238 - acc: 0.937 - ETA: 4s - loss: 0.3102 - acc: 0.937 - ETA: 4s - loss: 0.3227 - acc: 0.936 - ETA: 3s - loss: 0.3111 - acc: 0.936 - ETA: 3s - loss: 0.3196 - acc: 0.937 - ETA: 3s - loss: 0.3093 - acc: 0.937 - ETA: 3s - loss: 0.3187 - acc: 0.935 - ETA: 3s - loss: 0.3277 - acc: 0.932 - ETA: 3s - loss: 0.3427 - acc: 0.928 - ETA: 3s - loss: 0.3358 - acc: 0.927 - ETA: 3s - loss: 0.3447 - acc: 0.926 - ETA: 3s - loss: 0.3413 - acc: 0.925 - ETA: 3s - loss: 0.3338 - acc: 0.927 - ETA: 3s - loss: 0.3530 - acc: 0.925 - ETA: 3s - loss: 0.3458 - acc: 0.925 - ETA: 3s - loss: 0.3462 - acc: 0.923 - ETA: 3s - loss: 0.3511 - acc: 0.924 - ETA: 3s - loss: 0.3755 - acc: 0.924 - ETA: 3s - loss: 0.3759 - acc: 0.925 - ETA: 3s - loss: 0.3680 - acc: 0.926 - ETA: 3s - loss: 0.3728 - acc: 0.925 - ETA: 3s - loss: 0.3952 - acc: 0.924 - ETA: 2s - loss: 0.3929 - acc: 0.925 - ETA: 2s - loss: 0.3983 - acc: 0.924 - ETA: 2s - loss: 0.3903 - acc: 0.924 - ETA: 2s - loss: 0.3940 - acc: 0.924 - ETA: 2s - loss: 0.3886 - acc: 0.925 - ETA: 2s - loss: 0.3960 - acc: 0.923 - ETA: 2s - loss: 0.3932 - acc: 0.923 - ETA: 2s - loss: 0.3913 - acc: 0.922 - ETA: 2s - loss: 0.3859 - acc: 0.922 - ETA: 2s - loss: 0.3829 - acc: 0.921 - ETA: 2s - loss: 0.3789 - acc: 0.922 - ETA: 2s - loss: 0.3737 - acc: 0.922 - ETA: 2s - loss: 0.3748 - acc: 0.923 - ETA: 2s - loss: 0.3699 - acc: 0.923 - ETA: 2s - loss: 0.3646 - acc: 0.924 - ETA: 2s - loss: 0.3623 - acc: 0.924 - ETA: 2s - loss: 0.3591 - acc: 0.924 - ETA: 2s - loss: 0.3567 - acc: 0.924 - ETA: 2s - loss: 0.3510 - acc: 0.926 - ETA: 2s - loss: 0.3497 - acc: 0.925 - ETA: 1s - loss: 0.3536 - acc: 0.925 - ETA: 1s - loss: 0.3566 - acc: 0.924 - ETA: 1s - loss: 0.3618 - acc: 0.923 - ETA: 1s - loss: 0.3573 - acc: 0.924 - ETA: 1s - loss: 0.3542 - acc: 0.925 - ETA: 1s - loss: 0.3503 - acc: 0.926 - ETA: 1s - loss: 0.3513 - acc: 0.927 - ETA: 1s - loss: 0.3545 - acc: 0.927 - ETA: 1s - loss: 0.3513 - acc: 0.927 - ETA: 1s - loss: 0.3522 - acc: 0.927 - ETA: 1s - loss: 0.3541 - acc: 0.928 - ETA: 1s - loss: 0.3532 - acc: 0.927 - ETA: 1s - loss: 0.3514 - acc: 0.927 - ETA: 1s - loss: 0.3469 - acc: 0.928 - ETA: 1s - loss: 0.3486 - acc: 0.928 - ETA: 1s - loss: 0.3608 - acc: 0.927 - ETA: 0s - loss: 0.3593 - acc: 0.927 - ETA: 0s - loss: 0.3657 - acc: 0.927 - ETA: 0s - loss: 0.3677 - acc: 0.927 - ETA: 0s - loss: 0.3660 - acc: 0.927 - ETA: 0s - loss: 0.3720 - acc: 0.926 - ETA: 0s - loss: 0.3744 - acc: 0.926 - ETA: 0s - loss: 0.3743 - acc: 0.926 - ETA: 0s - loss: 0.3714 - acc: 0.927 - ETA: 0s - loss: 0.3808 - acc: 0.926 - ETA: 0s - loss: 0.3835 - acc: 0.927 - ETA: 0s - loss: 0.3821 - acc: 0.926 - ETA: 0s - loss: 0.3791 - acc: 0.927 - ETA: 0s - loss: 0.3764 - acc: 0.927 - ETA: 0s - loss: 0.3728 - acc: 0.928 - ETA: 0s - loss: 0.3734 - acc: 0.928 - ETA: 0s - loss: 0.3749 - acc: 0.928 - ETA: 0s - loss: 0.3769 - acc: 0.927 - ETA: 0s - loss: 0.3745 - acc: 0.928 - ETA: 0s - loss: 0.3708 - acc: 0.928 - 6s 1ms/step - loss: 0.3708 - acc: 0.9284 - val_loss: 0.2730 - val_acc: 0.9026\n",
      "Epoch 14/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1007 - acc: 0.937 - ETA: 4s - loss: 0.6765 - acc: 0.859 - ETA: 4s - loss: 0.5817 - acc: 0.892 - ETA: 4s - loss: 0.5312 - acc: 0.912 - ETA: 4s - loss: 0.4290 - acc: 0.923 - ETA: 4s - loss: 0.3670 - acc: 0.929 - ETA: 4s - loss: 0.3211 - acc: 0.937 - ETA: 4s - loss: 0.2906 - acc: 0.940 - ETA: 4s - loss: 0.2747 - acc: 0.942 - ETA: 4s - loss: 0.2746 - acc: 0.942 - ETA: 4s - loss: 0.2692 - acc: 0.937 - ETA: 4s - loss: 0.2576 - acc: 0.937 - ETA: 4s - loss: 0.2538 - acc: 0.934 - ETA: 4s - loss: 0.2419 - acc: 0.935 - ETA: 4s - loss: 0.2618 - acc: 0.933 - ETA: 4s - loss: 0.2546 - acc: 0.934 - ETA: 4s - loss: 0.2661 - acc: 0.936 - ETA: 3s - loss: 0.3011 - acc: 0.936 - ETA: 3s - loss: 0.2892 - acc: 0.937 - ETA: 3s - loss: 0.2997 - acc: 0.935 - ETA: 3s - loss: 0.3048 - acc: 0.936 - ETA: 3s - loss: 0.2996 - acc: 0.936 - ETA: 3s - loss: 0.2951 - acc: 0.934 - ETA: 3s - loss: 0.2919 - acc: 0.933 - ETA: 3s - loss: 0.2874 - acc: 0.933 - ETA: 3s - loss: 0.2815 - acc: 0.933 - ETA: 3s - loss: 0.2741 - acc: 0.935 - ETA: 3s - loss: 0.2924 - acc: 0.935 - ETA: 3s - loss: 0.2965 - acc: 0.936 - ETA: 3s - loss: 0.3016 - acc: 0.936 - ETA: 3s - loss: 0.3206 - acc: 0.935 - ETA: 3s - loss: 0.3152 - acc: 0.936 - ETA: 3s - loss: 0.3114 - acc: 0.935 - ETA: 3s - loss: 0.3073 - acc: 0.935 - ETA: 2s - loss: 0.3135 - acc: 0.935 - ETA: 2s - loss: 0.3080 - acc: 0.935 - ETA: 2s - loss: 0.3209 - acc: 0.935 - ETA: 2s - loss: 0.3170 - acc: 0.934 - ETA: 2s - loss: 0.3126 - acc: 0.934 - ETA: 2s - loss: 0.3141 - acc: 0.935 - ETA: 2s - loss: 0.3089 - acc: 0.937 - ETA: 2s - loss: 0.3110 - acc: 0.937 - ETA: 2s - loss: 0.3108 - acc: 0.937 - ETA: 2s - loss: 0.3050 - acc: 0.937 - ETA: 2s - loss: 0.3107 - acc: 0.937 - ETA: 2s - loss: 0.3069 - acc: 0.937 - ETA: 2s - loss: 0.3104 - acc: 0.937 - ETA: 2s - loss: 0.3128 - acc: 0.936 - ETA: 2s - loss: 0.3088 - acc: 0.936 - ETA: 2s - loss: 0.3126 - acc: 0.936 - ETA: 2s - loss: 0.3107 - acc: 0.936 - ETA: 2s - loss: 0.3136 - acc: 0.936 - ETA: 1s - loss: 0.3103 - acc: 0.937 - ETA: 1s - loss: 0.3189 - acc: 0.936 - ETA: 1s - loss: 0.3148 - acc: 0.937 - ETA: 1s - loss: 0.3109 - acc: 0.937 - ETA: 1s - loss: 0.3144 - acc: 0.937 - ETA: 1s - loss: 0.3228 - acc: 0.936 - ETA: 1s - loss: 0.3254 - acc: 0.936 - ETA: 1s - loss: 0.3287 - acc: 0.935 - ETA: 1s - loss: 0.3257 - acc: 0.936 - ETA: 1s - loss: 0.3242 - acc: 0.936 - ETA: 1s - loss: 0.3276 - acc: 0.936 - ETA: 1s - loss: 0.3268 - acc: 0.935 - ETA: 1s - loss: 0.3294 - acc: 0.934 - ETA: 1s - loss: 0.3268 - acc: 0.934 - ETA: 1s - loss: 0.3309 - acc: 0.933 - ETA: 1s - loss: 0.3293 - acc: 0.932 - ETA: 1s - loss: 0.3320 - acc: 0.932 - ETA: 1s - loss: 0.3373 - acc: 0.931 - ETA: 1s - loss: 0.3354 - acc: 0.931 - ETA: 1s - loss: 0.3346 - acc: 0.930 - ETA: 0s - loss: 0.3388 - acc: 0.930 - ETA: 0s - loss: 0.3400 - acc: 0.930 - ETA: 0s - loss: 0.3435 - acc: 0.929 - ETA: 0s - loss: 0.3406 - acc: 0.929 - ETA: 0s - loss: 0.3418 - acc: 0.929 - ETA: 0s - loss: 0.3398 - acc: 0.929 - ETA: 0s - loss: 0.3432 - acc: 0.929 - ETA: 0s - loss: 0.3423 - acc: 0.929 - ETA: 0s - loss: 0.3443 - acc: 0.929 - ETA: 0s - loss: 0.3413 - acc: 0.929 - ETA: 0s - loss: 0.3390 - acc: 0.930 - ETA: 0s - loss: 0.3363 - acc: 0.930 - ETA: 0s - loss: 0.3331 - acc: 0.930 - ETA: 0s - loss: 0.3308 - acc: 0.931 - ETA: 0s - loss: 0.3280 - acc: 0.931 - ETA: 0s - loss: 0.3309 - acc: 0.931 - ETA: 0s - loss: 0.3305 - acc: 0.931 - ETA: 0s - loss: 0.3328 - acc: 0.931 - 6s 1ms/step - loss: 0.3315 - acc: 0.9314 - val_loss: 0.2520 - val_acc: 0.9237\n",
      "Epoch 15/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0281 - acc: 1.000 - ETA: 5s - loss: 0.6113 - acc: 0.937 - ETA: 6s - loss: 0.4964 - acc: 0.916 - ETA: 6s - loss: 0.4098 - acc: 0.921 - ETA: 6s - loss: 0.3470 - acc: 0.931 - ETA: 5s - loss: 0.3680 - acc: 0.932 - ETA: 5s - loss: 0.3179 - acc: 0.937 - ETA: 5s - loss: 0.3873 - acc: 0.934 - ETA: 5s - loss: 0.4040 - acc: 0.926 - ETA: 5s - loss: 0.4065 - acc: 0.930 - ETA: 4s - loss: 0.3821 - acc: 0.926 - ETA: 4s - loss: 0.3622 - acc: 0.931 - ETA: 4s - loss: 0.3469 - acc: 0.930 - ETA: 4s - loss: 0.3248 - acc: 0.934 - ETA: 4s - loss: 0.3039 - acc: 0.937 - ETA: 4s - loss: 0.2939 - acc: 0.939 - ETA: 4s - loss: 0.3111 - acc: 0.937 - ETA: 4s - loss: 0.3001 - acc: 0.938 - ETA: 4s - loss: 0.3359 - acc: 0.936 - ETA: 4s - loss: 0.3284 - acc: 0.935 - ETA: 4s - loss: 0.3393 - acc: 0.933 - ETA: 3s - loss: 0.3505 - acc: 0.931 - ETA: 3s - loss: 0.3401 - acc: 0.932 - ETA: 3s - loss: 0.3300 - acc: 0.934 - ETA: 3s - loss: 0.3414 - acc: 0.932 - ETA: 3s - loss: 0.3309 - acc: 0.934 - ETA: 3s - loss: 0.3477 - acc: 0.935 - ETA: 3s - loss: 0.3389 - acc: 0.935 - ETA: 3s - loss: 0.3411 - acc: 0.935 - ETA: 3s - loss: 0.3434 - acc: 0.936 - ETA: 3s - loss: 0.3460 - acc: 0.936 - ETA: 3s - loss: 0.3375 - acc: 0.937 - ETA: 3s - loss: 0.3408 - acc: 0.937 - ETA: 3s - loss: 0.3658 - acc: 0.935 - ETA: 3s - loss: 0.3599 - acc: 0.936 - ETA: 3s - loss: 0.3499 - acc: 0.938 - ETA: 3s - loss: 0.3406 - acc: 0.939 - ETA: 3s - loss: 0.3325 - acc: 0.940 - ETA: 2s - loss: 0.3302 - acc: 0.939 - ETA: 2s - loss: 0.3250 - acc: 0.939 - ETA: 2s - loss: 0.3191 - acc: 0.940 - ETA: 2s - loss: 0.3148 - acc: 0.940 - ETA: 2s - loss: 0.3199 - acc: 0.939 - ETA: 2s - loss: 0.3301 - acc: 0.938 - ETA: 2s - loss: 0.3359 - acc: 0.936 - ETA: 2s - loss: 0.3384 - acc: 0.936 - ETA: 2s - loss: 0.3428 - acc: 0.934 - ETA: 2s - loss: 0.3406 - acc: 0.932 - ETA: 2s - loss: 0.3373 - acc: 0.932 - ETA: 2s - loss: 0.3393 - acc: 0.931 - ETA: 2s - loss: 0.3421 - acc: 0.930 - ETA: 2s - loss: 0.3428 - acc: 0.928 - ETA: 2s - loss: 0.3399 - acc: 0.928 - ETA: 2s - loss: 0.3426 - acc: 0.927 - ETA: 1s - loss: 0.3380 - acc: 0.928 - ETA: 1s - loss: 0.3342 - acc: 0.929 - ETA: 1s - loss: 0.3310 - acc: 0.930 - ETA: 1s - loss: 0.3278 - acc: 0.930 - ETA: 1s - loss: 0.3248 - acc: 0.930 - ETA: 1s - loss: 0.3195 - acc: 0.932 - ETA: 1s - loss: 0.3218 - acc: 0.931 - ETA: 1s - loss: 0.3241 - acc: 0.931 - ETA: 1s - loss: 0.3209 - acc: 0.931 - ETA: 1s - loss: 0.3296 - acc: 0.931 - ETA: 1s - loss: 0.3379 - acc: 0.930 - ETA: 1s - loss: 0.3403 - acc: 0.929 - ETA: 1s - loss: 0.3420 - acc: 0.930 - ETA: 1s - loss: 0.3403 - acc: 0.930 - ETA: 1s - loss: 0.3376 - acc: 0.930 - ETA: 1s - loss: 0.3356 - acc: 0.931 - ETA: 1s - loss: 0.3379 - acc: 0.931 - ETA: 0s - loss: 0.3393 - acc: 0.931 - ETA: 0s - loss: 0.3360 - acc: 0.932 - ETA: 0s - loss: 0.3378 - acc: 0.932 - ETA: 0s - loss: 0.3338 - acc: 0.932 - ETA: 0s - loss: 0.3346 - acc: 0.933 - ETA: 0s - loss: 0.3319 - acc: 0.933 - ETA: 0s - loss: 0.3280 - acc: 0.933 - ETA: 0s - loss: 0.3244 - acc: 0.934 - ETA: 0s - loss: 0.3220 - acc: 0.934 - ETA: 0s - loss: 0.3204 - acc: 0.935 - ETA: 0s - loss: 0.3182 - acc: 0.935 - ETA: 0s - loss: 0.3216 - acc: 0.935 - ETA: 0s - loss: 0.3207 - acc: 0.935 - ETA: 0s - loss: 0.3186 - acc: 0.936 - ETA: 0s - loss: 0.3167 - acc: 0.936 - ETA: 0s - loss: 0.3182 - acc: 0.936 - ETA: 0s - loss: 0.3150 - acc: 0.936 - 6s 1ms/step - loss: 0.3183 - acc: 0.9363 - val_loss: 0.2522 - val_acc: 0.9263\n",
      "Epoch 16/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0819 - acc: 1.000 - ETA: 5s - loss: 0.5874 - acc: 0.953 - ETA: 5s - loss: 0.5752 - acc: 0.946 - ETA: 4s - loss: 0.4240 - acc: 0.956 - ETA: 4s - loss: 0.3568 - acc: 0.956 - ETA: 4s - loss: 0.2963 - acc: 0.964 - ETA: 4s - loss: 0.3122 - acc: 0.963 - ETA: 4s - loss: 0.3441 - acc: 0.955 - ETA: 4s - loss: 0.3203 - acc: 0.959 - ETA: 4s - loss: 0.2952 - acc: 0.962 - ETA: 5s - loss: 0.2830 - acc: 0.960 - ETA: 5s - loss: 0.3056 - acc: 0.956 - ETA: 4s - loss: 0.3423 - acc: 0.957 - ETA: 4s - loss: 0.3237 - acc: 0.957 - ETA: 4s - loss: 0.3001 - acc: 0.960 - ETA: 4s - loss: 0.2901 - acc: 0.960 - ETA: 4s - loss: 0.2981 - acc: 0.958 - ETA: 4s - loss: 0.2914 - acc: 0.957 - ETA: 4s - loss: 0.2799 - acc: 0.959 - ETA: 4s - loss: 0.2712 - acc: 0.959 - ETA: 4s - loss: 0.2702 - acc: 0.959 - ETA: 4s - loss: 0.2633 - acc: 0.959 - ETA: 4s - loss: 0.2564 - acc: 0.956 - ETA: 4s - loss: 0.2510 - acc: 0.955 - ETA: 4s - loss: 0.2665 - acc: 0.953 - ETA: 4s - loss: 0.2621 - acc: 0.952 - ETA: 4s - loss: 0.2712 - acc: 0.951 - ETA: 4s - loss: 0.2635 - acc: 0.951 - ETA: 3s - loss: 0.2549 - acc: 0.952 - ETA: 3s - loss: 0.2581 - acc: 0.951 - ETA: 3s - loss: 0.2620 - acc: 0.952 - ETA: 3s - loss: 0.2691 - acc: 0.952 - ETA: 3s - loss: 0.2638 - acc: 0.951 - ETA: 3s - loss: 0.2709 - acc: 0.950 - ETA: 3s - loss: 0.2849 - acc: 0.950 - ETA: 3s - loss: 0.2833 - acc: 0.950 - ETA: 3s - loss: 0.3091 - acc: 0.948 - ETA: 3s - loss: 0.3021 - acc: 0.949 - ETA: 3s - loss: 0.2990 - acc: 0.948 - ETA: 3s - loss: 0.3024 - acc: 0.948 - ETA: 3s - loss: 0.3076 - acc: 0.947 - ETA: 3s - loss: 0.3118 - acc: 0.946 - ETA: 2s - loss: 0.3237 - acc: 0.945 - ETA: 2s - loss: 0.3195 - acc: 0.944 - ETA: 2s - loss: 0.3137 - acc: 0.945 - ETA: 2s - loss: 0.3140 - acc: 0.943 - ETA: 2s - loss: 0.3082 - acc: 0.944 - ETA: 2s - loss: 0.3029 - acc: 0.944 - ETA: 2s - loss: 0.3062 - acc: 0.944 - ETA: 2s - loss: 0.3029 - acc: 0.944 - ETA: 2s - loss: 0.3069 - acc: 0.943 - ETA: 2s - loss: 0.3021 - acc: 0.944 - ETA: 2s - loss: 0.2971 - acc: 0.945 - ETA: 2s - loss: 0.2948 - acc: 0.945 - ETA: 2s - loss: 0.3008 - acc: 0.944 - ETA: 2s - loss: 0.2966 - acc: 0.944 - ETA: 2s - loss: 0.2934 - acc: 0.944 - ETA: 1s - loss: 0.2903 - acc: 0.944 - ETA: 1s - loss: 0.2875 - acc: 0.944 - ETA: 1s - loss: 0.2848 - acc: 0.943 - ETA: 1s - loss: 0.2895 - acc: 0.943 - ETA: 1s - loss: 0.2878 - acc: 0.942 - ETA: 1s - loss: 0.2848 - acc: 0.942 - ETA: 1s - loss: 0.2870 - acc: 0.942 - ETA: 1s - loss: 0.2836 - acc: 0.943 - ETA: 1s - loss: 0.2821 - acc: 0.942 - ETA: 1s - loss: 0.2849 - acc: 0.942 - ETA: 1s - loss: 0.2817 - acc: 0.942 - ETA: 1s - loss: 0.2872 - acc: 0.942 - ETA: 1s - loss: 0.2899 - acc: 0.942 - ETA: 1s - loss: 0.3018 - acc: 0.941 - ETA: 1s - loss: 0.3234 - acc: 0.940 - ETA: 1s - loss: 0.3443 - acc: 0.939 - ETA: 0s - loss: 0.3515 - acc: 0.938 - ETA: 0s - loss: 0.3683 - acc: 0.936 - ETA: 0s - loss: 0.3913 - acc: 0.934 - ETA: 0s - loss: 0.4026 - acc: 0.933 - ETA: 0s - loss: 0.4100 - acc: 0.932 - ETA: 0s - loss: 0.4124 - acc: 0.932 - ETA: 0s - loss: 0.4144 - acc: 0.931 - ETA: 0s - loss: 0.4133 - acc: 0.930 - ETA: 0s - loss: 0.4163 - acc: 0.929 - ETA: 0s - loss: 0.4135 - acc: 0.929 - ETA: 0s - loss: 0.4238 - acc: 0.928 - ETA: 0s - loss: 0.4194 - acc: 0.929 - ETA: 0s - loss: 0.4178 - acc: 0.928 - ETA: 0s - loss: 0.4150 - acc: 0.928 - ETA: 0s - loss: 0.4165 - acc: 0.928 - ETA: 0s - loss: 0.4131 - acc: 0.929 - ETA: 0s - loss: 0.4098 - acc: 0.929 - ETA: 0s - loss: 0.4113 - acc: 0.929 - ETA: 0s - loss: 0.4112 - acc: 0.929 - 6s 1ms/step - loss: 0.4109 - acc: 0.9297 - val_loss: 0.3336 - val_acc: 0.9083\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0772 - acc: 1.000 - ETA: 4s - loss: 0.0640 - acc: 1.000 - ETA: 5s - loss: 0.0986 - acc: 0.968 - ETA: 5s - loss: 0.1292 - acc: 0.951 - ETA: 5s - loss: 0.2842 - acc: 0.947 - ETA: 5s - loss: 0.3511 - acc: 0.942 - ETA: 5s - loss: 0.3124 - acc: 0.949 - ETA: 5s - loss: 0.2820 - acc: 0.950 - ETA: 5s - loss: 0.2785 - acc: 0.946 - ETA: 5s - loss: 0.2598 - acc: 0.947 - ETA: 5s - loss: 0.2527 - acc: 0.944 - ETA: 4s - loss: 0.2821 - acc: 0.935 - ETA: 4s - loss: 0.2685 - acc: 0.937 - ETA: 5s - loss: 0.2632 - acc: 0.937 - ETA: 5s - loss: 0.2593 - acc: 0.939 - ETA: 5s - loss: 0.2552 - acc: 0.937 - ETA: 5s - loss: 0.2558 - acc: 0.932 - ETA: 5s - loss: 0.2528 - acc: 0.934 - ETA: 5s - loss: 0.2457 - acc: 0.936 - ETA: 5s - loss: 0.2627 - acc: 0.934 - ETA: 5s - loss: 0.2569 - acc: 0.935 - ETA: 5s - loss: 0.2734 - acc: 0.933 - ETA: 5s - loss: 0.2694 - acc: 0.934 - ETA: 5s - loss: 0.2698 - acc: 0.930 - ETA: 4s - loss: 0.2972 - acc: 0.931 - ETA: 4s - loss: 0.2879 - acc: 0.933 - ETA: 4s - loss: 0.2770 - acc: 0.935 - ETA: 4s - loss: 0.2743 - acc: 0.936 - ETA: 4s - loss: 0.2720 - acc: 0.935 - ETA: 4s - loss: 0.2851 - acc: 0.935 - ETA: 4s - loss: 0.2805 - acc: 0.936 - ETA: 4s - loss: 0.2807 - acc: 0.935 - ETA: 4s - loss: 0.2756 - acc: 0.935 - ETA: 4s - loss: 0.2706 - acc: 0.936 - ETA: 4s - loss: 0.2659 - acc: 0.936 - ETA: 4s - loss: 0.2737 - acc: 0.937 - ETA: 3s - loss: 0.2812 - acc: 0.936 - ETA: 3s - loss: 0.2881 - acc: 0.936 - ETA: 3s - loss: 0.2839 - acc: 0.936 - ETA: 3s - loss: 0.2818 - acc: 0.935 - ETA: 3s - loss: 0.2772 - acc: 0.936 - ETA: 3s - loss: 0.2946 - acc: 0.935 - ETA: 3s - loss: 0.2917 - acc: 0.935 - ETA: 3s - loss: 0.2869 - acc: 0.935 - ETA: 3s - loss: 0.2832 - acc: 0.935 - ETA: 3s - loss: 0.2779 - acc: 0.936 - ETA: 3s - loss: 0.2756 - acc: 0.935 - ETA: 3s - loss: 0.2737 - acc: 0.936 - ETA: 3s - loss: 0.2679 - acc: 0.938 - ETA: 3s - loss: 0.2658 - acc: 0.937 - ETA: 3s - loss: 0.2619 - acc: 0.938 - ETA: 2s - loss: 0.2756 - acc: 0.937 - ETA: 2s - loss: 0.2722 - acc: 0.936 - ETA: 2s - loss: 0.2771 - acc: 0.935 - ETA: 2s - loss: 0.2756 - acc: 0.934 - ETA: 2s - loss: 0.2734 - acc: 0.935 - ETA: 2s - loss: 0.2827 - acc: 0.935 - ETA: 2s - loss: 0.2890 - acc: 0.935 - ETA: 2s - loss: 0.2858 - acc: 0.936 - ETA: 2s - loss: 0.2832 - acc: 0.936 - ETA: 2s - loss: 0.2809 - acc: 0.936 - ETA: 2s - loss: 0.2814 - acc: 0.936 - ETA: 2s - loss: 0.2787 - acc: 0.936 - ETA: 2s - loss: 0.2876 - acc: 0.936 - ETA: 2s - loss: 0.2851 - acc: 0.936 - ETA: 1s - loss: 0.2809 - acc: 0.937 - ETA: 1s - loss: 0.2777 - acc: 0.938 - ETA: 1s - loss: 0.2822 - acc: 0.937 - ETA: 1s - loss: 0.2862 - acc: 0.937 - ETA: 1s - loss: 0.2905 - acc: 0.936 - ETA: 1s - loss: 0.2872 - acc: 0.937 - ETA: 1s - loss: 0.2843 - acc: 0.937 - ETA: 1s - loss: 0.2818 - acc: 0.937 - ETA: 1s - loss: 0.2805 - acc: 0.936 - ETA: 1s - loss: 0.2785 - acc: 0.936 - ETA: 1s - loss: 0.2762 - acc: 0.936 - ETA: 1s - loss: 0.2734 - acc: 0.937 - ETA: 1s - loss: 0.2718 - acc: 0.937 - ETA: 1s - loss: 0.2703 - acc: 0.937 - ETA: 0s - loss: 0.2686 - acc: 0.937 - ETA: 0s - loss: 0.2658 - acc: 0.938 - ETA: 0s - loss: 0.2645 - acc: 0.938 - ETA: 0s - loss: 0.2762 - acc: 0.937 - ETA: 0s - loss: 0.2739 - acc: 0.937 - ETA: 0s - loss: 0.2711 - acc: 0.938 - ETA: 0s - loss: 0.2760 - acc: 0.938 - ETA: 0s - loss: 0.2730 - acc: 0.938 - ETA: 0s - loss: 0.2731 - acc: 0.937 - ETA: 0s - loss: 0.2720 - acc: 0.937 - ETA: 0s - loss: 0.2700 - acc: 0.937 - ETA: 0s - loss: 0.2681 - acc: 0.938 - ETA: 0s - loss: 0.2692 - acc: 0.937 - ETA: 0s - loss: 0.2673 - acc: 0.938 - ETA: 0s - loss: 0.2652 - acc: 0.938 - 6s 1ms/step - loss: 0.2644 - acc: 0.9388 - val_loss: 0.3351 - val_acc: 0.9237\n",
      "Epoch 18/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0732 - acc: 0.937 - ETA: 4s - loss: 0.6333 - acc: 0.921 - ETA: 4s - loss: 0.6838 - acc: 0.919 - ETA: 4s - loss: 0.4894 - acc: 0.937 - ETA: 4s - loss: 0.5129 - acc: 0.918 - ETA: 4s - loss: 0.4515 - acc: 0.921 - ETA: 4s - loss: 0.3984 - acc: 0.927 - ETA: 4s - loss: 0.3573 - acc: 0.931 - ETA: 4s - loss: 0.3761 - acc: 0.927 - ETA: 4s - loss: 0.3457 - acc: 0.930 - ETA: 4s - loss: 0.3550 - acc: 0.929 - ETA: 4s - loss: 0.3316 - acc: 0.932 - ETA: 4s - loss: 0.3143 - acc: 0.932 - ETA: 4s - loss: 0.3002 - acc: 0.932 - ETA: 4s - loss: 0.3026 - acc: 0.930 - ETA: 4s - loss: 0.3190 - acc: 0.929 - ETA: 4s - loss: 0.3021 - acc: 0.933 - ETA: 3s - loss: 0.2977 - acc: 0.930 - ETA: 3s - loss: 0.3029 - acc: 0.923 - ETA: 3s - loss: 0.3010 - acc: 0.921 - ETA: 3s - loss: 0.2932 - acc: 0.921 - ETA: 3s - loss: 0.2848 - acc: 0.922 - ETA: 3s - loss: 0.2950 - acc: 0.922 - ETA: 3s - loss: 0.2905 - acc: 0.923 - ETA: 3s - loss: 0.2830 - acc: 0.925 - ETA: 3s - loss: 0.2771 - acc: 0.926 - ETA: 3s - loss: 0.2814 - acc: 0.928 - ETA: 3s - loss: 0.2742 - acc: 0.929 - ETA: 3s - loss: 0.2722 - acc: 0.928 - ETA: 3s - loss: 0.2665 - acc: 0.930 - ETA: 3s - loss: 0.2605 - acc: 0.931 - ETA: 3s - loss: 0.2662 - acc: 0.932 - ETA: 3s - loss: 0.2662 - acc: 0.932 - ETA: 3s - loss: 0.2722 - acc: 0.928 - ETA: 3s - loss: 0.2654 - acc: 0.929 - ETA: 3s - loss: 0.2593 - acc: 0.931 - ETA: 3s - loss: 0.2557 - acc: 0.932 - ETA: 2s - loss: 0.2552 - acc: 0.931 - ETA: 2s - loss: 0.2628 - acc: 0.930 - ETA: 2s - loss: 0.2646 - acc: 0.928 - ETA: 2s - loss: 0.2610 - acc: 0.929 - ETA: 2s - loss: 0.2578 - acc: 0.930 - ETA: 2s - loss: 0.2549 - acc: 0.931 - ETA: 2s - loss: 0.2609 - acc: 0.930 - ETA: 2s - loss: 0.2570 - acc: 0.931 - ETA: 2s - loss: 0.2534 - acc: 0.932 - ETA: 2s - loss: 0.2524 - acc: 0.932 - ETA: 2s - loss: 0.2562 - acc: 0.933 - ETA: 2s - loss: 0.2526 - acc: 0.933 - ETA: 2s - loss: 0.2487 - acc: 0.934 - ETA: 2s - loss: 0.2465 - acc: 0.934 - ETA: 2s - loss: 0.2498 - acc: 0.934 - ETA: 2s - loss: 0.2470 - acc: 0.934 - ETA: 1s - loss: 0.2423 - acc: 0.935 - ETA: 1s - loss: 0.2410 - acc: 0.935 - ETA: 1s - loss: 0.2392 - acc: 0.936 - ETA: 1s - loss: 0.2371 - acc: 0.936 - ETA: 1s - loss: 0.2370 - acc: 0.936 - ETA: 1s - loss: 0.2355 - acc: 0.936 - ETA: 1s - loss: 0.2394 - acc: 0.935 - ETA: 1s - loss: 0.2425 - acc: 0.935 - ETA: 1s - loss: 0.2394 - acc: 0.936 - ETA: 1s - loss: 0.2386 - acc: 0.936 - ETA: 1s - loss: 0.2424 - acc: 0.936 - ETA: 1s - loss: 0.2461 - acc: 0.935 - ETA: 1s - loss: 0.2483 - acc: 0.935 - ETA: 1s - loss: 0.2469 - acc: 0.935 - ETA: 1s - loss: 0.2504 - acc: 0.935 - ETA: 1s - loss: 0.2495 - acc: 0.935 - ETA: 1s - loss: 0.2474 - acc: 0.935 - ETA: 0s - loss: 0.2476 - acc: 0.935 - ETA: 0s - loss: 0.2452 - acc: 0.935 - ETA: 0s - loss: 0.2432 - acc: 0.935 - ETA: 0s - loss: 0.2456 - acc: 0.935 - ETA: 0s - loss: 0.2433 - acc: 0.935 - ETA: 0s - loss: 0.2462 - acc: 0.935 - ETA: 0s - loss: 0.2434 - acc: 0.936 - ETA: 0s - loss: 0.2440 - acc: 0.935 - ETA: 0s - loss: 0.2464 - acc: 0.935 - ETA: 0s - loss: 0.2488 - acc: 0.935 - ETA: 0s - loss: 0.2519 - acc: 0.935 - ETA: 0s - loss: 0.2507 - acc: 0.935 - ETA: 0s - loss: 0.2494 - acc: 0.936 - ETA: 0s - loss: 0.2487 - acc: 0.936 - ETA: 0s - loss: 0.2481 - acc: 0.936 - ETA: 0s - loss: 0.2463 - acc: 0.936 - ETA: 0s - loss: 0.2441 - acc: 0.937 - 5s 1ms/step - loss: 0.2435 - acc: 0.9371 - val_loss: 0.2876 - val_acc: 0.9167\n",
      "Epoch 19/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0315 - acc: 1.000 - ETA: 5s - loss: 0.1362 - acc: 0.906 - ETA: 5s - loss: 0.1058 - acc: 0.928 - ETA: 5s - loss: 0.1175 - acc: 0.937 - ETA: 5s - loss: 0.1208 - acc: 0.937 - ETA: 5s - loss: 0.1154 - acc: 0.945 - ETA: 5s - loss: 0.1036 - acc: 0.950 - ETA: 5s - loss: 0.2002 - acc: 0.946 - ETA: 5s - loss: 0.2685 - acc: 0.942 - ETA: 4s - loss: 0.2603 - acc: 0.942 - ETA: 4s - loss: 0.2450 - acc: 0.945 - ETA: 4s - loss: 0.2606 - acc: 0.945 - ETA: 4s - loss: 0.2433 - acc: 0.947 - ETA: 4s - loss: 0.2369 - acc: 0.947 - ETA: 4s - loss: 0.2226 - acc: 0.950 - ETA: 4s - loss: 0.2138 - acc: 0.951 - ETA: 4s - loss: 0.2041 - acc: 0.953 - ETA: 4s - loss: 0.1964 - acc: 0.954 - ETA: 4s - loss: 0.1935 - acc: 0.953 - ETA: 4s - loss: 0.1887 - acc: 0.952 - ETA: 4s - loss: 0.1821 - acc: 0.954 - ETA: 4s - loss: 0.1791 - acc: 0.954 - ETA: 3s - loss: 0.1723 - acc: 0.956 - ETA: 3s - loss: 0.1709 - acc: 0.956 - ETA: 3s - loss: 0.1644 - acc: 0.958 - ETA: 3s - loss: 0.1658 - acc: 0.958 - ETA: 3s - loss: 0.1630 - acc: 0.959 - ETA: 3s - loss: 0.1855 - acc: 0.957 - ETA: 3s - loss: 0.1816 - acc: 0.958 - ETA: 3s - loss: 0.1773 - acc: 0.959 - ETA: 3s - loss: 0.1741 - acc: 0.960 - ETA: 3s - loss: 0.1715 - acc: 0.960 - ETA: 3s - loss: 0.1696 - acc: 0.960 - ETA: 3s - loss: 0.1839 - acc: 0.957 - ETA: 3s - loss: 0.1845 - acc: 0.955 - ETA: 3s - loss: 0.1933 - acc: 0.953 - ETA: 3s - loss: 0.1900 - acc: 0.954 - ETA: 3s - loss: 0.1967 - acc: 0.953 - ETA: 3s - loss: 0.2039 - acc: 0.953 - ETA: 2s - loss: 0.2005 - acc: 0.953 - ETA: 2s - loss: 0.2005 - acc: 0.953 - ETA: 2s - loss: 0.2063 - acc: 0.953 - ETA: 2s - loss: 0.2041 - acc: 0.953 - ETA: 2s - loss: 0.2029 - acc: 0.952 - ETA: 2s - loss: 0.2000 - acc: 0.953 - ETA: 2s - loss: 0.1967 - acc: 0.953 - ETA: 2s - loss: 0.1943 - acc: 0.953 - ETA: 2s - loss: 0.1959 - acc: 0.952 - ETA: 2s - loss: 0.1962 - acc: 0.952 - ETA: 2s - loss: 0.1960 - acc: 0.951 - ETA: 2s - loss: 0.1949 - acc: 0.951 - ETA: 2s - loss: 0.1928 - acc: 0.951 - ETA: 2s - loss: 0.1994 - acc: 0.950 - ETA: 2s - loss: 0.1995 - acc: 0.949 - ETA: 2s - loss: 0.2050 - acc: 0.948 - ETA: 1s - loss: 0.2028 - acc: 0.948 - ETA: 1s - loss: 0.2076 - acc: 0.948 - ETA: 1s - loss: 0.2055 - acc: 0.948 - ETA: 1s - loss: 0.2035 - acc: 0.948 - ETA: 1s - loss: 0.2009 - acc: 0.949 - ETA: 1s - loss: 0.1986 - acc: 0.949 - ETA: 1s - loss: 0.1982 - acc: 0.949 - ETA: 1s - loss: 0.1968 - acc: 0.949 - ETA: 1s - loss: 0.1954 - acc: 0.949 - ETA: 1s - loss: 0.1947 - acc: 0.949 - ETA: 1s - loss: 0.1919 - acc: 0.950 - ETA: 1s - loss: 0.1969 - acc: 0.949 - ETA: 1s - loss: 0.1967 - acc: 0.949 - ETA: 1s - loss: 0.1955 - acc: 0.949 - ETA: 1s - loss: 0.1948 - acc: 0.949 - ETA: 1s - loss: 0.1924 - acc: 0.950 - ETA: 1s - loss: 0.1914 - acc: 0.950 - ETA: 0s - loss: 0.1946 - acc: 0.950 - ETA: 0s - loss: 0.1931 - acc: 0.950 - ETA: 0s - loss: 0.1965 - acc: 0.950 - ETA: 0s - loss: 0.1958 - acc: 0.950 - ETA: 0s - loss: 0.1937 - acc: 0.950 - ETA: 0s - loss: 0.1982 - acc: 0.950 - ETA: 0s - loss: 0.1962 - acc: 0.950 - ETA: 0s - loss: 0.1985 - acc: 0.950 - ETA: 0s - loss: 0.1998 - acc: 0.951 - ETA: 0s - loss: 0.1993 - acc: 0.951 - ETA: 0s - loss: 0.2016 - acc: 0.951 - ETA: 0s - loss: 0.2080 - acc: 0.950 - ETA: 0s - loss: 0.2058 - acc: 0.951 - ETA: 0s - loss: 0.2061 - acc: 0.951 - ETA: 0s - loss: 0.2054 - acc: 0.951 - ETA: 0s - loss: 0.2034 - acc: 0.952 - 6s 1ms/step - loss: 0.2032 - acc: 0.9523 - val_loss: 0.3878 - val_acc: 0.8763\n",
      "Epoch 20/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 0.2594 - acc: 0.937 - ETA: 4s - loss: 0.3197 - acc: 0.921 - ETA: 4s - loss: 0.3828 - acc: 0.937 - ETA: 4s - loss: 0.2866 - acc: 0.950 - ETA: 4s - loss: 0.2426 - acc: 0.956 - ETA: 4s - loss: 0.2131 - acc: 0.960 - ETA: 4s - loss: 0.2055 - acc: 0.953 - ETA: 4s - loss: 0.2101 - acc: 0.951 - ETA: 4s - loss: 0.1993 - acc: 0.952 - ETA: 4s - loss: 0.1857 - acc: 0.953 - ETA: 4s - loss: 0.1815 - acc: 0.951 - ETA: 4s - loss: 0.1751 - acc: 0.952 - ETA: 4s - loss: 0.1736 - acc: 0.949 - ETA: 4s - loss: 0.1656 - acc: 0.951 - ETA: 4s - loss: 0.1763 - acc: 0.953 - ETA: 4s - loss: 0.1697 - acc: 0.955 - ETA: 4s - loss: 0.1650 - acc: 0.955 - ETA: 4s - loss: 0.1908 - acc: 0.951 - ETA: 4s - loss: 0.1867 - acc: 0.952 - ETA: 4s - loss: 0.1853 - acc: 0.952 - ETA: 4s - loss: 0.1964 - acc: 0.948 - ETA: 3s - loss: 0.2019 - acc: 0.943 - ETA: 3s - loss: 0.2033 - acc: 0.938 - ETA: 3s - loss: 0.2303 - acc: 0.935 - ETA: 3s - loss: 0.2635 - acc: 0.930 - ETA: 3s - loss: 0.2656 - acc: 0.928 - ETA: 3s - loss: 0.2672 - acc: 0.925 - ETA: 3s - loss: 0.2820 - acc: 0.919 - ETA: 3s - loss: 0.2781 - acc: 0.918 - ETA: 3s - loss: 0.2775 - acc: 0.915 - ETA: 3s - loss: 0.2737 - acc: 0.914 - ETA: 3s - loss: 0.2735 - acc: 0.914 - ETA: 3s - loss: 0.2703 - acc: 0.913 - ETA: 3s - loss: 0.2704 - acc: 0.912 - ETA: 3s - loss: 0.2738 - acc: 0.911 - ETA: 3s - loss: 0.2863 - acc: 0.909 - ETA: 3s - loss: 0.2890 - acc: 0.907 - ETA: 2s - loss: 0.2915 - acc: 0.907 - ETA: 2s - loss: 0.2896 - acc: 0.907 - ETA: 2s - loss: 0.2874 - acc: 0.906 - ETA: 2s - loss: 0.2965 - acc: 0.904 - ETA: 2s - loss: 0.2950 - acc: 0.904 - ETA: 2s - loss: 0.2942 - acc: 0.902 - ETA: 2s - loss: 0.2978 - acc: 0.899 - ETA: 2s - loss: 0.2961 - acc: 0.897 - ETA: 2s - loss: 0.3027 - acc: 0.897 - ETA: 2s - loss: 0.3108 - acc: 0.894 - ETA: 2s - loss: 0.3110 - acc: 0.890 - ETA: 2s - loss: 0.3182 - acc: 0.890 - ETA: 2s - loss: 0.3225 - acc: 0.890 - ETA: 2s - loss: 0.3217 - acc: 0.891 - ETA: 2s - loss: 0.3213 - acc: 0.891 - ETA: 2s - loss: 0.3212 - acc: 0.891 - ETA: 2s - loss: 0.3242 - acc: 0.891 - ETA: 2s - loss: 0.3226 - acc: 0.892 - ETA: 2s - loss: 0.3256 - acc: 0.892 - ETA: 1s - loss: 0.3237 - acc: 0.892 - ETA: 1s - loss: 0.3240 - acc: 0.891 - ETA: 1s - loss: 0.3238 - acc: 0.891 - ETA: 1s - loss: 0.3214 - acc: 0.892 - ETA: 1s - loss: 0.3191 - acc: 0.893 - ETA: 1s - loss: 0.3228 - acc: 0.893 - ETA: 1s - loss: 0.3213 - acc: 0.893 - ETA: 1s - loss: 0.3189 - acc: 0.893 - ETA: 1s - loss: 0.3213 - acc: 0.894 - ETA: 1s - loss: 0.3233 - acc: 0.895 - ETA: 1s - loss: 0.3213 - acc: 0.895 - ETA: 1s - loss: 0.3249 - acc: 0.895 - ETA: 1s - loss: 0.3220 - acc: 0.895 - ETA: 1s - loss: 0.3209 - acc: 0.895 - ETA: 1s - loss: 0.3186 - acc: 0.896 - ETA: 1s - loss: 0.3225 - acc: 0.896 - ETA: 1s - loss: 0.3275 - acc: 0.895 - ETA: 1s - loss: 0.3256 - acc: 0.896 - ETA: 0s - loss: 0.3244 - acc: 0.896 - ETA: 0s - loss: 0.3273 - acc: 0.897 - ETA: 0s - loss: 0.3271 - acc: 0.896 - ETA: 0s - loss: 0.3301 - acc: 0.896 - ETA: 0s - loss: 0.3330 - acc: 0.897 - ETA: 0s - loss: 0.3327 - acc: 0.896 - ETA: 0s - loss: 0.3325 - acc: 0.896 - ETA: 0s - loss: 0.3318 - acc: 0.897 - ETA: 0s - loss: 0.3345 - acc: 0.897 - ETA: 0s - loss: 0.3324 - acc: 0.898 - ETA: 0s - loss: 0.3320 - acc: 0.898 - ETA: 0s - loss: 0.3304 - acc: 0.899 - ETA: 0s - loss: 0.3335 - acc: 0.899 - ETA: 0s - loss: 0.3316 - acc: 0.899 - ETA: 0s - loss: 0.3308 - acc: 0.900 - ETA: 0s - loss: 0.3333 - acc: 0.900 - 6s 1ms/step - loss: 0.3328 - acc: 0.9007 - val_loss: 0.2951 - val_acc: 0.9109\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1827 - acc: 0.937 - ETA: 5s - loss: 0.1342 - acc: 0.921 - ETA: 5s - loss: 0.1499 - acc: 0.919 - ETA: 5s - loss: 0.4519 - acc: 0.906 - ETA: 5s - loss: 0.5631 - acc: 0.903 - ETA: 5s - loss: 0.7082 - acc: 0.890 - ETA: 4s - loss: 0.8193 - acc: 0.888 - ETA: 4s - loss: 0.7435 - acc: 0.886 - ETA: 4s - loss: 0.7707 - acc: 0.877 - ETA: 4s - loss: 0.7604 - acc: 0.868 - ETA: 4s - loss: 0.7543 - acc: 0.854 - ETA: 4s - loss: 0.7420 - acc: 0.852 - ETA: 4s - loss: 0.7586 - acc: 0.856 - ETA: 4s - loss: 0.7524 - acc: 0.857 - ETA: 4s - loss: 0.7387 - acc: 0.863 - ETA: 4s - loss: 0.7244 - acc: 0.868 - ETA: 4s - loss: 0.6882 - acc: 0.876 - ETA: 4s - loss: 0.6971 - acc: 0.877 - ETA: 4s - loss: 0.6703 - acc: 0.880 - ETA: 4s - loss: 0.6918 - acc: 0.881 - ETA: 4s - loss: 0.6665 - acc: 0.884 - ETA: 3s - loss: 0.6631 - acc: 0.883 - ETA: 3s - loss: 0.6493 - acc: 0.880 - ETA: 3s - loss: 0.6444 - acc: 0.880 - ETA: 3s - loss: 0.6553 - acc: 0.879 - ETA: 3s - loss: 0.6413 - acc: 0.877 - ETA: 3s - loss: 0.6470 - acc: 0.871 - ETA: 3s - loss: 0.6368 - acc: 0.866 - ETA: 3s - loss: 0.6619 - acc: 0.863 - ETA: 3s - loss: 0.6621 - acc: 0.864 - ETA: 3s - loss: 0.6632 - acc: 0.863 - ETA: 3s - loss: 0.6471 - acc: 0.864 - ETA: 3s - loss: 0.6428 - acc: 0.865 - ETA: 3s - loss: 0.6402 - acc: 0.864 - ETA: 3s - loss: 0.6306 - acc: 0.862 - ETA: 3s - loss: 0.6183 - acc: 0.862 - ETA: 3s - loss: 0.6080 - acc: 0.862 - ETA: 3s - loss: 0.5973 - acc: 0.862 - ETA: 3s - loss: 0.6001 - acc: 0.861 - ETA: 2s - loss: 0.5995 - acc: 0.861 - ETA: 2s - loss: 0.6023 - acc: 0.861 - ETA: 2s - loss: 0.6016 - acc: 0.861 - ETA: 2s - loss: 0.5901 - acc: 0.862 - ETA: 2s - loss: 0.5826 - acc: 0.862 - ETA: 2s - loss: 0.5744 - acc: 0.862 - ETA: 2s - loss: 0.5641 - acc: 0.865 - ETA: 2s - loss: 0.5620 - acc: 0.866 - ETA: 2s - loss: 0.5554 - acc: 0.866 - ETA: 2s - loss: 0.5537 - acc: 0.868 - ETA: 2s - loss: 0.5501 - acc: 0.870 - ETA: 2s - loss: 0.5491 - acc: 0.871 - ETA: 2s - loss: 0.5452 - acc: 0.871 - ETA: 2s - loss: 0.5514 - acc: 0.870 - ETA: 2s - loss: 0.5464 - acc: 0.872 - ETA: 1s - loss: 0.5455 - acc: 0.872 - ETA: 1s - loss: 0.5385 - acc: 0.874 - ETA: 1s - loss: 0.5304 - acc: 0.875 - ETA: 1s - loss: 0.5278 - acc: 0.876 - ETA: 1s - loss: 0.5208 - acc: 0.877 - ETA: 1s - loss: 0.5186 - acc: 0.876 - ETA: 1s - loss: 0.5169 - acc: 0.872 - ETA: 1s - loss: 0.5142 - acc: 0.870 - ETA: 1s - loss: 0.5147 - acc: 0.870 - ETA: 1s - loss: 0.5102 - acc: 0.870 - ETA: 1s - loss: 0.5051 - acc: 0.870 - ETA: 1s - loss: 0.4998 - acc: 0.870 - ETA: 1s - loss: 0.4965 - acc: 0.870 - ETA: 1s - loss: 0.4935 - acc: 0.870 - ETA: 1s - loss: 0.4924 - acc: 0.868 - ETA: 1s - loss: 0.4884 - acc: 0.868 - ETA: 0s - loss: 0.4852 - acc: 0.867 - ETA: 0s - loss: 0.4822 - acc: 0.866 - ETA: 0s - loss: 0.4795 - acc: 0.866 - ETA: 0s - loss: 0.4777 - acc: 0.866 - ETA: 0s - loss: 0.4790 - acc: 0.865 - ETA: 0s - loss: 0.4765 - acc: 0.864 - ETA: 0s - loss: 0.4734 - acc: 0.864 - ETA: 0s - loss: 0.4705 - acc: 0.864 - ETA: 0s - loss: 0.4703 - acc: 0.864 - ETA: 0s - loss: 0.4652 - acc: 0.866 - ETA: 0s - loss: 0.4608 - acc: 0.866 - ETA: 0s - loss: 0.4567 - acc: 0.866 - ETA: 0s - loss: 0.4538 - acc: 0.866 - ETA: 0s - loss: 0.4534 - acc: 0.867 - ETA: 0s - loss: 0.4488 - acc: 0.868 - ETA: 0s - loss: 0.4446 - acc: 0.869 - 6s 1ms/step - loss: 0.4433 - acc: 0.8694 - val_loss: 0.3019 - val_acc: 0.8833\n",
      "Epoch 22/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1209 - acc: 0.937 - ETA: 5s - loss: 0.7414 - acc: 0.828 - ETA: 5s - loss: 0.4808 - acc: 0.866 - ETA: 4s - loss: 0.3578 - acc: 0.893 - ETA: 4s - loss: 0.3863 - acc: 0.899 - ETA: 4s - loss: 0.3447 - acc: 0.906 - ETA: 4s - loss: 0.3214 - acc: 0.898 - ETA: 4s - loss: 0.3439 - acc: 0.900 - ETA: 4s - loss: 0.3574 - acc: 0.905 - ETA: 4s - loss: 0.3405 - acc: 0.899 - ETA: 4s - loss: 0.3253 - acc: 0.899 - ETA: 4s - loss: 0.3799 - acc: 0.891 - ETA: 4s - loss: 0.3942 - acc: 0.888 - ETA: 4s - loss: 0.3847 - acc: 0.887 - ETA: 4s - loss: 0.3990 - acc: 0.886 - ETA: 4s - loss: 0.4118 - acc: 0.881 - ETA: 4s - loss: 0.3996 - acc: 0.880 - ETA: 4s - loss: 0.3899 - acc: 0.881 - ETA: 4s - loss: 0.3951 - acc: 0.880 - ETA: 4s - loss: 0.4191 - acc: 0.879 - ETA: 3s - loss: 0.4110 - acc: 0.881 - ETA: 3s - loss: 0.4238 - acc: 0.879 - ETA: 3s - loss: 0.4100 - acc: 0.881 - ETA: 3s - loss: 0.3993 - acc: 0.885 - ETA: 3s - loss: 0.3903 - acc: 0.888 - ETA: 3s - loss: 0.3954 - acc: 0.889 - ETA: 3s - loss: 0.3843 - acc: 0.890 - ETA: 3s - loss: 0.3803 - acc: 0.890 - ETA: 3s - loss: 0.3727 - acc: 0.892 - ETA: 3s - loss: 0.3808 - acc: 0.893 - ETA: 3s - loss: 0.3865 - acc: 0.892 - ETA: 3s - loss: 0.3806 - acc: 0.894 - ETA: 3s - loss: 0.3764 - acc: 0.895 - ETA: 3s - loss: 0.3696 - acc: 0.898 - ETA: 3s - loss: 0.3630 - acc: 0.898 - ETA: 3s - loss: 0.3603 - acc: 0.899 - ETA: 3s - loss: 0.3527 - acc: 0.900 - ETA: 3s - loss: 0.3509 - acc: 0.899 - ETA: 3s - loss: 0.3548 - acc: 0.899 - ETA: 3s - loss: 0.3490 - acc: 0.900 - ETA: 3s - loss: 0.3447 - acc: 0.900 - ETA: 3s - loss: 0.3501 - acc: 0.898 - ETA: 2s - loss: 0.3651 - acc: 0.896 - ETA: 2s - loss: 0.3658 - acc: 0.897 - ETA: 2s - loss: 0.3698 - acc: 0.896 - ETA: 2s - loss: 0.3661 - acc: 0.896 - ETA: 2s - loss: 0.3617 - acc: 0.896 - ETA: 2s - loss: 0.3580 - acc: 0.897 - ETA: 2s - loss: 0.4064 - acc: 0.893 - ETA: 2s - loss: 0.4068 - acc: 0.894 - ETA: 2s - loss: 0.4032 - acc: 0.894 - ETA: 2s - loss: 0.4061 - acc: 0.894 - ETA: 2s - loss: 0.4052 - acc: 0.893 - ETA: 2s - loss: 0.4160 - acc: 0.893 - ETA: 2s - loss: 0.4143 - acc: 0.892 - ETA: 2s - loss: 0.4164 - acc: 0.892 - ETA: 2s - loss: 0.4120 - acc: 0.892 - ETA: 2s - loss: 0.4070 - acc: 0.893 - ETA: 2s - loss: 0.4049 - acc: 0.893 - ETA: 2s - loss: 0.4080 - acc: 0.893 - ETA: 2s - loss: 0.4037 - acc: 0.894 - ETA: 1s - loss: 0.3991 - acc: 0.895 - ETA: 1s - loss: 0.3957 - acc: 0.895 - ETA: 1s - loss: 0.3950 - acc: 0.895 - ETA: 1s - loss: 0.3974 - acc: 0.894 - ETA: 1s - loss: 0.3979 - acc: 0.895 - ETA: 1s - loss: 0.3931 - acc: 0.896 - ETA: 1s - loss: 0.3954 - acc: 0.897 - ETA: 1s - loss: 0.3930 - acc: 0.898 - ETA: 1s - loss: 0.3890 - acc: 0.899 - ETA: 1s - loss: 0.3853 - acc: 0.899 - ETA: 1s - loss: 0.3804 - acc: 0.901 - ETA: 1s - loss: 0.3809 - acc: 0.901 - ETA: 1s - loss: 0.3785 - acc: 0.901 - ETA: 1s - loss: 0.3880 - acc: 0.902 - ETA: 1s - loss: 0.3883 - acc: 0.902 - ETA: 0s - loss: 0.3865 - acc: 0.903 - ETA: 0s - loss: 0.3884 - acc: 0.902 - ETA: 0s - loss: 0.3869 - acc: 0.901 - ETA: 0s - loss: 0.3886 - acc: 0.901 - ETA: 0s - loss: 0.3905 - acc: 0.901 - ETA: 0s - loss: 0.3891 - acc: 0.901 - ETA: 0s - loss: 0.3854 - acc: 0.901 - ETA: 0s - loss: 0.3829 - acc: 0.901 - ETA: 0s - loss: 0.3907 - acc: 0.900 - ETA: 0s - loss: 0.3921 - acc: 0.900 - ETA: 0s - loss: 0.3893 - acc: 0.900 - ETA: 0s - loss: 0.3873 - acc: 0.900 - ETA: 0s - loss: 0.3884 - acc: 0.900 - ETA: 0s - loss: 0.3871 - acc: 0.899 - ETA: 0s - loss: 0.3853 - acc: 0.899 - ETA: 0s - loss: 0.3833 - acc: 0.899 - 6s 2ms/step - loss: 0.3832 - acc: 0.8989 - val_loss: 0.2424 - val_acc: 0.8718\n",
      "Epoch 23/100\n",
      "4067/4067 [==============================] - ETA: 7s - loss: 0.2216 - acc: 0.750 - ETA: 7s - loss: 0.6694 - acc: 0.791 - ETA: 7s - loss: 0.4372 - acc: 0.837 - ETA: 7s - loss: 0.3757 - acc: 0.857 - ETA: 7s - loss: 0.3905 - acc: 0.840 - ETA: 7s - loss: 0.3609 - acc: 0.840 - ETA: 7s - loss: 0.3387 - acc: 0.841 - ETA: 7s - loss: 0.3242 - acc: 0.841 - ETA: 6s - loss: 0.3021 - acc: 0.849 - ETA: 6s - loss: 0.2833 - acc: 0.858 - ETA: 6s - loss: 0.2753 - acc: 0.863 - ETA: 6s - loss: 0.3066 - acc: 0.858 - ETA: 6s - loss: 0.2983 - acc: 0.857 - ETA: 6s - loss: 0.2839 - acc: 0.868 - ETA: 6s - loss: 0.2797 - acc: 0.872 - ETA: 6s - loss: 0.2741 - acc: 0.875 - ETA: 6s - loss: 0.2648 - acc: 0.878 - ETA: 6s - loss: 0.2849 - acc: 0.883 - ETA: 6s - loss: 0.2805 - acc: 0.886 - ETA: 6s - loss: 0.2761 - acc: 0.887 - ETA: 6s - loss: 0.2769 - acc: 0.885 - ETA: 6s - loss: 0.2728 - acc: 0.885 - ETA: 6s - loss: 0.2684 - acc: 0.888 - ETA: 6s - loss: 0.2602 - acc: 0.893 - ETA: 6s - loss: 0.2768 - acc: 0.894 - ETA: 6s - loss: 0.2707 - acc: 0.893 - ETA: 5s - loss: 0.2641 - acc: 0.893 - ETA: 5s - loss: 0.2817 - acc: 0.894 - ETA: 5s - loss: 0.2831 - acc: 0.891 - ETA: 5s - loss: 0.2950 - acc: 0.889 - ETA: 5s - loss: 0.2913 - acc: 0.889 - ETA: 5s - loss: 0.2869 - acc: 0.890 - ETA: 5s - loss: 0.2988 - acc: 0.890 - ETA: 5s - loss: 0.3002 - acc: 0.891 - ETA: 5s - loss: 0.2938 - acc: 0.894 - ETA: 5s - loss: 0.3019 - acc: 0.893 - ETA: 5s - loss: 0.2986 - acc: 0.894 - ETA: 5s - loss: 0.2964 - acc: 0.895 - ETA: 5s - loss: 0.2938 - acc: 0.896 - ETA: 5s - loss: 0.2897 - acc: 0.896 - ETA: 5s - loss: 0.2998 - acc: 0.894 - ETA: 5s - loss: 0.3078 - acc: 0.894 - ETA: 5s - loss: 0.3030 - acc: 0.896 - ETA: 5s - loss: 0.2962 - acc: 0.898 - ETA: 4s - loss: 0.2924 - acc: 0.899 - ETA: 4s - loss: 0.2884 - acc: 0.900 - ETA: 4s - loss: 0.2957 - acc: 0.901 - ETA: 4s - loss: 0.2958 - acc: 0.902 - ETA: 4s - loss: 0.3016 - acc: 0.902 - ETA: 4s - loss: 0.2987 - acc: 0.902 - ETA: 4s - loss: 0.2946 - acc: 0.904 - ETA: 4s - loss: 0.2981 - acc: 0.905 - ETA: 4s - loss: 0.2933 - acc: 0.907 - ETA: 4s - loss: 0.2998 - acc: 0.907 - ETA: 4s - loss: 0.2978 - acc: 0.907 - ETA: 4s - loss: 0.2975 - acc: 0.907 - ETA: 4s - loss: 0.2967 - acc: 0.906 - ETA: 4s - loss: 0.2934 - acc: 0.906 - ETA: 4s - loss: 0.2924 - acc: 0.906 - ETA: 4s - loss: 0.3008 - acc: 0.904 - ETA: 3s - loss: 0.2982 - acc: 0.905 - ETA: 3s - loss: 0.2957 - acc: 0.906 - ETA: 3s - loss: 0.2923 - acc: 0.907 - ETA: 3s - loss: 0.2893 - acc: 0.907 - ETA: 3s - loss: 0.2865 - acc: 0.908 - ETA: 3s - loss: 0.2878 - acc: 0.906 - ETA: 3s - loss: 0.2857 - acc: 0.907 - ETA: 3s - loss: 0.2900 - acc: 0.907 - ETA: 3s - loss: 0.2955 - acc: 0.907 - ETA: 3s - loss: 0.2929 - acc: 0.907 - ETA: 3s - loss: 0.2926 - acc: 0.906 - ETA: 3s - loss: 0.2983 - acc: 0.906 - ETA: 3s - loss: 0.2947 - acc: 0.907 - ETA: 3s - loss: 0.2928 - acc: 0.907 - ETA: 3s - loss: 0.2911 - acc: 0.908 - ETA: 3s - loss: 0.2890 - acc: 0.908 - ETA: 2s - loss: 0.2922 - acc: 0.909 - ETA: 2s - loss: 0.2907 - acc: 0.909 - ETA: 2s - loss: 0.2877 - acc: 0.909 - ETA: 2s - loss: 0.2891 - acc: 0.910 - ETA: 2s - loss: 0.2930 - acc: 0.909 - ETA: 2s - loss: 0.2929 - acc: 0.908 - ETA: 2s - loss: 0.2892 - acc: 0.909 - ETA: 2s - loss: 0.3019 - acc: 0.909 - ETA: 2s - loss: 0.3099 - acc: 0.908 - ETA: 2s - loss: 0.3070 - acc: 0.909 - ETA: 2s - loss: 0.3102 - acc: 0.910 - ETA: 2s - loss: 0.3077 - acc: 0.910 - ETA: 1s - loss: 0.3051 - acc: 0.910 - ETA: 1s - loss: 0.3083 - acc: 0.911 - ETA: 1s - loss: 0.3160 - acc: 0.911 - ETA: 1s - loss: 0.3319 - acc: 0.910 - ETA: 1s - loss: 0.3307 - acc: 0.910 - ETA: 1s - loss: 0.3290 - acc: 0.911 - ETA: 1s - loss: 0.3272 - acc: 0.911 - ETA: 1s - loss: 0.3259 - acc: 0.910 - ETA: 1s - loss: 0.3243 - acc: 0.910 - ETA: 1s - loss: 0.3204 - acc: 0.912 - ETA: 1s - loss: 0.3210 - acc: 0.911 - ETA: 1s - loss: 0.3194 - acc: 0.911 - ETA: 1s - loss: 0.3195 - acc: 0.910 - ETA: 0s - loss: 0.3236 - acc: 0.909 - ETA: 0s - loss: 0.3225 - acc: 0.909 - ETA: 0s - loss: 0.3235 - acc: 0.908 - ETA: 0s - loss: 0.3247 - acc: 0.907 - ETA: 0s - loss: 0.3238 - acc: 0.907 - ETA: 0s - loss: 0.3279 - acc: 0.906 - ETA: 0s - loss: 0.3271 - acc: 0.906 - ETA: 0s - loss: 0.3256 - acc: 0.906 - ETA: 0s - loss: 0.3297 - acc: 0.905 - ETA: 0s - loss: 0.3298 - acc: 0.905 - ETA: 0s - loss: 0.3279 - acc: 0.905 - ETA: 0s - loss: 0.3269 - acc: 0.905 - ETA: 0s - loss: 0.3246 - acc: 0.906 - ETA: 0s - loss: 0.3310 - acc: 0.906 - ETA: 0s - loss: 0.3332 - acc: 0.906 - 8s 2ms/step - loss: 0.3324 - acc: 0.9066 - val_loss: 0.2736 - val_acc: 0.8859\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0925 - acc: 0.937 - ETA: 6s - loss: 0.3229 - acc: 0.895 - ETA: 6s - loss: 0.3371 - acc: 0.900 - ETA: 6s - loss: 0.2742 - acc: 0.910 - ETA: 6s - loss: 0.2619 - acc: 0.916 - ETA: 6s - loss: 0.2412 - acc: 0.909 - ETA: 6s - loss: 0.2465 - acc: 0.899 - ETA: 6s - loss: 0.2364 - acc: 0.891 - ETA: 6s - loss: 0.2386 - acc: 0.889 - ETA: 6s - loss: 0.2166 - acc: 0.893 - ETA: 6s - loss: 0.2317 - acc: 0.889 - ETA: 6s - loss: 0.2267 - acc: 0.888 - ETA: 6s - loss: 0.2600 - acc: 0.884 - ETA: 6s - loss: 0.2527 - acc: 0.887 - ETA: 6s - loss: 0.2528 - acc: 0.887 - ETA: 5s - loss: 0.2429 - acc: 0.886 - ETA: 5s - loss: 0.2416 - acc: 0.883 - ETA: 5s - loss: 0.2915 - acc: 0.875 - ETA: 5s - loss: 0.2795 - acc: 0.877 - ETA: 5s - loss: 0.2896 - acc: 0.879 - ETA: 5s - loss: 0.2815 - acc: 0.881 - ETA: 5s - loss: 0.2750 - acc: 0.881 - ETA: 4s - loss: 0.2663 - acc: 0.885 - ETA: 4s - loss: 0.2605 - acc: 0.888 - ETA: 4s - loss: 0.2561 - acc: 0.888 - ETA: 4s - loss: 0.2486 - acc: 0.890 - ETA: 4s - loss: 0.2609 - acc: 0.892 - ETA: 4s - loss: 0.2552 - acc: 0.895 - ETA: 4s - loss: 0.2648 - acc: 0.895 - ETA: 4s - loss: 0.2599 - acc: 0.895 - ETA: 4s - loss: 0.2547 - acc: 0.897 - ETA: 4s - loss: 0.2526 - acc: 0.897 - ETA: 4s - loss: 0.2594 - acc: 0.899 - ETA: 4s - loss: 0.2760 - acc: 0.897 - ETA: 3s - loss: 0.2965 - acc: 0.896 - ETA: 3s - loss: 0.3026 - acc: 0.896 - ETA: 3s - loss: 0.2990 - acc: 0.897 - ETA: 3s - loss: 0.2921 - acc: 0.899 - ETA: 3s - loss: 0.2878 - acc: 0.900 - ETA: 3s - loss: 0.2846 - acc: 0.901 - ETA: 3s - loss: 0.2873 - acc: 0.901 - ETA: 3s - loss: 0.2845 - acc: 0.903 - ETA: 3s - loss: 0.2894 - acc: 0.903 - ETA: 3s - loss: 0.2872 - acc: 0.904 - ETA: 3s - loss: 0.2971 - acc: 0.904 - ETA: 3s - loss: 0.2926 - acc: 0.906 - ETA: 3s - loss: 0.2925 - acc: 0.905 - ETA: 3s - loss: 0.3046 - acc: 0.905 - ETA: 3s - loss: 0.3002 - acc: 0.905 - ETA: 3s - loss: 0.3041 - acc: 0.905 - ETA: 2s - loss: 0.3036 - acc: 0.904 - ETA: 2s - loss: 0.3012 - acc: 0.905 - ETA: 2s - loss: 0.3061 - acc: 0.905 - ETA: 2s - loss: 0.3024 - acc: 0.905 - ETA: 2s - loss: 0.2995 - acc: 0.906 - ETA: 2s - loss: 0.2969 - acc: 0.907 - ETA: 2s - loss: 0.2935 - acc: 0.907 - ETA: 2s - loss: 0.2893 - acc: 0.908 - ETA: 2s - loss: 0.2915 - acc: 0.909 - ETA: 2s - loss: 0.2891 - acc: 0.910 - ETA: 2s - loss: 0.2860 - acc: 0.910 - ETA: 2s - loss: 0.2824 - acc: 0.911 - ETA: 2s - loss: 0.2859 - acc: 0.911 - ETA: 2s - loss: 0.2830 - acc: 0.912 - ETA: 2s - loss: 0.2813 - acc: 0.912 - ETA: 2s - loss: 0.2774 - acc: 0.914 - ETA: 1s - loss: 0.2787 - acc: 0.913 - ETA: 1s - loss: 0.2764 - acc: 0.913 - ETA: 1s - loss: 0.2749 - acc: 0.913 - ETA: 1s - loss: 0.2766 - acc: 0.912 - ETA: 1s - loss: 0.2762 - acc: 0.911 - ETA: 1s - loss: 0.2801 - acc: 0.910 - ETA: 1s - loss: 0.2776 - acc: 0.910 - ETA: 1s - loss: 0.2745 - acc: 0.911 - ETA: 1s - loss: 0.2737 - acc: 0.911 - ETA: 1s - loss: 0.2708 - acc: 0.911 - ETA: 1s - loss: 0.2756 - acc: 0.910 - ETA: 1s - loss: 0.2732 - acc: 0.911 - ETA: 1s - loss: 0.2716 - acc: 0.910 - ETA: 1s - loss: 0.2840 - acc: 0.909 - ETA: 1s - loss: 0.3038 - acc: 0.906 - ETA: 0s - loss: 0.3383 - acc: 0.902 - ETA: 0s - loss: 0.3484 - acc: 0.902 - ETA: 0s - loss: 0.3752 - acc: 0.898 - ETA: 0s - loss: 0.3908 - acc: 0.896 - ETA: 0s - loss: 0.4010 - acc: 0.895 - ETA: 0s - loss: 0.4094 - acc: 0.894 - ETA: 0s - loss: 0.4164 - acc: 0.893 - ETA: 0s - loss: 0.4208 - acc: 0.894 - ETA: 0s - loss: 0.4172 - acc: 0.895 - ETA: 0s - loss: 0.4140 - acc: 0.895 - ETA: 0s - loss: 0.4113 - acc: 0.894 - ETA: 0s - loss: 0.4085 - acc: 0.894 - ETA: 0s - loss: 0.4051 - acc: 0.894 - ETA: 0s - loss: 0.4121 - acc: 0.894 - ETA: 0s - loss: 0.4094 - acc: 0.894 - ETA: 0s - loss: 0.4059 - acc: 0.895 - 6s 2ms/step - loss: 0.4039 - acc: 0.8957 - val_loss: 0.2712 - val_acc: 0.9058\n",
      "Epoch 25/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1178 - acc: 0.875 - ETA: 4s - loss: 0.3122 - acc: 0.953 - ETA: 4s - loss: 0.1959 - acc: 0.964 - ETA: 4s - loss: 0.4147 - acc: 0.937 - ETA: 4s - loss: 0.4712 - acc: 0.913 - ETA: 4s - loss: 0.4017 - acc: 0.921 - ETA: 4s - loss: 0.3975 - acc: 0.930 - ETA: 4s - loss: 0.3616 - acc: 0.937 - ETA: 4s - loss: 0.3395 - acc: 0.937 - ETA: 4s - loss: 0.3241 - acc: 0.933 - ETA: 4s - loss: 0.3124 - acc: 0.931 - ETA: 4s - loss: 0.2924 - acc: 0.933 - ETA: 4s - loss: 0.2779 - acc: 0.934 - ETA: 4s - loss: 0.2628 - acc: 0.937 - ETA: 4s - loss: 0.2609 - acc: 0.934 - ETA: 4s - loss: 0.2609 - acc: 0.929 - ETA: 4s - loss: 0.2572 - acc: 0.925 - ETA: 4s - loss: 0.2714 - acc: 0.924 - ETA: 4s - loss: 0.2617 - acc: 0.925 - ETA: 4s - loss: 0.2740 - acc: 0.925 - ETA: 4s - loss: 0.2685 - acc: 0.926 - ETA: 3s - loss: 0.2638 - acc: 0.927 - ETA: 3s - loss: 0.2592 - acc: 0.928 - ETA: 3s - loss: 0.2538 - acc: 0.928 - ETA: 3s - loss: 0.2467 - acc: 0.928 - ETA: 3s - loss: 0.2557 - acc: 0.926 - ETA: 3s - loss: 0.2566 - acc: 0.926 - ETA: 3s - loss: 0.2546 - acc: 0.925 - ETA: 3s - loss: 0.2603 - acc: 0.923 - ETA: 3s - loss: 0.2709 - acc: 0.923 - ETA: 3s - loss: 0.2679 - acc: 0.923 - ETA: 3s - loss: 0.2653 - acc: 0.924 - ETA: 3s - loss: 0.2608 - acc: 0.925 - ETA: 3s - loss: 0.2582 - acc: 0.924 - ETA: 3s - loss: 0.2565 - acc: 0.924 - ETA: 3s - loss: 0.2527 - acc: 0.925 - ETA: 2s - loss: 0.2576 - acc: 0.923 - ETA: 2s - loss: 0.2544 - acc: 0.923 - ETA: 2s - loss: 0.2676 - acc: 0.923 - ETA: 2s - loss: 0.2619 - acc: 0.925 - ETA: 2s - loss: 0.2664 - acc: 0.924 - ETA: 2s - loss: 0.2719 - acc: 0.922 - ETA: 2s - loss: 0.2726 - acc: 0.921 - ETA: 2s - loss: 0.2686 - acc: 0.922 - ETA: 2s - loss: 0.2647 - acc: 0.922 - ETA: 2s - loss: 0.2627 - acc: 0.921 - ETA: 2s - loss: 0.2606 - acc: 0.921 - ETA: 2s - loss: 0.2724 - acc: 0.920 - ETA: 2s - loss: 0.2769 - acc: 0.919 - ETA: 2s - loss: 0.2834 - acc: 0.917 - ETA: 2s - loss: 0.2878 - acc: 0.917 - ETA: 2s - loss: 0.2920 - acc: 0.917 - ETA: 1s - loss: 0.2885 - acc: 0.918 - ETA: 1s - loss: 0.2914 - acc: 0.919 - ETA: 1s - loss: 0.2889 - acc: 0.919 - ETA: 1s - loss: 0.2846 - acc: 0.920 - ETA: 1s - loss: 0.2829 - acc: 0.920 - ETA: 1s - loss: 0.2819 - acc: 0.919 - ETA: 1s - loss: 0.2789 - acc: 0.919 - ETA: 1s - loss: 0.2768 - acc: 0.918 - ETA: 1s - loss: 0.2734 - acc: 0.919 - ETA: 1s - loss: 0.2724 - acc: 0.919 - ETA: 1s - loss: 0.2695 - acc: 0.919 - ETA: 1s - loss: 0.2667 - acc: 0.920 - ETA: 1s - loss: 0.2652 - acc: 0.919 - ETA: 1s - loss: 0.2627 - acc: 0.920 - ETA: 1s - loss: 0.2609 - acc: 0.919 - ETA: 1s - loss: 0.2585 - acc: 0.919 - ETA: 1s - loss: 0.2621 - acc: 0.919 - ETA: 0s - loss: 0.2602 - acc: 0.919 - ETA: 0s - loss: 0.2570 - acc: 0.919 - ETA: 0s - loss: 0.2573 - acc: 0.919 - ETA: 0s - loss: 0.2543 - acc: 0.920 - ETA: 0s - loss: 0.2570 - acc: 0.919 - ETA: 0s - loss: 0.2598 - acc: 0.920 - ETA: 0s - loss: 0.2569 - acc: 0.920 - ETA: 0s - loss: 0.2550 - acc: 0.921 - ETA: 0s - loss: 0.2580 - acc: 0.921 - ETA: 0s - loss: 0.2563 - acc: 0.921 - ETA: 0s - loss: 0.2589 - acc: 0.922 - ETA: 0s - loss: 0.2615 - acc: 0.922 - ETA: 0s - loss: 0.2602 - acc: 0.922 - ETA: 0s - loss: 0.2580 - acc: 0.923 - ETA: 0s - loss: 0.2605 - acc: 0.923 - ETA: 0s - loss: 0.2582 - acc: 0.923 - 5s 1ms/step - loss: 0.2615 - acc: 0.9238 - val_loss: 0.2360 - val_acc: 0.9301\n",
      "Epoch 26/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.3067 - acc: 0.812 - ETA: 4s - loss: 0.7626 - acc: 0.890 - ETA: 4s - loss: 0.5117 - acc: 0.910 - ETA: 4s - loss: 0.3862 - acc: 0.931 - ETA: 4s - loss: 0.3269 - acc: 0.937 - ETA: 4s - loss: 0.2906 - acc: 0.945 - ETA: 4s - loss: 0.3109 - acc: 0.934 - ETA: 4s - loss: 0.2955 - acc: 0.937 - ETA: 4s - loss: 0.2839 - acc: 0.940 - ETA: 4s - loss: 0.2679 - acc: 0.942 - ETA: 4s - loss: 0.2562 - acc: 0.941 - ETA: 4s - loss: 0.2663 - acc: 0.937 - ETA: 4s - loss: 0.2691 - acc: 0.937 - ETA: 4s - loss: 0.2633 - acc: 0.934 - ETA: 4s - loss: 0.2810 - acc: 0.927 - ETA: 4s - loss: 0.2725 - acc: 0.928 - ETA: 4s - loss: 0.2661 - acc: 0.927 - ETA: 4s - loss: 0.2626 - acc: 0.927 - ETA: 3s - loss: 0.2573 - acc: 0.926 - ETA: 3s - loss: 0.2514 - acc: 0.925 - ETA: 3s - loss: 0.2449 - acc: 0.927 - ETA: 3s - loss: 0.2593 - acc: 0.926 - ETA: 3s - loss: 0.2688 - acc: 0.928 - ETA: 3s - loss: 0.2758 - acc: 0.928 - ETA: 3s - loss: 0.2711 - acc: 0.928 - ETA: 3s - loss: 0.2693 - acc: 0.929 - ETA: 3s - loss: 0.2790 - acc: 0.928 - ETA: 3s - loss: 0.2741 - acc: 0.928 - ETA: 3s - loss: 0.2765 - acc: 0.927 - ETA: 3s - loss: 0.2754 - acc: 0.929 - ETA: 3s - loss: 0.2833 - acc: 0.927 - ETA: 3s - loss: 0.2827 - acc: 0.928 - ETA: 3s - loss: 0.2790 - acc: 0.929 - ETA: 3s - loss: 0.2745 - acc: 0.931 - ETA: 3s - loss: 0.2707 - acc: 0.931 - ETA: 3s - loss: 0.2677 - acc: 0.931 - ETA: 3s - loss: 0.2643 - acc: 0.933 - ETA: 3s - loss: 0.2593 - acc: 0.934 - ETA: 2s - loss: 0.2563 - acc: 0.934 - ETA: 2s - loss: 0.2617 - acc: 0.935 - ETA: 2s - loss: 0.2760 - acc: 0.934 - ETA: 2s - loss: 0.2721 - acc: 0.935 - ETA: 2s - loss: 0.2685 - acc: 0.936 - ETA: 2s - loss: 0.2636 - acc: 0.937 - ETA: 2s - loss: 0.2668 - acc: 0.938 - ETA: 2s - loss: 0.2682 - acc: 0.937 - ETA: 2s - loss: 0.2659 - acc: 0.938 - ETA: 2s - loss: 0.2647 - acc: 0.938 - ETA: 2s - loss: 0.2628 - acc: 0.938 - ETA: 2s - loss: 0.2614 - acc: 0.938 - ETA: 2s - loss: 0.2591 - acc: 0.937 - ETA: 2s - loss: 0.2560 - acc: 0.937 - ETA: 2s - loss: 0.2532 - acc: 0.937 - ETA: 2s - loss: 0.2517 - acc: 0.937 - ETA: 2s - loss: 0.2565 - acc: 0.935 - ETA: 2s - loss: 0.2549 - acc: 0.936 - ETA: 1s - loss: 0.2527 - acc: 0.937 - ETA: 1s - loss: 0.2504 - acc: 0.937 - ETA: 1s - loss: 0.2492 - acc: 0.937 - ETA: 1s - loss: 0.2469 - acc: 0.937 - ETA: 1s - loss: 0.2441 - acc: 0.938 - ETA: 1s - loss: 0.2426 - acc: 0.938 - ETA: 1s - loss: 0.2453 - acc: 0.937 - ETA: 1s - loss: 0.2447 - acc: 0.937 - ETA: 1s - loss: 0.2443 - acc: 0.937 - ETA: 1s - loss: 0.2444 - acc: 0.937 - ETA: 1s - loss: 0.2423 - acc: 0.937 - ETA: 1s - loss: 0.2414 - acc: 0.937 - ETA: 1s - loss: 0.2413 - acc: 0.936 - ETA: 1s - loss: 0.2402 - acc: 0.937 - ETA: 1s - loss: 0.2409 - acc: 0.936 - ETA: 1s - loss: 0.2403 - acc: 0.936 - ETA: 1s - loss: 0.2400 - acc: 0.936 - ETA: 0s - loss: 0.2383 - acc: 0.936 - ETA: 0s - loss: 0.2372 - acc: 0.936 - ETA: 0s - loss: 0.2354 - acc: 0.936 - ETA: 0s - loss: 0.2339 - acc: 0.936 - ETA: 0s - loss: 0.2392 - acc: 0.936 - ETA: 0s - loss: 0.2376 - acc: 0.936 - ETA: 0s - loss: 0.2367 - acc: 0.935 - ETA: 0s - loss: 0.2385 - acc: 0.935 - ETA: 0s - loss: 0.2389 - acc: 0.933 - ETA: 0s - loss: 0.2389 - acc: 0.932 - ETA: 0s - loss: 0.2405 - acc: 0.932 - ETA: 0s - loss: 0.2418 - acc: 0.930 - ETA: 0s - loss: 0.2405 - acc: 0.930 - ETA: 0s - loss: 0.2412 - acc: 0.929 - ETA: 0s - loss: 0.2400 - acc: 0.929 - ETA: 0s - loss: 0.2390 - acc: 0.928 - 6s 1ms/step - loss: 0.2377 - acc: 0.9292 - val_loss: 0.3049 - val_acc: 0.8974\n",
      "Epoch 27/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0032 - acc: 1.000 - ETA: 4s - loss: 0.1378 - acc: 0.890 - ETA: 5s - loss: 0.3557 - acc: 0.875 - ETA: 5s - loss: 0.2911 - acc: 0.875 - ETA: 4s - loss: 0.3802 - acc: 0.865 - ETA: 4s - loss: 0.3603 - acc: 0.867 - ETA: 4s - loss: 0.3156 - acc: 0.878 - ETA: 4s - loss: 0.3118 - acc: 0.886 - ETA: 4s - loss: 0.3021 - acc: 0.882 - ETA: 4s - loss: 0.2945 - acc: 0.883 - ETA: 4s - loss: 0.2819 - acc: 0.887 - ETA: 4s - loss: 0.2846 - acc: 0.882 - ETA: 4s - loss: 0.2739 - acc: 0.885 - ETA: 4s - loss: 0.2634 - acc: 0.890 - ETA: 4s - loss: 0.2594 - acc: 0.892 - ETA: 4s - loss: 0.2604 - acc: 0.891 - ETA: 4s - loss: 0.2767 - acc: 0.891 - ETA: 4s - loss: 0.2923 - acc: 0.893 - ETA: 3s - loss: 0.2887 - acc: 0.893 - ETA: 3s - loss: 0.2833 - acc: 0.895 - ETA: 3s - loss: 0.2764 - acc: 0.897 - ETA: 3s - loss: 0.2802 - acc: 0.896 - ETA: 3s - loss: 0.2870 - acc: 0.899 - ETA: 3s - loss: 0.3095 - acc: 0.898 - ETA: 3s - loss: 0.3031 - acc: 0.899 - ETA: 3s - loss: 0.2945 - acc: 0.902 - ETA: 3s - loss: 0.2900 - acc: 0.902 - ETA: 3s - loss: 0.3065 - acc: 0.904 - ETA: 3s - loss: 0.3036 - acc: 0.902 - ETA: 3s - loss: 0.3084 - acc: 0.903 - ETA: 3s - loss: 0.3082 - acc: 0.903 - ETA: 3s - loss: 0.3065 - acc: 0.903 - ETA: 3s - loss: 0.3141 - acc: 0.903 - ETA: 3s - loss: 0.3112 - acc: 0.902 - ETA: 3s - loss: 0.3064 - acc: 0.903 - ETA: 2s - loss: 0.3256 - acc: 0.901 - ETA: 2s - loss: 0.3222 - acc: 0.902 - ETA: 2s - loss: 0.3237 - acc: 0.903 - ETA: 2s - loss: 0.3204 - acc: 0.903 - ETA: 2s - loss: 0.3154 - acc: 0.904 - ETA: 2s - loss: 0.3112 - acc: 0.905 - ETA: 2s - loss: 0.3060 - acc: 0.907 - ETA: 2s - loss: 0.3088 - acc: 0.908 - ETA: 2s - loss: 0.3026 - acc: 0.910 - ETA: 2s - loss: 0.3097 - acc: 0.910 - ETA: 2s - loss: 0.3151 - acc: 0.909 - ETA: 2s - loss: 0.3211 - acc: 0.909 - ETA: 2s - loss: 0.3243 - acc: 0.910 - ETA: 2s - loss: 0.3203 - acc: 0.910 - ETA: 2s - loss: 0.3201 - acc: 0.910 - ETA: 2s - loss: 0.3187 - acc: 0.911 - ETA: 2s - loss: 0.3145 - acc: 0.912 - ETA: 1s - loss: 0.3150 - acc: 0.912 - ETA: 1s - loss: 0.3141 - acc: 0.911 - ETA: 1s - loss: 0.3172 - acc: 0.912 - ETA: 1s - loss: 0.3270 - acc: 0.912 - ETA: 1s - loss: 0.3229 - acc: 0.913 - ETA: 1s - loss: 0.3223 - acc: 0.914 - ETA: 1s - loss: 0.3204 - acc: 0.914 - ETA: 1s - loss: 0.3183 - acc: 0.914 - ETA: 1s - loss: 0.3158 - acc: 0.914 - ETA: 1s - loss: 0.3136 - acc: 0.915 - ETA: 1s - loss: 0.3119 - acc: 0.915 - ETA: 1s - loss: 0.3090 - acc: 0.916 - ETA: 1s - loss: 0.3051 - acc: 0.916 - ETA: 1s - loss: 0.3019 - acc: 0.917 - ETA: 1s - loss: 0.3006 - acc: 0.917 - ETA: 1s - loss: 0.2975 - acc: 0.918 - ETA: 0s - loss: 0.2942 - acc: 0.919 - ETA: 0s - loss: 0.2923 - acc: 0.919 - ETA: 0s - loss: 0.2890 - acc: 0.920 - ETA: 0s - loss: 0.2909 - acc: 0.920 - ETA: 0s - loss: 0.2905 - acc: 0.920 - ETA: 0s - loss: 0.2938 - acc: 0.920 - ETA: 0s - loss: 0.2913 - acc: 0.921 - ETA: 0s - loss: 0.2893 - acc: 0.921 - ETA: 0s - loss: 0.2919 - acc: 0.921 - ETA: 0s - loss: 0.2906 - acc: 0.921 - ETA: 0s - loss: 0.2902 - acc: 0.920 - ETA: 0s - loss: 0.2880 - acc: 0.921 - ETA: 0s - loss: 0.2905 - acc: 0.920 - ETA: 0s - loss: 0.2943 - acc: 0.920 - ETA: 0s - loss: 0.2925 - acc: 0.920 - ETA: 0s - loss: 0.2925 - acc: 0.920 - ETA: 0s - loss: 0.2947 - acc: 0.919 - ETA: 0s - loss: 0.2927 - acc: 0.920 - ETA: 0s - loss: 0.2906 - acc: 0.920 - 5s 1ms/step - loss: 0.2904 - acc: 0.9208 - val_loss: 0.3097 - val_acc: 0.9199\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.3593 - acc: 0.875 - ETA: 5s - loss: 0.4135 - acc: 0.921 - ETA: 4s - loss: 0.4264 - acc: 0.928 - ETA: 4s - loss: 0.3423 - acc: 0.937 - ETA: 5s - loss: 0.3171 - acc: 0.937 - ETA: 5s - loss: 0.2989 - acc: 0.933 - ETA: 5s - loss: 0.3400 - acc: 0.926 - ETA: 5s - loss: 0.4359 - acc: 0.918 - ETA: 5s - loss: 0.4335 - acc: 0.923 - ETA: 4s - loss: 0.4340 - acc: 0.927 - ETA: 4s - loss: 0.4018 - acc: 0.928 - ETA: 4s - loss: 0.3897 - acc: 0.927 - ETA: 4s - loss: 0.3880 - acc: 0.929 - ETA: 4s - loss: 0.3978 - acc: 0.927 - ETA: 4s - loss: 0.4605 - acc: 0.921 - ETA: 4s - loss: 0.4439 - acc: 0.922 - ETA: 4s - loss: 0.4304 - acc: 0.923 - ETA: 4s - loss: 0.4383 - acc: 0.919 - ETA: 4s - loss: 0.4206 - acc: 0.918 - ETA: 4s - loss: 0.4074 - acc: 0.920 - ETA: 4s - loss: 0.4193 - acc: 0.921 - ETA: 4s - loss: 0.4062 - acc: 0.917 - ETA: 4s - loss: 0.3930 - acc: 0.916 - ETA: 4s - loss: 0.3843 - acc: 0.917 - ETA: 4s - loss: 0.3755 - acc: 0.917 - ETA: 4s - loss: 0.3618 - acc: 0.919 - ETA: 4s - loss: 0.3604 - acc: 0.919 - ETA: 4s - loss: 0.3492 - acc: 0.921 - ETA: 4s - loss: 0.3453 - acc: 0.920 - ETA: 4s - loss: 0.3439 - acc: 0.918 - ETA: 4s - loss: 0.3643 - acc: 0.916 - ETA: 4s - loss: 0.3551 - acc: 0.918 - ETA: 3s - loss: 0.3441 - acc: 0.921 - ETA: 3s - loss: 0.3435 - acc: 0.918 - ETA: 3s - loss: 0.3367 - acc: 0.919 - ETA: 3s - loss: 0.3344 - acc: 0.919 - ETA: 3s - loss: 0.3289 - acc: 0.920 - ETA: 3s - loss: 0.3237 - acc: 0.920 - ETA: 3s - loss: 0.3166 - acc: 0.921 - ETA: 3s - loss: 0.3103 - acc: 0.922 - ETA: 3s - loss: 0.3048 - acc: 0.923 - ETA: 3s - loss: 0.3107 - acc: 0.923 - ETA: 3s - loss: 0.3123 - acc: 0.922 - ETA: 3s - loss: 0.3047 - acc: 0.924 - ETA: 3s - loss: 0.3012 - acc: 0.924 - ETA: 3s - loss: 0.2967 - acc: 0.924 - ETA: 3s - loss: 0.3001 - acc: 0.924 - ETA: 2s - loss: 0.2950 - acc: 0.925 - ETA: 2s - loss: 0.2912 - acc: 0.924 - ETA: 2s - loss: 0.2978 - acc: 0.923 - ETA: 2s - loss: 0.3070 - acc: 0.923 - ETA: 2s - loss: 0.3036 - acc: 0.923 - ETA: 2s - loss: 0.3030 - acc: 0.923 - ETA: 2s - loss: 0.3074 - acc: 0.921 - ETA: 2s - loss: 0.3060 - acc: 0.920 - ETA: 2s - loss: 0.3025 - acc: 0.920 - ETA: 2s - loss: 0.2972 - acc: 0.922 - ETA: 2s - loss: 0.2949 - acc: 0.922 - ETA: 2s - loss: 0.2928 - acc: 0.922 - ETA: 2s - loss: 0.2905 - acc: 0.923 - ETA: 2s - loss: 0.2877 - acc: 0.923 - ETA: 2s - loss: 0.2872 - acc: 0.923 - ETA: 2s - loss: 0.2848 - acc: 0.923 - ETA: 2s - loss: 0.2937 - acc: 0.923 - ETA: 1s - loss: 0.2891 - acc: 0.925 - ETA: 1s - loss: 0.2869 - acc: 0.925 - ETA: 1s - loss: 0.2838 - acc: 0.926 - ETA: 1s - loss: 0.2878 - acc: 0.925 - ETA: 1s - loss: 0.2852 - acc: 0.926 - ETA: 1s - loss: 0.2902 - acc: 0.925 - ETA: 1s - loss: 0.2887 - acc: 0.924 - ETA: 1s - loss: 0.2853 - acc: 0.924 - ETA: 1s - loss: 0.2824 - acc: 0.925 - ETA: 1s - loss: 0.2803 - acc: 0.925 - ETA: 1s - loss: 0.2774 - acc: 0.926 - ETA: 1s - loss: 0.2745 - acc: 0.926 - ETA: 1s - loss: 0.2879 - acc: 0.925 - ETA: 1s - loss: 0.2851 - acc: 0.926 - ETA: 1s - loss: 0.2871 - acc: 0.926 - ETA: 0s - loss: 0.2867 - acc: 0.926 - ETA: 0s - loss: 0.2834 - acc: 0.927 - ETA: 0s - loss: 0.2802 - acc: 0.928 - ETA: 0s - loss: 0.2816 - acc: 0.928 - ETA: 0s - loss: 0.2809 - acc: 0.929 - ETA: 0s - loss: 0.2804 - acc: 0.928 - ETA: 0s - loss: 0.2797 - acc: 0.928 - ETA: 0s - loss: 0.2774 - acc: 0.929 - ETA: 0s - loss: 0.2757 - acc: 0.929 - ETA: 0s - loss: 0.2737 - acc: 0.930 - ETA: 0s - loss: 0.2759 - acc: 0.930 - ETA: 0s - loss: 0.2781 - acc: 0.930 - ETA: 0s - loss: 0.2757 - acc: 0.930 - ETA: 0s - loss: 0.2770 - acc: 0.930 - ETA: 0s - loss: 0.2786 - acc: 0.930 - 6s 1ms/step - loss: 0.2784 - acc: 0.9307 - val_loss: 0.2673 - val_acc: 0.9212\n",
      "Epoch 29/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 0.2920 - acc: 0.875 - ETA: 4s - loss: 0.2308 - acc: 0.921 - ETA: 4s - loss: 0.2050 - acc: 0.928 - ETA: 4s - loss: 0.1752 - acc: 0.931 - ETA: 4s - loss: 0.2374 - acc: 0.903 - ETA: 4s - loss: 0.2556 - acc: 0.898 - ETA: 4s - loss: 0.2494 - acc: 0.898 - ETA: 4s - loss: 0.2531 - acc: 0.900 - ETA: 4s - loss: 0.2485 - acc: 0.905 - ETA: 4s - loss: 0.2487 - acc: 0.897 - ETA: 4s - loss: 0.2454 - acc: 0.901 - ETA: 4s - loss: 0.2436 - acc: 0.904 - ETA: 4s - loss: 0.2389 - acc: 0.908 - ETA: 4s - loss: 0.2286 - acc: 0.910 - ETA: 4s - loss: 0.2257 - acc: 0.911 - ETA: 4s - loss: 0.2265 - acc: 0.909 - ETA: 4s - loss: 0.2208 - acc: 0.912 - ETA: 4s - loss: 0.2148 - acc: 0.915 - ETA: 4s - loss: 0.2095 - acc: 0.918 - ETA: 3s - loss: 0.2038 - acc: 0.921 - ETA: 3s - loss: 0.2083 - acc: 0.920 - ETA: 3s - loss: 0.2041 - acc: 0.920 - ETA: 3s - loss: 0.2166 - acc: 0.922 - ETA: 3s - loss: 0.2160 - acc: 0.922 - ETA: 3s - loss: 0.2160 - acc: 0.922 - ETA: 3s - loss: 0.2113 - acc: 0.925 - ETA: 3s - loss: 0.2205 - acc: 0.924 - ETA: 3s - loss: 0.2195 - acc: 0.925 - ETA: 3s - loss: 0.2250 - acc: 0.925 - ETA: 3s - loss: 0.2295 - acc: 0.926 - ETA: 3s - loss: 0.2305 - acc: 0.924 - ETA: 3s - loss: 0.2297 - acc: 0.925 - ETA: 3s - loss: 0.2314 - acc: 0.923 - ETA: 3s - loss: 0.2307 - acc: 0.924 - ETA: 3s - loss: 0.2322 - acc: 0.924 - ETA: 3s - loss: 0.2301 - acc: 0.925 - ETA: 3s - loss: 0.2271 - acc: 0.926 - ETA: 2s - loss: 0.2339 - acc: 0.927 - ETA: 2s - loss: 0.2305 - acc: 0.928 - ETA: 2s - loss: 0.2291 - acc: 0.927 - ETA: 2s - loss: 0.2367 - acc: 0.927 - ETA: 2s - loss: 0.2358 - acc: 0.927 - ETA: 2s - loss: 0.2424 - acc: 0.928 - ETA: 2s - loss: 0.2442 - acc: 0.928 - ETA: 2s - loss: 0.2426 - acc: 0.928 - ETA: 2s - loss: 0.2481 - acc: 0.925 - ETA: 2s - loss: 0.2489 - acc: 0.924 - ETA: 2s - loss: 0.2573 - acc: 0.921 - ETA: 2s - loss: 0.2577 - acc: 0.919 - ETA: 2s - loss: 0.2577 - acc: 0.919 - ETA: 2s - loss: 0.2581 - acc: 0.917 - ETA: 2s - loss: 0.2572 - acc: 0.916 - ETA: 2s - loss: 0.2560 - acc: 0.916 - ETA: 1s - loss: 0.2535 - acc: 0.917 - ETA: 1s - loss: 0.2501 - acc: 0.919 - ETA: 1s - loss: 0.2483 - acc: 0.919 - ETA: 1s - loss: 0.2471 - acc: 0.919 - ETA: 1s - loss: 0.2503 - acc: 0.919 - ETA: 1s - loss: 0.2603 - acc: 0.919 - ETA: 1s - loss: 0.2595 - acc: 0.919 - ETA: 1s - loss: 0.2637 - acc: 0.919 - ETA: 1s - loss: 0.2624 - acc: 0.919 - ETA: 1s - loss: 0.2714 - acc: 0.919 - ETA: 1s - loss: 0.2782 - acc: 0.918 - ETA: 1s - loss: 0.2769 - acc: 0.918 - ETA: 1s - loss: 0.2793 - acc: 0.918 - ETA: 1s - loss: 0.2781 - acc: 0.918 - ETA: 1s - loss: 0.2818 - acc: 0.918 - ETA: 1s - loss: 0.2915 - acc: 0.917 - ETA: 1s - loss: 0.2944 - acc: 0.917 - ETA: 1s - loss: 0.2933 - acc: 0.917 - ETA: 1s - loss: 0.2922 - acc: 0.917 - ETA: 1s - loss: 0.2905 - acc: 0.917 - ETA: 1s - loss: 0.2884 - acc: 0.918 - ETA: 0s - loss: 0.2870 - acc: 0.918 - ETA: 0s - loss: 0.2878 - acc: 0.919 - ETA: 0s - loss: 0.2897 - acc: 0.919 - ETA: 0s - loss: 0.2920 - acc: 0.918 - ETA: 0s - loss: 0.2902 - acc: 0.918 - ETA: 0s - loss: 0.2875 - acc: 0.919 - ETA: 0s - loss: 0.2866 - acc: 0.919 - ETA: 0s - loss: 0.2869 - acc: 0.919 - ETA: 0s - loss: 0.2844 - acc: 0.919 - ETA: 0s - loss: 0.2820 - acc: 0.920 - ETA: 0s - loss: 0.2806 - acc: 0.920 - ETA: 0s - loss: 0.2789 - acc: 0.920 - ETA: 0s - loss: 0.2776 - acc: 0.920 - ETA: 0s - loss: 0.2775 - acc: 0.921 - ETA: 0s - loss: 0.2762 - acc: 0.921 - 6s 1ms/step - loss: 0.2748 - acc: 0.9221 - val_loss: 0.2145 - val_acc: 0.9179\n",
      "Epoch 30/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.2798 - acc: 0.875 - ETA: 4s - loss: 0.1863 - acc: 0.906 - ETA: 4s - loss: 0.1849 - acc: 0.928 - ETA: 4s - loss: 0.1668 - acc: 0.937 - ETA: 4s - loss: 0.1739 - acc: 0.932 - ETA: 4s - loss: 0.2039 - acc: 0.933 - ETA: 4s - loss: 0.1930 - acc: 0.934 - ETA: 4s - loss: 0.1812 - acc: 0.937 - ETA: 4s - loss: 0.2186 - acc: 0.930 - ETA: 4s - loss: 0.2344 - acc: 0.921 - ETA: 4s - loss: 0.2436 - acc: 0.917 - ETA: 4s - loss: 0.2645 - acc: 0.911 - ETA: 4s - loss: 0.2973 - acc: 0.903 - ETA: 4s - loss: 0.2997 - acc: 0.904 - ETA: 4s - loss: 0.3017 - acc: 0.905 - ETA: 4s - loss: 0.3054 - acc: 0.902 - ETA: 4s - loss: 0.3003 - acc: 0.904 - ETA: 3s - loss: 0.2948 - acc: 0.906 - ETA: 3s - loss: 0.2914 - acc: 0.905 - ETA: 3s - loss: 0.2997 - acc: 0.900 - ETA: 3s - loss: 0.2967 - acc: 0.903 - ETA: 3s - loss: 0.3098 - acc: 0.901 - ETA: 3s - loss: 0.3034 - acc: 0.903 - ETA: 3s - loss: 0.3008 - acc: 0.902 - ETA: 3s - loss: 0.3034 - acc: 0.900 - ETA: 3s - loss: 0.3017 - acc: 0.899 - ETA: 3s - loss: 0.2954 - acc: 0.901 - ETA: 3s - loss: 0.2921 - acc: 0.902 - ETA: 3s - loss: 0.2899 - acc: 0.903 - ETA: 3s - loss: 0.2905 - acc: 0.904 - ETA: 3s - loss: 0.2914 - acc: 0.903 - ETA: 3s - loss: 0.2911 - acc: 0.901 - ETA: 3s - loss: 0.2864 - acc: 0.902 - ETA: 3s - loss: 0.2813 - acc: 0.905 - ETA: 2s - loss: 0.2770 - acc: 0.907 - ETA: 2s - loss: 0.2760 - acc: 0.907 - ETA: 2s - loss: 0.2718 - acc: 0.909 - ETA: 2s - loss: 0.2723 - acc: 0.909 - ETA: 2s - loss: 0.2715 - acc: 0.909 - ETA: 2s - loss: 0.2697 - acc: 0.907 - ETA: 2s - loss: 0.2683 - acc: 0.908 - ETA: 2s - loss: 0.2638 - acc: 0.910 - ETA: 2s - loss: 0.2648 - acc: 0.910 - ETA: 2s - loss: 0.2611 - acc: 0.911 - ETA: 2s - loss: 0.2611 - acc: 0.911 - ETA: 2s - loss: 0.2599 - acc: 0.909 - ETA: 2s - loss: 0.2566 - acc: 0.910 - ETA: 2s - loss: 0.2567 - acc: 0.911 - ETA: 2s - loss: 0.2529 - acc: 0.912 - ETA: 2s - loss: 0.2523 - acc: 0.913 - ETA: 2s - loss: 0.2491 - acc: 0.914 - ETA: 2s - loss: 0.2476 - acc: 0.915 - ETA: 1s - loss: 0.2475 - acc: 0.915 - ETA: 1s - loss: 0.2451 - acc: 0.916 - ETA: 1s - loss: 0.2490 - acc: 0.914 - ETA: 1s - loss: 0.2537 - acc: 0.911 - ETA: 1s - loss: 0.2521 - acc: 0.911 - ETA: 1s - loss: 0.2510 - acc: 0.911 - ETA: 1s - loss: 0.2489 - acc: 0.912 - ETA: 1s - loss: 0.2472 - acc: 0.913 - ETA: 1s - loss: 0.2480 - acc: 0.913 - ETA: 1s - loss: 0.2469 - acc: 0.914 - ETA: 1s - loss: 0.2472 - acc: 0.915 - ETA: 1s - loss: 0.2463 - acc: 0.914 - ETA: 1s - loss: 0.2444 - acc: 0.915 - ETA: 1s - loss: 0.2426 - acc: 0.915 - ETA: 1s - loss: 0.2403 - acc: 0.916 - ETA: 1s - loss: 0.2415 - acc: 0.915 - ETA: 1s - loss: 0.2392 - acc: 0.916 - ETA: 0s - loss: 0.2386 - acc: 0.916 - ETA: 0s - loss: 0.2382 - acc: 0.916 - ETA: 0s - loss: 0.2359 - acc: 0.917 - ETA: 0s - loss: 0.2334 - acc: 0.918 - ETA: 0s - loss: 0.2326 - acc: 0.918 - ETA: 0s - loss: 0.2331 - acc: 0.917 - ETA: 0s - loss: 0.2328 - acc: 0.918 - ETA: 0s - loss: 0.2308 - acc: 0.918 - ETA: 0s - loss: 0.2289 - acc: 0.919 - ETA: 0s - loss: 0.2270 - acc: 0.920 - ETA: 0s - loss: 0.2251 - acc: 0.921 - ETA: 0s - loss: 0.2252 - acc: 0.920 - ETA: 0s - loss: 0.2235 - acc: 0.921 - ETA: 0s - loss: 0.2228 - acc: 0.921 - ETA: 0s - loss: 0.2213 - acc: 0.922 - ETA: 0s - loss: 0.2235 - acc: 0.922 - ETA: 0s - loss: 0.2246 - acc: 0.922 - 5s 1ms/step - loss: 0.2245 - acc: 0.9223 - val_loss: 0.2524 - val_acc: 0.9058\n",
      "Epoch 31/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0422 - acc: 1.000 - ETA: 5s - loss: 0.1848 - acc: 0.921 - ETA: 5s - loss: 0.2000 - acc: 0.910 - ETA: 5s - loss: 0.2868 - acc: 0.906 - ETA: 5s - loss: 0.2607 - acc: 0.916 - ETA: 6s - loss: 0.2348 - acc: 0.924 - ETA: 6s - loss: 0.2279 - acc: 0.918 - ETA: 6s - loss: 0.2070 - acc: 0.924 - ETA: 5s - loss: 0.2087 - acc: 0.923 - ETA: 5s - loss: 0.1947 - acc: 0.925 - ETA: 5s - loss: 0.2028 - acc: 0.921 - ETA: 5s - loss: 0.2121 - acc: 0.919 - ETA: 5s - loss: 0.2223 - acc: 0.921 - ETA: 4s - loss: 0.2217 - acc: 0.918 - ETA: 4s - loss: 0.2166 - acc: 0.920 - ETA: 4s - loss: 0.2346 - acc: 0.921 - ETA: 4s - loss: 0.2336 - acc: 0.918 - ETA: 4s - loss: 0.2271 - acc: 0.920 - ETA: 4s - loss: 0.2184 - acc: 0.924 - ETA: 4s - loss: 0.2229 - acc: 0.921 - ETA: 4s - loss: 0.2149 - acc: 0.924 - ETA: 4s - loss: 0.2102 - acc: 0.926 - ETA: 4s - loss: 0.2068 - acc: 0.927 - ETA: 4s - loss: 0.2086 - acc: 0.927 - ETA: 3s - loss: 0.2097 - acc: 0.927 - ETA: 3s - loss: 0.2187 - acc: 0.928 - ETA: 3s - loss: 0.2173 - acc: 0.927 - ETA: 3s - loss: 0.2142 - acc: 0.928 - ETA: 3s - loss: 0.2126 - acc: 0.928 - ETA: 3s - loss: 0.2059 - acc: 0.930 - ETA: 3s - loss: 0.2175 - acc: 0.929 - ETA: 3s - loss: 0.2209 - acc: 0.927 - ETA: 3s - loss: 0.2307 - acc: 0.926 - ETA: 3s - loss: 0.2443 - acc: 0.924 - ETA: 3s - loss: 0.2515 - acc: 0.924 - ETA: 3s - loss: 0.2682 - acc: 0.924 - ETA: 3s - loss: 0.2637 - acc: 0.924 - ETA: 3s - loss: 0.2596 - acc: 0.925 - ETA: 3s - loss: 0.2675 - acc: 0.924 - ETA: 3s - loss: 0.2654 - acc: 0.924 - ETA: 2s - loss: 0.2693 - acc: 0.925 - ETA: 2s - loss: 0.2724 - acc: 0.925 - ETA: 2s - loss: 0.2718 - acc: 0.926 - ETA: 2s - loss: 0.2698 - acc: 0.926 - ETA: 2s - loss: 0.2736 - acc: 0.927 - ETA: 2s - loss: 0.2786 - acc: 0.927 - ETA: 2s - loss: 0.2745 - acc: 0.928 - ETA: 2s - loss: 0.2726 - acc: 0.928 - ETA: 2s - loss: 0.2791 - acc: 0.928 - ETA: 2s - loss: 0.2753 - acc: 0.929 - ETA: 2s - loss: 0.2789 - acc: 0.928 - ETA: 2s - loss: 0.2827 - acc: 0.929 - ETA: 2s - loss: 0.2783 - acc: 0.930 - ETA: 2s - loss: 0.2813 - acc: 0.931 - ETA: 2s - loss: 0.2907 - acc: 0.930 - ETA: 1s - loss: 0.2870 - acc: 0.931 - ETA: 1s - loss: 0.2876 - acc: 0.931 - ETA: 1s - loss: 0.2840 - acc: 0.932 - ETA: 1s - loss: 0.2885 - acc: 0.931 - ETA: 1s - loss: 0.2845 - acc: 0.932 - ETA: 1s - loss: 0.2805 - acc: 0.933 - ETA: 1s - loss: 0.2788 - acc: 0.932 - ETA: 1s - loss: 0.2756 - acc: 0.933 - ETA: 1s - loss: 0.2723 - acc: 0.933 - ETA: 1s - loss: 0.2695 - acc: 0.933 - ETA: 1s - loss: 0.2710 - acc: 0.934 - ETA: 1s - loss: 0.2696 - acc: 0.933 - ETA: 1s - loss: 0.2779 - acc: 0.932 - ETA: 1s - loss: 0.2748 - acc: 0.933 - ETA: 1s - loss: 0.2718 - acc: 0.933 - ETA: 1s - loss: 0.2740 - acc: 0.934 - ETA: 0s - loss: 0.2793 - acc: 0.933 - ETA: 0s - loss: 0.2786 - acc: 0.933 - ETA: 0s - loss: 0.2811 - acc: 0.933 - ETA: 0s - loss: 0.2805 - acc: 0.932 - ETA: 0s - loss: 0.2879 - acc: 0.931 - ETA: 0s - loss: 0.2862 - acc: 0.931 - ETA: 0s - loss: 0.2898 - acc: 0.930 - ETA: 0s - loss: 0.2920 - acc: 0.930 - ETA: 0s - loss: 0.2906 - acc: 0.930 - ETA: 0s - loss: 0.2926 - acc: 0.930 - ETA: 0s - loss: 0.2949 - acc: 0.929 - ETA: 0s - loss: 0.3018 - acc: 0.929 - ETA: 0s - loss: 0.3095 - acc: 0.927 - ETA: 0s - loss: 0.3071 - acc: 0.928 - ETA: 0s - loss: 0.3087 - acc: 0.928 - ETA: 0s - loss: 0.3058 - acc: 0.929 - ETA: 0s - loss: 0.3082 - acc: 0.929 - ETA: 0s - loss: 0.3087 - acc: 0.929 - 6s 1ms/step - loss: 0.3080 - acc: 0.9292 - val_loss: 0.3359 - val_acc: 0.9128\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 1.1320 - acc: 0.812 - ETA: 4s - loss: 0.3775 - acc: 0.921 - ETA: 4s - loss: 0.4061 - acc: 0.937 - ETA: 4s - loss: 0.2949 - acc: 0.950 - ETA: 4s - loss: 0.3935 - acc: 0.947 - ETA: 4s - loss: 0.3352 - acc: 0.945 - ETA: 4s - loss: 0.3105 - acc: 0.944 - ETA: 4s - loss: 0.2812 - acc: 0.940 - ETA: 4s - loss: 0.3999 - acc: 0.934 - ETA: 4s - loss: 0.4114 - acc: 0.930 - ETA: 4s - loss: 0.3801 - acc: 0.931 - ETA: 4s - loss: 0.4202 - acc: 0.924 - ETA: 4s - loss: 0.3937 - acc: 0.925 - ETA: 4s - loss: 0.3735 - acc: 0.924 - ETA: 4s - loss: 0.3641 - acc: 0.921 - ETA: 4s - loss: 0.3511 - acc: 0.922 - ETA: 4s - loss: 0.3396 - acc: 0.920 - ETA: 4s - loss: 0.3450 - acc: 0.920 - ETA: 4s - loss: 0.3344 - acc: 0.919 - ETA: 3s - loss: 0.3297 - acc: 0.916 - ETA: 3s - loss: 0.3172 - acc: 0.919 - ETA: 3s - loss: 0.3231 - acc: 0.919 - ETA: 3s - loss: 0.3136 - acc: 0.920 - ETA: 3s - loss: 0.3050 - acc: 0.922 - ETA: 3s - loss: 0.2964 - acc: 0.923 - ETA: 3s - loss: 0.2873 - acc: 0.925 - ETA: 3s - loss: 0.2801 - acc: 0.926 - ETA: 3s - loss: 0.2772 - acc: 0.926 - ETA: 3s - loss: 0.2736 - acc: 0.925 - ETA: 3s - loss: 0.2679 - acc: 0.927 - ETA: 3s - loss: 0.2631 - acc: 0.927 - ETA: 3s - loss: 0.2590 - acc: 0.927 - ETA: 3s - loss: 0.2521 - acc: 0.928 - ETA: 3s - loss: 0.2484 - acc: 0.929 - ETA: 3s - loss: 0.2416 - acc: 0.931 - ETA: 3s - loss: 0.2477 - acc: 0.931 - ETA: 3s - loss: 0.2420 - acc: 0.932 - ETA: 3s - loss: 0.2391 - acc: 0.932 - ETA: 3s - loss: 0.2380 - acc: 0.932 - ETA: 2s - loss: 0.2425 - acc: 0.933 - ETA: 2s - loss: 0.2391 - acc: 0.934 - ETA: 2s - loss: 0.2393 - acc: 0.933 - ETA: 2s - loss: 0.2386 - acc: 0.932 - ETA: 2s - loss: 0.2355 - acc: 0.933 - ETA: 2s - loss: 0.2331 - acc: 0.934 - ETA: 2s - loss: 0.2306 - acc: 0.935 - ETA: 2s - loss: 0.2336 - acc: 0.936 - ETA: 2s - loss: 0.2310 - acc: 0.935 - ETA: 2s - loss: 0.2284 - acc: 0.936 - ETA: 2s - loss: 0.2258 - acc: 0.937 - ETA: 2s - loss: 0.2244 - acc: 0.936 - ETA: 2s - loss: 0.2219 - acc: 0.937 - ETA: 2s - loss: 0.2257 - acc: 0.937 - ETA: 2s - loss: 0.2222 - acc: 0.938 - ETA: 2s - loss: 0.2236 - acc: 0.938 - ETA: 2s - loss: 0.2243 - acc: 0.938 - ETA: 2s - loss: 0.2215 - acc: 0.939 - ETA: 1s - loss: 0.2259 - acc: 0.938 - ETA: 1s - loss: 0.2249 - acc: 0.938 - ETA: 1s - loss: 0.2324 - acc: 0.935 - ETA: 1s - loss: 0.2314 - acc: 0.935 - ETA: 1s - loss: 0.2304 - acc: 0.935 - ETA: 1s - loss: 0.2289 - acc: 0.935 - ETA: 1s - loss: 0.2267 - acc: 0.935 - ETA: 1s - loss: 0.2248 - acc: 0.935 - ETA: 1s - loss: 0.2228 - acc: 0.935 - ETA: 1s - loss: 0.2215 - acc: 0.935 - ETA: 1s - loss: 0.2185 - acc: 0.935 - ETA: 1s - loss: 0.2188 - acc: 0.934 - ETA: 1s - loss: 0.2172 - acc: 0.935 - ETA: 1s - loss: 0.2192 - acc: 0.935 - ETA: 1s - loss: 0.2189 - acc: 0.935 - ETA: 1s - loss: 0.2219 - acc: 0.934 - ETA: 0s - loss: 0.2201 - acc: 0.935 - ETA: 0s - loss: 0.2250 - acc: 0.934 - ETA: 0s - loss: 0.2295 - acc: 0.934 - ETA: 0s - loss: 0.2271 - acc: 0.934 - ETA: 0s - loss: 0.2272 - acc: 0.933 - ETA: 0s - loss: 0.2263 - acc: 0.933 - ETA: 0s - loss: 0.2257 - acc: 0.933 - ETA: 0s - loss: 0.2248 - acc: 0.932 - ETA: 0s - loss: 0.2252 - acc: 0.931 - ETA: 0s - loss: 0.2238 - acc: 0.931 - ETA: 0s - loss: 0.2243 - acc: 0.931 - ETA: 0s - loss: 0.2230 - acc: 0.931 - ETA: 0s - loss: 0.2228 - acc: 0.931 - ETA: 0s - loss: 0.2224 - acc: 0.931 - ETA: 0s - loss: 0.2212 - acc: 0.932 - ETA: 0s - loss: 0.2197 - acc: 0.932 - 6s 1ms/step - loss: 0.2196 - acc: 0.9326 - val_loss: 0.2519 - val_acc: 0.9269\n",
      "Epoch 33/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0364 - acc: 1.000 - ETA: 6s - loss: 0.4248 - acc: 0.937 - ETA: 6s - loss: 0.2736 - acc: 0.962 - ETA: 5s - loss: 0.1955 - acc: 0.976 - ETA: 5s - loss: 0.1611 - acc: 0.971 - ETA: 5s - loss: 0.1589 - acc: 0.961 - ETA: 5s - loss: 0.1480 - acc: 0.957 - ETA: 5s - loss: 0.1349 - acc: 0.960 - ETA: 5s - loss: 0.1221 - acc: 0.965 - ETA: 5s - loss: 0.1604 - acc: 0.962 - ETA: 5s - loss: 0.1568 - acc: 0.962 - ETA: 4s - loss: 0.1482 - acc: 0.963 - ETA: 4s - loss: 0.1571 - acc: 0.957 - ETA: 4s - loss: 0.1522 - acc: 0.956 - ETA: 4s - loss: 0.1485 - acc: 0.956 - ETA: 4s - loss: 0.1427 - acc: 0.958 - ETA: 4s - loss: 0.1398 - acc: 0.957 - ETA: 4s - loss: 0.1423 - acc: 0.954 - ETA: 4s - loss: 0.1375 - acc: 0.956 - ETA: 4s - loss: 0.1584 - acc: 0.953 - ETA: 4s - loss: 0.1569 - acc: 0.951 - ETA: 4s - loss: 0.1534 - acc: 0.949 - ETA: 4s - loss: 0.1534 - acc: 0.951 - ETA: 4s - loss: 0.1590 - acc: 0.948 - ETA: 4s - loss: 0.1594 - acc: 0.948 - ETA: 4s - loss: 0.1601 - acc: 0.948 - ETA: 4s - loss: 0.1576 - acc: 0.950 - ETA: 4s - loss: 0.1540 - acc: 0.950 - ETA: 4s - loss: 0.1556 - acc: 0.949 - ETA: 3s - loss: 0.1527 - acc: 0.951 - ETA: 3s - loss: 0.1527 - acc: 0.950 - ETA: 3s - loss: 0.1641 - acc: 0.949 - ETA: 3s - loss: 0.1626 - acc: 0.949 - ETA: 3s - loss: 0.1733 - acc: 0.947 - ETA: 3s - loss: 0.1693 - acc: 0.948 - ETA: 3s - loss: 0.1767 - acc: 0.949 - ETA: 3s - loss: 0.1739 - acc: 0.950 - ETA: 3s - loss: 0.1708 - acc: 0.951 - ETA: 3s - loss: 0.1695 - acc: 0.951 - ETA: 3s - loss: 0.1673 - acc: 0.951 - ETA: 3s - loss: 0.1738 - acc: 0.951 - ETA: 3s - loss: 0.1729 - acc: 0.951 - ETA: 3s - loss: 0.1737 - acc: 0.950 - ETA: 3s - loss: 0.1698 - acc: 0.951 - ETA: 3s - loss: 0.1691 - acc: 0.951 - ETA: 3s - loss: 0.1662 - acc: 0.951 - ETA: 3s - loss: 0.1689 - acc: 0.950 - ETA: 2s - loss: 0.1656 - acc: 0.951 - ETA: 2s - loss: 0.1657 - acc: 0.950 - ETA: 2s - loss: 0.1656 - acc: 0.950 - ETA: 2s - loss: 0.1644 - acc: 0.950 - ETA: 2s - loss: 0.1613 - acc: 0.951 - ETA: 2s - loss: 0.1592 - acc: 0.952 - ETA: 2s - loss: 0.1574 - acc: 0.952 - ETA: 2s - loss: 0.1590 - acc: 0.952 - ETA: 2s - loss: 0.1578 - acc: 0.952 - ETA: 2s - loss: 0.1604 - acc: 0.952 - ETA: 2s - loss: 0.1591 - acc: 0.952 - ETA: 2s - loss: 0.1581 - acc: 0.953 - ETA: 2s - loss: 0.1589 - acc: 0.952 - ETA: 2s - loss: 0.1568 - acc: 0.953 - ETA: 2s - loss: 0.1580 - acc: 0.953 - ETA: 2s - loss: 0.1570 - acc: 0.953 - ETA: 1s - loss: 0.1562 - acc: 0.952 - ETA: 1s - loss: 0.1625 - acc: 0.951 - ETA: 1s - loss: 0.1608 - acc: 0.951 - ETA: 1s - loss: 0.1599 - acc: 0.951 - ETA: 1s - loss: 0.1592 - acc: 0.951 - ETA: 1s - loss: 0.1598 - acc: 0.950 - ETA: 1s - loss: 0.1582 - acc: 0.951 - ETA: 1s - loss: 0.1573 - acc: 0.950 - ETA: 1s - loss: 0.1570 - acc: 0.950 - ETA: 1s - loss: 0.1575 - acc: 0.950 - ETA: 1s - loss: 0.1685 - acc: 0.948 - ETA: 1s - loss: 0.1672 - acc: 0.949 - ETA: 1s - loss: 0.1670 - acc: 0.948 - ETA: 1s - loss: 0.1714 - acc: 0.948 - ETA: 1s - loss: 0.1704 - acc: 0.948 - ETA: 1s - loss: 0.1689 - acc: 0.949 - ETA: 1s - loss: 0.1683 - acc: 0.949 - ETA: 0s - loss: 0.1680 - acc: 0.949 - ETA: 0s - loss: 0.1667 - acc: 0.950 - ETA: 0s - loss: 0.1675 - acc: 0.949 - ETA: 0s - loss: 0.1679 - acc: 0.949 - ETA: 0s - loss: 0.1673 - acc: 0.949 - ETA: 0s - loss: 0.1665 - acc: 0.949 - ETA: 0s - loss: 0.1655 - acc: 0.949 - ETA: 0s - loss: 0.1650 - acc: 0.949 - ETA: 0s - loss: 0.1645 - acc: 0.949 - ETA: 0s - loss: 0.1635 - acc: 0.949 - ETA: 0s - loss: 0.1638 - acc: 0.949 - ETA: 0s - loss: 0.1631 - acc: 0.948 - ETA: 0s - loss: 0.1624 - acc: 0.948 - ETA: 0s - loss: 0.1618 - acc: 0.948 - ETA: 0s - loss: 0.1619 - acc: 0.947 - ETA: 0s - loss: 0.1646 - acc: 0.948 - 6s 1ms/step - loss: 0.1642 - acc: 0.9481 - val_loss: 0.2357 - val_acc: 0.9186\n",
      "Epoch 34/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1604 - acc: 0.875 - ETA: 6s - loss: 0.0748 - acc: 0.958 - ETA: 5s - loss: 0.1192 - acc: 0.968 - ETA: 5s - loss: 0.1525 - acc: 0.951 - ETA: 5s - loss: 0.1332 - acc: 0.954 - ETA: 5s - loss: 0.1091 - acc: 0.964 - ETA: 5s - loss: 0.1076 - acc: 0.963 - ETA: 5s - loss: 0.1102 - acc: 0.959 - ETA: 4s - loss: 0.1006 - acc: 0.964 - ETA: 4s - loss: 0.0910 - acc: 0.968 - ETA: 4s - loss: 0.1048 - acc: 0.967 - ETA: 4s - loss: 0.1380 - acc: 0.964 - ETA: 4s - loss: 0.1311 - acc: 0.964 - ETA: 4s - loss: 0.1314 - acc: 0.963 - ETA: 4s - loss: 0.1301 - acc: 0.964 - ETA: 4s - loss: 0.1447 - acc: 0.964 - ETA: 4s - loss: 0.1391 - acc: 0.966 - ETA: 4s - loss: 0.1373 - acc: 0.966 - ETA: 4s - loss: 0.1380 - acc: 0.962 - ETA: 4s - loss: 0.1424 - acc: 0.958 - ETA: 3s - loss: 0.1362 - acc: 0.960 - ETA: 3s - loss: 0.1377 - acc: 0.958 - ETA: 3s - loss: 0.1359 - acc: 0.959 - ETA: 3s - loss: 0.1401 - acc: 0.958 - ETA: 3s - loss: 0.1375 - acc: 0.958 - ETA: 3s - loss: 0.1352 - acc: 0.957 - ETA: 3s - loss: 0.1332 - acc: 0.957 - ETA: 3s - loss: 0.1296 - acc: 0.957 - ETA: 3s - loss: 0.1278 - acc: 0.957 - ETA: 3s - loss: 0.1249 - acc: 0.957 - ETA: 3s - loss: 0.1232 - acc: 0.957 - ETA: 3s - loss: 0.1221 - acc: 0.957 - ETA: 3s - loss: 0.1202 - acc: 0.958 - ETA: 3s - loss: 0.1220 - acc: 0.958 - ETA: 3s - loss: 0.1198 - acc: 0.958 - ETA: 3s - loss: 0.1216 - acc: 0.957 - ETA: 3s - loss: 0.1204 - acc: 0.958 - ETA: 2s - loss: 0.1215 - acc: 0.957 - ETA: 2s - loss: 0.1225 - acc: 0.955 - ETA: 2s - loss: 0.1214 - acc: 0.956 - ETA: 2s - loss: 0.1208 - acc: 0.955 - ETA: 2s - loss: 0.1203 - acc: 0.955 - ETA: 2s - loss: 0.1191 - acc: 0.955 - ETA: 2s - loss: 0.1197 - acc: 0.954 - ETA: 2s - loss: 0.1182 - acc: 0.955 - ETA: 2s - loss: 0.1164 - acc: 0.955 - ETA: 2s - loss: 0.1203 - acc: 0.955 - ETA: 2s - loss: 0.1208 - acc: 0.954 - ETA: 2s - loss: 0.1228 - acc: 0.953 - ETA: 2s - loss: 0.1222 - acc: 0.953 - ETA: 2s - loss: 0.1237 - acc: 0.952 - ETA: 2s - loss: 0.1235 - acc: 0.952 - ETA: 2s - loss: 0.1233 - acc: 0.952 - ETA: 2s - loss: 0.1278 - acc: 0.951 - ETA: 2s - loss: 0.1371 - acc: 0.948 - ETA: 1s - loss: 0.1425 - acc: 0.946 - ETA: 1s - loss: 0.1503 - acc: 0.944 - ETA: 1s - loss: 0.1544 - acc: 0.942 - ETA: 1s - loss: 0.1554 - acc: 0.942 - ETA: 1s - loss: 0.1553 - acc: 0.941 - ETA: 1s - loss: 0.1583 - acc: 0.940 - ETA: 1s - loss: 0.1609 - acc: 0.940 - ETA: 1s - loss: 0.1599 - acc: 0.939 - ETA: 1s - loss: 0.1646 - acc: 0.939 - ETA: 1s - loss: 0.1690 - acc: 0.940 - ETA: 1s - loss: 0.1680 - acc: 0.940 - ETA: 1s - loss: 0.1669 - acc: 0.941 - ETA: 1s - loss: 0.1667 - acc: 0.940 - ETA: 1s - loss: 0.1667 - acc: 0.940 - ETA: 1s - loss: 0.1678 - acc: 0.940 - ETA: 1s - loss: 0.1662 - acc: 0.940 - ETA: 0s - loss: 0.1671 - acc: 0.940 - ETA: 0s - loss: 0.1667 - acc: 0.940 - ETA: 0s - loss: 0.1698 - acc: 0.939 - ETA: 0s - loss: 0.1691 - acc: 0.940 - ETA: 0s - loss: 0.1685 - acc: 0.939 - ETA: 0s - loss: 0.1671 - acc: 0.940 - ETA: 0s - loss: 0.1666 - acc: 0.940 - ETA: 0s - loss: 0.1656 - acc: 0.941 - ETA: 0s - loss: 0.1651 - acc: 0.941 - ETA: 0s - loss: 0.1643 - acc: 0.941 - ETA: 0s - loss: 0.1637 - acc: 0.941 - ETA: 0s - loss: 0.1635 - acc: 0.941 - ETA: 0s - loss: 0.1627 - acc: 0.942 - ETA: 0s - loss: 0.1621 - acc: 0.942 - ETA: 0s - loss: 0.1614 - acc: 0.942 - ETA: 0s - loss: 0.1614 - acc: 0.942 - ETA: 0s - loss: 0.1604 - acc: 0.943 - 6s 1ms/step - loss: 0.1594 - acc: 0.9437 - val_loss: 0.2339 - val_acc: 0.9282\n",
      "Epoch 35/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.2009 - acc: 0.812 - ETA: 5s - loss: 0.1417 - acc: 0.921 - ETA: 5s - loss: 0.1336 - acc: 0.928 - ETA: 5s - loss: 0.1058 - acc: 0.943 - ETA: 4s - loss: 0.1262 - acc: 0.951 - ETA: 4s - loss: 0.1215 - acc: 0.949 - ETA: 4s - loss: 0.1275 - acc: 0.947 - ETA: 4s - loss: 0.1158 - acc: 0.951 - ETA: 4s - loss: 0.1027 - acc: 0.957 - ETA: 4s - loss: 0.1098 - acc: 0.957 - ETA: 4s - loss: 0.2666 - acc: 0.943 - ETA: 4s - loss: 0.2676 - acc: 0.943 - ETA: 4s - loss: 0.2534 - acc: 0.945 - ETA: 4s - loss: 0.2564 - acc: 0.945 - ETA: 4s - loss: 0.2623 - acc: 0.947 - ETA: 4s - loss: 0.2702 - acc: 0.948 - ETA: 4s - loss: 0.2588 - acc: 0.949 - ETA: 4s - loss: 0.2477 - acc: 0.949 - ETA: 3s - loss: 0.2452 - acc: 0.950 - ETA: 3s - loss: 0.2357 - acc: 0.950 - ETA: 3s - loss: 0.2300 - acc: 0.948 - ETA: 3s - loss: 0.2522 - acc: 0.948 - ETA: 3s - loss: 0.2616 - acc: 0.948 - ETA: 3s - loss: 0.2551 - acc: 0.949 - ETA: 3s - loss: 0.2693 - acc: 0.948 - ETA: 3s - loss: 0.2771 - acc: 0.947 - ETA: 3s - loss: 0.2864 - acc: 0.945 - ETA: 3s - loss: 0.2913 - acc: 0.946 - ETA: 3s - loss: 0.3122 - acc: 0.944 - ETA: 3s - loss: 0.3087 - acc: 0.943 - ETA: 3s - loss: 0.3382 - acc: 0.940 - ETA: 3s - loss: 0.3353 - acc: 0.941 - ETA: 3s - loss: 0.3630 - acc: 0.938 - ETA: 3s - loss: 0.3663 - acc: 0.938 - ETA: 3s - loss: 0.3621 - acc: 0.939 - ETA: 3s - loss: 0.3743 - acc: 0.938 - ETA: 3s - loss: 0.3695 - acc: 0.937 - ETA: 2s - loss: 0.3721 - acc: 0.937 - ETA: 2s - loss: 0.3785 - acc: 0.935 - ETA: 2s - loss: 0.3812 - acc: 0.935 - ETA: 2s - loss: 0.3784 - acc: 0.934 - ETA: 2s - loss: 0.3823 - acc: 0.934 - ETA: 2s - loss: 0.3850 - acc: 0.934 - ETA: 2s - loss: 0.4026 - acc: 0.933 - ETA: 2s - loss: 0.3976 - acc: 0.932 - ETA: 2s - loss: 0.3906 - acc: 0.933 - ETA: 2s - loss: 0.3914 - acc: 0.934 - ETA: 2s - loss: 0.3998 - acc: 0.934 - ETA: 2s - loss: 0.4036 - acc: 0.932 - ETA: 2s - loss: 0.4212 - acc: 0.930 - ETA: 2s - loss: 0.4232 - acc: 0.930 - ETA: 2s - loss: 0.4261 - acc: 0.929 - ETA: 2s - loss: 0.4410 - acc: 0.927 - ETA: 1s - loss: 0.4485 - acc: 0.927 - ETA: 1s - loss: 0.4734 - acc: 0.926 - ETA: 1s - loss: 0.4808 - acc: 0.925 - ETA: 1s - loss: 0.4895 - acc: 0.924 - ETA: 1s - loss: 0.4864 - acc: 0.924 - ETA: 1s - loss: 0.4958 - acc: 0.923 - ETA: 1s - loss: 0.4907 - acc: 0.923 - ETA: 1s - loss: 0.4954 - acc: 0.924 - ETA: 1s - loss: 0.5010 - acc: 0.923 - ETA: 1s - loss: 0.4952 - acc: 0.924 - ETA: 1s - loss: 0.4976 - acc: 0.923 - ETA: 1s - loss: 0.4941 - acc: 0.923 - ETA: 1s - loss: 0.5174 - acc: 0.921 - ETA: 1s - loss: 0.5326 - acc: 0.920 - ETA: 1s - loss: 0.5278 - acc: 0.919 - ETA: 1s - loss: 0.5204 - acc: 0.920 - ETA: 1s - loss: 0.5278 - acc: 0.920 - ETA: 1s - loss: 0.5393 - acc: 0.919 - ETA: 1s - loss: 0.5355 - acc: 0.919 - ETA: 1s - loss: 0.5365 - acc: 0.918 - ETA: 1s - loss: 0.5398 - acc: 0.918 - ETA: 0s - loss: 0.5381 - acc: 0.918 - ETA: 0s - loss: 0.5457 - acc: 0.918 - ETA: 0s - loss: 0.5444 - acc: 0.918 - ETA: 0s - loss: 0.5529 - acc: 0.917 - ETA: 0s - loss: 0.5468 - acc: 0.917 - ETA: 0s - loss: 0.5438 - acc: 0.917 - ETA: 0s - loss: 0.5442 - acc: 0.918 - ETA: 0s - loss: 0.5444 - acc: 0.917 - ETA: 0s - loss: 0.5407 - acc: 0.917 - ETA: 0s - loss: 0.5375 - acc: 0.917 - ETA: 0s - loss: 0.5343 - acc: 0.917 - ETA: 0s - loss: 0.5374 - acc: 0.917 - ETA: 0s - loss: 0.5319 - acc: 0.918 - ETA: 0s - loss: 0.5346 - acc: 0.918 - ETA: 0s - loss: 0.5387 - acc: 0.918 - ETA: 0s - loss: 0.5464 - acc: 0.917 - ETA: 0s - loss: 0.5402 - acc: 0.918 - ETA: 0s - loss: 0.5490 - acc: 0.918 - 6s 1ms/step - loss: 0.5477 - acc: 0.9176 - val_loss: 0.4250 - val_acc: 0.9058\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0624 - acc: 1.000 - ETA: 5s - loss: 0.1944 - acc: 0.921 - ETA: 5s - loss: 0.3140 - acc: 0.937 - ETA: 5s - loss: 0.5884 - acc: 0.900 - ETA: 5s - loss: 0.5978 - acc: 0.895 - ETA: 5s - loss: 0.5378 - acc: 0.892 - ETA: 5s - loss: 0.7950 - acc: 0.882 - ETA: 5s - loss: 0.7358 - acc: 0.894 - ETA: 5s - loss: 0.7790 - acc: 0.897 - ETA: 5s - loss: 0.7318 - acc: 0.907 - ETA: 4s - loss: 0.7111 - acc: 0.908 - ETA: 4s - loss: 0.6780 - acc: 0.915 - ETA: 4s - loss: 0.6497 - acc: 0.916 - ETA: 4s - loss: 0.6739 - acc: 0.917 - ETA: 4s - loss: 0.6706 - acc: 0.920 - ETA: 4s - loss: 0.6498 - acc: 0.925 - ETA: 4s - loss: 0.6794 - acc: 0.924 - ETA: 4s - loss: 0.6612 - acc: 0.926 - ETA: 4s - loss: 0.6547 - acc: 0.922 - ETA: 4s - loss: 0.6613 - acc: 0.923 - ETA: 4s - loss: 0.6526 - acc: 0.921 - ETA: 4s - loss: 0.6448 - acc: 0.921 - ETA: 4s - loss: 0.6347 - acc: 0.923 - ETA: 4s - loss: 0.6140 - acc: 0.920 - ETA: 4s - loss: 0.6250 - acc: 0.917 - ETA: 3s - loss: 0.6537 - acc: 0.915 - ETA: 3s - loss: 0.6721 - acc: 0.913 - ETA: 3s - loss: 0.6618 - acc: 0.914 - ETA: 3s - loss: 0.6532 - acc: 0.916 - ETA: 3s - loss: 0.6370 - acc: 0.916 - ETA: 3s - loss: 0.6181 - acc: 0.917 - ETA: 3s - loss: 0.6006 - acc: 0.919 - ETA: 3s - loss: 0.5895 - acc: 0.919 - ETA: 3s - loss: 0.6173 - acc: 0.917 - ETA: 3s - loss: 0.6125 - acc: 0.917 - ETA: 3s - loss: 0.6069 - acc: 0.918 - ETA: 3s - loss: 0.6016 - acc: 0.919 - ETA: 3s - loss: 0.6057 - acc: 0.917 - ETA: 3s - loss: 0.6014 - acc: 0.918 - ETA: 2s - loss: 0.5891 - acc: 0.918 - ETA: 2s - loss: 0.5786 - acc: 0.918 - ETA: 2s - loss: 0.5910 - acc: 0.917 - ETA: 2s - loss: 0.5884 - acc: 0.917 - ETA: 2s - loss: 0.5766 - acc: 0.918 - ETA: 2s - loss: 0.5741 - acc: 0.918 - ETA: 2s - loss: 0.5856 - acc: 0.919 - ETA: 2s - loss: 0.5903 - acc: 0.919 - ETA: 2s - loss: 0.5888 - acc: 0.919 - ETA: 2s - loss: 0.5835 - acc: 0.917 - ETA: 2s - loss: 0.5728 - acc: 0.919 - ETA: 2s - loss: 0.5702 - acc: 0.919 - ETA: 2s - loss: 0.5674 - acc: 0.920 - ETA: 2s - loss: 0.5593 - acc: 0.920 - ETA: 2s - loss: 0.5569 - acc: 0.920 - ETA: 1s - loss: 0.5669 - acc: 0.921 - ETA: 1s - loss: 0.5651 - acc: 0.921 - ETA: 1s - loss: 0.5637 - acc: 0.921 - ETA: 1s - loss: 0.5548 - acc: 0.922 - ETA: 1s - loss: 0.5555 - acc: 0.922 - ETA: 1s - loss: 0.5480 - acc: 0.923 - ETA: 1s - loss: 0.5435 - acc: 0.922 - ETA: 1s - loss: 0.5363 - acc: 0.923 - ETA: 1s - loss: 0.5319 - acc: 0.924 - ETA: 1s - loss: 0.5349 - acc: 0.924 - ETA: 1s - loss: 0.5283 - acc: 0.925 - ETA: 1s - loss: 0.5337 - acc: 0.924 - ETA: 1s - loss: 0.5265 - acc: 0.925 - ETA: 1s - loss: 0.5215 - acc: 0.925 - ETA: 1s - loss: 0.5148 - acc: 0.925 - ETA: 1s - loss: 0.5163 - acc: 0.925 - ETA: 1s - loss: 0.5134 - acc: 0.925 - ETA: 1s - loss: 0.5143 - acc: 0.925 - ETA: 0s - loss: 0.5103 - acc: 0.926 - ETA: 0s - loss: 0.5105 - acc: 0.926 - ETA: 0s - loss: 0.5159 - acc: 0.926 - ETA: 0s - loss: 0.5122 - acc: 0.926 - ETA: 0s - loss: 0.5157 - acc: 0.926 - ETA: 0s - loss: 0.5109 - acc: 0.926 - ETA: 0s - loss: 0.5153 - acc: 0.926 - ETA: 0s - loss: 0.5108 - acc: 0.926 - ETA: 0s - loss: 0.5068 - acc: 0.927 - ETA: 0s - loss: 0.5031 - acc: 0.927 - ETA: 0s - loss: 0.5038 - acc: 0.926 - ETA: 0s - loss: 0.5029 - acc: 0.926 - ETA: 0s - loss: 0.5127 - acc: 0.925 - ETA: 0s - loss: 0.5114 - acc: 0.925 - ETA: 0s - loss: 0.5066 - acc: 0.926 - ETA: 0s - loss: 0.5031 - acc: 0.925 - ETA: 0s - loss: 0.5028 - acc: 0.925 - ETA: 0s - loss: 0.4998 - acc: 0.925 - 6s 1ms/step - loss: 0.4976 - acc: 0.9257 - val_loss: 0.3819 - val_acc: 0.9167\n",
      "Epoch 37/100\n",
      "4067/4067 [==============================] - ETA: 7s - loss: 0.0411 - acc: 1.000 - ETA: 6s - loss: 0.4622 - acc: 0.916 - ETA: 6s - loss: 0.5331 - acc: 0.925 - ETA: 6s - loss: 0.4976 - acc: 0.929 - ETA: 5s - loss: 0.4972 - acc: 0.926 - ETA: 5s - loss: 0.5713 - acc: 0.910 - ETA: 5s - loss: 0.6463 - acc: 0.910 - ETA: 5s - loss: 0.5797 - acc: 0.920 - ETA: 5s - loss: 0.5389 - acc: 0.918 - ETA: 5s - loss: 0.4703 - acc: 0.929 - ETA: 5s - loss: 0.4705 - acc: 0.923 - ETA: 5s - loss: 0.5128 - acc: 0.921 - ETA: 5s - loss: 0.5238 - acc: 0.918 - ETA: 5s - loss: 0.5293 - acc: 0.918 - ETA: 5s - loss: 0.5212 - acc: 0.919 - ETA: 5s - loss: 0.5109 - acc: 0.922 - ETA: 5s - loss: 0.5149 - acc: 0.923 - ETA: 5s - loss: 0.5402 - acc: 0.924 - ETA: 5s - loss: 0.5415 - acc: 0.924 - ETA: 4s - loss: 0.5156 - acc: 0.924 - ETA: 4s - loss: 0.4897 - acc: 0.926 - ETA: 4s - loss: 0.4690 - acc: 0.926 - ETA: 4s - loss: 0.4530 - acc: 0.929 - ETA: 4s - loss: 0.4559 - acc: 0.930 - ETA: 4s - loss: 0.4781 - acc: 0.930 - ETA: 4s - loss: 0.4763 - acc: 0.930 - ETA: 4s - loss: 0.4585 - acc: 0.931 - ETA: 4s - loss: 0.4449 - acc: 0.930 - ETA: 4s - loss: 0.4484 - acc: 0.929 - ETA: 4s - loss: 0.4456 - acc: 0.930 - ETA: 4s - loss: 0.4342 - acc: 0.931 - ETA: 3s - loss: 0.4279 - acc: 0.932 - ETA: 3s - loss: 0.4311 - acc: 0.932 - ETA: 3s - loss: 0.4343 - acc: 0.932 - ETA: 3s - loss: 0.4291 - acc: 0.933 - ETA: 3s - loss: 0.4461 - acc: 0.931 - ETA: 3s - loss: 0.4553 - acc: 0.932 - ETA: 3s - loss: 0.4461 - acc: 0.932 - ETA: 3s - loss: 0.4379 - acc: 0.933 - ETA: 3s - loss: 0.4304 - acc: 0.934 - ETA: 3s - loss: 0.4331 - acc: 0.934 - ETA: 3s - loss: 0.4229 - acc: 0.935 - ETA: 3s - loss: 0.4227 - acc: 0.936 - ETA: 3s - loss: 0.4249 - acc: 0.935 - ETA: 3s - loss: 0.4165 - acc: 0.935 - ETA: 3s - loss: 0.4264 - acc: 0.935 - ETA: 3s - loss: 0.4186 - acc: 0.935 - ETA: 3s - loss: 0.4247 - acc: 0.934 - ETA: 3s - loss: 0.4269 - acc: 0.935 - ETA: 3s - loss: 0.4299 - acc: 0.935 - ETA: 2s - loss: 0.4243 - acc: 0.936 - ETA: 2s - loss: 0.4254 - acc: 0.936 - ETA: 2s - loss: 0.4299 - acc: 0.936 - ETA: 2s - loss: 0.4253 - acc: 0.937 - ETA: 2s - loss: 0.4213 - acc: 0.937 - ETA: 2s - loss: 0.4185 - acc: 0.937 - ETA: 2s - loss: 0.4144 - acc: 0.937 - ETA: 2s - loss: 0.4071 - acc: 0.938 - ETA: 2s - loss: 0.4083 - acc: 0.938 - ETA: 2s - loss: 0.4041 - acc: 0.938 - ETA: 2s - loss: 0.4001 - acc: 0.937 - ETA: 2s - loss: 0.3964 - acc: 0.938 - ETA: 2s - loss: 0.3947 - acc: 0.937 - ETA: 2s - loss: 0.3975 - acc: 0.937 - ETA: 2s - loss: 0.3952 - acc: 0.936 - ETA: 2s - loss: 0.3911 - acc: 0.937 - ETA: 2s - loss: 0.3884 - acc: 0.936 - ETA: 2s - loss: 0.3862 - acc: 0.936 - ETA: 1s - loss: 0.3813 - acc: 0.937 - ETA: 1s - loss: 0.3949 - acc: 0.936 - ETA: 1s - loss: 0.3908 - acc: 0.937 - ETA: 1s - loss: 0.3942 - acc: 0.936 - ETA: 1s - loss: 0.3910 - acc: 0.936 - ETA: 1s - loss: 0.3895 - acc: 0.935 - ETA: 1s - loss: 0.3954 - acc: 0.935 - ETA: 1s - loss: 0.3913 - acc: 0.935 - ETA: 1s - loss: 0.3876 - acc: 0.935 - ETA: 1s - loss: 0.3899 - acc: 0.935 - ETA: 1s - loss: 0.3854 - acc: 0.936 - ETA: 1s - loss: 0.3915 - acc: 0.936 - ETA: 1s - loss: 0.3884 - acc: 0.937 - ETA: 1s - loss: 0.3859 - acc: 0.937 - ETA: 1s - loss: 0.3874 - acc: 0.937 - ETA: 1s - loss: 0.3857 - acc: 0.936 - ETA: 1s - loss: 0.3882 - acc: 0.936 - ETA: 1s - loss: 0.3907 - acc: 0.935 - ETA: 1s - loss: 0.3885 - acc: 0.935 - ETA: 0s - loss: 0.3927 - acc: 0.934 - ETA: 0s - loss: 0.3884 - acc: 0.935 - ETA: 0s - loss: 0.3894 - acc: 0.935 - ETA: 0s - loss: 0.3859 - acc: 0.935 - ETA: 0s - loss: 0.3834 - acc: 0.935 - ETA: 0s - loss: 0.3806 - acc: 0.935 - ETA: 0s - loss: 0.3778 - acc: 0.935 - ETA: 0s - loss: 0.3748 - acc: 0.936 - ETA: 0s - loss: 0.3751 - acc: 0.936 - ETA: 0s - loss: 0.3806 - acc: 0.935 - ETA: 0s - loss: 0.3762 - acc: 0.936 - ETA: 0s - loss: 0.3822 - acc: 0.936 - ETA: 0s - loss: 0.3845 - acc: 0.936 - ETA: 0s - loss: 0.3821 - acc: 0.936 - ETA: 0s - loss: 0.3797 - acc: 0.936 - ETA: 0s - loss: 0.3755 - acc: 0.937 - ETA: 0s - loss: 0.3758 - acc: 0.937 - 6s 2ms/step - loss: 0.3756 - acc: 0.9378 - val_loss: 0.3829 - val_acc: 0.9218\n",
      "Epoch 38/100\n",
      "4067/4067 [==============================] - ETA: 7s - loss: 0.0929 - acc: 0.937 - ETA: 6s - loss: 0.1098 - acc: 0.937 - ETA: 6s - loss: 0.1324 - acc: 0.925 - ETA: 6s - loss: 0.1198 - acc: 0.937 - ETA: 6s - loss: 0.2145 - acc: 0.937 - ETA: 5s - loss: 0.2343 - acc: 0.927 - ETA: 5s - loss: 0.2400 - acc: 0.937 - ETA: 5s - loss: 0.2192 - acc: 0.941 - ETA: 5s - loss: 0.2544 - acc: 0.940 - ETA: 5s - loss: 0.2657 - acc: 0.940 - ETA: 5s - loss: 0.2581 - acc: 0.940 - ETA: 5s - loss: 0.2391 - acc: 0.942 - ETA: 5s - loss: 0.2298 - acc: 0.943 - ETA: 5s - loss: 0.2485 - acc: 0.945 - ETA: 5s - loss: 0.2354 - acc: 0.949 - ETA: 4s - loss: 0.2807 - acc: 0.945 - ETA: 4s - loss: 0.2718 - acc: 0.946 - ETA: 4s - loss: 0.2608 - acc: 0.949 - ETA: 4s - loss: 0.2988 - acc: 0.947 - ETA: 4s - loss: 0.2944 - acc: 0.945 - ETA: 4s - loss: 0.3121 - acc: 0.941 - ETA: 4s - loss: 0.3208 - acc: 0.939 - ETA: 4s - loss: 0.3095 - acc: 0.940 - ETA: 4s - loss: 0.3566 - acc: 0.937 - ETA: 4s - loss: 0.3631 - acc: 0.937 - ETA: 4s - loss: 0.3571 - acc: 0.936 - ETA: 4s - loss: 0.3479 - acc: 0.938 - ETA: 4s - loss: 0.3568 - acc: 0.937 - ETA: 4s - loss: 0.3530 - acc: 0.936 - ETA: 4s - loss: 0.3686 - acc: 0.936 - ETA: 4s - loss: 0.3728 - acc: 0.935 - ETA: 4s - loss: 0.3794 - acc: 0.935 - ETA: 3s - loss: 0.3838 - acc: 0.933 - ETA: 3s - loss: 0.3865 - acc: 0.933 - ETA: 3s - loss: 0.3792 - acc: 0.933 - ETA: 3s - loss: 0.3723 - acc: 0.933 - ETA: 3s - loss: 0.3861 - acc: 0.932 - ETA: 3s - loss: 0.3852 - acc: 0.931 - ETA: 3s - loss: 0.3822 - acc: 0.930 - ETA: 3s - loss: 0.3815 - acc: 0.929 - ETA: 3s - loss: 0.3748 - acc: 0.930 - ETA: 3s - loss: 0.3771 - acc: 0.931 - ETA: 3s - loss: 0.3792 - acc: 0.931 - ETA: 3s - loss: 0.3715 - acc: 0.933 - ETA: 3s - loss: 0.3637 - acc: 0.934 - ETA: 3s - loss: 0.3568 - acc: 0.934 - ETA: 3s - loss: 0.3671 - acc: 0.934 - ETA: 2s - loss: 0.3617 - acc: 0.935 - ETA: 2s - loss: 0.3652 - acc: 0.935 - ETA: 2s - loss: 0.3609 - acc: 0.936 - ETA: 2s - loss: 0.3635 - acc: 0.936 - ETA: 2s - loss: 0.3735 - acc: 0.936 - ETA: 2s - loss: 0.3710 - acc: 0.936 - ETA: 2s - loss: 0.3827 - acc: 0.935 - ETA: 2s - loss: 0.3765 - acc: 0.936 - ETA: 2s - loss: 0.3704 - acc: 0.937 - ETA: 2s - loss: 0.3660 - acc: 0.938 - ETA: 2s - loss: 0.3614 - acc: 0.939 - ETA: 2s - loss: 0.3634 - acc: 0.939 - ETA: 2s - loss: 0.3662 - acc: 0.940 - ETA: 2s - loss: 0.3730 - acc: 0.939 - ETA: 2s - loss: 0.3668 - acc: 0.939 - ETA: 2s - loss: 0.3693 - acc: 0.939 - ETA: 2s - loss: 0.3661 - acc: 0.938 - ETA: 2s - loss: 0.3680 - acc: 0.937 - ETA: 2s - loss: 0.3643 - acc: 0.938 - ETA: 2s - loss: 0.3668 - acc: 0.938 - ETA: 1s - loss: 0.3636 - acc: 0.939 - ETA: 1s - loss: 0.3598 - acc: 0.939 - ETA: 1s - loss: 0.3622 - acc: 0.939 - ETA: 1s - loss: 0.3704 - acc: 0.938 - ETA: 1s - loss: 0.3723 - acc: 0.937 - ETA: 1s - loss: 0.3732 - acc: 0.938 - ETA: 1s - loss: 0.3682 - acc: 0.939 - ETA: 1s - loss: 0.3629 - acc: 0.940 - ETA: 1s - loss: 0.3584 - acc: 0.940 - ETA: 1s - loss: 0.3576 - acc: 0.939 - ETA: 1s - loss: 0.3549 - acc: 0.939 - ETA: 1s - loss: 0.3555 - acc: 0.940 - ETA: 1s - loss: 0.3536 - acc: 0.940 - ETA: 1s - loss: 0.3508 - acc: 0.939 - ETA: 1s - loss: 0.3494 - acc: 0.939 - ETA: 0s - loss: 0.3466 - acc: 0.939 - ETA: 0s - loss: 0.3446 - acc: 0.939 - ETA: 0s - loss: 0.3416 - acc: 0.940 - ETA: 0s - loss: 0.3375 - acc: 0.940 - ETA: 0s - loss: 0.3379 - acc: 0.941 - ETA: 0s - loss: 0.3381 - acc: 0.941 - ETA: 0s - loss: 0.3408 - acc: 0.941 - ETA: 0s - loss: 0.3468 - acc: 0.941 - ETA: 0s - loss: 0.3444 - acc: 0.941 - ETA: 0s - loss: 0.3503 - acc: 0.940 - ETA: 0s - loss: 0.3485 - acc: 0.940 - ETA: 0s - loss: 0.3465 - acc: 0.941 - ETA: 0s - loss: 0.3444 - acc: 0.941 - ETA: 0s - loss: 0.3469 - acc: 0.941 - ETA: 0s - loss: 0.3440 - acc: 0.941 - ETA: 0s - loss: 0.3418 - acc: 0.941 - ETA: 0s - loss: 0.3550 - acc: 0.940 - 6s 2ms/step - loss: 0.3552 - acc: 0.9400 - val_loss: 0.3991 - val_acc: 0.9314\n",
      "Epoch 39/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 0.1141 - acc: 0.937 - ETA: 5s - loss: 0.0911 - acc: 0.953 - ETA: 5s - loss: 0.0976 - acc: 0.958 - ETA: 5s - loss: 0.1037 - acc: 0.953 - ETA: 5s - loss: 0.2322 - acc: 0.931 - ETA: 5s - loss: 0.2921 - acc: 0.932 - ETA: 6s - loss: 0.2593 - acc: 0.937 - ETA: 5s - loss: 0.2431 - acc: 0.937 - ETA: 5s - loss: 0.3206 - acc: 0.937 - ETA: 5s - loss: 0.3934 - acc: 0.934 - ETA: 5s - loss: 0.3574 - acc: 0.940 - ETA: 5s - loss: 0.3447 - acc: 0.935 - ETA: 5s - loss: 0.3661 - acc: 0.928 - ETA: 5s - loss: 0.3471 - acc: 0.931 - ETA: 5s - loss: 0.3799 - acc: 0.933 - ETA: 5s - loss: 0.3546 - acc: 0.937 - ETA: 5s - loss: 0.3398 - acc: 0.934 - ETA: 5s - loss: 0.3519 - acc: 0.933 - ETA: 5s - loss: 0.3371 - acc: 0.936 - ETA: 5s - loss: 0.3240 - acc: 0.938 - ETA: 4s - loss: 0.3163 - acc: 0.938 - ETA: 4s - loss: 0.3013 - acc: 0.941 - ETA: 4s - loss: 0.3439 - acc: 0.939 - ETA: 4s - loss: 0.3382 - acc: 0.936 - ETA: 4s - loss: 0.3248 - acc: 0.938 - ETA: 4s - loss: 0.3153 - acc: 0.940 - ETA: 4s - loss: 0.3215 - acc: 0.941 - ETA: 4s - loss: 0.3136 - acc: 0.943 - ETA: 4s - loss: 0.3054 - acc: 0.944 - ETA: 4s - loss: 0.2979 - acc: 0.946 - ETA: 4s - loss: 0.3399 - acc: 0.942 - ETA: 4s - loss: 0.3290 - acc: 0.943 - ETA: 4s - loss: 0.3361 - acc: 0.940 - ETA: 4s - loss: 0.3400 - acc: 0.941 - ETA: 4s - loss: 0.3326 - acc: 0.942 - ETA: 4s - loss: 0.3253 - acc: 0.944 - ETA: 4s - loss: 0.3203 - acc: 0.944 - ETA: 3s - loss: 0.3144 - acc: 0.945 - ETA: 3s - loss: 0.3161 - acc: 0.946 - ETA: 3s - loss: 0.3407 - acc: 0.944 - ETA: 3s - loss: 0.3348 - acc: 0.944 - ETA: 3s - loss: 0.3293 - acc: 0.945 - ETA: 3s - loss: 0.3232 - acc: 0.946 - ETA: 3s - loss: 0.3185 - acc: 0.947 - ETA: 3s - loss: 0.3110 - acc: 0.948 - ETA: 3s - loss: 0.3135 - acc: 0.948 - ETA: 3s - loss: 0.3093 - acc: 0.948 - ETA: 3s - loss: 0.3059 - acc: 0.947 - ETA: 3s - loss: 0.3011 - acc: 0.948 - ETA: 3s - loss: 0.2968 - acc: 0.949 - ETA: 3s - loss: 0.2930 - acc: 0.950 - ETA: 3s - loss: 0.2968 - acc: 0.949 - ETA: 3s - loss: 0.2917 - acc: 0.949 - ETA: 3s - loss: 0.2878 - acc: 0.950 - ETA: 3s - loss: 0.2860 - acc: 0.951 - ETA: 3s - loss: 0.2849 - acc: 0.950 - ETA: 3s - loss: 0.2965 - acc: 0.949 - ETA: 2s - loss: 0.2924 - acc: 0.950 - ETA: 2s - loss: 0.2911 - acc: 0.950 - ETA: 2s - loss: 0.3013 - acc: 0.949 - ETA: 2s - loss: 0.3040 - acc: 0.948 - ETA: 2s - loss: 0.3032 - acc: 0.948 - ETA: 2s - loss: 0.3066 - acc: 0.948 - ETA: 2s - loss: 0.3033 - acc: 0.948 - ETA: 2s - loss: 0.3019 - acc: 0.947 - ETA: 2s - loss: 0.3011 - acc: 0.947 - ETA: 2s - loss: 0.2969 - acc: 0.947 - ETA: 2s - loss: 0.3063 - acc: 0.946 - ETA: 2s - loss: 0.3164 - acc: 0.945 - ETA: 2s - loss: 0.3198 - acc: 0.945 - ETA: 2s - loss: 0.3296 - acc: 0.944 - ETA: 2s - loss: 0.3444 - acc: 0.943 - ETA: 2s - loss: 0.3408 - acc: 0.944 - ETA: 1s - loss: 0.3360 - acc: 0.944 - ETA: 1s - loss: 0.3374 - acc: 0.944 - ETA: 1s - loss: 0.3347 - acc: 0.944 - ETA: 1s - loss: 0.3320 - acc: 0.944 - ETA: 1s - loss: 0.3342 - acc: 0.944 - ETA: 1s - loss: 0.3356 - acc: 0.944 - ETA: 1s - loss: 0.3376 - acc: 0.944 - ETA: 1s - loss: 0.3348 - acc: 0.945 - ETA: 1s - loss: 0.3420 - acc: 0.945 - ETA: 1s - loss: 0.3449 - acc: 0.945 - ETA: 1s - loss: 0.3423 - acc: 0.945 - ETA: 1s - loss: 0.3390 - acc: 0.945 - ETA: 1s - loss: 0.3348 - acc: 0.945 - ETA: 1s - loss: 0.3364 - acc: 0.945 - ETA: 1s - loss: 0.3347 - acc: 0.945 - ETA: 1s - loss: 0.3324 - acc: 0.945 - ETA: 1s - loss: 0.3303 - acc: 0.945 - ETA: 1s - loss: 0.3368 - acc: 0.944 - ETA: 0s - loss: 0.3375 - acc: 0.945 - ETA: 0s - loss: 0.3334 - acc: 0.945 - ETA: 0s - loss: 0.3398 - acc: 0.945 - ETA: 0s - loss: 0.3414 - acc: 0.945 - ETA: 0s - loss: 0.3442 - acc: 0.945 - ETA: 0s - loss: 0.3406 - acc: 0.946 - ETA: 0s - loss: 0.3369 - acc: 0.946 - ETA: 0s - loss: 0.3336 - acc: 0.946 - ETA: 0s - loss: 0.3356 - acc: 0.946 - ETA: 0s - loss: 0.3436 - acc: 0.945 - ETA: 0s - loss: 0.3500 - acc: 0.945 - ETA: 0s - loss: 0.3501 - acc: 0.945 - ETA: 0s - loss: 0.3485 - acc: 0.945 - ETA: 0s - loss: 0.3452 - acc: 0.945 - ETA: 0s - loss: 0.3546 - acc: 0.945 - 7s 2ms/step - loss: 0.3568 - acc: 0.9452 - val_loss: 0.4125 - val_acc: 0.9006\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0435 - acc: 1.000 - ETA: 5s - loss: 0.4744 - acc: 0.859 - ETA: 5s - loss: 0.4568 - acc: 0.892 - ETA: 5s - loss: 0.4343 - acc: 0.918 - ETA: 5s - loss: 0.4490 - acc: 0.913 - ETA: 5s - loss: 0.3997 - acc: 0.920 - ETA: 5s - loss: 0.3573 - acc: 0.926 - ETA: 5s - loss: 0.3219 - acc: 0.934 - ETA: 5s - loss: 0.3635 - acc: 0.928 - ETA: 5s - loss: 0.3625 - acc: 0.924 - ETA: 5s - loss: 0.4050 - acc: 0.928 - ETA: 5s - loss: 0.3849 - acc: 0.925 - ETA: 5s - loss: 0.3614 - acc: 0.929 - ETA: 5s - loss: 0.3728 - acc: 0.932 - ETA: 5s - loss: 0.3610 - acc: 0.930 - ETA: 5s - loss: 0.3504 - acc: 0.931 - ETA: 5s - loss: 0.3438 - acc: 0.931 - ETA: 5s - loss: 0.3487 - acc: 0.933 - ETA: 4s - loss: 0.3466 - acc: 0.934 - ETA: 4s - loss: 0.3303 - acc: 0.937 - ETA: 4s - loss: 0.3247 - acc: 0.937 - ETA: 4s - loss: 0.3326 - acc: 0.938 - ETA: 4s - loss: 0.3455 - acc: 0.936 - ETA: 4s - loss: 0.3397 - acc: 0.935 - ETA: 4s - loss: 0.3328 - acc: 0.935 - ETA: 4s - loss: 0.3301 - acc: 0.933 - ETA: 4s - loss: 0.3394 - acc: 0.932 - ETA: 4s - loss: 0.3347 - acc: 0.931 - ETA: 4s - loss: 0.3588 - acc: 0.926 - ETA: 4s - loss: 0.3533 - acc: 0.925 - ETA: 4s - loss: 0.3450 - acc: 0.926 - ETA: 4s - loss: 0.3413 - acc: 0.926 - ETA: 4s - loss: 0.3465 - acc: 0.927 - ETA: 4s - loss: 0.3534 - acc: 0.926 - ETA: 4s - loss: 0.3499 - acc: 0.927 - ETA: 4s - loss: 0.3441 - acc: 0.927 - ETA: 3s - loss: 0.3481 - acc: 0.928 - ETA: 3s - loss: 0.3513 - acc: 0.928 - ETA: 3s - loss: 0.3447 - acc: 0.929 - ETA: 3s - loss: 0.3474 - acc: 0.929 - ETA: 3s - loss: 0.3433 - acc: 0.929 - ETA: 3s - loss: 0.3386 - acc: 0.930 - ETA: 3s - loss: 0.3347 - acc: 0.930 - ETA: 3s - loss: 0.3384 - acc: 0.931 - ETA: 3s - loss: 0.3339 - acc: 0.931 - ETA: 3s - loss: 0.3290 - acc: 0.931 - ETA: 3s - loss: 0.3336 - acc: 0.930 - ETA: 3s - loss: 0.3269 - acc: 0.931 - ETA: 3s - loss: 0.3275 - acc: 0.932 - ETA: 3s - loss: 0.3311 - acc: 0.932 - ETA: 2s - loss: 0.3342 - acc: 0.932 - ETA: 2s - loss: 0.3387 - acc: 0.931 - ETA: 2s - loss: 0.3347 - acc: 0.932 - ETA: 2s - loss: 0.3456 - acc: 0.931 - ETA: 2s - loss: 0.3429 - acc: 0.931 - ETA: 2s - loss: 0.3434 - acc: 0.932 - ETA: 2s - loss: 0.3680 - acc: 0.931 - ETA: 2s - loss: 0.3644 - acc: 0.931 - ETA: 2s - loss: 0.3753 - acc: 0.930 - ETA: 2s - loss: 0.3702 - acc: 0.931 - ETA: 2s - loss: 0.3652 - acc: 0.931 - ETA: 2s - loss: 0.3661 - acc: 0.931 - ETA: 2s - loss: 0.3611 - acc: 0.932 - ETA: 2s - loss: 0.3633 - acc: 0.931 - ETA: 2s - loss: 0.3595 - acc: 0.931 - ETA: 2s - loss: 0.3562 - acc: 0.932 - ETA: 2s - loss: 0.3512 - acc: 0.933 - ETA: 1s - loss: 0.3529 - acc: 0.933 - ETA: 1s - loss: 0.3492 - acc: 0.933 - ETA: 1s - loss: 0.3457 - acc: 0.933 - ETA: 1s - loss: 0.3407 - acc: 0.934 - ETA: 1s - loss: 0.3358 - acc: 0.935 - ETA: 1s - loss: 0.3321 - acc: 0.935 - ETA: 1s - loss: 0.3269 - acc: 0.936 - ETA: 1s - loss: 0.3225 - acc: 0.937 - ETA: 1s - loss: 0.3301 - acc: 0.937 - ETA: 1s - loss: 0.3279 - acc: 0.938 - ETA: 1s - loss: 0.3300 - acc: 0.938 - ETA: 1s - loss: 0.3270 - acc: 0.938 - ETA: 1s - loss: 0.3231 - acc: 0.939 - ETA: 1s - loss: 0.3336 - acc: 0.938 - ETA: 1s - loss: 0.3371 - acc: 0.937 - ETA: 1s - loss: 0.3343 - acc: 0.938 - ETA: 0s - loss: 0.3335 - acc: 0.937 - ETA: 0s - loss: 0.3314 - acc: 0.937 - ETA: 0s - loss: 0.3379 - acc: 0.937 - ETA: 0s - loss: 0.3370 - acc: 0.936 - ETA: 0s - loss: 0.3380 - acc: 0.936 - ETA: 0s - loss: 0.3436 - acc: 0.936 - ETA: 0s - loss: 0.3408 - acc: 0.936 - ETA: 0s - loss: 0.3377 - acc: 0.936 - ETA: 0s - loss: 0.3354 - acc: 0.936 - ETA: 0s - loss: 0.3448 - acc: 0.936 - ETA: 0s - loss: 0.3460 - acc: 0.936 - ETA: 0s - loss: 0.3549 - acc: 0.936 - ETA: 0s - loss: 0.3517 - acc: 0.936 - ETA: 0s - loss: 0.3536 - acc: 0.936 - ETA: 0s - loss: 0.3547 - acc: 0.936 - 6s 1ms/step - loss: 0.3573 - acc: 0.9361 - val_loss: 0.4175 - val_acc: 0.9314\n",
      "Epoch 41/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 2.1459 - acc: 0.812 - ETA: 4s - loss: 0.5987 - acc: 0.937 - ETA: 4s - loss: 0.3959 - acc: 0.937 - ETA: 5s - loss: 0.5691 - acc: 0.937 - ETA: 5s - loss: 0.5632 - acc: 0.942 - ETA: 5s - loss: 0.5124 - acc: 0.942 - ETA: 5s - loss: 0.4558 - acc: 0.945 - ETA: 5s - loss: 0.3971 - acc: 0.947 - ETA: 5s - loss: 0.3740 - acc: 0.946 - ETA: 5s - loss: 0.3493 - acc: 0.947 - ETA: 5s - loss: 0.3265 - acc: 0.951 - ETA: 5s - loss: 0.3147 - acc: 0.948 - ETA: 5s - loss: 0.3681 - acc: 0.947 - ETA: 5s - loss: 0.3751 - acc: 0.947 - ETA: 5s - loss: 0.3488 - acc: 0.949 - ETA: 4s - loss: 0.3484 - acc: 0.948 - ETA: 4s - loss: 0.3236 - acc: 0.952 - ETA: 4s - loss: 0.3307 - acc: 0.951 - ETA: 4s - loss: 0.3172 - acc: 0.951 - ETA: 4s - loss: 0.3052 - acc: 0.953 - ETA: 4s - loss: 0.2930 - acc: 0.954 - ETA: 4s - loss: 0.3165 - acc: 0.953 - ETA: 4s - loss: 0.3017 - acc: 0.956 - ETA: 4s - loss: 0.3076 - acc: 0.956 - ETA: 4s - loss: 0.3113 - acc: 0.956 - ETA: 4s - loss: 0.3071 - acc: 0.956 - ETA: 3s - loss: 0.2973 - acc: 0.957 - ETA: 3s - loss: 0.3138 - acc: 0.957 - ETA: 3s - loss: 0.3045 - acc: 0.958 - ETA: 3s - loss: 0.3106 - acc: 0.958 - ETA: 3s - loss: 0.3182 - acc: 0.957 - ETA: 3s - loss: 0.3121 - acc: 0.957 - ETA: 3s - loss: 0.3085 - acc: 0.956 - ETA: 3s - loss: 0.3025 - acc: 0.957 - ETA: 3s - loss: 0.2950 - acc: 0.958 - ETA: 3s - loss: 0.3006 - acc: 0.956 - ETA: 3s - loss: 0.3027 - acc: 0.956 - ETA: 3s - loss: 0.2940 - acc: 0.958 - ETA: 3s - loss: 0.3078 - acc: 0.956 - ETA: 3s - loss: 0.3283 - acc: 0.956 - ETA: 3s - loss: 0.3300 - acc: 0.957 - ETA: 3s - loss: 0.3239 - acc: 0.957 - ETA: 2s - loss: 0.3159 - acc: 0.958 - ETA: 2s - loss: 0.3185 - acc: 0.958 - ETA: 2s - loss: 0.3114 - acc: 0.959 - ETA: 2s - loss: 0.3313 - acc: 0.957 - ETA: 2s - loss: 0.3329 - acc: 0.957 - ETA: 2s - loss: 0.3288 - acc: 0.957 - ETA: 2s - loss: 0.3307 - acc: 0.957 - ETA: 2s - loss: 0.3268 - acc: 0.957 - ETA: 2s - loss: 0.3226 - acc: 0.958 - ETA: 2s - loss: 0.3254 - acc: 0.958 - ETA: 2s - loss: 0.3265 - acc: 0.959 - ETA: 2s - loss: 0.3211 - acc: 0.959 - ETA: 2s - loss: 0.3298 - acc: 0.959 - ETA: 2s - loss: 0.3263 - acc: 0.958 - ETA: 2s - loss: 0.3223 - acc: 0.958 - ETA: 2s - loss: 0.3313 - acc: 0.957 - ETA: 2s - loss: 0.3344 - acc: 0.956 - ETA: 1s - loss: 0.3425 - acc: 0.955 - ETA: 1s - loss: 0.3448 - acc: 0.955 - ETA: 1s - loss: 0.3412 - acc: 0.956 - ETA: 1s - loss: 0.3383 - acc: 0.956 - ETA: 1s - loss: 0.3413 - acc: 0.956 - ETA: 1s - loss: 0.3375 - acc: 0.956 - ETA: 1s - loss: 0.3349 - acc: 0.956 - ETA: 1s - loss: 0.3307 - acc: 0.956 - ETA: 1s - loss: 0.3306 - acc: 0.956 - ETA: 1s - loss: 0.3271 - acc: 0.955 - ETA: 1s - loss: 0.3234 - acc: 0.955 - ETA: 1s - loss: 0.3246 - acc: 0.955 - ETA: 1s - loss: 0.3226 - acc: 0.955 - ETA: 1s - loss: 0.3188 - acc: 0.955 - ETA: 1s - loss: 0.3345 - acc: 0.954 - ETA: 1s - loss: 0.3413 - acc: 0.952 - ETA: 1s - loss: 0.3390 - acc: 0.952 - ETA: 0s - loss: 0.3390 - acc: 0.950 - ETA: 0s - loss: 0.3448 - acc: 0.947 - ETA: 0s - loss: 0.3461 - acc: 0.945 - ETA: 0s - loss: 0.3486 - acc: 0.942 - ETA: 0s - loss: 0.3497 - acc: 0.940 - ETA: 0s - loss: 0.3508 - acc: 0.938 - ETA: 0s - loss: 0.3520 - acc: 0.936 - ETA: 0s - loss: 0.3552 - acc: 0.934 - ETA: 0s - loss: 0.3671 - acc: 0.930 - ETA: 0s - loss: 0.3670 - acc: 0.929 - ETA: 0s - loss: 0.3718 - acc: 0.929 - ETA: 0s - loss: 0.3754 - acc: 0.927 - ETA: 0s - loss: 0.3770 - acc: 0.927 - ETA: 0s - loss: 0.3777 - acc: 0.927 - ETA: 0s - loss: 0.3815 - acc: 0.927 - ETA: 0s - loss: 0.3842 - acc: 0.927 - ETA: 0s - loss: 0.3893 - acc: 0.927 - 6s 1ms/step - loss: 0.3877 - acc: 0.9282 - val_loss: 1.1408 - val_acc: 0.8994\n",
      "Epoch 42/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 1.3677 - acc: 0.875 - ETA: 4s - loss: 0.6959 - acc: 0.937 - ETA: 4s - loss: 0.7114 - acc: 0.937 - ETA: 4s - loss: 0.8465 - acc: 0.918 - ETA: 4s - loss: 0.7837 - acc: 0.923 - ETA: 4s - loss: 0.7999 - acc: 0.921 - ETA: 4s - loss: 0.7468 - acc: 0.920 - ETA: 4s - loss: 0.6714 - acc: 0.913 - ETA: 4s - loss: 0.6958 - acc: 0.911 - ETA: 4s - loss: 0.6726 - acc: 0.909 - ETA: 4s - loss: 0.6702 - acc: 0.897 - ETA: 4s - loss: 0.6892 - acc: 0.892 - ETA: 4s - loss: 0.7100 - acc: 0.883 - ETA: 4s - loss: 0.6935 - acc: 0.884 - ETA: 4s - loss: 0.6716 - acc: 0.884 - ETA: 4s - loss: 0.7012 - acc: 0.877 - ETA: 4s - loss: 0.6795 - acc: 0.877 - ETA: 4s - loss: 0.7074 - acc: 0.876 - ETA: 4s - loss: 0.7258 - acc: 0.876 - ETA: 4s - loss: 0.7186 - acc: 0.879 - ETA: 4s - loss: 0.7021 - acc: 0.878 - ETA: 4s - loss: 0.7136 - acc: 0.875 - ETA: 4s - loss: 0.7242 - acc: 0.871 - ETA: 4s - loss: 0.7521 - acc: 0.864 - ETA: 4s - loss: 0.7436 - acc: 0.862 - ETA: 4s - loss: 0.7257 - acc: 0.857 - ETA: 4s - loss: 0.7156 - acc: 0.858 - ETA: 3s - loss: 0.7018 - acc: 0.857 - ETA: 3s - loss: 0.7211 - acc: 0.855 - ETA: 3s - loss: 0.7274 - acc: 0.853 - ETA: 3s - loss: 0.7192 - acc: 0.855 - ETA: 3s - loss: 0.7210 - acc: 0.853 - ETA: 3s - loss: 0.7052 - acc: 0.852 - ETA: 3s - loss: 0.7013 - acc: 0.850 - ETA: 3s - loss: 0.6869 - acc: 0.850 - ETA: 3s - loss: 0.6855 - acc: 0.848 - ETA: 3s - loss: 0.6826 - acc: 0.846 - ETA: 3s - loss: 0.6871 - acc: 0.845 - ETA: 3s - loss: 0.6754 - acc: 0.843 - ETA: 3s - loss: 0.6671 - acc: 0.843 - ETA: 3s - loss: 0.6545 - acc: 0.844 - ETA: 2s - loss: 0.6521 - acc: 0.844 - ETA: 2s - loss: 0.6400 - acc: 0.845 - ETA: 2s - loss: 0.6449 - acc: 0.846 - ETA: 2s - loss: 0.6325 - acc: 0.847 - ETA: 2s - loss: 0.6192 - acc: 0.849 - ETA: 2s - loss: 0.6142 - acc: 0.852 - ETA: 2s - loss: 0.6079 - acc: 0.854 - ETA: 2s - loss: 0.6035 - acc: 0.856 - ETA: 2s - loss: 0.5961 - acc: 0.857 - ETA: 2s - loss: 0.5851 - acc: 0.860 - ETA: 2s - loss: 0.5817 - acc: 0.862 - ETA: 2s - loss: 0.5740 - acc: 0.862 - ETA: 2s - loss: 0.5650 - acc: 0.863 - ETA: 2s - loss: 0.5635 - acc: 0.864 - ETA: 2s - loss: 0.5608 - acc: 0.865 - ETA: 1s - loss: 0.5580 - acc: 0.866 - ETA: 1s - loss: 0.5494 - acc: 0.868 - ETA: 1s - loss: 0.5419 - acc: 0.869 - ETA: 1s - loss: 0.5374 - acc: 0.869 - ETA: 1s - loss: 0.5477 - acc: 0.867 - ETA: 1s - loss: 0.5549 - acc: 0.866 - ETA: 1s - loss: 0.5542 - acc: 0.866 - ETA: 1s - loss: 0.5479 - acc: 0.866 - ETA: 1s - loss: 0.5416 - acc: 0.867 - ETA: 1s - loss: 0.5407 - acc: 0.868 - ETA: 1s - loss: 0.5357 - acc: 0.867 - ETA: 1s - loss: 0.5302 - acc: 0.867 - ETA: 1s - loss: 0.5256 - acc: 0.867 - ETA: 1s - loss: 0.5260 - acc: 0.867 - ETA: 1s - loss: 0.5211 - acc: 0.867 - ETA: 1s - loss: 0.5181 - acc: 0.867 - ETA: 0s - loss: 0.5216 - acc: 0.866 - ETA: 0s - loss: 0.5285 - acc: 0.865 - ETA: 0s - loss: 0.5218 - acc: 0.866 - ETA: 0s - loss: 0.5173 - acc: 0.867 - ETA: 0s - loss: 0.5169 - acc: 0.867 - ETA: 0s - loss: 0.5178 - acc: 0.866 - ETA: 0s - loss: 0.5130 - acc: 0.866 - ETA: 0s - loss: 0.5166 - acc: 0.866 - ETA: 0s - loss: 0.5116 - acc: 0.868 - ETA: 0s - loss: 0.5196 - acc: 0.867 - ETA: 0s - loss: 0.5195 - acc: 0.868 - ETA: 0s - loss: 0.5153 - acc: 0.868 - ETA: 0s - loss: 0.5116 - acc: 0.868 - ETA: 0s - loss: 0.5080 - acc: 0.869 - ETA: 0s - loss: 0.5048 - acc: 0.870 - ETA: 0s - loss: 0.5015 - acc: 0.870 - 6s 1ms/step - loss: 0.4999 - acc: 0.8704 - val_loss: 0.4241 - val_acc: 0.8885\n",
      "Epoch 43/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0916 - acc: 0.875 - ETA: 4s - loss: 0.3341 - acc: 0.953 - ETA: 4s - loss: 0.3651 - acc: 0.937 - ETA: 4s - loss: 0.3398 - acc: 0.887 - ETA: 4s - loss: 0.2884 - acc: 0.908 - ETA: 4s - loss: 0.3313 - acc: 0.906 - ETA: 4s - loss: 0.3101 - acc: 0.904 - ETA: 4s - loss: 0.2932 - acc: 0.903 - ETA: 4s - loss: 0.2797 - acc: 0.910 - ETA: 4s - loss: 0.2709 - acc: 0.904 - ETA: 4s - loss: 0.2618 - acc: 0.909 - ETA: 4s - loss: 0.2500 - acc: 0.909 - ETA: 4s - loss: 0.2450 - acc: 0.910 - ETA: 4s - loss: 0.2608 - acc: 0.909 - ETA: 4s - loss: 0.2756 - acc: 0.910 - ETA: 4s - loss: 0.2905 - acc: 0.908 - ETA: 4s - loss: 0.3061 - acc: 0.906 - ETA: 4s - loss: 0.3183 - acc: 0.904 - ETA: 3s - loss: 0.3467 - acc: 0.902 - ETA: 3s - loss: 0.3388 - acc: 0.900 - ETA: 3s - loss: 0.3791 - acc: 0.899 - ETA: 3s - loss: 0.4007 - acc: 0.894 - ETA: 3s - loss: 0.4031 - acc: 0.895 - ETA: 3s - loss: 0.3997 - acc: 0.893 - ETA: 3s - loss: 0.4055 - acc: 0.892 - ETA: 3s - loss: 0.3948 - acc: 0.893 - ETA: 3s - loss: 0.3882 - acc: 0.891 - ETA: 3s - loss: 0.4064 - acc: 0.889 - ETA: 3s - loss: 0.4071 - acc: 0.892 - ETA: 3s - loss: 0.3997 - acc: 0.890 - ETA: 3s - loss: 0.4034 - acc: 0.892 - ETA: 3s - loss: 0.4084 - acc: 0.892 - ETA: 3s - loss: 0.4025 - acc: 0.891 - ETA: 3s - loss: 0.3943 - acc: 0.892 - ETA: 3s - loss: 0.3859 - acc: 0.893 - ETA: 3s - loss: 0.3991 - acc: 0.893 - ETA: 3s - loss: 0.4110 - acc: 0.891 - ETA: 2s - loss: 0.4117 - acc: 0.892 - ETA: 2s - loss: 0.4077 - acc: 0.892 - ETA: 2s - loss: 0.4011 - acc: 0.893 - ETA: 2s - loss: 0.3947 - acc: 0.893 - ETA: 2s - loss: 0.3900 - acc: 0.894 - ETA: 2s - loss: 0.3933 - acc: 0.894 - ETA: 2s - loss: 0.3944 - acc: 0.894 - ETA: 2s - loss: 0.3902 - acc: 0.895 - ETA: 2s - loss: 0.3879 - acc: 0.895 - ETA: 2s - loss: 0.3845 - acc: 0.895 - ETA: 2s - loss: 0.3856 - acc: 0.893 - ETA: 2s - loss: 0.3888 - acc: 0.893 - ETA: 2s - loss: 0.3989 - acc: 0.891 - ETA: 2s - loss: 0.3939 - acc: 0.893 - ETA: 2s - loss: 0.3890 - acc: 0.894 - ETA: 2s - loss: 0.3922 - acc: 0.894 - ETA: 2s - loss: 0.3865 - acc: 0.896 - ETA: 2s - loss: 0.3874 - acc: 0.897 - ETA: 2s - loss: 0.3909 - acc: 0.897 - ETA: 2s - loss: 0.3894 - acc: 0.898 - ETA: 2s - loss: 0.3881 - acc: 0.898 - ETA: 2s - loss: 0.3865 - acc: 0.898 - ETA: 2s - loss: 0.3829 - acc: 0.899 - ETA: 2s - loss: 0.3800 - acc: 0.900 - ETA: 2s - loss: 0.3810 - acc: 0.900 - ETA: 1s - loss: 0.3820 - acc: 0.900 - ETA: 1s - loss: 0.3787 - acc: 0.900 - ETA: 1s - loss: 0.3750 - acc: 0.901 - ETA: 1s - loss: 0.3739 - acc: 0.901 - ETA: 1s - loss: 0.3767 - acc: 0.901 - ETA: 1s - loss: 0.3798 - acc: 0.901 - ETA: 1s - loss: 0.3759 - acc: 0.902 - ETA: 1s - loss: 0.3720 - acc: 0.902 - ETA: 1s - loss: 0.3744 - acc: 0.902 - ETA: 1s - loss: 0.3714 - acc: 0.903 - ETA: 1s - loss: 0.3694 - acc: 0.903 - ETA: 1s - loss: 0.3718 - acc: 0.903 - ETA: 1s - loss: 0.3694 - acc: 0.904 - ETA: 1s - loss: 0.3677 - acc: 0.905 - ETA: 1s - loss: 0.3651 - acc: 0.905 - ETA: 1s - loss: 0.3621 - acc: 0.906 - ETA: 1s - loss: 0.3605 - acc: 0.906 - ETA: 1s - loss: 0.3580 - acc: 0.907 - ETA: 1s - loss: 0.3549 - acc: 0.908 - ETA: 0s - loss: 0.3522 - acc: 0.909 - ETA: 0s - loss: 0.3510 - acc: 0.908 - ETA: 0s - loss: 0.3479 - acc: 0.909 - ETA: 0s - loss: 0.3484 - acc: 0.908 - ETA: 0s - loss: 0.3478 - acc: 0.908 - ETA: 0s - loss: 0.3456 - acc: 0.909 - ETA: 0s - loss: 0.3441 - acc: 0.909 - ETA: 0s - loss: 0.3479 - acc: 0.909 - ETA: 0s - loss: 0.3459 - acc: 0.910 - ETA: 0s - loss: 0.3435 - acc: 0.910 - ETA: 0s - loss: 0.3419 - acc: 0.910 - ETA: 0s - loss: 0.3387 - acc: 0.911 - ETA: 0s - loss: 0.3362 - acc: 0.912 - ETA: 0s - loss: 0.3344 - acc: 0.912 - ETA: 0s - loss: 0.3355 - acc: 0.912 - ETA: 0s - loss: 0.3321 - acc: 0.914 - ETA: 0s - loss: 0.3305 - acc: 0.913 - ETA: 0s - loss: 0.3293 - acc: 0.912 - 6s 2ms/step - loss: 0.3282 - acc: 0.9130 - val_loss: 0.4344 - val_acc: 0.9147\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 4s - loss: 0.0783 - acc: 1.000 - ETA: 4s - loss: 0.1673 - acc: 0.937 - ETA: 4s - loss: 0.1295 - acc: 0.946 - ETA: 4s - loss: 0.2198 - acc: 0.943 - ETA: 4s - loss: 0.2886 - acc: 0.937 - ETA: 4s - loss: 0.3202 - acc: 0.937 - ETA: 4s - loss: 0.3400 - acc: 0.944 - ETA: 4s - loss: 0.3226 - acc: 0.937 - ETA: 4s - loss: 0.3352 - acc: 0.937 - ETA: 4s - loss: 0.3240 - acc: 0.926 - ETA: 4s - loss: 0.3220 - acc: 0.925 - ETA: 4s - loss: 0.3069 - acc: 0.926 - ETA: 4s - loss: 0.2856 - acc: 0.932 - ETA: 4s - loss: 0.3031 - acc: 0.928 - ETA: 4s - loss: 0.2877 - acc: 0.931 - ETA: 4s - loss: 0.2809 - acc: 0.929 - ETA: 4s - loss: 0.2819 - acc: 0.926 - ETA: 4s - loss: 0.2729 - acc: 0.926 - ETA: 3s - loss: 0.2628 - acc: 0.928 - ETA: 3s - loss: 0.2603 - acc: 0.925 - ETA: 3s - loss: 0.2527 - acc: 0.927 - ETA: 3s - loss: 0.2693 - acc: 0.925 - ETA: 3s - loss: 0.2636 - acc: 0.926 - ETA: 3s - loss: 0.2876 - acc: 0.925 - ETA: 3s - loss: 0.3128 - acc: 0.924 - ETA: 3s - loss: 0.3184 - acc: 0.924 - ETA: 3s - loss: 0.3260 - acc: 0.924 - ETA: 3s - loss: 0.3308 - acc: 0.924 - ETA: 3s - loss: 0.3264 - acc: 0.923 - ETA: 3s - loss: 0.3313 - acc: 0.922 - ETA: 3s - loss: 0.3218 - acc: 0.925 - ETA: 3s - loss: 0.3157 - acc: 0.924 - ETA: 3s - loss: 0.3081 - acc: 0.927 - ETA: 3s - loss: 0.3175 - acc: 0.926 - ETA: 3s - loss: 0.3205 - acc: 0.927 - ETA: 3s - loss: 0.3136 - acc: 0.928 - ETA: 3s - loss: 0.3125 - acc: 0.927 - ETA: 3s - loss: 0.3070 - acc: 0.929 - ETA: 2s - loss: 0.3020 - acc: 0.930 - ETA: 2s - loss: 0.3056 - acc: 0.930 - ETA: 2s - loss: 0.3040 - acc: 0.929 - ETA: 2s - loss: 0.3081 - acc: 0.928 - ETA: 2s - loss: 0.3157 - acc: 0.928 - ETA: 2s - loss: 0.3102 - acc: 0.929 - ETA: 2s - loss: 0.3088 - acc: 0.928 - ETA: 2s - loss: 0.3066 - acc: 0.927 - ETA: 2s - loss: 0.3037 - acc: 0.927 - ETA: 2s - loss: 0.3000 - acc: 0.927 - ETA: 2s - loss: 0.3059 - acc: 0.926 - ETA: 2s - loss: 0.3041 - acc: 0.927 - ETA: 2s - loss: 0.3108 - acc: 0.926 - ETA: 2s - loss: 0.3065 - acc: 0.927 - ETA: 2s - loss: 0.3043 - acc: 0.927 - ETA: 2s - loss: 0.2998 - acc: 0.927 - ETA: 2s - loss: 0.2969 - acc: 0.928 - ETA: 1s - loss: 0.2961 - acc: 0.928 - ETA: 1s - loss: 0.2963 - acc: 0.928 - ETA: 1s - loss: 0.2934 - acc: 0.928 - ETA: 1s - loss: 0.2914 - acc: 0.929 - ETA: 1s - loss: 0.2884 - acc: 0.929 - ETA: 1s - loss: 0.2864 - acc: 0.929 - ETA: 1s - loss: 0.2860 - acc: 0.928 - ETA: 1s - loss: 0.2863 - acc: 0.927 - ETA: 1s - loss: 0.2847 - acc: 0.927 - ETA: 1s - loss: 0.2835 - acc: 0.927 - ETA: 1s - loss: 0.2835 - acc: 0.926 - ETA: 1s - loss: 0.2818 - acc: 0.926 - ETA: 1s - loss: 0.2787 - acc: 0.927 - ETA: 1s - loss: 0.2769 - acc: 0.928 - ETA: 1s - loss: 0.2757 - acc: 0.927 - ETA: 1s - loss: 0.2747 - acc: 0.926 - ETA: 0s - loss: 0.2739 - acc: 0.927 - ETA: 0s - loss: 0.2725 - acc: 0.927 - ETA: 0s - loss: 0.2693 - acc: 0.928 - ETA: 0s - loss: 0.2677 - acc: 0.928 - ETA: 0s - loss: 0.2661 - acc: 0.929 - ETA: 0s - loss: 0.2652 - acc: 0.929 - ETA: 0s - loss: 0.2638 - acc: 0.929 - ETA: 0s - loss: 0.2620 - acc: 0.929 - ETA: 0s - loss: 0.2590 - acc: 0.930 - ETA: 0s - loss: 0.2572 - acc: 0.930 - ETA: 0s - loss: 0.2544 - acc: 0.931 - ETA: 0s - loss: 0.2549 - acc: 0.930 - ETA: 0s - loss: 0.2534 - acc: 0.930 - ETA: 0s - loss: 0.2549 - acc: 0.930 - ETA: 0s - loss: 0.2539 - acc: 0.930 - 6s 1ms/step - loss: 0.2525 - acc: 0.9309 - val_loss: 0.3623 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0970 - acc: 1.000 - ETA: 4s - loss: 0.1118 - acc: 0.968 - ETA: 4s - loss: 0.2721 - acc: 0.946 - ETA: 4s - loss: 0.2180 - acc: 0.956 - ETA: 4s - loss: 0.2158 - acc: 0.951 - ETA: 4s - loss: 0.2111 - acc: 0.945 - ETA: 4s - loss: 0.1981 - acc: 0.944 - ETA: 4s - loss: 0.1818 - acc: 0.946 - ETA: 4s - loss: 0.1755 - acc: 0.945 - ETA: 4s - loss: 0.2005 - acc: 0.948 - ETA: 4s - loss: 0.1923 - acc: 0.949 - ETA: 4s - loss: 0.1927 - acc: 0.943 - ETA: 4s - loss: 0.1941 - acc: 0.937 - ETA: 4s - loss: 0.1864 - acc: 0.939 - ETA: 4s - loss: 0.2155 - acc: 0.937 - ETA: 4s - loss: 0.2067 - acc: 0.940 - ETA: 4s - loss: 0.2001 - acc: 0.940 - ETA: 3s - loss: 0.1948 - acc: 0.941 - ETA: 3s - loss: 0.1920 - acc: 0.942 - ETA: 3s - loss: 0.1889 - acc: 0.940 - ETA: 3s - loss: 0.2627 - acc: 0.938 - ETA: 3s - loss: 0.3408 - acc: 0.933 - ETA: 3s - loss: 0.3761 - acc: 0.931 - ETA: 3s - loss: 0.4064 - acc: 0.931 - ETA: 3s - loss: 0.4213 - acc: 0.930 - ETA: 3s - loss: 0.4201 - acc: 0.931 - ETA: 3s - loss: 0.4461 - acc: 0.929 - ETA: 3s - loss: 0.4564 - acc: 0.926 - ETA: 3s - loss: 0.4780 - acc: 0.926 - ETA: 3s - loss: 0.4824 - acc: 0.926 - ETA: 3s - loss: 0.4733 - acc: 0.927 - ETA: 3s - loss: 0.4708 - acc: 0.926 - ETA: 3s - loss: 0.4780 - acc: 0.927 - ETA: 3s - loss: 0.4696 - acc: 0.928 - ETA: 3s - loss: 0.4779 - acc: 0.927 - ETA: 2s - loss: 0.5036 - acc: 0.927 - ETA: 2s - loss: 0.4906 - acc: 0.929 - ETA: 2s - loss: 0.4812 - acc: 0.930 - ETA: 2s - loss: 0.4711 - acc: 0.932 - ETA: 2s - loss: 0.4627 - acc: 0.932 - ETA: 2s - loss: 0.4634 - acc: 0.931 - ETA: 2s - loss: 0.4575 - acc: 0.932 - ETA: 2s - loss: 0.4493 - acc: 0.932 - ETA: 2s - loss: 0.4405 - acc: 0.933 - ETA: 2s - loss: 0.4327 - acc: 0.934 - ETA: 2s - loss: 0.4326 - acc: 0.934 - ETA: 2s - loss: 0.4320 - acc: 0.934 - ETA: 2s - loss: 0.4338 - acc: 0.933 - ETA: 2s - loss: 0.4340 - acc: 0.934 - ETA: 2s - loss: 0.4257 - acc: 0.935 - ETA: 2s - loss: 0.4194 - acc: 0.935 - ETA: 2s - loss: 0.4195 - acc: 0.935 - ETA: 1s - loss: 0.4146 - acc: 0.935 - ETA: 1s - loss: 0.4097 - acc: 0.935 - ETA: 1s - loss: 0.4035 - acc: 0.936 - ETA: 1s - loss: 0.4057 - acc: 0.935 - ETA: 1s - loss: 0.3990 - acc: 0.936 - ETA: 1s - loss: 0.3985 - acc: 0.937 - ETA: 1s - loss: 0.3994 - acc: 0.937 - ETA: 1s - loss: 0.4003 - acc: 0.937 - ETA: 1s - loss: 0.3944 - acc: 0.938 - ETA: 1s - loss: 0.3897 - acc: 0.938 - ETA: 1s - loss: 0.3892 - acc: 0.938 - ETA: 1s - loss: 0.3857 - acc: 0.939 - ETA: 1s - loss: 0.3880 - acc: 0.939 - ETA: 1s - loss: 0.3904 - acc: 0.939 - ETA: 1s - loss: 0.3873 - acc: 0.939 - ETA: 1s - loss: 0.3840 - acc: 0.940 - ETA: 1s - loss: 0.3802 - acc: 0.940 - ETA: 1s - loss: 0.3754 - acc: 0.940 - ETA: 1s - loss: 0.3716 - acc: 0.940 - ETA: 0s - loss: 0.3697 - acc: 0.940 - ETA: 0s - loss: 0.3665 - acc: 0.941 - ETA: 0s - loss: 0.3667 - acc: 0.941 - ETA: 0s - loss: 0.3680 - acc: 0.940 - ETA: 0s - loss: 0.3689 - acc: 0.940 - ETA: 0s - loss: 0.3653 - acc: 0.940 - ETA: 0s - loss: 0.3610 - acc: 0.941 - ETA: 0s - loss: 0.3581 - acc: 0.941 - ETA: 0s - loss: 0.3553 - acc: 0.941 - ETA: 0s - loss: 0.3606 - acc: 0.941 - ETA: 0s - loss: 0.3576 - acc: 0.941 - ETA: 0s - loss: 0.3552 - acc: 0.941 - ETA: 0s - loss: 0.3525 - acc: 0.942 - ETA: 0s - loss: 0.3489 - acc: 0.942 - ETA: 0s - loss: 0.3454 - acc: 0.943 - ETA: 0s - loss: 0.3461 - acc: 0.943 - ETA: 0s - loss: 0.3423 - acc: 0.944 - ETA: 0s - loss: 0.3395 - acc: 0.944 - 6s 1ms/step - loss: 0.3414 - acc: 0.9439 - val_loss: 0.4593 - val_acc: 0.9321\n",
      "Epoch 46/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.2517 - acc: 0.875 - ETA: 6s - loss: 0.3241 - acc: 0.791 - ETA: 7s - loss: 0.4591 - acc: 0.762 - ETA: 6s - loss: 0.3558 - acc: 0.812 - ETA: 6s - loss: 0.2857 - acc: 0.850 - ETA: 6s - loss: 0.3039 - acc: 0.875 - ETA: 6s - loss: 0.2681 - acc: 0.891 - ETA: 6s - loss: 0.2470 - acc: 0.897 - ETA: 6s - loss: 0.3315 - acc: 0.898 - ETA: 5s - loss: 0.3686 - acc: 0.903 - ETA: 5s - loss: 0.5114 - acc: 0.897 - ETA: 5s - loss: 0.5299 - acc: 0.895 - ETA: 5s - loss: 0.4977 - acc: 0.900 - ETA: 5s - loss: 0.5275 - acc: 0.898 - ETA: 5s - loss: 0.5032 - acc: 0.892 - ETA: 5s - loss: 0.4976 - acc: 0.883 - ETA: 5s - loss: 0.4972 - acc: 0.871 - ETA: 5s - loss: 0.5129 - acc: 0.865 - ETA: 5s - loss: 0.5049 - acc: 0.856 - ETA: 5s - loss: 0.5143 - acc: 0.856 - ETA: 5s - loss: 0.5010 - acc: 0.856 - ETA: 5s - loss: 0.4946 - acc: 0.855 - ETA: 5s - loss: 0.5012 - acc: 0.850 - ETA: 4s - loss: 0.4895 - acc: 0.847 - ETA: 4s - loss: 0.5268 - acc: 0.841 - ETA: 4s - loss: 0.5141 - acc: 0.843 - ETA: 4s - loss: 0.5039 - acc: 0.843 - ETA: 4s - loss: 0.4987 - acc: 0.843 - ETA: 4s - loss: 0.4872 - acc: 0.846 - ETA: 4s - loss: 0.4790 - acc: 0.846 - ETA: 4s - loss: 0.4726 - acc: 0.846 - ETA: 4s - loss: 0.4631 - acc: 0.848 - ETA: 4s - loss: 0.4559 - acc: 0.845 - ETA: 4s - loss: 0.4645 - acc: 0.842 - ETA: 4s - loss: 0.4714 - acc: 0.841 - ETA: 4s - loss: 0.4684 - acc: 0.837 - ETA: 4s - loss: 0.4608 - acc: 0.836 - ETA: 4s - loss: 0.4552 - acc: 0.834 - ETA: 3s - loss: 0.4505 - acc: 0.833 - ETA: 3s - loss: 0.4450 - acc: 0.832 - ETA: 3s - loss: 0.4468 - acc: 0.832 - ETA: 3s - loss: 0.4370 - acc: 0.834 - ETA: 3s - loss: 0.4300 - acc: 0.837 - ETA: 3s - loss: 0.4439 - acc: 0.838 - ETA: 3s - loss: 0.4483 - acc: 0.838 - ETA: 3s - loss: 0.4597 - acc: 0.840 - ETA: 3s - loss: 0.4500 - acc: 0.843 - ETA: 3s - loss: 0.4428 - acc: 0.846 - ETA: 3s - loss: 0.4378 - acc: 0.847 - ETA: 3s - loss: 0.4422 - acc: 0.847 - ETA: 3s - loss: 0.4706 - acc: 0.847 - ETA: 3s - loss: 0.4817 - acc: 0.848 - ETA: 3s - loss: 0.4756 - acc: 0.849 - ETA: 3s - loss: 0.4700 - acc: 0.850 - ETA: 3s - loss: 0.4749 - acc: 0.850 - ETA: 3s - loss: 0.4673 - acc: 0.851 - ETA: 3s - loss: 0.4611 - acc: 0.852 - ETA: 2s - loss: 0.4554 - acc: 0.854 - ETA: 2s - loss: 0.4513 - acc: 0.854 - ETA: 2s - loss: 0.4491 - acc: 0.854 - ETA: 2s - loss: 0.4438 - acc: 0.854 - ETA: 2s - loss: 0.4401 - acc: 0.853 - ETA: 2s - loss: 0.4358 - acc: 0.853 - ETA: 2s - loss: 0.4334 - acc: 0.852 - ETA: 2s - loss: 0.4305 - acc: 0.852 - ETA: 2s - loss: 0.4264 - acc: 0.853 - ETA: 2s - loss: 0.4243 - acc: 0.853 - ETA: 2s - loss: 0.4182 - acc: 0.854 - ETA: 2s - loss: 0.4160 - acc: 0.853 - ETA: 2s - loss: 0.4130 - acc: 0.853 - ETA: 2s - loss: 0.4109 - acc: 0.853 - ETA: 2s - loss: 0.4081 - acc: 0.853 - ETA: 2s - loss: 0.4056 - acc: 0.852 - ETA: 1s - loss: 0.4027 - acc: 0.852 - ETA: 1s - loss: 0.3998 - acc: 0.853 - ETA: 1s - loss: 0.3969 - acc: 0.853 - ETA: 1s - loss: 0.3934 - acc: 0.854 - ETA: 1s - loss: 0.3887 - acc: 0.855 - ETA: 1s - loss: 0.3843 - acc: 0.856 - ETA: 1s - loss: 0.3812 - acc: 0.858 - ETA: 1s - loss: 0.3786 - acc: 0.859 - ETA: 1s - loss: 0.3761 - acc: 0.860 - ETA: 1s - loss: 0.3725 - acc: 0.861 - ETA: 1s - loss: 0.3726 - acc: 0.863 - ETA: 1s - loss: 0.3716 - acc: 0.864 - ETA: 1s - loss: 0.3682 - acc: 0.866 - ETA: 1s - loss: 0.3646 - acc: 0.867 - ETA: 1s - loss: 0.3721 - acc: 0.866 - ETA: 1s - loss: 0.3751 - acc: 0.867 - ETA: 1s - loss: 0.3730 - acc: 0.868 - ETA: 0s - loss: 0.3701 - acc: 0.867 - ETA: 0s - loss: 0.3664 - acc: 0.869 - ETA: 0s - loss: 0.3641 - acc: 0.869 - ETA: 0s - loss: 0.3614 - acc: 0.870 - ETA: 0s - loss: 0.3590 - acc: 0.871 - ETA: 0s - loss: 0.3572 - acc: 0.871 - ETA: 0s - loss: 0.3594 - acc: 0.870 - ETA: 0s - loss: 0.3563 - acc: 0.871 - ETA: 0s - loss: 0.3527 - acc: 0.872 - ETA: 0s - loss: 0.3515 - acc: 0.872 - ETA: 0s - loss: 0.3537 - acc: 0.872 - ETA: 0s - loss: 0.3547 - acc: 0.872 - ETA: 0s - loss: 0.3597 - acc: 0.873 - ETA: 0s - loss: 0.3568 - acc: 0.874 - ETA: 0s - loss: 0.3582 - acc: 0.874 - 6s 2ms/step - loss: 0.3566 - acc: 0.8756 - val_loss: 0.5121 - val_acc: 0.9160\n",
      "Epoch 47/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0890 - acc: 0.937 - ETA: 5s - loss: 0.1116 - acc: 0.921 - ETA: 5s - loss: 0.0995 - acc: 0.937 - ETA: 5s - loss: 0.1810 - acc: 0.950 - ETA: 5s - loss: 0.3079 - acc: 0.947 - ETA: 5s - loss: 0.2811 - acc: 0.945 - ETA: 5s - loss: 0.2627 - acc: 0.944 - ETA: 5s - loss: 0.3011 - acc: 0.940 - ETA: 5s - loss: 0.2683 - acc: 0.943 - ETA: 5s - loss: 0.2510 - acc: 0.942 - ETA: 5s - loss: 0.2330 - acc: 0.944 - ETA: 5s - loss: 0.2485 - acc: 0.945 - ETA: 5s - loss: 0.2403 - acc: 0.947 - ETA: 5s - loss: 0.2286 - acc: 0.950 - ETA: 5s - loss: 0.2187 - acc: 0.951 - ETA: 5s - loss: 0.2109 - acc: 0.953 - ETA: 4s - loss: 0.2013 - acc: 0.955 - ETA: 4s - loss: 0.2182 - acc: 0.955 - ETA: 4s - loss: 0.2119 - acc: 0.957 - ETA: 4s - loss: 0.2081 - acc: 0.955 - ETA: 4s - loss: 0.2153 - acc: 0.953 - ETA: 4s - loss: 0.2109 - acc: 0.952 - ETA: 4s - loss: 0.2062 - acc: 0.952 - ETA: 4s - loss: 0.2205 - acc: 0.951 - ETA: 4s - loss: 0.2162 - acc: 0.950 - ETA: 4s - loss: 0.2334 - acc: 0.945 - ETA: 4s - loss: 0.2261 - acc: 0.946 - ETA: 4s - loss: 0.2360 - acc: 0.942 - ETA: 4s - loss: 0.2283 - acc: 0.944 - ETA: 4s - loss: 0.2240 - acc: 0.943 - ETA: 4s - loss: 0.2223 - acc: 0.941 - ETA: 3s - loss: 0.2308 - acc: 0.940 - ETA: 3s - loss: 0.2258 - acc: 0.940 - ETA: 3s - loss: 0.2232 - acc: 0.940 - ETA: 3s - loss: 0.2225 - acc: 0.938 - ETA: 3s - loss: 0.2182 - acc: 0.938 - ETA: 3s - loss: 0.2143 - acc: 0.939 - ETA: 3s - loss: 0.2135 - acc: 0.940 - ETA: 3s - loss: 0.2112 - acc: 0.941 - ETA: 3s - loss: 0.2120 - acc: 0.940 - ETA: 3s - loss: 0.2175 - acc: 0.941 - ETA: 3s - loss: 0.2238 - acc: 0.941 - ETA: 3s - loss: 0.2220 - acc: 0.941 - ETA: 3s - loss: 0.2205 - acc: 0.941 - ETA: 3s - loss: 0.2181 - acc: 0.941 - ETA: 3s - loss: 0.2151 - acc: 0.942 - ETA: 3s - loss: 0.2112 - acc: 0.943 - ETA: 2s - loss: 0.2088 - acc: 0.944 - ETA: 2s - loss: 0.2067 - acc: 0.944 - ETA: 2s - loss: 0.2133 - acc: 0.944 - ETA: 2s - loss: 0.2188 - acc: 0.944 - ETA: 2s - loss: 0.2164 - acc: 0.944 - ETA: 2s - loss: 0.2143 - acc: 0.944 - ETA: 2s - loss: 0.2126 - acc: 0.943 - ETA: 2s - loss: 0.2182 - acc: 0.944 - ETA: 2s - loss: 0.2312 - acc: 0.944 - ETA: 2s - loss: 0.2363 - acc: 0.944 - ETA: 2s - loss: 0.2319 - acc: 0.945 - ETA: 2s - loss: 0.2363 - acc: 0.945 - ETA: 2s - loss: 0.2346 - acc: 0.945 - ETA: 2s - loss: 0.2342 - acc: 0.944 - ETA: 2s - loss: 0.2324 - acc: 0.945 - ETA: 2s - loss: 0.2296 - acc: 0.945 - ETA: 2s - loss: 0.2275 - acc: 0.946 - ETA: 2s - loss: 0.2251 - acc: 0.946 - ETA: 2s - loss: 0.2235 - acc: 0.946 - ETA: 2s - loss: 0.2218 - acc: 0.947 - ETA: 1s - loss: 0.2270 - acc: 0.947 - ETA: 1s - loss: 0.2314 - acc: 0.946 - ETA: 1s - loss: 0.2305 - acc: 0.946 - ETA: 1s - loss: 0.2307 - acc: 0.944 - ETA: 1s - loss: 0.2285 - acc: 0.945 - ETA: 1s - loss: 0.2278 - acc: 0.944 - ETA: 1s - loss: 0.2301 - acc: 0.945 - ETA: 1s - loss: 0.2345 - acc: 0.944 - ETA: 1s - loss: 0.2368 - acc: 0.944 - ETA: 1s - loss: 0.2350 - acc: 0.944 - ETA: 1s - loss: 0.2330 - acc: 0.944 - ETA: 1s - loss: 0.2324 - acc: 0.943 - ETA: 1s - loss: 0.2304 - acc: 0.943 - ETA: 1s - loss: 0.2286 - acc: 0.943 - ETA: 0s - loss: 0.2271 - acc: 0.943 - ETA: 0s - loss: 0.2250 - acc: 0.943 - ETA: 0s - loss: 0.2241 - acc: 0.944 - ETA: 0s - loss: 0.2277 - acc: 0.943 - ETA: 0s - loss: 0.2307 - acc: 0.943 - ETA: 0s - loss: 0.2298 - acc: 0.943 - ETA: 0s - loss: 0.2277 - acc: 0.944 - ETA: 0s - loss: 0.2262 - acc: 0.944 - ETA: 0s - loss: 0.2288 - acc: 0.944 - ETA: 0s - loss: 0.2273 - acc: 0.944 - ETA: 0s - loss: 0.2261 - acc: 0.944 - ETA: 0s - loss: 0.2247 - acc: 0.944 - ETA: 0s - loss: 0.2232 - acc: 0.944 - ETA: 0s - loss: 0.2227 - acc: 0.943 - ETA: 0s - loss: 0.2214 - acc: 0.943 - ETA: 0s - loss: 0.2245 - acc: 0.943 - 6s 1ms/step - loss: 0.2246 - acc: 0.9432 - val_loss: 0.4678 - val_acc: 0.9192\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0512 - acc: 1.000 - ETA: 4s - loss: 0.1076 - acc: 0.921 - ETA: 4s - loss: 0.1165 - acc: 0.928 - ETA: 4s - loss: 0.1034 - acc: 0.937 - ETA: 4s - loss: 0.1013 - acc: 0.942 - ETA: 4s - loss: 0.1291 - acc: 0.925 - ETA: 4s - loss: 0.1349 - acc: 0.924 - ETA: 4s - loss: 0.1312 - acc: 0.926 - ETA: 4s - loss: 0.1703 - acc: 0.925 - ETA: 4s - loss: 0.1767 - acc: 0.912 - ETA: 4s - loss: 0.2025 - acc: 0.913 - ETA: 4s - loss: 0.1903 - acc: 0.917 - ETA: 4s - loss: 0.1863 - acc: 0.915 - ETA: 4s - loss: 0.2046 - acc: 0.917 - ETA: 4s - loss: 0.1980 - acc: 0.917 - ETA: 4s - loss: 0.1935 - acc: 0.918 - ETA: 4s - loss: 0.1909 - acc: 0.917 - ETA: 4s - loss: 0.1898 - acc: 0.915 - ETA: 4s - loss: 0.1799 - acc: 0.919 - ETA: 4s - loss: 0.1769 - acc: 0.920 - ETA: 4s - loss: 0.1757 - acc: 0.920 - ETA: 4s - loss: 0.1706 - acc: 0.923 - ETA: 4s - loss: 0.1672 - acc: 0.924 - ETA: 4s - loss: 0.1642 - acc: 0.925 - ETA: 4s - loss: 0.1697 - acc: 0.925 - ETA: 4s - loss: 0.1791 - acc: 0.924 - ETA: 4s - loss: 0.1904 - acc: 0.923 - ETA: 4s - loss: 0.1886 - acc: 0.923 - ETA: 3s - loss: 0.1847 - acc: 0.924 - ETA: 3s - loss: 0.1835 - acc: 0.923 - ETA: 3s - loss: 0.1805 - acc: 0.923 - ETA: 3s - loss: 0.1775 - acc: 0.924 - ETA: 3s - loss: 0.1755 - acc: 0.925 - ETA: 3s - loss: 0.1735 - acc: 0.925 - ETA: 3s - loss: 0.1712 - acc: 0.926 - ETA: 3s - loss: 0.1706 - acc: 0.926 - ETA: 3s - loss: 0.1715 - acc: 0.924 - ETA: 3s - loss: 0.1809 - acc: 0.923 - ETA: 3s - loss: 0.1794 - acc: 0.924 - ETA: 3s - loss: 0.1783 - acc: 0.923 - ETA: 3s - loss: 0.1967 - acc: 0.922 - ETA: 3s - loss: 0.1944 - acc: 0.923 - ETA: 3s - loss: 0.1915 - acc: 0.924 - ETA: 3s - loss: 0.1878 - acc: 0.926 - ETA: 3s - loss: 0.1861 - acc: 0.925 - ETA: 3s - loss: 0.1846 - acc: 0.925 - ETA: 3s - loss: 0.1814 - acc: 0.927 - ETA: 3s - loss: 0.1873 - acc: 0.928 - ETA: 3s - loss: 0.1873 - acc: 0.928 - ETA: 2s - loss: 0.1862 - acc: 0.928 - ETA: 2s - loss: 0.1845 - acc: 0.929 - ETA: 2s - loss: 0.1834 - acc: 0.929 - ETA: 2s - loss: 0.1893 - acc: 0.930 - ETA: 2s - loss: 0.1869 - acc: 0.931 - ETA: 2s - loss: 0.1923 - acc: 0.931 - ETA: 2s - loss: 0.1895 - acc: 0.933 - ETA: 2s - loss: 0.1887 - acc: 0.933 - ETA: 2s - loss: 0.1871 - acc: 0.933 - ETA: 2s - loss: 0.1843 - acc: 0.935 - ETA: 2s - loss: 0.1896 - acc: 0.934 - ETA: 2s - loss: 0.1961 - acc: 0.933 - ETA: 2s - loss: 0.2002 - acc: 0.933 - ETA: 2s - loss: 0.1981 - acc: 0.934 - ETA: 2s - loss: 0.2030 - acc: 0.934 - ETA: 1s - loss: 0.2011 - acc: 0.934 - ETA: 1s - loss: 0.2180 - acc: 0.932 - ETA: 1s - loss: 0.2226 - acc: 0.932 - ETA: 1s - loss: 0.2204 - acc: 0.933 - ETA: 1s - loss: 0.2180 - acc: 0.933 - ETA: 1s - loss: 0.2268 - acc: 0.934 - ETA: 1s - loss: 0.2256 - acc: 0.934 - ETA: 1s - loss: 0.2233 - acc: 0.934 - ETA: 1s - loss: 0.2291 - acc: 0.934 - ETA: 1s - loss: 0.2267 - acc: 0.935 - ETA: 1s - loss: 0.2244 - acc: 0.935 - ETA: 1s - loss: 0.2226 - acc: 0.936 - ETA: 1s - loss: 0.2199 - acc: 0.936 - ETA: 1s - loss: 0.2187 - acc: 0.937 - ETA: 1s - loss: 0.2160 - acc: 0.938 - ETA: 0s - loss: 0.2141 - acc: 0.938 - ETA: 0s - loss: 0.2129 - acc: 0.939 - ETA: 0s - loss: 0.2155 - acc: 0.939 - ETA: 0s - loss: 0.2142 - acc: 0.940 - ETA: 0s - loss: 0.2136 - acc: 0.940 - ETA: 0s - loss: 0.2158 - acc: 0.940 - ETA: 0s - loss: 0.2142 - acc: 0.941 - ETA: 0s - loss: 0.2134 - acc: 0.941 - ETA: 0s - loss: 0.2121 - acc: 0.941 - ETA: 0s - loss: 0.2108 - acc: 0.942 - ETA: 0s - loss: 0.2097 - acc: 0.942 - ETA: 0s - loss: 0.2076 - acc: 0.943 - ETA: 0s - loss: 0.2100 - acc: 0.943 - ETA: 0s - loss: 0.2085 - acc: 0.943 - ETA: 0s - loss: 0.2109 - acc: 0.944 - ETA: 0s - loss: 0.2131 - acc: 0.943 - 6s 1ms/step - loss: 0.2122 - acc: 0.9439 - val_loss: 0.4087 - val_acc: 0.9481\n",
      "Epoch 49/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 2.0369 - acc: 0.875 - ETA: 5s - loss: 0.5316 - acc: 0.968 - ETA: 5s - loss: 0.5051 - acc: 0.937 - ETA: 5s - loss: 0.3869 - acc: 0.950 - ETA: 5s - loss: 0.3185 - acc: 0.956 - ETA: 5s - loss: 0.2800 - acc: 0.953 - ETA: 4s - loss: 0.3093 - acc: 0.947 - ETA: 4s - loss: 0.2792 - acc: 0.951 - ETA: 4s - loss: 0.2545 - acc: 0.955 - ETA: 4s - loss: 0.2378 - acc: 0.955 - ETA: 4s - loss: 0.2259 - acc: 0.955 - ETA: 4s - loss: 0.2503 - acc: 0.954 - ETA: 4s - loss: 0.2421 - acc: 0.951 - ETA: 4s - loss: 0.2358 - acc: 0.950 - ETA: 4s - loss: 0.2267 - acc: 0.950 - ETA: 4s - loss: 0.2187 - acc: 0.951 - ETA: 4s - loss: 0.2325 - acc: 0.950 - ETA: 4s - loss: 0.2225 - acc: 0.951 - ETA: 4s - loss: 0.2137 - acc: 0.953 - ETA: 4s - loss: 0.2402 - acc: 0.953 - ETA: 4s - loss: 0.2327 - acc: 0.952 - ETA: 3s - loss: 0.2388 - acc: 0.954 - ETA: 3s - loss: 0.2288 - acc: 0.956 - ETA: 3s - loss: 0.2254 - acc: 0.956 - ETA: 3s - loss: 0.2480 - acc: 0.954 - ETA: 3s - loss: 0.2430 - acc: 0.956 - ETA: 3s - loss: 0.2479 - acc: 0.957 - ETA: 3s - loss: 0.2549 - acc: 0.957 - ETA: 3s - loss: 0.2487 - acc: 0.958 - ETA: 3s - loss: 0.2497 - acc: 0.957 - ETA: 3s - loss: 0.2437 - acc: 0.958 - ETA: 3s - loss: 0.2404 - acc: 0.958 - ETA: 3s - loss: 0.2381 - acc: 0.957 - ETA: 3s - loss: 0.2335 - acc: 0.958 - ETA: 3s - loss: 0.2410 - acc: 0.957 - ETA: 3s - loss: 0.2364 - acc: 0.958 - ETA: 3s - loss: 0.2407 - acc: 0.958 - ETA: 3s - loss: 0.2357 - acc: 0.959 - ETA: 3s - loss: 0.2310 - acc: 0.959 - ETA: 3s - loss: 0.2373 - acc: 0.958 - ETA: 3s - loss: 0.2348 - acc: 0.958 - ETA: 3s - loss: 0.2409 - acc: 0.956 - ETA: 2s - loss: 0.2462 - acc: 0.955 - ETA: 2s - loss: 0.2452 - acc: 0.954 - ETA: 2s - loss: 0.2411 - acc: 0.954 - ETA: 2s - loss: 0.2385 - acc: 0.954 - ETA: 2s - loss: 0.2362 - acc: 0.954 - ETA: 2s - loss: 0.2327 - acc: 0.954 - ETA: 2s - loss: 0.2286 - acc: 0.955 - ETA: 2s - loss: 0.2330 - acc: 0.955 - ETA: 2s - loss: 0.2298 - acc: 0.955 - ETA: 2s - loss: 0.2259 - acc: 0.956 - ETA: 2s - loss: 0.2241 - acc: 0.956 - ETA: 2s - loss: 0.2224 - acc: 0.955 - ETA: 2s - loss: 0.2193 - acc: 0.956 - ETA: 2s - loss: 0.2234 - acc: 0.955 - ETA: 2s - loss: 0.2208 - acc: 0.956 - ETA: 1s - loss: 0.2186 - acc: 0.955 - ETA: 1s - loss: 0.2165 - acc: 0.956 - ETA: 1s - loss: 0.2149 - acc: 0.955 - ETA: 1s - loss: 0.2123 - acc: 0.956 - ETA: 1s - loss: 0.2103 - acc: 0.956 - ETA: 1s - loss: 0.2081 - acc: 0.956 - ETA: 1s - loss: 0.2061 - acc: 0.956 - ETA: 1s - loss: 0.2042 - acc: 0.957 - ETA: 1s - loss: 0.2078 - acc: 0.957 - ETA: 1s - loss: 0.2057 - acc: 0.957 - ETA: 1s - loss: 0.2033 - acc: 0.957 - ETA: 1s - loss: 0.2017 - acc: 0.958 - ETA: 1s - loss: 0.2009 - acc: 0.958 - ETA: 1s - loss: 0.1997 - acc: 0.958 - ETA: 1s - loss: 0.2039 - acc: 0.957 - ETA: 1s - loss: 0.2030 - acc: 0.957 - ETA: 1s - loss: 0.2060 - acc: 0.957 - ETA: 1s - loss: 0.2046 - acc: 0.957 - ETA: 0s - loss: 0.2074 - acc: 0.957 - ETA: 0s - loss: 0.2055 - acc: 0.957 - ETA: 0s - loss: 0.2081 - acc: 0.957 - ETA: 0s - loss: 0.2110 - acc: 0.956 - ETA: 0s - loss: 0.2084 - acc: 0.957 - ETA: 0s - loss: 0.2076 - acc: 0.957 - ETA: 0s - loss: 0.2062 - acc: 0.957 - ETA: 0s - loss: 0.2088 - acc: 0.957 - ETA: 0s - loss: 0.2065 - acc: 0.958 - ETA: 0s - loss: 0.2100 - acc: 0.958 - ETA: 0s - loss: 0.2125 - acc: 0.957 - ETA: 0s - loss: 0.2102 - acc: 0.958 - ETA: 0s - loss: 0.2084 - acc: 0.958 - ETA: 0s - loss: 0.2067 - acc: 0.958 - ETA: 0s - loss: 0.2050 - acc: 0.959 - 6s 1ms/step - loss: 0.2048 - acc: 0.9587 - val_loss: 0.4626 - val_acc: 0.9301\n",
      "Epoch 50/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0180 - acc: 1.000 - ETA: 5s - loss: 0.1021 - acc: 0.953 - ETA: 5s - loss: 0.0863 - acc: 0.964 - ETA: 5s - loss: 0.0643 - acc: 0.975 - ETA: 5s - loss: 0.0552 - acc: 0.980 - ETA: 5s - loss: 0.0723 - acc: 0.964 - ETA: 5s - loss: 0.0748 - acc: 0.960 - ETA: 5s - loss: 0.0687 - acc: 0.964 - ETA: 5s - loss: 0.1184 - acc: 0.955 - ETA: 5s - loss: 0.1144 - acc: 0.958 - ETA: 5s - loss: 0.1072 - acc: 0.960 - ETA: 4s - loss: 0.1366 - acc: 0.956 - ETA: 4s - loss: 0.1276 - acc: 0.958 - ETA: 4s - loss: 0.1480 - acc: 0.956 - ETA: 4s - loss: 0.1457 - acc: 0.953 - ETA: 4s - loss: 0.1424 - acc: 0.955 - ETA: 4s - loss: 0.1379 - acc: 0.955 - ETA: 4s - loss: 0.1356 - acc: 0.955 - ETA: 4s - loss: 0.1330 - acc: 0.956 - ETA: 4s - loss: 0.1301 - acc: 0.957 - ETA: 4s - loss: 0.1271 - acc: 0.958 - ETA: 4s - loss: 0.1269 - acc: 0.956 - ETA: 4s - loss: 0.1245 - acc: 0.957 - ETA: 4s - loss: 0.1233 - acc: 0.957 - ETA: 4s - loss: 0.1231 - acc: 0.957 - ETA: 4s - loss: 0.1367 - acc: 0.956 - ETA: 4s - loss: 0.1327 - acc: 0.957 - ETA: 4s - loss: 0.1478 - acc: 0.957 - ETA: 4s - loss: 0.1440 - acc: 0.958 - ETA: 4s - loss: 0.1647 - acc: 0.956 - ETA: 4s - loss: 0.1620 - acc: 0.957 - ETA: 4s - loss: 0.1589 - acc: 0.957 - ETA: 4s - loss: 0.1560 - acc: 0.957 - ETA: 4s - loss: 0.1559 - acc: 0.956 - ETA: 4s - loss: 0.1654 - acc: 0.957 - ETA: 4s - loss: 0.1632 - acc: 0.956 - ETA: 4s - loss: 0.1605 - acc: 0.957 - ETA: 4s - loss: 0.1566 - acc: 0.959 - ETA: 4s - loss: 0.1534 - acc: 0.959 - ETA: 4s - loss: 0.1612 - acc: 0.959 - ETA: 3s - loss: 0.1797 - acc: 0.958 - ETA: 3s - loss: 0.1873 - acc: 0.958 - ETA: 3s - loss: 0.1846 - acc: 0.959 - ETA: 3s - loss: 0.1998 - acc: 0.959 - ETA: 3s - loss: 0.1982 - acc: 0.958 - ETA: 3s - loss: 0.1946 - acc: 0.959 - ETA: 3s - loss: 0.1927 - acc: 0.959 - ETA: 3s - loss: 0.1901 - acc: 0.960 - ETA: 3s - loss: 0.1878 - acc: 0.960 - ETA: 3s - loss: 0.1853 - acc: 0.961 - ETA: 3s - loss: 0.1824 - acc: 0.961 - ETA: 3s - loss: 0.1807 - acc: 0.961 - ETA: 3s - loss: 0.1875 - acc: 0.960 - ETA: 3s - loss: 0.1846 - acc: 0.960 - ETA: 3s - loss: 0.1898 - acc: 0.960 - ETA: 3s - loss: 0.1871 - acc: 0.961 - ETA: 2s - loss: 0.1848 - acc: 0.962 - ETA: 2s - loss: 0.1841 - acc: 0.961 - ETA: 2s - loss: 0.1810 - acc: 0.962 - ETA: 2s - loss: 0.1787 - acc: 0.962 - ETA: 2s - loss: 0.1831 - acc: 0.962 - ETA: 2s - loss: 0.1942 - acc: 0.961 - ETA: 2s - loss: 0.1932 - acc: 0.961 - ETA: 2s - loss: 0.1929 - acc: 0.961 - ETA: 2s - loss: 0.1924 - acc: 0.961 - ETA: 2s - loss: 0.1911 - acc: 0.961 - ETA: 2s - loss: 0.1967 - acc: 0.960 - ETA: 2s - loss: 0.1962 - acc: 0.960 - ETA: 2s - loss: 0.1947 - acc: 0.960 - ETA: 2s - loss: 0.1996 - acc: 0.958 - ETA: 2s - loss: 0.1978 - acc: 0.958 - ETA: 1s - loss: 0.1956 - acc: 0.959 - ETA: 1s - loss: 0.1936 - acc: 0.959 - ETA: 1s - loss: 0.1920 - acc: 0.959 - ETA: 1s - loss: 0.1909 - acc: 0.959 - ETA: 1s - loss: 0.1890 - acc: 0.960 - ETA: 1s - loss: 0.1866 - acc: 0.960 - ETA: 1s - loss: 0.1855 - acc: 0.960 - ETA: 1s - loss: 0.1832 - acc: 0.961 - ETA: 1s - loss: 0.1816 - acc: 0.961 - ETA: 1s - loss: 0.1900 - acc: 0.960 - ETA: 1s - loss: 0.1884 - acc: 0.960 - ETA: 1s - loss: 0.1920 - acc: 0.960 - ETA: 1s - loss: 0.1971 - acc: 0.959 - ETA: 1s - loss: 0.1958 - acc: 0.959 - ETA: 1s - loss: 0.1964 - acc: 0.959 - ETA: 1s - loss: 0.1952 - acc: 0.959 - ETA: 0s - loss: 0.1989 - acc: 0.958 - ETA: 0s - loss: 0.2076 - acc: 0.957 - ETA: 0s - loss: 0.2063 - acc: 0.957 - ETA: 0s - loss: 0.2051 - acc: 0.957 - ETA: 0s - loss: 0.2085 - acc: 0.957 - ETA: 0s - loss: 0.2085 - acc: 0.956 - ETA: 0s - loss: 0.2071 - acc: 0.957 - ETA: 0s - loss: 0.2061 - acc: 0.957 - ETA: 0s - loss: 0.2083 - acc: 0.957 - ETA: 0s - loss: 0.2059 - acc: 0.957 - ETA: 0s - loss: 0.2041 - acc: 0.957 - ETA: 0s - loss: 0.2066 - acc: 0.957 - ETA: 0s - loss: 0.2068 - acc: 0.957 - ETA: 0s - loss: 0.2092 - acc: 0.957 - ETA: 0s - loss: 0.2085 - acc: 0.957 - 7s 2ms/step - loss: 0.2081 - acc: 0.9575 - val_loss: 0.4652 - val_acc: 0.9250\n",
      "Epoch 51/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1263 - acc: 0.937 - ETA: 5s - loss: 0.3427 - acc: 0.937 - ETA: 5s - loss: 0.2627 - acc: 0.928 - ETA: 5s - loss: 0.2074 - acc: 0.937 - ETA: 5s - loss: 0.1954 - acc: 0.932 - ETA: 5s - loss: 0.1724 - acc: 0.937 - ETA: 5s - loss: 0.1661 - acc: 0.937 - ETA: 5s - loss: 0.2236 - acc: 0.934 - ETA: 5s - loss: 0.2166 - acc: 0.934 - ETA: 5s - loss: 0.2074 - acc: 0.937 - ETA: 5s - loss: 0.1879 - acc: 0.944 - ETA: 4s - loss: 0.1705 - acc: 0.949 - ETA: 4s - loss: 0.1649 - acc: 0.950 - ETA: 4s - loss: 0.1603 - acc: 0.951 - ETA: 4s - loss: 0.1564 - acc: 0.951 - ETA: 4s - loss: 0.1916 - acc: 0.949 - ETA: 4s - loss: 0.1875 - acc: 0.950 - ETA: 4s - loss: 0.2066 - acc: 0.950 - ETA: 4s - loss: 0.2230 - acc: 0.949 - ETA: 4s - loss: 0.2501 - acc: 0.947 - ETA: 4s - loss: 0.2384 - acc: 0.949 - ETA: 4s - loss: 0.2465 - acc: 0.951 - ETA: 4s - loss: 0.2391 - acc: 0.952 - ETA: 4s - loss: 0.2531 - acc: 0.951 - ETA: 4s - loss: 0.2626 - acc: 0.951 - ETA: 4s - loss: 0.2551 - acc: 0.953 - ETA: 4s - loss: 0.2459 - acc: 0.954 - ETA: 4s - loss: 0.2427 - acc: 0.952 - ETA: 4s - loss: 0.2373 - acc: 0.951 - ETA: 4s - loss: 0.2373 - acc: 0.948 - ETA: 4s - loss: 0.2378 - acc: 0.947 - ETA: 4s - loss: 0.2344 - acc: 0.946 - ETA: 3s - loss: 0.2317 - acc: 0.945 - ETA: 3s - loss: 0.2389 - acc: 0.944 - ETA: 3s - loss: 0.2389 - acc: 0.944 - ETA: 3s - loss: 0.2488 - acc: 0.943 - ETA: 3s - loss: 0.2487 - acc: 0.940 - ETA: 3s - loss: 0.2484 - acc: 0.939 - ETA: 3s - loss: 0.2555 - acc: 0.939 - ETA: 3s - loss: 0.2609 - acc: 0.939 - ETA: 3s - loss: 0.2666 - acc: 0.938 - ETA: 3s - loss: 0.2624 - acc: 0.939 - ETA: 3s - loss: 0.2582 - acc: 0.941 - ETA: 3s - loss: 0.2568 - acc: 0.940 - ETA: 3s - loss: 0.2633 - acc: 0.940 - ETA: 3s - loss: 0.2668 - acc: 0.941 - ETA: 3s - loss: 0.2694 - acc: 0.942 - ETA: 3s - loss: 0.2741 - acc: 0.942 - ETA: 3s - loss: 0.2722 - acc: 0.942 - ETA: 2s - loss: 0.2767 - acc: 0.942 - ETA: 2s - loss: 0.2726 - acc: 0.942 - ETA: 2s - loss: 0.2745 - acc: 0.938 - ETA: 2s - loss: 0.2785 - acc: 0.933 - ETA: 2s - loss: 0.2776 - acc: 0.931 - ETA: 2s - loss: 0.2797 - acc: 0.928 - ETA: 2s - loss: 0.2800 - acc: 0.926 - ETA: 2s - loss: 0.2782 - acc: 0.925 - ETA: 2s - loss: 0.2764 - acc: 0.924 - ETA: 2s - loss: 0.2749 - acc: 0.923 - ETA: 2s - loss: 0.2781 - acc: 0.923 - ETA: 2s - loss: 0.2774 - acc: 0.922 - ETA: 2s - loss: 0.2761 - acc: 0.921 - ETA: 2s - loss: 0.2739 - acc: 0.922 - ETA: 2s - loss: 0.2705 - acc: 0.923 - ETA: 2s - loss: 0.2686 - acc: 0.923 - ETA: 1s - loss: 0.2679 - acc: 0.924 - ETA: 1s - loss: 0.2665 - acc: 0.924 - ETA: 1s - loss: 0.2648 - acc: 0.924 - ETA: 1s - loss: 0.2638 - acc: 0.923 - ETA: 1s - loss: 0.2621 - acc: 0.923 - ETA: 1s - loss: 0.2594 - acc: 0.924 - ETA: 1s - loss: 0.2585 - acc: 0.924 - ETA: 1s - loss: 0.2587 - acc: 0.924 - ETA: 1s - loss: 0.2575 - acc: 0.924 - ETA: 1s - loss: 0.2557 - acc: 0.925 - ETA: 1s - loss: 0.2543 - acc: 0.925 - ETA: 1s - loss: 0.2528 - acc: 0.924 - ETA: 1s - loss: 0.2523 - acc: 0.924 - ETA: 1s - loss: 0.2506 - acc: 0.924 - ETA: 1s - loss: 0.2483 - acc: 0.925 - ETA: 1s - loss: 0.2544 - acc: 0.925 - ETA: 1s - loss: 0.2516 - acc: 0.926 - ETA: 1s - loss: 0.2506 - acc: 0.926 - ETA: 0s - loss: 0.2484 - acc: 0.926 - ETA: 0s - loss: 0.2470 - acc: 0.926 - ETA: 0s - loss: 0.2446 - acc: 0.927 - ETA: 0s - loss: 0.2425 - acc: 0.927 - ETA: 0s - loss: 0.2414 - acc: 0.927 - ETA: 0s - loss: 0.2391 - acc: 0.928 - ETA: 0s - loss: 0.2390 - acc: 0.928 - ETA: 0s - loss: 0.2366 - acc: 0.928 - ETA: 0s - loss: 0.2359 - acc: 0.928 - ETA: 0s - loss: 0.2354 - acc: 0.928 - ETA: 0s - loss: 0.2344 - acc: 0.928 - ETA: 0s - loss: 0.2327 - acc: 0.929 - ETA: 0s - loss: 0.2311 - acc: 0.929 - ETA: 0s - loss: 0.2294 - acc: 0.930 - ETA: 0s - loss: 0.2279 - acc: 0.930 - ETA: 0s - loss: 0.2266 - acc: 0.930 - 6s 2ms/step - loss: 0.2268 - acc: 0.9304 - val_loss: 0.3563 - val_acc: 0.9224\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1252 - acc: 0.937 - ETA: 6s - loss: 0.0887 - acc: 0.958 - ETA: 5s - loss: 0.1137 - acc: 0.937 - ETA: 5s - loss: 0.0919 - acc: 0.944 - ETA: 5s - loss: 0.0816 - acc: 0.954 - ETA: 5s - loss: 0.0806 - acc: 0.955 - ETA: 5s - loss: 0.0911 - acc: 0.948 - ETA: 5s - loss: 0.0894 - acc: 0.950 - ETA: 5s - loss: 0.1254 - acc: 0.948 - ETA: 5s - loss: 0.1242 - acc: 0.947 - ETA: 5s - loss: 0.1226 - acc: 0.948 - ETA: 5s - loss: 0.1213 - acc: 0.949 - ETA: 4s - loss: 0.1280 - acc: 0.946 - ETA: 4s - loss: 0.1283 - acc: 0.944 - ETA: 4s - loss: 0.1273 - acc: 0.945 - ETA: 4s - loss: 0.1249 - acc: 0.947 - ETA: 4s - loss: 0.1272 - acc: 0.948 - ETA: 4s - loss: 0.1226 - acc: 0.950 - ETA: 4s - loss: 0.1229 - acc: 0.950 - ETA: 4s - loss: 0.1206 - acc: 0.951 - ETA: 4s - loss: 0.1299 - acc: 0.950 - ETA: 4s - loss: 0.1265 - acc: 0.952 - ETA: 4s - loss: 0.1242 - acc: 0.952 - ETA: 4s - loss: 0.1307 - acc: 0.952 - ETA: 4s - loss: 0.1280 - acc: 0.954 - ETA: 4s - loss: 0.1277 - acc: 0.953 - ETA: 4s - loss: 0.1272 - acc: 0.951 - ETA: 4s - loss: 0.1409 - acc: 0.949 - ETA: 4s - loss: 0.1393 - acc: 0.950 - ETA: 4s - loss: 0.1384 - acc: 0.949 - ETA: 4s - loss: 0.1356 - acc: 0.950 - ETA: 4s - loss: 0.1330 - acc: 0.951 - ETA: 3s - loss: 0.1343 - acc: 0.950 - ETA: 3s - loss: 0.1325 - acc: 0.951 - ETA: 3s - loss: 0.1319 - acc: 0.951 - ETA: 3s - loss: 0.1312 - acc: 0.951 - ETA: 3s - loss: 0.1322 - acc: 0.949 - ETA: 3s - loss: 0.1297 - acc: 0.951 - ETA: 3s - loss: 0.1297 - acc: 0.950 - ETA: 3s - loss: 0.1309 - acc: 0.950 - ETA: 3s - loss: 0.1379 - acc: 0.950 - ETA: 3s - loss: 0.1372 - acc: 0.950 - ETA: 3s - loss: 0.1361 - acc: 0.950 - ETA: 3s - loss: 0.1369 - acc: 0.949 - ETA: 3s - loss: 0.1359 - acc: 0.949 - ETA: 3s - loss: 0.1350 - acc: 0.949 - ETA: 3s - loss: 0.1355 - acc: 0.949 - ETA: 2s - loss: 0.1351 - acc: 0.949 - ETA: 2s - loss: 0.1341 - acc: 0.949 - ETA: 2s - loss: 0.1329 - acc: 0.950 - ETA: 2s - loss: 0.1315 - acc: 0.950 - ETA: 2s - loss: 0.1316 - acc: 0.950 - ETA: 2s - loss: 0.1305 - acc: 0.950 - ETA: 2s - loss: 0.1295 - acc: 0.951 - ETA: 2s - loss: 0.1287 - acc: 0.951 - ETA: 2s - loss: 0.1280 - acc: 0.951 - ETA: 2s - loss: 0.1279 - acc: 0.950 - ETA: 2s - loss: 0.1257 - acc: 0.951 - ETA: 2s - loss: 0.1247 - acc: 0.952 - ETA: 2s - loss: 0.1243 - acc: 0.952 - ETA: 2s - loss: 0.1233 - acc: 0.952 - ETA: 2s - loss: 0.1265 - acc: 0.952 - ETA: 2s - loss: 0.1257 - acc: 0.952 - ETA: 2s - loss: 0.1254 - acc: 0.952 - ETA: 2s - loss: 0.1241 - acc: 0.953 - ETA: 2s - loss: 0.1228 - acc: 0.953 - ETA: 1s - loss: 0.1282 - acc: 0.953 - ETA: 1s - loss: 0.1281 - acc: 0.953 - ETA: 1s - loss: 0.1290 - acc: 0.952 - ETA: 1s - loss: 0.1292 - acc: 0.952 - ETA: 1s - loss: 0.1285 - acc: 0.952 - ETA: 1s - loss: 0.1280 - acc: 0.952 - ETA: 1s - loss: 0.1272 - acc: 0.952 - ETA: 1s - loss: 0.1265 - acc: 0.952 - ETA: 1s - loss: 0.1258 - acc: 0.952 - ETA: 1s - loss: 0.1248 - acc: 0.953 - ETA: 1s - loss: 0.1245 - acc: 0.952 - ETA: 1s - loss: 0.1306 - acc: 0.952 - ETA: 1s - loss: 0.1297 - acc: 0.952 - ETA: 1s - loss: 0.1286 - acc: 0.953 - ETA: 0s - loss: 0.1286 - acc: 0.952 - ETA: 0s - loss: 0.1279 - acc: 0.952 - ETA: 0s - loss: 0.1272 - acc: 0.952 - ETA: 0s - loss: 0.1299 - acc: 0.952 - ETA: 0s - loss: 0.1289 - acc: 0.952 - ETA: 0s - loss: 0.1430 - acc: 0.951 - ETA: 0s - loss: 0.1648 - acc: 0.949 - ETA: 0s - loss: 0.2227 - acc: 0.945 - ETA: 0s - loss: 0.2723 - acc: 0.943 - ETA: 0s - loss: 0.3394 - acc: 0.938 - ETA: 0s - loss: 0.3954 - acc: 0.935 - ETA: 0s - loss: 0.4584 - acc: 0.931 - ETA: 0s - loss: 0.5191 - acc: 0.928 - ETA: 0s - loss: 0.5680 - acc: 0.924 - ETA: 0s - loss: 0.6150 - acc: 0.922 - 6s 2ms/step - loss: 0.6502 - acc: 0.9203 - val_loss: 4.9240 - val_acc: 0.6340\n",
      "Epoch 53/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 4.1443 - acc: 0.687 - ETA: 5s - loss: 6.4214 - acc: 0.578 - ETA: 5s - loss: 5.6626 - acc: 0.614 - ETA: 5s - loss: 5.2750 - acc: 0.645 - ETA: 5s - loss: 4.8270 - acc: 0.677 - ETA: 5s - loss: 5.0271 - acc: 0.666 - ETA: 5s - loss: 5.1578 - acc: 0.658 - ETA: 5s - loss: 5.0086 - acc: 0.665 - ETA: 5s - loss: 4.8545 - acc: 0.668 - ETA: 5s - loss: 4.7326 - acc: 0.673 - ETA: 4s - loss: 4.7735 - acc: 0.665 - ETA: 4s - loss: 4.8046 - acc: 0.666 - ETA: 4s - loss: 4.8147 - acc: 0.664 - ETA: 4s - loss: 4.8896 - acc: 0.661 - ETA: 4s - loss: 4.8576 - acc: 0.663 - ETA: 4s - loss: 4.8739 - acc: 0.664 - ETA: 4s - loss: 4.9559 - acc: 0.659 - ETA: 4s - loss: 5.0053 - acc: 0.657 - ETA: 4s - loss: 4.9894 - acc: 0.660 - ETA: 4s - loss: 5.0132 - acc: 0.659 - ETA: 4s - loss: 5.0336 - acc: 0.658 - ETA: 4s - loss: 5.1010 - acc: 0.655 - ETA: 4s - loss: 5.1011 - acc: 0.655 - ETA: 4s - loss: 5.1478 - acc: 0.652 - ETA: 3s - loss: 5.1507 - acc: 0.648 - ETA: 3s - loss: 5.1629 - acc: 0.649 - ETA: 3s - loss: 5.2018 - acc: 0.645 - ETA: 3s - loss: 5.1737 - acc: 0.647 - ETA: 3s - loss: 5.1350 - acc: 0.649 - ETA: 3s - loss: 5.1243 - acc: 0.649 - ETA: 3s - loss: 5.1452 - acc: 0.649 - ETA: 3s - loss: 5.1770 - acc: 0.647 - ETA: 3s - loss: 5.2397 - acc: 0.644 - ETA: 3s - loss: 5.1941 - acc: 0.647 - ETA: 3s - loss: 5.2231 - acc: 0.645 - ETA: 3s - loss: 5.2446 - acc: 0.643 - ETA: 3s - loss: 5.2542 - acc: 0.642 - ETA: 3s - loss: 5.2038 - acc: 0.646 - ETA: 3s - loss: 5.1358 - acc: 0.650 - ETA: 3s - loss: 5.1342 - acc: 0.651 - ETA: 3s - loss: 5.0970 - acc: 0.654 - ETA: 3s - loss: 5.0644 - acc: 0.656 - ETA: 3s - loss: 5.0396 - acc: 0.657 - ETA: 3s - loss: 4.9956 - acc: 0.659 - ETA: 3s - loss: 4.8791 - acc: 0.666 - ETA: 3s - loss: 4.8075 - acc: 0.670 - ETA: 3s - loss: 4.7512 - acc: 0.674 - ETA: 3s - loss: 4.6745 - acc: 0.679 - ETA: 3s - loss: 4.6023 - acc: 0.684 - ETA: 3s - loss: 4.5441 - acc: 0.687 - ETA: 3s - loss: 4.4899 - acc: 0.690 - ETA: 2s - loss: 4.4229 - acc: 0.694 - ETA: 2s - loss: 4.3585 - acc: 0.698 - ETA: 2s - loss: 4.2652 - acc: 0.703 - ETA: 2s - loss: 4.2090 - acc: 0.706 - ETA: 2s - loss: 4.1572 - acc: 0.710 - ETA: 2s - loss: 4.1150 - acc: 0.712 - ETA: 2s - loss: 4.0657 - acc: 0.713 - ETA: 2s - loss: 4.0196 - acc: 0.715 - ETA: 2s - loss: 3.9751 - acc: 0.716 - ETA: 2s - loss: 3.9007 - acc: 0.720 - ETA: 2s - loss: 3.8300 - acc: 0.721 - ETA: 2s - loss: 3.7692 - acc: 0.722 - ETA: 2s - loss: 3.7051 - acc: 0.722 - ETA: 2s - loss: 3.6430 - acc: 0.723 - ETA: 2s - loss: 3.5881 - acc: 0.724 - ETA: 2s - loss: 3.5427 - acc: 0.723 - ETA: 1s - loss: 3.4862 - acc: 0.723 - ETA: 1s - loss: 3.4305 - acc: 0.724 - ETA: 1s - loss: 3.3762 - acc: 0.726 - ETA: 1s - loss: 3.3304 - acc: 0.727 - ETA: 1s - loss: 3.2789 - acc: 0.729 - ETA: 1s - loss: 3.2294 - acc: 0.731 - ETA: 1s - loss: 3.1836 - acc: 0.734 - ETA: 1s - loss: 3.1522 - acc: 0.736 - ETA: 1s - loss: 3.1066 - acc: 0.739 - ETA: 1s - loss: 3.0871 - acc: 0.739 - ETA: 1s - loss: 3.0469 - acc: 0.742 - ETA: 1s - loss: 3.0080 - acc: 0.745 - ETA: 1s - loss: 2.9797 - acc: 0.747 - ETA: 1s - loss: 2.9378 - acc: 0.750 - ETA: 0s - loss: 2.8993 - acc: 0.753 - ETA: 0s - loss: 2.8601 - acc: 0.756 - ETA: 0s - loss: 2.8246 - acc: 0.758 - ETA: 0s - loss: 2.7887 - acc: 0.760 - ETA: 0s - loss: 2.7540 - acc: 0.761 - ETA: 0s - loss: 2.7197 - acc: 0.762 - ETA: 0s - loss: 2.6851 - acc: 0.765 - ETA: 0s - loss: 2.6574 - acc: 0.766 - ETA: 0s - loss: 2.6300 - acc: 0.766 - ETA: 0s - loss: 2.5990 - acc: 0.768 - ETA: 0s - loss: 2.5690 - acc: 0.768 - ETA: 0s - loss: 2.5432 - acc: 0.770 - ETA: 0s - loss: 2.5144 - acc: 0.771 - ETA: 0s - loss: 2.4857 - acc: 0.774 - ETA: 0s - loss: 2.4619 - acc: 0.775 - 6s 2ms/step - loss: 2.4602 - acc: 0.7758 - val_loss: 0.5891 - val_acc: 0.8712\n",
      "Epoch 54/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.2350 - acc: 0.875 - ETA: 5s - loss: 0.1410 - acc: 0.906 - ETA: 5s - loss: 0.3194 - acc: 0.866 - ETA: 4s - loss: 0.3888 - acc: 0.856 - ETA: 4s - loss: 0.5017 - acc: 0.855 - ETA: 4s - loss: 0.4497 - acc: 0.839 - ETA: 4s - loss: 0.4073 - acc: 0.838 - ETA: 4s - loss: 0.3731 - acc: 0.838 - ETA: 4s - loss: 0.3538 - acc: 0.832 - ETA: 4s - loss: 0.3852 - acc: 0.819 - ETA: 4s - loss: 0.3677 - acc: 0.822 - ETA: 4s - loss: 0.3844 - acc: 0.821 - ETA: 4s - loss: 0.3950 - acc: 0.820 - ETA: 4s - loss: 0.4047 - acc: 0.825 - ETA: 4s - loss: 0.3851 - acc: 0.829 - ETA: 4s - loss: 0.3686 - acc: 0.834 - ETA: 4s - loss: 0.3833 - acc: 0.834 - ETA: 4s - loss: 0.3887 - acc: 0.836 - ETA: 4s - loss: 0.3942 - acc: 0.835 - ETA: 3s - loss: 0.3968 - acc: 0.838 - ETA: 3s - loss: 0.3879 - acc: 0.838 - ETA: 3s - loss: 0.3786 - acc: 0.841 - ETA: 3s - loss: 0.3719 - acc: 0.845 - ETA: 3s - loss: 0.3753 - acc: 0.842 - ETA: 3s - loss: 0.3728 - acc: 0.839 - ETA: 3s - loss: 0.3816 - acc: 0.837 - ETA: 3s - loss: 0.3870 - acc: 0.839 - ETA: 3s - loss: 0.3807 - acc: 0.841 - ETA: 3s - loss: 0.3736 - acc: 0.841 - ETA: 3s - loss: 0.3692 - acc: 0.841 - ETA: 3s - loss: 0.3649 - acc: 0.838 - ETA: 3s - loss: 0.3704 - acc: 0.840 - ETA: 3s - loss: 0.3667 - acc: 0.838 - ETA: 3s - loss: 0.3626 - acc: 0.838 - ETA: 3s - loss: 0.3778 - acc: 0.837 - ETA: 3s - loss: 0.3731 - acc: 0.836 - ETA: 3s - loss: 0.3712 - acc: 0.837 - ETA: 3s - loss: 0.3721 - acc: 0.838 - ETA: 3s - loss: 0.3786 - acc: 0.838 - ETA: 3s - loss: 0.3854 - acc: 0.837 - ETA: 3s - loss: 0.3809 - acc: 0.838 - ETA: 3s - loss: 0.3782 - acc: 0.839 - ETA: 2s - loss: 0.3816 - acc: 0.840 - ETA: 2s - loss: 0.3776 - acc: 0.841 - ETA: 2s - loss: 0.3756 - acc: 0.841 - ETA: 2s - loss: 0.3735 - acc: 0.841 - ETA: 2s - loss: 0.3714 - acc: 0.842 - ETA: 2s - loss: 0.3695 - acc: 0.842 - ETA: 2s - loss: 0.3815 - acc: 0.842 - ETA: 2s - loss: 0.3858 - acc: 0.843 - ETA: 2s - loss: 0.3826 - acc: 0.846 - ETA: 2s - loss: 0.3807 - acc: 0.844 - ETA: 2s - loss: 0.3754 - acc: 0.845 - ETA: 2s - loss: 0.3735 - acc: 0.845 - ETA: 2s - loss: 0.3680 - acc: 0.846 - ETA: 2s - loss: 0.3717 - acc: 0.845 - ETA: 2s - loss: 0.3664 - acc: 0.847 - ETA: 2s - loss: 0.3770 - acc: 0.846 - ETA: 2s - loss: 0.3723 - acc: 0.847 - ETA: 2s - loss: 0.3753 - acc: 0.847 - ETA: 2s - loss: 0.3727 - acc: 0.848 - ETA: 1s - loss: 0.3747 - acc: 0.849 - ETA: 1s - loss: 0.3768 - acc: 0.849 - ETA: 1s - loss: 0.3735 - acc: 0.850 - ETA: 1s - loss: 0.3691 - acc: 0.851 - ETA: 1s - loss: 0.3674 - acc: 0.851 - ETA: 1s - loss: 0.3637 - acc: 0.852 - ETA: 1s - loss: 0.3603 - acc: 0.855 - ETA: 1s - loss: 0.3576 - acc: 0.855 - ETA: 1s - loss: 0.3545 - acc: 0.856 - ETA: 1s - loss: 0.3508 - acc: 0.857 - ETA: 1s - loss: 0.3478 - acc: 0.858 - ETA: 1s - loss: 0.3551 - acc: 0.858 - ETA: 1s - loss: 0.3515 - acc: 0.858 - ETA: 1s - loss: 0.3523 - acc: 0.860 - ETA: 1s - loss: 0.3533 - acc: 0.861 - ETA: 0s - loss: 0.3502 - acc: 0.863 - ETA: 0s - loss: 0.3480 - acc: 0.864 - ETA: 0s - loss: 0.3467 - acc: 0.865 - ETA: 0s - loss: 0.3442 - acc: 0.866 - ETA: 0s - loss: 0.3469 - acc: 0.867 - ETA: 0s - loss: 0.3445 - acc: 0.868 - ETA: 0s - loss: 0.3465 - acc: 0.869 - ETA: 0s - loss: 0.3484 - acc: 0.869 - ETA: 0s - loss: 0.3454 - acc: 0.870 - ETA: 0s - loss: 0.3422 - acc: 0.871 - ETA: 0s - loss: 0.3402 - acc: 0.872 - ETA: 0s - loss: 0.3375 - acc: 0.873 - ETA: 0s - loss: 0.3353 - acc: 0.873 - ETA: 0s - loss: 0.3338 - acc: 0.873 - ETA: 0s - loss: 0.3308 - acc: 0.874 - 6s 1ms/step - loss: 0.3285 - acc: 0.8753 - val_loss: 0.5510 - val_acc: 0.8885\n",
      "Epoch 55/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1024 - acc: 0.875 - ETA: 5s - loss: 0.1976 - acc: 0.906 - ETA: 6s - loss: 0.3408 - acc: 0.906 - ETA: 5s - loss: 0.2633 - acc: 0.923 - ETA: 5s - loss: 0.2374 - acc: 0.916 - ETA: 5s - loss: 0.2790 - acc: 0.920 - ETA: 5s - loss: 0.2971 - acc: 0.927 - ETA: 5s - loss: 0.2790 - acc: 0.928 - ETA: 5s - loss: 0.2628 - acc: 0.927 - ETA: 5s - loss: 0.2867 - acc: 0.925 - ETA: 5s - loss: 0.2776 - acc: 0.927 - ETA: 5s - loss: 0.2922 - acc: 0.928 - ETA: 4s - loss: 0.2796 - acc: 0.928 - ETA: 4s - loss: 0.2719 - acc: 0.926 - ETA: 4s - loss: 0.2718 - acc: 0.922 - ETA: 4s - loss: 0.2817 - acc: 0.926 - ETA: 4s - loss: 0.2696 - acc: 0.927 - ETA: 4s - loss: 0.2603 - acc: 0.928 - ETA: 4s - loss: 0.2521 - acc: 0.929 - ETA: 4s - loss: 0.2410 - acc: 0.932 - ETA: 4s - loss: 0.2361 - acc: 0.932 - ETA: 4s - loss: 0.2462 - acc: 0.931 - ETA: 4s - loss: 0.2565 - acc: 0.930 - ETA: 4s - loss: 0.2525 - acc: 0.930 - ETA: 4s - loss: 0.2518 - acc: 0.928 - ETA: 4s - loss: 0.2454 - acc: 0.929 - ETA: 4s - loss: 0.2545 - acc: 0.928 - ETA: 4s - loss: 0.2498 - acc: 0.930 - ETA: 4s - loss: 0.2473 - acc: 0.931 - ETA: 3s - loss: 0.2443 - acc: 0.929 - ETA: 3s - loss: 0.2746 - acc: 0.927 - ETA: 3s - loss: 0.2715 - acc: 0.926 - ETA: 3s - loss: 0.2890 - acc: 0.925 - ETA: 3s - loss: 0.2858 - acc: 0.926 - ETA: 3s - loss: 0.2802 - acc: 0.926 - ETA: 3s - loss: 0.2842 - acc: 0.927 - ETA: 3s - loss: 0.2894 - acc: 0.927 - ETA: 3s - loss: 0.2931 - acc: 0.927 - ETA: 3s - loss: 0.2895 - acc: 0.928 - ETA: 3s - loss: 0.2842 - acc: 0.928 - ETA: 3s - loss: 0.2802 - acc: 0.928 - ETA: 3s - loss: 0.2743 - acc: 0.930 - ETA: 3s - loss: 0.2719 - acc: 0.928 - ETA: 3s - loss: 0.2682 - acc: 0.929 - ETA: 2s - loss: 0.2668 - acc: 0.929 - ETA: 2s - loss: 0.2628 - acc: 0.930 - ETA: 2s - loss: 0.2606 - acc: 0.930 - ETA: 2s - loss: 0.2569 - acc: 0.930 - ETA: 2s - loss: 0.2528 - acc: 0.932 - ETA: 2s - loss: 0.2658 - acc: 0.930 - ETA: 2s - loss: 0.2613 - acc: 0.931 - ETA: 2s - loss: 0.2590 - acc: 0.931 - ETA: 2s - loss: 0.2640 - acc: 0.930 - ETA: 2s - loss: 0.2612 - acc: 0.930 - ETA: 2s - loss: 0.2575 - acc: 0.931 - ETA: 2s - loss: 0.2594 - acc: 0.932 - ETA: 2s - loss: 0.2567 - acc: 0.933 - ETA: 2s - loss: 0.2556 - acc: 0.933 - ETA: 2s - loss: 0.2526 - acc: 0.933 - ETA: 1s - loss: 0.2558 - acc: 0.933 - ETA: 1s - loss: 0.2528 - acc: 0.934 - ETA: 1s - loss: 0.2525 - acc: 0.935 - ETA: 1s - loss: 0.2493 - acc: 0.935 - ETA: 1s - loss: 0.2473 - acc: 0.936 - ETA: 1s - loss: 0.2438 - acc: 0.936 - ETA: 1s - loss: 0.2414 - acc: 0.937 - ETA: 1s - loss: 0.2400 - acc: 0.937 - ETA: 1s - loss: 0.2392 - acc: 0.937 - ETA: 1s - loss: 0.2435 - acc: 0.937 - ETA: 1s - loss: 0.2527 - acc: 0.937 - ETA: 1s - loss: 0.2506 - acc: 0.937 - ETA: 1s - loss: 0.2534 - acc: 0.937 - ETA: 1s - loss: 0.2515 - acc: 0.937 - ETA: 1s - loss: 0.2540 - acc: 0.937 - ETA: 1s - loss: 0.2619 - acc: 0.937 - ETA: 0s - loss: 0.2644 - acc: 0.936 - ETA: 0s - loss: 0.2720 - acc: 0.935 - ETA: 0s - loss: 0.2693 - acc: 0.936 - ETA: 0s - loss: 0.2772 - acc: 0.935 - ETA: 0s - loss: 0.2762 - acc: 0.935 - ETA: 0s - loss: 0.2764 - acc: 0.934 - ETA: 0s - loss: 0.2757 - acc: 0.934 - ETA: 0s - loss: 0.2746 - acc: 0.934 - ETA: 0s - loss: 0.2737 - acc: 0.934 - ETA: 0s - loss: 0.2724 - acc: 0.933 - ETA: 0s - loss: 0.2699 - acc: 0.933 - ETA: 0s - loss: 0.2679 - acc: 0.934 - ETA: 0s - loss: 0.2659 - acc: 0.934 - ETA: 0s - loss: 0.2674 - acc: 0.934 - ETA: 0s - loss: 0.2689 - acc: 0.934 - ETA: 0s - loss: 0.2680 - acc: 0.934 - ETA: 0s - loss: 0.2659 - acc: 0.934 - 6s 2ms/step - loss: 0.2646 - acc: 0.9343 - val_loss: 0.6338 - val_acc: 0.9077\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0285 - acc: 1.000 - ETA: 5s - loss: 0.1193 - acc: 0.937 - ETA: 5s - loss: 0.0833 - acc: 0.955 - ETA: 5s - loss: 0.1193 - acc: 0.956 - ETA: 5s - loss: 0.1075 - acc: 0.961 - ETA: 5s - loss: 0.1601 - acc: 0.960 - ETA: 5s - loss: 0.1591 - acc: 0.950 - ETA: 5s - loss: 0.1489 - acc: 0.954 - ETA: 5s - loss: 0.1857 - acc: 0.950 - ETA: 5s - loss: 0.1757 - acc: 0.950 - ETA: 4s - loss: 0.1729 - acc: 0.951 - ETA: 4s - loss: 0.2093 - acc: 0.943 - ETA: 4s - loss: 0.2289 - acc: 0.942 - ETA: 4s - loss: 0.2285 - acc: 0.939 - ETA: 4s - loss: 0.2437 - acc: 0.937 - ETA: 4s - loss: 0.2384 - acc: 0.938 - ETA: 4s - loss: 0.2321 - acc: 0.937 - ETA: 4s - loss: 0.2450 - acc: 0.937 - ETA: 4s - loss: 0.2355 - acc: 0.939 - ETA: 4s - loss: 0.2484 - acc: 0.938 - ETA: 4s - loss: 0.2503 - acc: 0.936 - ETA: 4s - loss: 0.2448 - acc: 0.936 - ETA: 4s - loss: 0.2407 - acc: 0.936 - ETA: 4s - loss: 0.2687 - acc: 0.933 - ETA: 4s - loss: 0.2648 - acc: 0.933 - ETA: 4s - loss: 0.2592 - acc: 0.935 - ETA: 4s - loss: 0.2857 - acc: 0.932 - ETA: 4s - loss: 0.2933 - acc: 0.932 - ETA: 4s - loss: 0.3022 - acc: 0.932 - ETA: 4s - loss: 0.3106 - acc: 0.931 - ETA: 4s - loss: 0.3027 - acc: 0.931 - ETA: 4s - loss: 0.2975 - acc: 0.931 - ETA: 4s - loss: 0.3046 - acc: 0.932 - ETA: 4s - loss: 0.3111 - acc: 0.932 - ETA: 3s - loss: 0.3057 - acc: 0.933 - ETA: 4s - loss: 0.3019 - acc: 0.933 - ETA: 3s - loss: 0.2983 - acc: 0.932 - ETA: 3s - loss: 0.3037 - acc: 0.932 - ETA: 3s - loss: 0.2995 - acc: 0.930 - ETA: 3s - loss: 0.3033 - acc: 0.931 - ETA: 3s - loss: 0.2954 - acc: 0.933 - ETA: 3s - loss: 0.2897 - acc: 0.934 - ETA: 3s - loss: 0.2930 - acc: 0.934 - ETA: 3s - loss: 0.2889 - acc: 0.935 - ETA: 3s - loss: 0.2834 - acc: 0.935 - ETA: 3s - loss: 0.2803 - acc: 0.935 - ETA: 3s - loss: 0.2753 - acc: 0.936 - ETA: 3s - loss: 0.2808 - acc: 0.935 - ETA: 3s - loss: 0.2773 - acc: 0.936 - ETA: 2s - loss: 0.2740 - acc: 0.936 - ETA: 2s - loss: 0.2721 - acc: 0.936 - ETA: 2s - loss: 0.2686 - acc: 0.936 - ETA: 2s - loss: 0.2647 - acc: 0.937 - ETA: 2s - loss: 0.2633 - acc: 0.937 - ETA: 2s - loss: 0.2613 - acc: 0.936 - ETA: 2s - loss: 0.2637 - acc: 0.937 - ETA: 2s - loss: 0.2669 - acc: 0.936 - ETA: 2s - loss: 0.2686 - acc: 0.937 - ETA: 2s - loss: 0.2653 - acc: 0.937 - ETA: 2s - loss: 0.2627 - acc: 0.937 - ETA: 2s - loss: 0.2729 - acc: 0.937 - ETA: 2s - loss: 0.2772 - acc: 0.937 - ETA: 2s - loss: 0.2808 - acc: 0.936 - ETA: 2s - loss: 0.2782 - acc: 0.937 - ETA: 2s - loss: 0.2748 - acc: 0.937 - ETA: 1s - loss: 0.2774 - acc: 0.937 - ETA: 1s - loss: 0.2769 - acc: 0.936 - ETA: 1s - loss: 0.2734 - acc: 0.937 - ETA: 1s - loss: 0.2757 - acc: 0.937 - ETA: 1s - loss: 0.2786 - acc: 0.936 - ETA: 1s - loss: 0.2814 - acc: 0.936 - ETA: 1s - loss: 0.2772 - acc: 0.936 - ETA: 1s - loss: 0.2849 - acc: 0.936 - ETA: 1s - loss: 0.2867 - acc: 0.937 - ETA: 1s - loss: 0.2890 - acc: 0.936 - ETA: 1s - loss: 0.2851 - acc: 0.937 - ETA: 1s - loss: 0.2810 - acc: 0.938 - ETA: 1s - loss: 0.2787 - acc: 0.938 - ETA: 1s - loss: 0.2770 - acc: 0.939 - ETA: 0s - loss: 0.2741 - acc: 0.939 - ETA: 0s - loss: 0.2707 - acc: 0.940 - ETA: 0s - loss: 0.2675 - acc: 0.940 - ETA: 0s - loss: 0.2656 - acc: 0.940 - ETA: 0s - loss: 0.2668 - acc: 0.940 - ETA: 0s - loss: 0.2686 - acc: 0.940 - ETA: 0s - loss: 0.2709 - acc: 0.940 - ETA: 0s - loss: 0.2686 - acc: 0.939 - ETA: 0s - loss: 0.2705 - acc: 0.939 - ETA: 0s - loss: 0.2677 - acc: 0.940 - ETA: 0s - loss: 0.2658 - acc: 0.940 - ETA: 0s - loss: 0.2647 - acc: 0.940 - ETA: 0s - loss: 0.2643 - acc: 0.940 - ETA: 0s - loss: 0.2646 - acc: 0.940 - 6s 1ms/step - loss: 0.2668 - acc: 0.9400 - val_loss: 0.5670 - val_acc: 0.9096\n",
      "Epoch 57/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0278 - acc: 1.000 - ETA: 5s - loss: 0.3318 - acc: 0.968 - ETA: 5s - loss: 0.2725 - acc: 0.955 - ETA: 5s - loss: 0.2214 - acc: 0.962 - ETA: 4s - loss: 0.1917 - acc: 0.961 - ETA: 4s - loss: 0.1773 - acc: 0.953 - ETA: 4s - loss: 0.1605 - acc: 0.953 - ETA: 4s - loss: 0.1456 - acc: 0.960 - ETA: 4s - loss: 0.1478 - acc: 0.955 - ETA: 4s - loss: 0.1439 - acc: 0.953 - ETA: 4s - loss: 0.1377 - acc: 0.955 - ETA: 4s - loss: 0.1389 - acc: 0.952 - ETA: 4s - loss: 0.1641 - acc: 0.952 - ETA: 4s - loss: 0.1595 - acc: 0.954 - ETA: 4s - loss: 0.1508 - acc: 0.957 - ETA: 4s - loss: 0.1682 - acc: 0.957 - ETA: 4s - loss: 0.1831 - acc: 0.956 - ETA: 4s - loss: 0.1833 - acc: 0.955 - ETA: 4s - loss: 0.2151 - acc: 0.951 - ETA: 3s - loss: 0.2087 - acc: 0.951 - ETA: 3s - loss: 0.2047 - acc: 0.950 - ETA: 3s - loss: 0.1997 - acc: 0.951 - ETA: 3s - loss: 0.2090 - acc: 0.949 - ETA: 3s - loss: 0.2183 - acc: 0.950 - ETA: 3s - loss: 0.2137 - acc: 0.951 - ETA: 3s - loss: 0.2218 - acc: 0.950 - ETA: 3s - loss: 0.2313 - acc: 0.948 - ETA: 3s - loss: 0.2414 - acc: 0.946 - ETA: 3s - loss: 0.2378 - acc: 0.947 - ETA: 3s - loss: 0.2314 - acc: 0.948 - ETA: 3s - loss: 0.2398 - acc: 0.947 - ETA: 3s - loss: 0.2372 - acc: 0.947 - ETA: 3s - loss: 0.2419 - acc: 0.948 - ETA: 3s - loss: 0.2383 - acc: 0.947 - ETA: 3s - loss: 0.2450 - acc: 0.947 - ETA: 3s - loss: 0.2387 - acc: 0.949 - ETA: 3s - loss: 0.2352 - acc: 0.949 - ETA: 3s - loss: 0.2413 - acc: 0.949 - ETA: 3s - loss: 0.2377 - acc: 0.950 - ETA: 3s - loss: 0.2346 - acc: 0.951 - ETA: 3s - loss: 0.2303 - acc: 0.951 - ETA: 3s - loss: 0.2270 - acc: 0.952 - ETA: 2s - loss: 0.2318 - acc: 0.952 - ETA: 2s - loss: 0.2389 - acc: 0.950 - ETA: 2s - loss: 0.2355 - acc: 0.951 - ETA: 2s - loss: 0.2325 - acc: 0.952 - ETA: 2s - loss: 0.2322 - acc: 0.951 - ETA: 2s - loss: 0.2299 - acc: 0.951 - ETA: 2s - loss: 0.2279 - acc: 0.951 - ETA: 2s - loss: 0.2254 - acc: 0.952 - ETA: 2s - loss: 0.2228 - acc: 0.952 - ETA: 2s - loss: 0.2207 - acc: 0.952 - ETA: 2s - loss: 0.2180 - acc: 0.953 - ETA: 2s - loss: 0.2228 - acc: 0.952 - ETA: 2s - loss: 0.2209 - acc: 0.952 - ETA: 2s - loss: 0.2188 - acc: 0.952 - ETA: 2s - loss: 0.2184 - acc: 0.952 - ETA: 2s - loss: 0.2159 - acc: 0.953 - ETA: 2s - loss: 0.2155 - acc: 0.952 - ETA: 2s - loss: 0.2133 - acc: 0.952 - ETA: 2s - loss: 0.2125 - acc: 0.952 - ETA: 2s - loss: 0.2113 - acc: 0.952 - ETA: 2s - loss: 0.2095 - acc: 0.952 - ETA: 2s - loss: 0.2072 - acc: 0.953 - ETA: 2s - loss: 0.2113 - acc: 0.953 - ETA: 2s - loss: 0.2102 - acc: 0.953 - ETA: 2s - loss: 0.2076 - acc: 0.954 - ETA: 1s - loss: 0.2057 - acc: 0.954 - ETA: 1s - loss: 0.2109 - acc: 0.953 - ETA: 1s - loss: 0.2080 - acc: 0.954 - ETA: 1s - loss: 0.2116 - acc: 0.953 - ETA: 1s - loss: 0.2090 - acc: 0.954 - ETA: 1s - loss: 0.2118 - acc: 0.954 - ETA: 1s - loss: 0.2093 - acc: 0.954 - ETA: 1s - loss: 0.2087 - acc: 0.954 - ETA: 1s - loss: 0.2092 - acc: 0.953 - ETA: 1s - loss: 0.2072 - acc: 0.953 - ETA: 1s - loss: 0.2070 - acc: 0.952 - ETA: 1s - loss: 0.2114 - acc: 0.951 - ETA: 1s - loss: 0.2097 - acc: 0.951 - ETA: 1s - loss: 0.2087 - acc: 0.950 - ETA: 0s - loss: 0.2122 - acc: 0.949 - ETA: 0s - loss: 0.2093 - acc: 0.950 - ETA: 0s - loss: 0.2064 - acc: 0.951 - ETA: 0s - loss: 0.2040 - acc: 0.952 - ETA: 0s - loss: 0.2029 - acc: 0.952 - ETA: 0s - loss: 0.2054 - acc: 0.951 - ETA: 0s - loss: 0.2043 - acc: 0.951 - ETA: 0s - loss: 0.2036 - acc: 0.951 - ETA: 0s - loss: 0.2057 - acc: 0.951 - ETA: 0s - loss: 0.2048 - acc: 0.951 - ETA: 0s - loss: 0.2112 - acc: 0.950 - ETA: 0s - loss: 0.2181 - acc: 0.950 - ETA: 0s - loss: 0.2161 - acc: 0.950 - ETA: 0s - loss: 0.2167 - acc: 0.949 - 6s 1ms/step - loss: 0.2202 - acc: 0.9493 - val_loss: 0.4239 - val_acc: 0.9147\n",
      "Epoch 58/100\n",
      "4067/4067 [==============================] - ETA: 4s - loss: 0.1263 - acc: 0.937 - ETA: 5s - loss: 0.4048 - acc: 0.890 - ETA: 5s - loss: 0.2539 - acc: 0.937 - ETA: 5s - loss: 0.3293 - acc: 0.931 - ETA: 4s - loss: 0.3504 - acc: 0.932 - ETA: 4s - loss: 0.2932 - acc: 0.941 - ETA: 4s - loss: 0.3069 - acc: 0.947 - ETA: 4s - loss: 0.2778 - acc: 0.951 - ETA: 4s - loss: 0.2633 - acc: 0.945 - ETA: 4s - loss: 0.2778 - acc: 0.946 - ETA: 4s - loss: 0.2550 - acc: 0.951 - ETA: 4s - loss: 0.2415 - acc: 0.952 - ETA: 4s - loss: 0.2552 - acc: 0.951 - ETA: 4s - loss: 0.2655 - acc: 0.950 - ETA: 4s - loss: 0.2524 - acc: 0.950 - ETA: 4s - loss: 0.2429 - acc: 0.951 - ETA: 4s - loss: 0.2361 - acc: 0.951 - ETA: 4s - loss: 0.2319 - acc: 0.950 - ETA: 4s - loss: 0.2451 - acc: 0.947 - ETA: 4s - loss: 0.2387 - acc: 0.947 - ETA: 4s - loss: 0.2291 - acc: 0.948 - ETA: 4s - loss: 0.2234 - acc: 0.948 - ETA: 3s - loss: 0.2257 - acc: 0.947 - ETA: 3s - loss: 0.2208 - acc: 0.948 - ETA: 4s - loss: 0.2211 - acc: 0.946 - ETA: 4s - loss: 0.2323 - acc: 0.945 - ETA: 4s - loss: 0.2280 - acc: 0.945 - ETA: 3s - loss: 0.2244 - acc: 0.947 - ETA: 3s - loss: 0.2332 - acc: 0.946 - ETA: 3s - loss: 0.2418 - acc: 0.946 - ETA: 3s - loss: 0.2493 - acc: 0.946 - ETA: 3s - loss: 0.2432 - acc: 0.947 - ETA: 3s - loss: 0.2492 - acc: 0.947 - ETA: 3s - loss: 0.2462 - acc: 0.947 - ETA: 3s - loss: 0.2481 - acc: 0.946 - ETA: 3s - loss: 0.2436 - acc: 0.947 - ETA: 3s - loss: 0.2375 - acc: 0.948 - ETA: 3s - loss: 0.2445 - acc: 0.948 - ETA: 3s - loss: 0.2506 - acc: 0.948 - ETA: 3s - loss: 0.2474 - acc: 0.949 - ETA: 3s - loss: 0.2458 - acc: 0.948 - ETA: 3s - loss: 0.2515 - acc: 0.948 - ETA: 3s - loss: 0.2504 - acc: 0.947 - ETA: 3s - loss: 0.2480 - acc: 0.946 - ETA: 3s - loss: 0.2443 - acc: 0.947 - ETA: 3s - loss: 0.2396 - acc: 0.947 - ETA: 3s - loss: 0.2369 - acc: 0.948 - ETA: 2s - loss: 0.2338 - acc: 0.948 - ETA: 2s - loss: 0.2302 - acc: 0.948 - ETA: 2s - loss: 0.2269 - acc: 0.949 - ETA: 2s - loss: 0.2247 - acc: 0.949 - ETA: 2s - loss: 0.2217 - acc: 0.949 - ETA: 2s - loss: 0.2255 - acc: 0.949 - ETA: 2s - loss: 0.2230 - acc: 0.949 - ETA: 2s - loss: 0.2266 - acc: 0.949 - ETA: 2s - loss: 0.2230 - acc: 0.950 - ETA: 2s - loss: 0.2263 - acc: 0.950 - ETA: 2s - loss: 0.2248 - acc: 0.950 - ETA: 2s - loss: 0.2292 - acc: 0.949 - ETA: 2s - loss: 0.2265 - acc: 0.949 - ETA: 2s - loss: 0.2300 - acc: 0.949 - ETA: 2s - loss: 0.2270 - acc: 0.950 - ETA: 1s - loss: 0.2276 - acc: 0.948 - ETA: 1s - loss: 0.2254 - acc: 0.947 - ETA: 1s - loss: 0.2217 - acc: 0.948 - ETA: 1s - loss: 0.2307 - acc: 0.947 - ETA: 1s - loss: 0.2275 - acc: 0.948 - ETA: 1s - loss: 0.2256 - acc: 0.948 - ETA: 1s - loss: 0.2232 - acc: 0.948 - ETA: 1s - loss: 0.2211 - acc: 0.948 - ETA: 1s - loss: 0.2186 - acc: 0.948 - ETA: 1s - loss: 0.2284 - acc: 0.948 - ETA: 1s - loss: 0.2270 - acc: 0.947 - ETA: 1s - loss: 0.2290 - acc: 0.948 - ETA: 1s - loss: 0.2274 - acc: 0.948 - ETA: 1s - loss: 0.2245 - acc: 0.949 - ETA: 1s - loss: 0.2221 - acc: 0.949 - ETA: 0s - loss: 0.2250 - acc: 0.949 - ETA: 0s - loss: 0.2274 - acc: 0.949 - ETA: 0s - loss: 0.2272 - acc: 0.949 - ETA: 0s - loss: 0.2248 - acc: 0.950 - ETA: 0s - loss: 0.2229 - acc: 0.950 - ETA: 0s - loss: 0.2203 - acc: 0.951 - ETA: 0s - loss: 0.2182 - acc: 0.951 - ETA: 0s - loss: 0.2164 - acc: 0.951 - ETA: 0s - loss: 0.2142 - acc: 0.951 - ETA: 0s - loss: 0.2163 - acc: 0.951 - ETA: 0s - loss: 0.2143 - acc: 0.951 - ETA: 0s - loss: 0.2126 - acc: 0.951 - ETA: 0s - loss: 0.2133 - acc: 0.950 - ETA: 0s - loss: 0.2234 - acc: 0.950 - ETA: 0s - loss: 0.2216 - acc: 0.950 - 6s 2ms/step - loss: 0.2208 - acc: 0.9506 - val_loss: 0.4929 - val_acc: 0.9237\n",
      "Epoch 59/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1437 - acc: 0.875 - ETA: 6s - loss: 0.1038 - acc: 0.916 - ETA: 6s - loss: 0.0958 - acc: 0.925 - ETA: 6s - loss: 0.0953 - acc: 0.928 - ETA: 6s - loss: 0.0795 - acc: 0.943 - ETA: 6s - loss: 0.0724 - acc: 0.953 - ETA: 6s - loss: 0.0687 - acc: 0.959 - ETA: 6s - loss: 0.0658 - acc: 0.962 - ETA: 6s - loss: 0.1429 - acc: 0.955 - ETA: 6s - loss: 0.1876 - acc: 0.957 - ETA: 6s - loss: 0.1755 - acc: 0.958 - ETA: 6s - loss: 0.1700 - acc: 0.953 - ETA: 6s - loss: 0.1896 - acc: 0.952 - ETA: 6s - loss: 0.1812 - acc: 0.953 - ETA: 6s - loss: 0.1755 - acc: 0.952 - ETA: 6s - loss: 0.1805 - acc: 0.951 - ETA: 6s - loss: 0.1770 - acc: 0.950 - ETA: 6s - loss: 0.2041 - acc: 0.950 - ETA: 6s - loss: 0.1954 - acc: 0.952 - ETA: 6s - loss: 0.1862 - acc: 0.955 - ETA: 6s - loss: 0.2040 - acc: 0.954 - ETA: 5s - loss: 0.2144 - acc: 0.952 - ETA: 5s - loss: 0.2081 - acc: 0.952 - ETA: 5s - loss: 0.2008 - acc: 0.952 - ETA: 5s - loss: 0.1964 - acc: 0.953 - ETA: 5s - loss: 0.2106 - acc: 0.951 - ETA: 5s - loss: 0.2044 - acc: 0.952 - ETA: 5s - loss: 0.1978 - acc: 0.954 - ETA: 5s - loss: 0.1924 - acc: 0.955 - ETA: 5s - loss: 0.1879 - acc: 0.957 - ETA: 5s - loss: 0.1826 - acc: 0.958 - ETA: 4s - loss: 0.1791 - acc: 0.959 - ETA: 4s - loss: 0.1803 - acc: 0.957 - ETA: 4s - loss: 0.1790 - acc: 0.956 - ETA: 4s - loss: 0.1758 - acc: 0.955 - ETA: 4s - loss: 0.1853 - acc: 0.954 - ETA: 4s - loss: 0.1797 - acc: 0.956 - ETA: 4s - loss: 0.1768 - acc: 0.956 - ETA: 4s - loss: 0.1714 - acc: 0.957 - ETA: 4s - loss: 0.1744 - acc: 0.956 - ETA: 3s - loss: 0.1717 - acc: 0.956 - ETA: 3s - loss: 0.1691 - acc: 0.956 - ETA: 3s - loss: 0.1673 - acc: 0.956 - ETA: 3s - loss: 0.1645 - acc: 0.957 - ETA: 3s - loss: 0.1799 - acc: 0.956 - ETA: 3s - loss: 0.1761 - acc: 0.958 - ETA: 3s - loss: 0.1822 - acc: 0.958 - ETA: 3s - loss: 0.1795 - acc: 0.958 - ETA: 3s - loss: 0.1772 - acc: 0.958 - ETA: 3s - loss: 0.1755 - acc: 0.957 - ETA: 3s - loss: 0.1715 - acc: 0.958 - ETA: 3s - loss: 0.1694 - acc: 0.958 - ETA: 2s - loss: 0.1736 - acc: 0.958 - ETA: 2s - loss: 0.1784 - acc: 0.958 - ETA: 2s - loss: 0.1841 - acc: 0.958 - ETA: 2s - loss: 0.1821 - acc: 0.958 - ETA: 2s - loss: 0.1869 - acc: 0.958 - ETA: 2s - loss: 0.1844 - acc: 0.958 - ETA: 2s - loss: 0.1892 - acc: 0.959 - ETA: 2s - loss: 0.1874 - acc: 0.959 - ETA: 2s - loss: 0.1998 - acc: 0.958 - ETA: 2s - loss: 0.1960 - acc: 0.958 - ETA: 2s - loss: 0.1985 - acc: 0.957 - ETA: 2s - loss: 0.2082 - acc: 0.957 - ETA: 2s - loss: 0.2141 - acc: 0.956 - ETA: 2s - loss: 0.2233 - acc: 0.955 - ETA: 2s - loss: 0.2208 - acc: 0.956 - ETA: 1s - loss: 0.2249 - acc: 0.955 - ETA: 1s - loss: 0.2215 - acc: 0.956 - ETA: 1s - loss: 0.2197 - acc: 0.956 - ETA: 1s - loss: 0.2168 - acc: 0.956 - ETA: 1s - loss: 0.2144 - acc: 0.957 - ETA: 1s - loss: 0.2167 - acc: 0.957 - ETA: 1s - loss: 0.2145 - acc: 0.957 - ETA: 1s - loss: 0.2129 - acc: 0.957 - ETA: 1s - loss: 0.2123 - acc: 0.956 - ETA: 1s - loss: 0.2117 - acc: 0.955 - ETA: 1s - loss: 0.2087 - acc: 0.956 - ETA: 1s - loss: 0.2068 - acc: 0.956 - ETA: 1s - loss: 0.2044 - acc: 0.956 - ETA: 1s - loss: 0.2025 - acc: 0.956 - ETA: 0s - loss: 0.2019 - acc: 0.956 - ETA: 0s - loss: 0.2005 - acc: 0.957 - ETA: 0s - loss: 0.1988 - acc: 0.957 - ETA: 0s - loss: 0.1981 - acc: 0.956 - ETA: 0s - loss: 0.2010 - acc: 0.956 - ETA: 0s - loss: 0.2003 - acc: 0.956 - ETA: 0s - loss: 0.1995 - acc: 0.956 - ETA: 0s - loss: 0.2027 - acc: 0.955 - ETA: 0s - loss: 0.2094 - acc: 0.955 - ETA: 0s - loss: 0.2120 - acc: 0.954 - ETA: 0s - loss: 0.2139 - acc: 0.955 - ETA: 0s - loss: 0.2123 - acc: 0.955 - ETA: 0s - loss: 0.2154 - acc: 0.954 - ETA: 0s - loss: 0.2140 - acc: 0.955 - ETA: 0s - loss: 0.2134 - acc: 0.955 - 6s 1ms/step - loss: 0.2133 - acc: 0.9550 - val_loss: 0.4415 - val_acc: 0.9321\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 8s - loss: 0.0756 - acc: 0.937 - ETA: 9s - loss: 0.1248 - acc: 0.937 - ETA: 8s - loss: 0.1416 - acc: 0.925 - ETA: 7s - loss: 0.1143 - acc: 0.946 - ETA: 7s - loss: 0.4848 - acc: 0.937 - ETA: 6s - loss: 0.3908 - acc: 0.942 - ETA: 6s - loss: 0.3473 - acc: 0.950 - ETA: 6s - loss: 0.3107 - acc: 0.952 - ETA: 6s - loss: 0.3243 - acc: 0.953 - ETA: 6s - loss: 0.3348 - acc: 0.953 - ETA: 6s - loss: 0.3112 - acc: 0.957 - ETA: 6s - loss: 0.3334 - acc: 0.956 - ETA: 5s - loss: 0.3082 - acc: 0.958 - ETA: 5s - loss: 0.2867 - acc: 0.960 - ETA: 5s - loss: 0.3048 - acc: 0.957 - ETA: 5s - loss: 0.2916 - acc: 0.957 - ETA: 5s - loss: 0.2792 - acc: 0.958 - ETA: 5s - loss: 0.2676 - acc: 0.960 - ETA: 5s - loss: 0.2580 - acc: 0.960 - ETA: 5s - loss: 0.2426 - acc: 0.963 - ETA: 5s - loss: 0.2318 - acc: 0.965 - ETA: 5s - loss: 0.2435 - acc: 0.962 - ETA: 4s - loss: 0.2387 - acc: 0.962 - ETA: 4s - loss: 0.2332 - acc: 0.962 - ETA: 4s - loss: 0.2449 - acc: 0.958 - ETA: 4s - loss: 0.2669 - acc: 0.958 - ETA: 4s - loss: 0.2605 - acc: 0.958 - ETA: 4s - loss: 0.2691 - acc: 0.958 - ETA: 4s - loss: 0.2752 - acc: 0.958 - ETA: 4s - loss: 0.2658 - acc: 0.959 - ETA: 4s - loss: 0.2579 - acc: 0.960 - ETA: 4s - loss: 0.2636 - acc: 0.960 - ETA: 4s - loss: 0.2569 - acc: 0.961 - ETA: 4s - loss: 0.2621 - acc: 0.961 - ETA: 4s - loss: 0.2651 - acc: 0.961 - ETA: 3s - loss: 0.2586 - acc: 0.962 - ETA: 3s - loss: 0.2549 - acc: 0.962 - ETA: 3s - loss: 0.2579 - acc: 0.962 - ETA: 3s - loss: 0.2508 - acc: 0.964 - ETA: 3s - loss: 0.2471 - acc: 0.963 - ETA: 3s - loss: 0.2442 - acc: 0.963 - ETA: 3s - loss: 0.2501 - acc: 0.961 - ETA: 3s - loss: 0.2552 - acc: 0.960 - ETA: 3s - loss: 0.2506 - acc: 0.960 - ETA: 3s - loss: 0.2509 - acc: 0.960 - ETA: 3s - loss: 0.2455 - acc: 0.961 - ETA: 3s - loss: 0.2501 - acc: 0.960 - ETA: 3s - loss: 0.2544 - acc: 0.960 - ETA: 3s - loss: 0.2498 - acc: 0.961 - ETA: 2s - loss: 0.2448 - acc: 0.961 - ETA: 2s - loss: 0.2464 - acc: 0.961 - ETA: 2s - loss: 0.2464 - acc: 0.960 - ETA: 2s - loss: 0.2450 - acc: 0.958 - ETA: 2s - loss: 0.2435 - acc: 0.958 - ETA: 2s - loss: 0.2467 - acc: 0.958 - ETA: 2s - loss: 0.2448 - acc: 0.958 - ETA: 2s - loss: 0.2426 - acc: 0.958 - ETA: 2s - loss: 0.2382 - acc: 0.959 - ETA: 2s - loss: 0.2342 - acc: 0.960 - ETA: 2s - loss: 0.2374 - acc: 0.960 - ETA: 2s - loss: 0.2335 - acc: 0.960 - ETA: 2s - loss: 0.2332 - acc: 0.960 - ETA: 2s - loss: 0.2303 - acc: 0.960 - ETA: 1s - loss: 0.2272 - acc: 0.961 - ETA: 1s - loss: 0.2272 - acc: 0.959 - ETA: 1s - loss: 0.2257 - acc: 0.959 - ETA: 1s - loss: 0.2244 - acc: 0.958 - ETA: 1s - loss: 0.2243 - acc: 0.956 - ETA: 1s - loss: 0.2323 - acc: 0.956 - ETA: 1s - loss: 0.2305 - acc: 0.955 - ETA: 1s - loss: 0.2306 - acc: 0.954 - ETA: 1s - loss: 0.2318 - acc: 0.954 - ETA: 1s - loss: 0.2298 - acc: 0.954 - ETA: 1s - loss: 0.2281 - acc: 0.953 - ETA: 1s - loss: 0.2267 - acc: 0.953 - ETA: 1s - loss: 0.2257 - acc: 0.952 - ETA: 1s - loss: 0.2233 - acc: 0.953 - ETA: 0s - loss: 0.2225 - acc: 0.952 - ETA: 0s - loss: 0.2253 - acc: 0.951 - ETA: 0s - loss: 0.2228 - acc: 0.952 - ETA: 0s - loss: 0.2217 - acc: 0.952 - ETA: 0s - loss: 0.2197 - acc: 0.952 - ETA: 0s - loss: 0.2186 - acc: 0.953 - ETA: 0s - loss: 0.2178 - acc: 0.952 - ETA: 0s - loss: 0.2161 - acc: 0.953 - ETA: 0s - loss: 0.2138 - acc: 0.953 - ETA: 0s - loss: 0.2121 - acc: 0.954 - ETA: 0s - loss: 0.2106 - acc: 0.954 - ETA: 0s - loss: 0.2138 - acc: 0.954 - ETA: 0s - loss: 0.2124 - acc: 0.954 - ETA: 0s - loss: 0.2149 - acc: 0.954 - ETA: 0s - loss: 0.2133 - acc: 0.955 - ETA: 0s - loss: 0.2167 - acc: 0.954 - ETA: 0s - loss: 0.2161 - acc: 0.954 - ETA: 0s - loss: 0.2152 - acc: 0.954 - ETA: 0s - loss: 0.2178 - acc: 0.954 - 6s 2ms/step - loss: 0.2169 - acc: 0.9550 - val_loss: 0.4444 - val_acc: 0.9314\n",
      "Epoch 61/100\n",
      "4067/4067 [==============================] - ETA: 7s - loss: 0.0573 - acc: 1.000 - ETA: 7s - loss: 0.0775 - acc: 0.979 - ETA: 7s - loss: 0.0936 - acc: 0.975 - ETA: 8s - loss: 0.0793 - acc: 0.982 - ETA: 8s - loss: 0.1771 - acc: 0.972 - ETA: 8s - loss: 0.1558 - acc: 0.971 - ETA: 7s - loss: 0.1442 - acc: 0.971 - ETA: 7s - loss: 0.1434 - acc: 0.966 - ETA: 7s - loss: 0.2013 - acc: 0.959 - ETA: 8s - loss: 0.1950 - acc: 0.958 - ETA: 8s - loss: 0.2405 - acc: 0.950 - ETA: 8s - loss: 0.2215 - acc: 0.954 - ETA: 7s - loss: 0.2137 - acc: 0.953 - ETA: 7s - loss: 0.1994 - acc: 0.956 - ETA: 7s - loss: 0.1929 - acc: 0.957 - ETA: 7s - loss: 0.2149 - acc: 0.958 - ETA: 7s - loss: 0.2069 - acc: 0.959 - ETA: 7s - loss: 0.2298 - acc: 0.957 - ETA: 6s - loss: 0.2241 - acc: 0.952 - ETA: 6s - loss: 0.2449 - acc: 0.950 - ETA: 6s - loss: 0.2360 - acc: 0.951 - ETA: 6s - loss: 0.2591 - acc: 0.947 - ETA: 6s - loss: 0.2545 - acc: 0.945 - ETA: 6s - loss: 0.2467 - acc: 0.946 - ETA: 6s - loss: 0.2379 - acc: 0.949 - ETA: 6s - loss: 0.2531 - acc: 0.947 - ETA: 6s - loss: 0.2613 - acc: 0.947 - ETA: 6s - loss: 0.2536 - acc: 0.947 - ETA: 5s - loss: 0.2430 - acc: 0.949 - ETA: 5s - loss: 0.2386 - acc: 0.948 - ETA: 5s - loss: 0.2351 - acc: 0.948 - ETA: 5s - loss: 0.2467 - acc: 0.947 - ETA: 5s - loss: 0.2384 - acc: 0.948 - ETA: 5s - loss: 0.2347 - acc: 0.948 - ETA: 5s - loss: 0.2314 - acc: 0.948 - ETA: 5s - loss: 0.2269 - acc: 0.949 - ETA: 5s - loss: 0.2411 - acc: 0.948 - ETA: 5s - loss: 0.2336 - acc: 0.950 - ETA: 5s - loss: 0.2280 - acc: 0.951 - ETA: 5s - loss: 0.2232 - acc: 0.952 - ETA: 4s - loss: 0.2193 - acc: 0.952 - ETA: 4s - loss: 0.2266 - acc: 0.952 - ETA: 4s - loss: 0.2238 - acc: 0.952 - ETA: 4s - loss: 0.2402 - acc: 0.951 - ETA: 4s - loss: 0.2343 - acc: 0.952 - ETA: 4s - loss: 0.2288 - acc: 0.953 - ETA: 4s - loss: 0.2231 - acc: 0.954 - ETA: 4s - loss: 0.2197 - acc: 0.954 - ETA: 4s - loss: 0.2188 - acc: 0.954 - ETA: 4s - loss: 0.2152 - acc: 0.955 - ETA: 3s - loss: 0.2116 - acc: 0.955 - ETA: 3s - loss: 0.2190 - acc: 0.955 - ETA: 3s - loss: 0.2174 - acc: 0.954 - ETA: 3s - loss: 0.2228 - acc: 0.954 - ETA: 3s - loss: 0.2206 - acc: 0.955 - ETA: 3s - loss: 0.2253 - acc: 0.955 - ETA: 3s - loss: 0.2234 - acc: 0.954 - ETA: 3s - loss: 0.2199 - acc: 0.954 - ETA: 3s - loss: 0.2197 - acc: 0.954 - ETA: 3s - loss: 0.2177 - acc: 0.954 - ETA: 3s - loss: 0.2137 - acc: 0.955 - ETA: 3s - loss: 0.2270 - acc: 0.953 - ETA: 2s - loss: 0.2232 - acc: 0.954 - ETA: 2s - loss: 0.2259 - acc: 0.955 - ETA: 2s - loss: 0.2225 - acc: 0.955 - ETA: 2s - loss: 0.2198 - acc: 0.955 - ETA: 2s - loss: 0.2159 - acc: 0.956 - ETA: 2s - loss: 0.2193 - acc: 0.956 - ETA: 2s - loss: 0.2179 - acc: 0.955 - ETA: 2s - loss: 0.2173 - acc: 0.956 - ETA: 2s - loss: 0.2208 - acc: 0.955 - ETA: 2s - loss: 0.2178 - acc: 0.956 - ETA: 2s - loss: 0.2154 - acc: 0.956 - ETA: 1s - loss: 0.2133 - acc: 0.956 - ETA: 1s - loss: 0.2116 - acc: 0.956 - ETA: 1s - loss: 0.2092 - acc: 0.956 - ETA: 1s - loss: 0.2072 - acc: 0.956 - ETA: 1s - loss: 0.2050 - acc: 0.956 - ETA: 1s - loss: 0.2049 - acc: 0.955 - ETA: 1s - loss: 0.2026 - acc: 0.955 - ETA: 1s - loss: 0.2065 - acc: 0.955 - ETA: 1s - loss: 0.2043 - acc: 0.956 - ETA: 1s - loss: 0.2070 - acc: 0.956 - ETA: 1s - loss: 0.2060 - acc: 0.956 - ETA: 1s - loss: 0.2106 - acc: 0.955 - ETA: 0s - loss: 0.2093 - acc: 0.954 - ETA: 0s - loss: 0.2084 - acc: 0.954 - ETA: 0s - loss: 0.2116 - acc: 0.953 - ETA: 0s - loss: 0.2141 - acc: 0.954 - ETA: 0s - loss: 0.2123 - acc: 0.954 - ETA: 0s - loss: 0.2117 - acc: 0.953 - ETA: 0s - loss: 0.2114 - acc: 0.953 - ETA: 0s - loss: 0.2093 - acc: 0.953 - ETA: 0s - loss: 0.2079 - acc: 0.954 - ETA: 0s - loss: 0.2065 - acc: 0.954 - ETA: 0s - loss: 0.2060 - acc: 0.954 - ETA: 0s - loss: 0.2087 - acc: 0.954 - ETA: 0s - loss: 0.2075 - acc: 0.954 - ETA: 0s - loss: 0.2058 - acc: 0.954 - ETA: 0s - loss: 0.2041 - acc: 0.955 - 7s 2ms/step - loss: 0.2031 - acc: 0.9550 - val_loss: 0.4064 - val_acc: 0.9462\n",
      "Epoch 62/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0335 - acc: 1.000 - ETA: 5s - loss: 0.0538 - acc: 0.984 - ETA: 6s - loss: 0.3913 - acc: 0.958 - ETA: 6s - loss: 0.3039 - acc: 0.968 - ETA: 6s - loss: 0.2706 - acc: 0.968 - ETA: 6s - loss: 0.2435 - acc: 0.968 - ETA: 6s - loss: 0.2224 - acc: 0.964 - ETA: 6s - loss: 0.1969 - acc: 0.968 - ETA: 6s - loss: 0.1785 - acc: 0.972 - ETA: 5s - loss: 0.1675 - acc: 0.970 - ETA: 5s - loss: 0.1604 - acc: 0.970 - ETA: 5s - loss: 0.1539 - acc: 0.970 - ETA: 5s - loss: 0.1456 - acc: 0.972 - ETA: 5s - loss: 0.1385 - acc: 0.972 - ETA: 5s - loss: 0.1325 - acc: 0.973 - ETA: 5s - loss: 0.1268 - acc: 0.974 - ETA: 5s - loss: 0.1295 - acc: 0.972 - ETA: 5s - loss: 0.1490 - acc: 0.971 - ETA: 5s - loss: 0.1482 - acc: 0.968 - ETA: 5s - loss: 0.1450 - acc: 0.968 - ETA: 5s - loss: 0.1442 - acc: 0.966 - ETA: 4s - loss: 0.1394 - acc: 0.967 - ETA: 4s - loss: 0.1542 - acc: 0.965 - ETA: 4s - loss: 0.1555 - acc: 0.962 - ETA: 4s - loss: 0.2036 - acc: 0.959 - ETA: 4s - loss: 0.2175 - acc: 0.955 - ETA: 4s - loss: 0.2102 - acc: 0.956 - ETA: 4s - loss: 0.2222 - acc: 0.956 - ETA: 4s - loss: 0.2148 - acc: 0.957 - ETA: 4s - loss: 0.2141 - acc: 0.957 - ETA: 4s - loss: 0.2074 - acc: 0.958 - ETA: 3s - loss: 0.2035 - acc: 0.958 - ETA: 3s - loss: 0.1985 - acc: 0.959 - ETA: 3s - loss: 0.1947 - acc: 0.960 - ETA: 3s - loss: 0.1926 - acc: 0.959 - ETA: 3s - loss: 0.2018 - acc: 0.958 - ETA: 3s - loss: 0.2090 - acc: 0.958 - ETA: 3s - loss: 0.2144 - acc: 0.959 - ETA: 3s - loss: 0.2455 - acc: 0.956 - ETA: 3s - loss: 0.2444 - acc: 0.953 - ETA: 3s - loss: 0.2387 - acc: 0.953 - ETA: 3s - loss: 0.2351 - acc: 0.954 - ETA: 3s - loss: 0.2333 - acc: 0.953 - ETA: 3s - loss: 0.2304 - acc: 0.952 - ETA: 3s - loss: 0.2279 - acc: 0.951 - ETA: 3s - loss: 0.2329 - acc: 0.951 - ETA: 2s - loss: 0.2388 - acc: 0.949 - ETA: 2s - loss: 0.2358 - acc: 0.949 - ETA: 2s - loss: 0.2328 - acc: 0.949 - ETA: 2s - loss: 0.2298 - acc: 0.949 - ETA: 2s - loss: 0.2261 - acc: 0.949 - ETA: 2s - loss: 0.2258 - acc: 0.949 - ETA: 2s - loss: 0.2223 - acc: 0.950 - ETA: 2s - loss: 0.2185 - acc: 0.950 - ETA: 2s - loss: 0.2144 - acc: 0.951 - ETA: 2s - loss: 0.2118 - acc: 0.951 - ETA: 2s - loss: 0.2085 - acc: 0.952 - ETA: 2s - loss: 0.2184 - acc: 0.952 - ETA: 2s - loss: 0.2158 - acc: 0.952 - ETA: 2s - loss: 0.2145 - acc: 0.952 - ETA: 2s - loss: 0.2163 - acc: 0.952 - ETA: 2s - loss: 0.2155 - acc: 0.951 - ETA: 2s - loss: 0.2131 - acc: 0.951 - ETA: 2s - loss: 0.2170 - acc: 0.951 - ETA: 1s - loss: 0.2150 - acc: 0.951 - ETA: 1s - loss: 0.2123 - acc: 0.952 - ETA: 1s - loss: 0.2100 - acc: 0.952 - ETA: 1s - loss: 0.2079 - acc: 0.953 - ETA: 1s - loss: 0.2052 - acc: 0.953 - ETA: 1s - loss: 0.2030 - acc: 0.953 - ETA: 1s - loss: 0.2003 - acc: 0.954 - ETA: 1s - loss: 0.2047 - acc: 0.953 - ETA: 1s - loss: 0.2090 - acc: 0.952 - ETA: 1s - loss: 0.2097 - acc: 0.952 - ETA: 1s - loss: 0.2077 - acc: 0.952 - ETA: 1s - loss: 0.2060 - acc: 0.952 - ETA: 1s - loss: 0.2040 - acc: 0.953 - ETA: 1s - loss: 0.2024 - acc: 0.953 - ETA: 1s - loss: 0.2005 - acc: 0.954 - ETA: 1s - loss: 0.1989 - acc: 0.954 - ETA: 1s - loss: 0.1983 - acc: 0.954 - ETA: 0s - loss: 0.2027 - acc: 0.954 - ETA: 0s - loss: 0.2090 - acc: 0.952 - ETA: 0s - loss: 0.2113 - acc: 0.953 - ETA: 0s - loss: 0.2132 - acc: 0.953 - ETA: 0s - loss: 0.2126 - acc: 0.953 - ETA: 0s - loss: 0.2111 - acc: 0.953 - ETA: 0s - loss: 0.2148 - acc: 0.952 - ETA: 0s - loss: 0.2138 - acc: 0.952 - ETA: 0s - loss: 0.2124 - acc: 0.952 - ETA: 0s - loss: 0.2113 - acc: 0.951 - ETA: 0s - loss: 0.2103 - acc: 0.952 - ETA: 0s - loss: 0.2080 - acc: 0.952 - ETA: 0s - loss: 0.2057 - acc: 0.953 - ETA: 0s - loss: 0.2050 - acc: 0.952 - ETA: 0s - loss: 0.2078 - acc: 0.952 - ETA: 0s - loss: 0.2069 - acc: 0.952 - 6s 2ms/step - loss: 0.2068 - acc: 0.9523 - val_loss: 0.5174 - val_acc: 0.9301\n",
      "Epoch 63/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 6.0252e-05 - acc: 1.000 - ETA: 5s - loss: 0.0790 - acc: 0.9688    - ETA: 5s - loss: 0.0704 - acc: 0.973 - ETA: 5s - loss: 0.0789 - acc: 0.962 - ETA: 5s - loss: 0.1557 - acc: 0.956 - ETA: 5s - loss: 0.1443 - acc: 0.957 - ETA: 5s - loss: 0.1487 - acc: 0.957 - ETA: 5s - loss: 0.1808 - acc: 0.957 - ETA: 4s - loss: 0.1983 - acc: 0.957 - ETA: 4s - loss: 0.1909 - acc: 0.955 - ETA: 4s - loss: 0.1808 - acc: 0.953 - ETA: 4s - loss: 0.1705 - acc: 0.954 - ETA: 4s - loss: 0.1647 - acc: 0.952 - ETA: 4s - loss: 0.1564 - acc: 0.954 - ETA: 4s - loss: 0.1784 - acc: 0.952 - ETA: 4s - loss: 0.1807 - acc: 0.945 - ETA: 4s - loss: 0.1794 - acc: 0.942 - ETA: 4s - loss: 0.1738 - acc: 0.944 - ETA: 4s - loss: 0.1863 - acc: 0.944 - ETA: 4s - loss: 0.1772 - acc: 0.947 - ETA: 4s - loss: 0.1796 - acc: 0.946 - ETA: 4s - loss: 0.1711 - acc: 0.948 - ETA: 4s - loss: 0.1801 - acc: 0.950 - ETA: 4s - loss: 0.1757 - acc: 0.950 - ETA: 4s - loss: 0.1698 - acc: 0.951 - ETA: 3s - loss: 0.1686 - acc: 0.950 - ETA: 3s - loss: 0.1655 - acc: 0.951 - ETA: 3s - loss: 0.1733 - acc: 0.952 - ETA: 3s - loss: 0.1810 - acc: 0.952 - ETA: 3s - loss: 0.1799 - acc: 0.952 - ETA: 3s - loss: 0.1779 - acc: 0.953 - ETA: 3s - loss: 0.1738 - acc: 0.955 - ETA: 3s - loss: 0.1732 - acc: 0.953 - ETA: 3s - loss: 0.1701 - acc: 0.953 - ETA: 3s - loss: 0.1842 - acc: 0.952 - ETA: 3s - loss: 0.2010 - acc: 0.950 - ETA: 3s - loss: 0.1993 - acc: 0.950 - ETA: 3s - loss: 0.1966 - acc: 0.951 - ETA: 3s - loss: 0.1945 - acc: 0.951 - ETA: 3s - loss: 0.2003 - acc: 0.952 - ETA: 3s - loss: 0.2124 - acc: 0.951 - ETA: 3s - loss: 0.2076 - acc: 0.952 - ETA: 2s - loss: 0.2202 - acc: 0.952 - ETA: 2s - loss: 0.2296 - acc: 0.951 - ETA: 2s - loss: 0.2420 - acc: 0.949 - ETA: 2s - loss: 0.2375 - acc: 0.950 - ETA: 2s - loss: 0.2328 - acc: 0.951 - ETA: 2s - loss: 0.2284 - acc: 0.952 - ETA: 2s - loss: 0.2247 - acc: 0.953 - ETA: 2s - loss: 0.2281 - acc: 0.953 - ETA: 2s - loss: 0.2273 - acc: 0.953 - ETA: 2s - loss: 0.2240 - acc: 0.954 - ETA: 2s - loss: 0.2335 - acc: 0.954 - ETA: 2s - loss: 0.2364 - acc: 0.955 - ETA: 2s - loss: 0.2340 - acc: 0.955 - ETA: 2s - loss: 0.2303 - acc: 0.956 - ETA: 2s - loss: 0.2268 - acc: 0.956 - ETA: 1s - loss: 0.2238 - acc: 0.956 - ETA: 1s - loss: 0.2266 - acc: 0.956 - ETA: 1s - loss: 0.2322 - acc: 0.956 - ETA: 1s - loss: 0.2300 - acc: 0.956 - ETA: 1s - loss: 0.2266 - acc: 0.957 - ETA: 1s - loss: 0.2297 - acc: 0.957 - ETA: 1s - loss: 0.2268 - acc: 0.957 - ETA: 1s - loss: 0.2302 - acc: 0.956 - ETA: 1s - loss: 0.2299 - acc: 0.956 - ETA: 1s - loss: 0.2293 - acc: 0.956 - ETA: 1s - loss: 0.2271 - acc: 0.957 - ETA: 1s - loss: 0.2248 - acc: 0.957 - ETA: 1s - loss: 0.2226 - acc: 0.957 - ETA: 1s - loss: 0.2206 - acc: 0.958 - ETA: 1s - loss: 0.2186 - acc: 0.958 - ETA: 1s - loss: 0.2160 - acc: 0.959 - ETA: 0s - loss: 0.2185 - acc: 0.959 - ETA: 0s - loss: 0.2210 - acc: 0.959 - ETA: 0s - loss: 0.2195 - acc: 0.958 - ETA: 0s - loss: 0.2187 - acc: 0.958 - ETA: 0s - loss: 0.2225 - acc: 0.958 - ETA: 0s - loss: 0.2249 - acc: 0.957 - ETA: 0s - loss: 0.2266 - acc: 0.958 - ETA: 0s - loss: 0.2244 - acc: 0.958 - ETA: 0s - loss: 0.2265 - acc: 0.958 - ETA: 0s - loss: 0.2242 - acc: 0.958 - ETA: 0s - loss: 0.2220 - acc: 0.958 - ETA: 0s - loss: 0.2240 - acc: 0.958 - ETA: 0s - loss: 0.2289 - acc: 0.958 - ETA: 0s - loss: 0.2261 - acc: 0.958 - ETA: 0s - loss: 0.2245 - acc: 0.958 - ETA: 0s - loss: 0.2275 - acc: 0.957 - 6s 1ms/step - loss: 0.2273 - acc: 0.9580 - val_loss: 0.4031 - val_acc: 0.9410\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0554 - acc: 1.000 - ETA: 6s - loss: 0.0858 - acc: 0.958 - ETA: 5s - loss: 0.0776 - acc: 0.958 - ETA: 5s - loss: 0.0536 - acc: 0.972 - ETA: 5s - loss: 0.0502 - acc: 0.974 - ETA: 5s - loss: 0.0580 - acc: 0.973 - ETA: 6s - loss: 0.0517 - acc: 0.977 - ETA: 6s - loss: 0.1060 - acc: 0.975 - ETA: 6s - loss: 0.1361 - acc: 0.971 - ETA: 6s - loss: 0.1241 - acc: 0.972 - ETA: 5s - loss: 0.1241 - acc: 0.968 - ETA: 5s - loss: 0.1199 - acc: 0.968 - ETA: 5s - loss: 0.1166 - acc: 0.968 - ETA: 5s - loss: 0.1120 - acc: 0.967 - ETA: 5s - loss: 0.1091 - acc: 0.967 - ETA: 5s - loss: 0.1086 - acc: 0.964 - ETA: 5s - loss: 0.1014 - acc: 0.967 - ETA: 5s - loss: 0.1398 - acc: 0.966 - ETA: 4s - loss: 0.2190 - acc: 0.960 - ETA: 4s - loss: 0.2066 - acc: 0.963 - ETA: 4s - loss: 0.2130 - acc: 0.964 - ETA: 4s - loss: 0.2037 - acc: 0.964 - ETA: 4s - loss: 0.2058 - acc: 0.963 - ETA: 4s - loss: 0.2012 - acc: 0.964 - ETA: 4s - loss: 0.1926 - acc: 0.965 - ETA: 4s - loss: 0.1892 - acc: 0.965 - ETA: 4s - loss: 0.1960 - acc: 0.966 - ETA: 4s - loss: 0.2002 - acc: 0.965 - ETA: 4s - loss: 0.1931 - acc: 0.967 - ETA: 3s - loss: 0.2025 - acc: 0.966 - ETA: 3s - loss: 0.2205 - acc: 0.963 - ETA: 3s - loss: 0.2159 - acc: 0.962 - ETA: 3s - loss: 0.2128 - acc: 0.963 - ETA: 3s - loss: 0.2125 - acc: 0.961 - ETA: 3s - loss: 0.2087 - acc: 0.962 - ETA: 3s - loss: 0.2044 - acc: 0.963 - ETA: 3s - loss: 0.2023 - acc: 0.962 - ETA: 3s - loss: 0.2121 - acc: 0.961 - ETA: 3s - loss: 0.2080 - acc: 0.961 - ETA: 3s - loss: 0.2150 - acc: 0.961 - ETA: 3s - loss: 0.2140 - acc: 0.960 - ETA: 3s - loss: 0.2205 - acc: 0.960 - ETA: 3s - loss: 0.2348 - acc: 0.959 - ETA: 3s - loss: 0.2287 - acc: 0.960 - ETA: 3s - loss: 0.2288 - acc: 0.960 - ETA: 3s - loss: 0.2361 - acc: 0.959 - ETA: 3s - loss: 0.2408 - acc: 0.959 - ETA: 3s - loss: 0.2471 - acc: 0.958 - ETA: 2s - loss: 0.2510 - acc: 0.958 - ETA: 2s - loss: 0.2467 - acc: 0.958 - ETA: 2s - loss: 0.2500 - acc: 0.958 - ETA: 2s - loss: 0.2447 - acc: 0.959 - ETA: 2s - loss: 0.2404 - acc: 0.959 - ETA: 2s - loss: 0.2428 - acc: 0.959 - ETA: 2s - loss: 0.2413 - acc: 0.959 - ETA: 2s - loss: 0.2373 - acc: 0.959 - ETA: 2s - loss: 0.2336 - acc: 0.959 - ETA: 2s - loss: 0.2353 - acc: 0.960 - ETA: 2s - loss: 0.2447 - acc: 0.959 - ETA: 2s - loss: 0.2706 - acc: 0.957 - ETA: 2s - loss: 0.2681 - acc: 0.957 - ETA: 1s - loss: 0.2636 - acc: 0.957 - ETA: 1s - loss: 0.2623 - acc: 0.957 - ETA: 1s - loss: 0.2674 - acc: 0.957 - ETA: 1s - loss: 0.2756 - acc: 0.956 - ETA: 1s - loss: 0.2772 - acc: 0.956 - ETA: 1s - loss: 0.2723 - acc: 0.957 - ETA: 1s - loss: 0.2707 - acc: 0.955 - ETA: 1s - loss: 0.2683 - acc: 0.956 - ETA: 1s - loss: 0.2718 - acc: 0.955 - ETA: 1s - loss: 0.2684 - acc: 0.955 - ETA: 1s - loss: 0.2716 - acc: 0.954 - ETA: 1s - loss: 0.2704 - acc: 0.955 - ETA: 1s - loss: 0.2773 - acc: 0.954 - ETA: 1s - loss: 0.2837 - acc: 0.953 - ETA: 0s - loss: 0.2805 - acc: 0.954 - ETA: 0s - loss: 0.2786 - acc: 0.954 - ETA: 0s - loss: 0.2803 - acc: 0.953 - ETA: 0s - loss: 0.2791 - acc: 0.953 - ETA: 0s - loss: 0.2814 - acc: 0.953 - ETA: 0s - loss: 0.2869 - acc: 0.953 - ETA: 0s - loss: 0.2864 - acc: 0.953 - ETA: 0s - loss: 0.2829 - acc: 0.954 - ETA: 0s - loss: 0.2862 - acc: 0.953 - ETA: 0s - loss: 0.2937 - acc: 0.953 - ETA: 0s - loss: 0.2908 - acc: 0.953 - ETA: 0s - loss: 0.2972 - acc: 0.952 - ETA: 0s - loss: 0.2951 - acc: 0.952 - ETA: 0s - loss: 0.3015 - acc: 0.951 - ETA: 0s - loss: 0.3024 - acc: 0.951 - ETA: 0s - loss: 0.3011 - acc: 0.951 - 6s 1ms/step - loss: 0.3012 - acc: 0.9513 - val_loss: 0.4854 - val_acc: 0.9218\n",
      "Epoch 65/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1612 - acc: 0.875 - ETA: 4s - loss: 0.0585 - acc: 0.968 - ETA: 5s - loss: 0.3387 - acc: 0.955 - ETA: 5s - loss: 0.2382 - acc: 0.968 - ETA: 4s - loss: 0.1958 - acc: 0.971 - ETA: 4s - loss: 0.1769 - acc: 0.964 - ETA: 4s - loss: 0.1716 - acc: 0.957 - ETA: 4s - loss: 0.1757 - acc: 0.951 - ETA: 4s - loss: 0.2007 - acc: 0.952 - ETA: 4s - loss: 0.1835 - acc: 0.957 - ETA: 4s - loss: 0.1685 - acc: 0.961 - ETA: 4s - loss: 0.1639 - acc: 0.963 - ETA: 4s - loss: 0.1525 - acc: 0.966 - ETA: 4s - loss: 0.1676 - acc: 0.967 - ETA: 4s - loss: 0.1991 - acc: 0.966 - ETA: 4s - loss: 0.1975 - acc: 0.963 - ETA: 4s - loss: 0.2090 - acc: 0.963 - ETA: 4s - loss: 0.1985 - acc: 0.965 - ETA: 4s - loss: 0.1920 - acc: 0.964 - ETA: 3s - loss: 0.1832 - acc: 0.965 - ETA: 3s - loss: 0.1769 - acc: 0.966 - ETA: 3s - loss: 0.1713 - acc: 0.965 - ETA: 3s - loss: 0.1650 - acc: 0.966 - ETA: 3s - loss: 0.1606 - acc: 0.966 - ETA: 3s - loss: 0.1578 - acc: 0.964 - ETA: 3s - loss: 0.1813 - acc: 0.963 - ETA: 3s - loss: 0.1777 - acc: 0.962 - ETA: 3s - loss: 0.1772 - acc: 0.962 - ETA: 3s - loss: 0.1797 - acc: 0.961 - ETA: 3s - loss: 0.1897 - acc: 0.958 - ETA: 3s - loss: 0.1872 - acc: 0.958 - ETA: 3s - loss: 0.1852 - acc: 0.957 - ETA: 3s - loss: 0.1839 - acc: 0.956 - ETA: 3s - loss: 0.1800 - acc: 0.956 - ETA: 3s - loss: 0.1762 - acc: 0.957 - ETA: 3s - loss: 0.1723 - acc: 0.958 - ETA: 2s - loss: 0.1693 - acc: 0.958 - ETA: 2s - loss: 0.1661 - acc: 0.959 - ETA: 2s - loss: 0.1830 - acc: 0.957 - ETA: 2s - loss: 0.1786 - acc: 0.958 - ETA: 2s - loss: 0.1828 - acc: 0.958 - ETA: 2s - loss: 0.1922 - acc: 0.958 - ETA: 2s - loss: 0.1886 - acc: 0.958 - ETA: 2s - loss: 0.1937 - acc: 0.959 - ETA: 2s - loss: 0.1917 - acc: 0.959 - ETA: 2s - loss: 0.1904 - acc: 0.959 - ETA: 2s - loss: 0.1889 - acc: 0.959 - ETA: 2s - loss: 0.1870 - acc: 0.959 - ETA: 2s - loss: 0.1838 - acc: 0.960 - ETA: 2s - loss: 0.1867 - acc: 0.959 - ETA: 2s - loss: 0.1880 - acc: 0.957 - ETA: 2s - loss: 0.1863 - acc: 0.957 - ETA: 2s - loss: 0.1856 - acc: 0.957 - ETA: 2s - loss: 0.1862 - acc: 0.956 - ETA: 2s - loss: 0.1834 - acc: 0.957 - ETA: 2s - loss: 0.1821 - acc: 0.957 - ETA: 2s - loss: 0.1866 - acc: 0.957 - ETA: 2s - loss: 0.1852 - acc: 0.957 - ETA: 2s - loss: 0.1854 - acc: 0.956 - ETA: 1s - loss: 0.1837 - acc: 0.956 - ETA: 1s - loss: 0.1814 - acc: 0.956 - ETA: 1s - loss: 0.1820 - acc: 0.956 - ETA: 1s - loss: 0.1845 - acc: 0.957 - ETA: 1s - loss: 0.1886 - acc: 0.956 - ETA: 1s - loss: 0.1946 - acc: 0.955 - ETA: 1s - loss: 0.1995 - acc: 0.954 - ETA: 1s - loss: 0.1984 - acc: 0.954 - ETA: 1s - loss: 0.2017 - acc: 0.954 - ETA: 1s - loss: 0.2016 - acc: 0.952 - ETA: 1s - loss: 0.2005 - acc: 0.952 - ETA: 1s - loss: 0.2045 - acc: 0.951 - ETA: 1s - loss: 0.2036 - acc: 0.951 - ETA: 1s - loss: 0.2070 - acc: 0.950 - ETA: 1s - loss: 0.2066 - acc: 0.949 - ETA: 0s - loss: 0.2051 - acc: 0.949 - ETA: 0s - loss: 0.2037 - acc: 0.949 - ETA: 0s - loss: 0.2045 - acc: 0.948 - ETA: 0s - loss: 0.2078 - acc: 0.947 - ETA: 0s - loss: 0.2057 - acc: 0.948 - ETA: 0s - loss: 0.2042 - acc: 0.947 - ETA: 0s - loss: 0.2036 - acc: 0.946 - ETA: 0s - loss: 0.2033 - acc: 0.947 - ETA: 0s - loss: 0.2063 - acc: 0.947 - ETA: 0s - loss: 0.2103 - acc: 0.945 - ETA: 0s - loss: 0.2091 - acc: 0.946 - ETA: 0s - loss: 0.2096 - acc: 0.944 - ETA: 0s - loss: 0.2097 - acc: 0.944 - ETA: 0s - loss: 0.2165 - acc: 0.943 - ETA: 0s - loss: 0.2170 - acc: 0.942 - ETA: 0s - loss: 0.2197 - acc: 0.942 - ETA: 0s - loss: 0.2187 - acc: 0.942 - ETA: 0s - loss: 0.2172 - acc: 0.942 - 6s 1ms/step - loss: 0.2170 - acc: 0.9427 - val_loss: 0.5442 - val_acc: 0.8942\n",
      "Epoch 66/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.2570 - acc: 0.812 - ETA: 6s - loss: 0.1466 - acc: 0.895 - ETA: 5s - loss: 0.1083 - acc: 0.947 - ETA: 5s - loss: 0.1152 - acc: 0.937 - ETA: 5s - loss: 0.1072 - acc: 0.942 - ETA: 5s - loss: 0.1643 - acc: 0.945 - ETA: 5s - loss: 0.1552 - acc: 0.944 - ETA: 5s - loss: 0.1441 - acc: 0.943 - ETA: 5s - loss: 0.1301 - acc: 0.951 - ETA: 5s - loss: 0.1247 - acc: 0.952 - ETA: 5s - loss: 0.1560 - acc: 0.948 - ETA: 5s - loss: 0.2144 - acc: 0.943 - ETA: 4s - loss: 0.2163 - acc: 0.937 - ETA: 4s - loss: 0.2073 - acc: 0.937 - ETA: 4s - loss: 0.2009 - acc: 0.937 - ETA: 4s - loss: 0.1988 - acc: 0.934 - ETA: 4s - loss: 0.1983 - acc: 0.936 - ETA: 4s - loss: 0.1934 - acc: 0.936 - ETA: 4s - loss: 0.2088 - acc: 0.932 - ETA: 4s - loss: 0.2233 - acc: 0.930 - ETA: 4s - loss: 0.2132 - acc: 0.934 - ETA: 4s - loss: 0.2236 - acc: 0.934 - ETA: 4s - loss: 0.2175 - acc: 0.935 - ETA: 4s - loss: 0.2098 - acc: 0.938 - ETA: 4s - loss: 0.2072 - acc: 0.938 - ETA: 4s - loss: 0.2006 - acc: 0.940 - ETA: 4s - loss: 0.2012 - acc: 0.940 - ETA: 4s - loss: 0.1956 - acc: 0.941 - ETA: 4s - loss: 0.1900 - acc: 0.943 - ETA: 3s - loss: 0.1888 - acc: 0.943 - ETA: 3s - loss: 0.1960 - acc: 0.942 - ETA: 3s - loss: 0.1946 - acc: 0.941 - ETA: 3s - loss: 0.2027 - acc: 0.942 - ETA: 3s - loss: 0.1995 - acc: 0.942 - ETA: 3s - loss: 0.1982 - acc: 0.941 - ETA: 3s - loss: 0.2050 - acc: 0.941 - ETA: 3s - loss: 0.2029 - acc: 0.940 - ETA: 3s - loss: 0.2043 - acc: 0.941 - ETA: 3s - loss: 0.2015 - acc: 0.941 - ETA: 3s - loss: 0.2066 - acc: 0.942 - ETA: 3s - loss: 0.2015 - acc: 0.943 - ETA: 3s - loss: 0.1992 - acc: 0.945 - ETA: 3s - loss: 0.1963 - acc: 0.945 - ETA: 3s - loss: 0.1921 - acc: 0.946 - ETA: 3s - loss: 0.1895 - acc: 0.947 - ETA: 3s - loss: 0.1850 - acc: 0.949 - ETA: 3s - loss: 0.1815 - acc: 0.949 - ETA: 2s - loss: 0.1948 - acc: 0.949 - ETA: 2s - loss: 0.1996 - acc: 0.949 - ETA: 2s - loss: 0.1994 - acc: 0.950 - ETA: 2s - loss: 0.2046 - acc: 0.950 - ETA: 2s - loss: 0.2033 - acc: 0.950 - ETA: 2s - loss: 0.2141 - acc: 0.951 - ETA: 2s - loss: 0.2120 - acc: 0.950 - ETA: 2s - loss: 0.2166 - acc: 0.950 - ETA: 2s - loss: 0.2156 - acc: 0.949 - ETA: 2s - loss: 0.2271 - acc: 0.949 - ETA: 2s - loss: 0.2258 - acc: 0.948 - ETA: 2s - loss: 0.2240 - acc: 0.949 - ETA: 2s - loss: 0.2227 - acc: 0.948 - ETA: 2s - loss: 0.2207 - acc: 0.949 - ETA: 2s - loss: 0.2172 - acc: 0.949 - ETA: 2s - loss: 0.2160 - acc: 0.950 - ETA: 2s - loss: 0.2208 - acc: 0.949 - ETA: 2s - loss: 0.2266 - acc: 0.948 - ETA: 2s - loss: 0.2237 - acc: 0.949 - ETA: 2s - loss: 0.2209 - acc: 0.949 - ETA: 1s - loss: 0.2195 - acc: 0.948 - ETA: 1s - loss: 0.2168 - acc: 0.948 - ETA: 1s - loss: 0.2140 - acc: 0.949 - ETA: 1s - loss: 0.2217 - acc: 0.949 - ETA: 1s - loss: 0.2246 - acc: 0.949 - ETA: 1s - loss: 0.2223 - acc: 0.949 - ETA: 1s - loss: 0.2195 - acc: 0.950 - ETA: 1s - loss: 0.2187 - acc: 0.951 - ETA: 1s - loss: 0.2165 - acc: 0.951 - ETA: 1s - loss: 0.2139 - acc: 0.951 - ETA: 1s - loss: 0.2113 - acc: 0.952 - ETA: 1s - loss: 0.2087 - acc: 0.953 - ETA: 1s - loss: 0.2058 - acc: 0.954 - ETA: 1s - loss: 0.2035 - acc: 0.954 - ETA: 0s - loss: 0.2031 - acc: 0.954 - ETA: 0s - loss: 0.2007 - acc: 0.955 - ETA: 0s - loss: 0.2027 - acc: 0.955 - ETA: 0s - loss: 0.2052 - acc: 0.955 - ETA: 0s - loss: 0.2074 - acc: 0.955 - ETA: 0s - loss: 0.2056 - acc: 0.955 - ETA: 0s - loss: 0.2100 - acc: 0.955 - ETA: 0s - loss: 0.2123 - acc: 0.954 - ETA: 0s - loss: 0.2142 - acc: 0.954 - ETA: 0s - loss: 0.2160 - acc: 0.955 - ETA: 0s - loss: 0.2134 - acc: 0.955 - ETA: 0s - loss: 0.2115 - acc: 0.956 - ETA: 0s - loss: 0.2107 - acc: 0.956 - ETA: 0s - loss: 0.2104 - acc: 0.955 - 6s 1ms/step - loss: 0.2138 - acc: 0.9550 - val_loss: 0.5196 - val_acc: 0.8872\n",
      "Epoch 67/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0703 - acc: 0.937 - ETA: 4s - loss: 0.1368 - acc: 0.921 - ETA: 4s - loss: 0.2668 - acc: 0.919 - ETA: 4s - loss: 0.3023 - acc: 0.925 - ETA: 4s - loss: 0.3093 - acc: 0.932 - ETA: 4s - loss: 0.2576 - acc: 0.941 - ETA: 4s - loss: 0.3473 - acc: 0.937 - ETA: 4s - loss: 0.3447 - acc: 0.937 - ETA: 5s - loss: 0.3184 - acc: 0.940 - ETA: 5s - loss: 0.2865 - acc: 0.945 - ETA: 4s - loss: 0.2614 - acc: 0.950 - ETA: 4s - loss: 0.2467 - acc: 0.949 - ETA: 4s - loss: 0.2311 - acc: 0.950 - ETA: 4s - loss: 0.2239 - acc: 0.949 - ETA: 4s - loss: 0.2125 - acc: 0.951 - ETA: 4s - loss: 0.2481 - acc: 0.950 - ETA: 4s - loss: 0.2382 - acc: 0.949 - ETA: 4s - loss: 0.2297 - acc: 0.949 - ETA: 4s - loss: 0.2210 - acc: 0.949 - ETA: 4s - loss: 0.2152 - acc: 0.950 - ETA: 4s - loss: 0.2074 - acc: 0.951 - ETA: 4s - loss: 0.2198 - acc: 0.950 - ETA: 4s - loss: 0.2151 - acc: 0.950 - ETA: 4s - loss: 0.2280 - acc: 0.949 - ETA: 4s - loss: 0.2220 - acc: 0.950 - ETA: 4s - loss: 0.2196 - acc: 0.948 - ETA: 3s - loss: 0.2128 - acc: 0.949 - ETA: 3s - loss: 0.2226 - acc: 0.947 - ETA: 3s - loss: 0.2427 - acc: 0.946 - ETA: 3s - loss: 0.2517 - acc: 0.942 - ETA: 3s - loss: 0.2616 - acc: 0.941 - ETA: 3s - loss: 0.2541 - acc: 0.941 - ETA: 3s - loss: 0.2495 - acc: 0.940 - ETA: 3s - loss: 0.2418 - acc: 0.942 - ETA: 3s - loss: 0.2533 - acc: 0.940 - ETA: 3s - loss: 0.2475 - acc: 0.941 - ETA: 3s - loss: 0.2420 - acc: 0.941 - ETA: 3s - loss: 0.2382 - acc: 0.941 - ETA: 3s - loss: 0.2420 - acc: 0.941 - ETA: 2s - loss: 0.2393 - acc: 0.941 - ETA: 2s - loss: 0.2340 - acc: 0.943 - ETA: 2s - loss: 0.2303 - acc: 0.943 - ETA: 2s - loss: 0.2268 - acc: 0.944 - ETA: 2s - loss: 0.2243 - acc: 0.944 - ETA: 2s - loss: 0.2217 - acc: 0.945 - ETA: 2s - loss: 0.2277 - acc: 0.944 - ETA: 2s - loss: 0.2271 - acc: 0.944 - ETA: 2s - loss: 0.2243 - acc: 0.944 - ETA: 2s - loss: 0.2204 - acc: 0.945 - ETA: 2s - loss: 0.2248 - acc: 0.944 - ETA: 2s - loss: 0.2221 - acc: 0.944 - ETA: 2s - loss: 0.2251 - acc: 0.944 - ETA: 2s - loss: 0.2234 - acc: 0.943 - ETA: 2s - loss: 0.2285 - acc: 0.942 - ETA: 2s - loss: 0.2308 - acc: 0.943 - ETA: 2s - loss: 0.2294 - acc: 0.944 - ETA: 2s - loss: 0.2262 - acc: 0.944 - ETA: 1s - loss: 0.2244 - acc: 0.944 - ETA: 1s - loss: 0.2211 - acc: 0.945 - ETA: 1s - loss: 0.2246 - acc: 0.945 - ETA: 1s - loss: 0.2227 - acc: 0.945 - ETA: 1s - loss: 0.2200 - acc: 0.945 - ETA: 1s - loss: 0.2168 - acc: 0.946 - ETA: 1s - loss: 0.2135 - acc: 0.947 - ETA: 1s - loss: 0.2220 - acc: 0.946 - ETA: 1s - loss: 0.2195 - acc: 0.947 - ETA: 1s - loss: 0.2224 - acc: 0.946 - ETA: 1s - loss: 0.2263 - acc: 0.946 - ETA: 1s - loss: 0.2293 - acc: 0.945 - ETA: 1s - loss: 0.2272 - acc: 0.945 - ETA: 1s - loss: 0.2296 - acc: 0.945 - ETA: 1s - loss: 0.2303 - acc: 0.945 - ETA: 1s - loss: 0.2296 - acc: 0.945 - ETA: 0s - loss: 0.2310 - acc: 0.944 - ETA: 0s - loss: 0.2338 - acc: 0.944 - ETA: 0s - loss: 0.2413 - acc: 0.943 - ETA: 0s - loss: 0.2392 - acc: 0.943 - ETA: 0s - loss: 0.2384 - acc: 0.943 - ETA: 0s - loss: 0.2358 - acc: 0.943 - ETA: 0s - loss: 0.2331 - acc: 0.944 - ETA: 0s - loss: 0.2349 - acc: 0.944 - ETA: 0s - loss: 0.2334 - acc: 0.944 - ETA: 0s - loss: 0.2369 - acc: 0.944 - ETA: 0s - loss: 0.2347 - acc: 0.944 - ETA: 0s - loss: 0.2328 - acc: 0.944 - ETA: 0s - loss: 0.2361 - acc: 0.944 - ETA: 0s - loss: 0.2422 - acc: 0.943 - ETA: 0s - loss: 0.2406 - acc: 0.943 - ETA: 0s - loss: 0.2386 - acc: 0.943 - 6s 1ms/step - loss: 0.2386 - acc: 0.9434 - val_loss: 0.5007 - val_acc: 0.9051\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 4s - loss: 0.0831 - acc: 0.937 - ETA: 4s - loss: 0.0855 - acc: 0.953 - ETA: 4s - loss: 0.0772 - acc: 0.946 - ETA: 4s - loss: 0.0640 - acc: 0.962 - ETA: 4s - loss: 0.1500 - acc: 0.951 - ETA: 4s - loss: 0.1290 - acc: 0.960 - ETA: 4s - loss: 0.1859 - acc: 0.950 - ETA: 4s - loss: 0.1853 - acc: 0.940 - ETA: 4s - loss: 0.2083 - acc: 0.942 - ETA: 4s - loss: 0.1988 - acc: 0.939 - ETA: 4s - loss: 0.1860 - acc: 0.943 - ETA: 4s - loss: 0.1772 - acc: 0.944 - ETA: 4s - loss: 0.1764 - acc: 0.945 - ETA: 4s - loss: 0.1702 - acc: 0.945 - ETA: 4s - loss: 0.1634 - acc: 0.946 - ETA: 4s - loss: 0.2011 - acc: 0.945 - ETA: 4s - loss: 0.1934 - acc: 0.946 - ETA: 3s - loss: 0.1884 - acc: 0.945 - ETA: 3s - loss: 0.1974 - acc: 0.947 - ETA: 3s - loss: 0.2069 - acc: 0.948 - ETA: 3s - loss: 0.2007 - acc: 0.946 - ETA: 3s - loss: 0.1929 - acc: 0.949 - ETA: 3s - loss: 0.1862 - acc: 0.950 - ETA: 3s - loss: 0.1835 - acc: 0.949 - ETA: 3s - loss: 0.2114 - acc: 0.947 - ETA: 3s - loss: 0.2074 - acc: 0.947 - ETA: 3s - loss: 0.2025 - acc: 0.949 - ETA: 3s - loss: 0.1979 - acc: 0.949 - ETA: 3s - loss: 0.2200 - acc: 0.945 - ETA: 3s - loss: 0.2158 - acc: 0.944 - ETA: 3s - loss: 0.2133 - acc: 0.945 - ETA: 3s - loss: 0.2292 - acc: 0.945 - ETA: 3s - loss: 0.2249 - acc: 0.945 - ETA: 3s - loss: 0.2296 - acc: 0.946 - ETA: 3s - loss: 0.2341 - acc: 0.946 - ETA: 3s - loss: 0.2285 - acc: 0.947 - ETA: 3s - loss: 0.2248 - acc: 0.948 - ETA: 3s - loss: 0.2398 - acc: 0.948 - ETA: 3s - loss: 0.2400 - acc: 0.947 - ETA: 3s - loss: 0.2390 - acc: 0.948 - ETA: 2s - loss: 0.2349 - acc: 0.948 - ETA: 2s - loss: 0.2380 - acc: 0.949 - ETA: 2s - loss: 0.2324 - acc: 0.950 - ETA: 2s - loss: 0.2277 - acc: 0.950 - ETA: 2s - loss: 0.2230 - acc: 0.951 - ETA: 2s - loss: 0.2264 - acc: 0.952 - ETA: 2s - loss: 0.2409 - acc: 0.950 - ETA: 2s - loss: 0.2375 - acc: 0.949 - ETA: 2s - loss: 0.2399 - acc: 0.950 - ETA: 2s - loss: 0.2433 - acc: 0.950 - ETA: 2s - loss: 0.2387 - acc: 0.951 - ETA: 2s - loss: 0.2415 - acc: 0.951 - ETA: 2s - loss: 0.2501 - acc: 0.951 - ETA: 2s - loss: 0.2459 - acc: 0.952 - ETA: 2s - loss: 0.2422 - acc: 0.953 - ETA: 2s - loss: 0.2477 - acc: 0.952 - ETA: 1s - loss: 0.2460 - acc: 0.952 - ETA: 1s - loss: 0.2445 - acc: 0.953 - ETA: 1s - loss: 0.2422 - acc: 0.953 - ETA: 1s - loss: 0.2396 - acc: 0.954 - ETA: 1s - loss: 0.2424 - acc: 0.952 - ETA: 1s - loss: 0.2388 - acc: 0.953 - ETA: 1s - loss: 0.2430 - acc: 0.952 - ETA: 1s - loss: 0.2412 - acc: 0.952 - ETA: 1s - loss: 0.2392 - acc: 0.952 - ETA: 1s - loss: 0.2369 - acc: 0.953 - ETA: 1s - loss: 0.2348 - acc: 0.953 - ETA: 1s - loss: 0.2379 - acc: 0.952 - ETA: 1s - loss: 0.2359 - acc: 0.952 - ETA: 1s - loss: 0.2329 - acc: 0.953 - ETA: 1s - loss: 0.2298 - acc: 0.953 - ETA: 1s - loss: 0.2276 - acc: 0.953 - ETA: 1s - loss: 0.2301 - acc: 0.953 - ETA: 1s - loss: 0.2283 - acc: 0.952 - ETA: 1s - loss: 0.2273 - acc: 0.952 - ETA: 1s - loss: 0.2254 - acc: 0.952 - ETA: 0s - loss: 0.2240 - acc: 0.953 - ETA: 0s - loss: 0.2230 - acc: 0.952 - ETA: 0s - loss: 0.2265 - acc: 0.952 - ETA: 0s - loss: 0.2251 - acc: 0.952 - ETA: 0s - loss: 0.2285 - acc: 0.952 - ETA: 0s - loss: 0.2319 - acc: 0.952 - ETA: 0s - loss: 0.2356 - acc: 0.951 - ETA: 0s - loss: 0.2339 - acc: 0.952 - ETA: 0s - loss: 0.2370 - acc: 0.952 - ETA: 0s - loss: 0.2341 - acc: 0.952 - ETA: 0s - loss: 0.2320 - acc: 0.952 - ETA: 0s - loss: 0.2304 - acc: 0.952 - ETA: 0s - loss: 0.2289 - acc: 0.952 - ETA: 0s - loss: 0.2309 - acc: 0.952 - ETA: 0s - loss: 0.2342 - acc: 0.952 - ETA: 0s - loss: 0.2364 - acc: 0.952 - ETA: 0s - loss: 0.2347 - acc: 0.952 - ETA: 0s - loss: 0.2330 - acc: 0.952 - 6s 1ms/step - loss: 0.2364 - acc: 0.9516 - val_loss: 0.3897 - val_acc: 0.8994\n",
      "Epoch 69/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0598 - acc: 1.000 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.1030 - acc: 0.946 - ETA: 4s - loss: 0.1973 - acc: 0.937 - ETA: 4s - loss: 0.1715 - acc: 0.942 - ETA: 4s - loss: 0.1567 - acc: 0.945 - ETA: 4s - loss: 0.1636 - acc: 0.927 - ETA: 4s - loss: 0.1651 - acc: 0.920 - ETA: 4s - loss: 0.2325 - acc: 0.922 - ETA: 4s - loss: 0.2130 - acc: 0.926 - ETA: 4s - loss: 0.2001 - acc: 0.927 - ETA: 4s - loss: 0.2208 - acc: 0.928 - ETA: 4s - loss: 0.2159 - acc: 0.930 - ETA: 4s - loss: 0.2290 - acc: 0.929 - ETA: 4s - loss: 0.2251 - acc: 0.923 - ETA: 4s - loss: 0.2623 - acc: 0.917 - ETA: 4s - loss: 0.2526 - acc: 0.919 - ETA: 4s - loss: 0.2485 - acc: 0.918 - ETA: 4s - loss: 0.2396 - acc: 0.921 - ETA: 4s - loss: 0.2489 - acc: 0.922 - ETA: 4s - loss: 0.2616 - acc: 0.919 - ETA: 4s - loss: 0.2674 - acc: 0.916 - ETA: 4s - loss: 0.2587 - acc: 0.918 - ETA: 4s - loss: 0.2544 - acc: 0.917 - ETA: 4s - loss: 0.2496 - acc: 0.918 - ETA: 3s - loss: 0.2469 - acc: 0.919 - ETA: 3s - loss: 0.2517 - acc: 0.920 - ETA: 3s - loss: 0.2450 - acc: 0.922 - ETA: 3s - loss: 0.2383 - acc: 0.923 - ETA: 3s - loss: 0.2464 - acc: 0.922 - ETA: 3s - loss: 0.2408 - acc: 0.924 - ETA: 3s - loss: 0.2373 - acc: 0.925 - ETA: 3s - loss: 0.2332 - acc: 0.927 - ETA: 3s - loss: 0.2290 - acc: 0.927 - ETA: 3s - loss: 0.2253 - acc: 0.929 - ETA: 3s - loss: 0.2303 - acc: 0.930 - ETA: 3s - loss: 0.2248 - acc: 0.931 - ETA: 3s - loss: 0.2396 - acc: 0.931 - ETA: 3s - loss: 0.2383 - acc: 0.931 - ETA: 3s - loss: 0.2363 - acc: 0.931 - ETA: 3s - loss: 0.2312 - acc: 0.933 - ETA: 2s - loss: 0.2261 - acc: 0.934 - ETA: 2s - loss: 0.2324 - acc: 0.934 - ETA: 2s - loss: 0.2293 - acc: 0.935 - ETA: 2s - loss: 0.2281 - acc: 0.934 - ETA: 2s - loss: 0.2324 - acc: 0.934 - ETA: 2s - loss: 0.2303 - acc: 0.933 - ETA: 2s - loss: 0.2343 - acc: 0.934 - ETA: 2s - loss: 0.2315 - acc: 0.934 - ETA: 2s - loss: 0.2280 - acc: 0.934 - ETA: 2s - loss: 0.2388 - acc: 0.933 - ETA: 2s - loss: 0.2450 - acc: 0.933 - ETA: 2s - loss: 0.2509 - acc: 0.932 - ETA: 2s - loss: 0.2480 - acc: 0.932 - ETA: 2s - loss: 0.2449 - acc: 0.932 - ETA: 2s - loss: 0.2409 - acc: 0.933 - ETA: 1s - loss: 0.2437 - acc: 0.933 - ETA: 1s - loss: 0.2405 - acc: 0.933 - ETA: 1s - loss: 0.2377 - acc: 0.934 - ETA: 1s - loss: 0.2346 - acc: 0.934 - ETA: 1s - loss: 0.2405 - acc: 0.934 - ETA: 1s - loss: 0.2383 - acc: 0.935 - ETA: 1s - loss: 0.2371 - acc: 0.934 - ETA: 1s - loss: 0.2362 - acc: 0.934 - ETA: 1s - loss: 0.2349 - acc: 0.934 - ETA: 1s - loss: 0.2343 - acc: 0.933 - ETA: 1s - loss: 0.2318 - acc: 0.934 - ETA: 1s - loss: 0.2341 - acc: 0.934 - ETA: 1s - loss: 0.2328 - acc: 0.934 - ETA: 1s - loss: 0.2415 - acc: 0.934 - ETA: 1s - loss: 0.2406 - acc: 0.934 - ETA: 1s - loss: 0.2387 - acc: 0.935 - ETA: 1s - loss: 0.2367 - acc: 0.935 - ETA: 1s - loss: 0.2336 - acc: 0.936 - ETA: 1s - loss: 0.2407 - acc: 0.936 - ETA: 1s - loss: 0.2388 - acc: 0.936 - ETA: 0s - loss: 0.2366 - acc: 0.936 - ETA: 0s - loss: 0.2381 - acc: 0.936 - ETA: 0s - loss: 0.2363 - acc: 0.937 - ETA: 0s - loss: 0.2382 - acc: 0.938 - ETA: 0s - loss: 0.2372 - acc: 0.938 - ETA: 0s - loss: 0.2353 - acc: 0.938 - ETA: 0s - loss: 0.2334 - acc: 0.938 - ETA: 0s - loss: 0.2352 - acc: 0.938 - ETA: 0s - loss: 0.2333 - acc: 0.939 - ETA: 0s - loss: 0.2313 - acc: 0.939 - ETA: 0s - loss: 0.2294 - acc: 0.939 - ETA: 0s - loss: 0.2277 - acc: 0.939 - ETA: 0s - loss: 0.2251 - acc: 0.940 - ETA: 0s - loss: 0.2242 - acc: 0.941 - ETA: 0s - loss: 0.2220 - acc: 0.941 - 6s 1ms/step - loss: 0.2207 - acc: 0.9415 - val_loss: 0.3590 - val_acc: 0.9237\n",
      "Epoch 70/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0311 - acc: 1.000 - ETA: 5s - loss: 0.3766 - acc: 0.953 - ETA: 5s - loss: 0.2901 - acc: 0.946 - ETA: 5s - loss: 0.3783 - acc: 0.931 - ETA: 5s - loss: 0.3852 - acc: 0.932 - ETA: 4s - loss: 0.3358 - acc: 0.933 - ETA: 4s - loss: 0.2910 - acc: 0.944 - ETA: 4s - loss: 0.2601 - acc: 0.946 - ETA: 4s - loss: 0.2479 - acc: 0.940 - ETA: 4s - loss: 0.2450 - acc: 0.930 - ETA: 4s - loss: 0.2323 - acc: 0.935 - ETA: 4s - loss: 0.2223 - acc: 0.935 - ETA: 4s - loss: 0.2132 - acc: 0.937 - ETA: 4s - loss: 0.2013 - acc: 0.940 - ETA: 4s - loss: 0.1958 - acc: 0.940 - ETA: 4s - loss: 0.1888 - acc: 0.941 - ETA: 4s - loss: 0.1834 - acc: 0.940 - ETA: 4s - loss: 0.1795 - acc: 0.937 - ETA: 4s - loss: 0.2106 - acc: 0.935 - ETA: 4s - loss: 0.2097 - acc: 0.933 - ETA: 3s - loss: 0.2033 - acc: 0.933 - ETA: 3s - loss: 0.1976 - acc: 0.933 - ETA: 3s - loss: 0.2201 - acc: 0.934 - ETA: 3s - loss: 0.2306 - acc: 0.932 - ETA: 3s - loss: 0.2248 - acc: 0.933 - ETA: 3s - loss: 0.2371 - acc: 0.931 - ETA: 3s - loss: 0.2365 - acc: 0.930 - ETA: 3s - loss: 0.2420 - acc: 0.929 - ETA: 3s - loss: 0.2550 - acc: 0.930 - ETA: 3s - loss: 0.2485 - acc: 0.932 - ETA: 3s - loss: 0.2433 - acc: 0.933 - ETA: 3s - loss: 0.2386 - acc: 0.934 - ETA: 3s - loss: 0.2378 - acc: 0.934 - ETA: 3s - loss: 0.2337 - acc: 0.935 - ETA: 3s - loss: 0.2381 - acc: 0.936 - ETA: 3s - loss: 0.2333 - acc: 0.938 - ETA: 2s - loss: 0.2495 - acc: 0.937 - ETA: 2s - loss: 0.2453 - acc: 0.938 - ETA: 2s - loss: 0.2431 - acc: 0.939 - ETA: 2s - loss: 0.2398 - acc: 0.939 - ETA: 2s - loss: 0.2456 - acc: 0.939 - ETA: 2s - loss: 0.2420 - acc: 0.939 - ETA: 2s - loss: 0.2470 - acc: 0.940 - ETA: 2s - loss: 0.2421 - acc: 0.941 - ETA: 2s - loss: 0.2382 - acc: 0.940 - ETA: 2s - loss: 0.2341 - acc: 0.941 - ETA: 2s - loss: 0.2311 - acc: 0.941 - ETA: 2s - loss: 0.2281 - acc: 0.941 - ETA: 2s - loss: 0.2246 - acc: 0.942 - ETA: 2s - loss: 0.2293 - acc: 0.942 - ETA: 2s - loss: 0.2318 - acc: 0.943 - ETA: 2s - loss: 0.2273 - acc: 0.944 - ETA: 2s - loss: 0.2298 - acc: 0.945 - ETA: 2s - loss: 0.2265 - acc: 0.944 - ETA: 2s - loss: 0.2250 - acc: 0.944 - ETA: 1s - loss: 0.2280 - acc: 0.944 - ETA: 1s - loss: 0.2254 - acc: 0.945 - ETA: 1s - loss: 0.2246 - acc: 0.945 - ETA: 1s - loss: 0.2215 - acc: 0.945 - ETA: 1s - loss: 0.2181 - acc: 0.946 - ETA: 1s - loss: 0.2162 - acc: 0.947 - ETA: 1s - loss: 0.2136 - acc: 0.947 - ETA: 1s - loss: 0.2109 - acc: 0.948 - ETA: 1s - loss: 0.2079 - acc: 0.948 - ETA: 1s - loss: 0.2176 - acc: 0.948 - ETA: 1s - loss: 0.2150 - acc: 0.949 - ETA: 1s - loss: 0.2151 - acc: 0.948 - ETA: 1s - loss: 0.2124 - acc: 0.949 - ETA: 1s - loss: 0.2113 - acc: 0.949 - ETA: 1s - loss: 0.2101 - acc: 0.949 - ETA: 1s - loss: 0.2074 - acc: 0.950 - ETA: 1s - loss: 0.2064 - acc: 0.949 - ETA: 0s - loss: 0.2037 - acc: 0.950 - ETA: 0s - loss: 0.2021 - acc: 0.950 - ETA: 0s - loss: 0.2008 - acc: 0.950 - ETA: 0s - loss: 0.1996 - acc: 0.950 - ETA: 0s - loss: 0.1981 - acc: 0.951 - ETA: 0s - loss: 0.1970 - acc: 0.951 - ETA: 0s - loss: 0.2048 - acc: 0.950 - ETA: 0s - loss: 0.2027 - acc: 0.950 - ETA: 0s - loss: 0.2008 - acc: 0.951 - ETA: 0s - loss: 0.1984 - acc: 0.951 - ETA: 0s - loss: 0.2009 - acc: 0.951 - ETA: 0s - loss: 0.2036 - acc: 0.951 - ETA: 0s - loss: 0.2026 - acc: 0.951 - ETA: 0s - loss: 0.2003 - acc: 0.952 - ETA: 0s - loss: 0.1984 - acc: 0.952 - ETA: 0s - loss: 0.1966 - acc: 0.952 - ETA: 0s - loss: 0.1946 - acc: 0.953 - ETA: 0s - loss: 0.1933 - acc: 0.953 - ETA: 0s - loss: 0.1927 - acc: 0.953 - ETA: 0s - loss: 0.1918 - acc: 0.953 - 6s 1ms/step - loss: 0.1917 - acc: 0.9535 - val_loss: 0.3801 - val_acc: 0.9372\n",
      "Epoch 71/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0074 - acc: 0.937 - ETA: 7s - loss: 0.3978 - acc: 0.958 - ETA: 7s - loss: 0.2447 - acc: 0.975 - ETA: 7s - loss: 0.1964 - acc: 0.973 - ETA: 7s - loss: 0.1589 - acc: 0.979 - ETA: 7s - loss: 0.1388 - acc: 0.977 - ETA: 6s - loss: 0.1204 - acc: 0.973 - ETA: 6s - loss: 0.1091 - acc: 0.972 - ETA: 6s - loss: 0.1529 - acc: 0.972 - ETA: 6s - loss: 0.1886 - acc: 0.968 - ETA: 6s - loss: 0.1653 - acc: 0.972 - ETA: 6s - loss: 0.1584 - acc: 0.974 - ETA: 6s - loss: 0.1991 - acc: 0.968 - ETA: 6s - loss: 0.1911 - acc: 0.965 - ETA: 5s - loss: 0.1745 - acc: 0.968 - ETA: 5s - loss: 0.1653 - acc: 0.967 - ETA: 5s - loss: 0.1611 - acc: 0.967 - ETA: 5s - loss: 0.1520 - acc: 0.969 - ETA: 5s - loss: 0.1642 - acc: 0.965 - ETA: 5s - loss: 0.1643 - acc: 0.965 - ETA: 5s - loss: 0.1689 - acc: 0.964 - ETA: 5s - loss: 0.1972 - acc: 0.960 - ETA: 5s - loss: 0.1934 - acc: 0.961 - ETA: 5s - loss: 0.2064 - acc: 0.959 - ETA: 4s - loss: 0.2002 - acc: 0.960 - ETA: 4s - loss: 0.2244 - acc: 0.960 - ETA: 4s - loss: 0.2326 - acc: 0.960 - ETA: 4s - loss: 0.2459 - acc: 0.959 - ETA: 4s - loss: 0.2406 - acc: 0.959 - ETA: 4s - loss: 0.2348 - acc: 0.960 - ETA: 4s - loss: 0.2283 - acc: 0.959 - ETA: 4s - loss: 0.2469 - acc: 0.959 - ETA: 4s - loss: 0.2514 - acc: 0.959 - ETA: 4s - loss: 0.2448 - acc: 0.959 - ETA: 4s - loss: 0.2415 - acc: 0.959 - ETA: 4s - loss: 0.2476 - acc: 0.958 - ETA: 3s - loss: 0.2530 - acc: 0.957 - ETA: 3s - loss: 0.2463 - acc: 0.958 - ETA: 3s - loss: 0.2438 - acc: 0.957 - ETA: 3s - loss: 0.2392 - acc: 0.957 - ETA: 3s - loss: 0.2351 - acc: 0.956 - ETA: 3s - loss: 0.2294 - acc: 0.957 - ETA: 3s - loss: 0.2253 - acc: 0.956 - ETA: 3s - loss: 0.2300 - acc: 0.956 - ETA: 3s - loss: 0.2254 - acc: 0.957 - ETA: 3s - loss: 0.2296 - acc: 0.958 - ETA: 3s - loss: 0.2244 - acc: 0.958 - ETA: 2s - loss: 0.2291 - acc: 0.957 - ETA: 2s - loss: 0.2250 - acc: 0.958 - ETA: 2s - loss: 0.2213 - acc: 0.958 - ETA: 2s - loss: 0.2180 - acc: 0.958 - ETA: 2s - loss: 0.2133 - acc: 0.958 - ETA: 2s - loss: 0.2099 - acc: 0.958 - ETA: 2s - loss: 0.2072 - acc: 0.958 - ETA: 2s - loss: 0.2052 - acc: 0.959 - ETA: 2s - loss: 0.2156 - acc: 0.959 - ETA: 2s - loss: 0.2144 - acc: 0.959 - ETA: 2s - loss: 0.2119 - acc: 0.959 - ETA: 2s - loss: 0.2108 - acc: 0.959 - ETA: 2s - loss: 0.2080 - acc: 0.959 - ETA: 2s - loss: 0.2057 - acc: 0.959 - ETA: 2s - loss: 0.2064 - acc: 0.958 - ETA: 2s - loss: 0.2035 - acc: 0.959 - ETA: 2s - loss: 0.2038 - acc: 0.958 - ETA: 2s - loss: 0.2078 - acc: 0.958 - ETA: 1s - loss: 0.2078 - acc: 0.958 - ETA: 1s - loss: 0.2062 - acc: 0.958 - ETA: 1s - loss: 0.2058 - acc: 0.957 - ETA: 1s - loss: 0.2047 - acc: 0.956 - ETA: 1s - loss: 0.2030 - acc: 0.957 - ETA: 1s - loss: 0.2021 - acc: 0.957 - ETA: 1s - loss: 0.2031 - acc: 0.956 - ETA: 1s - loss: 0.2022 - acc: 0.955 - ETA: 1s - loss: 0.2022 - acc: 0.955 - ETA: 1s - loss: 0.2027 - acc: 0.954 - ETA: 1s - loss: 0.2073 - acc: 0.954 - ETA: 1s - loss: 0.2058 - acc: 0.954 - ETA: 1s - loss: 0.2048 - acc: 0.954 - ETA: 1s - loss: 0.2031 - acc: 0.954 - ETA: 1s - loss: 0.2031 - acc: 0.953 - ETA: 1s - loss: 0.2013 - acc: 0.953 - ETA: 1s - loss: 0.2008 - acc: 0.954 - ETA: 1s - loss: 0.1994 - acc: 0.954 - ETA: 1s - loss: 0.2030 - acc: 0.954 - ETA: 1s - loss: 0.2073 - acc: 0.954 - ETA: 0s - loss: 0.2075 - acc: 0.954 - ETA: 0s - loss: 0.2057 - acc: 0.954 - ETA: 0s - loss: 0.2047 - acc: 0.954 - ETA: 0s - loss: 0.2043 - acc: 0.954 - ETA: 0s - loss: 0.2054 - acc: 0.954 - ETA: 0s - loss: 0.2040 - acc: 0.954 - ETA: 0s - loss: 0.2025 - acc: 0.955 - ETA: 0s - loss: 0.2009 - acc: 0.955 - ETA: 0s - loss: 0.2004 - acc: 0.956 - ETA: 0s - loss: 0.1991 - acc: 0.956 - ETA: 0s - loss: 0.1974 - acc: 0.956 - ETA: 0s - loss: 0.1996 - acc: 0.955 - ETA: 0s - loss: 0.2031 - acc: 0.955 - ETA: 0s - loss: 0.2025 - acc: 0.955 - ETA: 0s - loss: 0.2017 - acc: 0.955 - ETA: 0s - loss: 0.2000 - acc: 0.955 - ETA: 0s - loss: 0.1992 - acc: 0.955 - 7s 2ms/step - loss: 0.2027 - acc: 0.9550 - val_loss: 0.3578 - val_acc: 0.9295\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.2467 - acc: 0.875 - ETA: 6s - loss: 0.2211 - acc: 0.921 - ETA: 6s - loss: 0.1697 - acc: 0.937 - ETA: 7s - loss: 0.1466 - acc: 0.945 - ETA: 7s - loss: 0.1465 - acc: 0.937 - ETA: 7s - loss: 0.1500 - acc: 0.937 - ETA: 8s - loss: 0.1557 - acc: 0.937 - ETA: 8s - loss: 0.1510 - acc: 0.942 - ETA: 8s - loss: 0.1427 - acc: 0.941 - ETA: 8s - loss: 0.1432 - acc: 0.941 - ETA: 8s - loss: 0.1333 - acc: 0.947 - ETA: 8s - loss: 0.1301 - acc: 0.949 - ETA: 8s - loss: 0.1286 - acc: 0.948 - ETA: 9s - loss: 0.1219 - acc: 0.950 - ETA: 8s - loss: 0.1559 - acc: 0.949 - ETA: 8s - loss: 0.1543 - acc: 0.948 - ETA: 8s - loss: 0.1646 - acc: 0.950 - ETA: 8s - loss: 0.1569 - acc: 0.953 - ETA: 8s - loss: 0.1526 - acc: 0.954 - ETA: 8s - loss: 0.1480 - acc: 0.956 - ETA: 8s - loss: 0.1473 - acc: 0.956 - ETA: 8s - loss: 0.1429 - acc: 0.956 - ETA: 8s - loss: 0.1413 - acc: 0.955 - ETA: 7s - loss: 0.1393 - acc: 0.954 - ETA: 7s - loss: 0.1597 - acc: 0.954 - ETA: 7s - loss: 0.1596 - acc: 0.952 - ETA: 7s - loss: 0.1552 - acc: 0.952 - ETA: 7s - loss: 0.1539 - acc: 0.950 - ETA: 7s - loss: 0.1504 - acc: 0.952 - ETA: 7s - loss: 0.1487 - acc: 0.952 - ETA: 6s - loss: 0.1458 - acc: 0.953 - ETA: 6s - loss: 0.1433 - acc: 0.954 - ETA: 6s - loss: 0.1427 - acc: 0.954 - ETA: 6s - loss: 0.1417 - acc: 0.953 - ETA: 6s - loss: 0.1384 - acc: 0.953 - ETA: 5s - loss: 0.1426 - acc: 0.952 - ETA: 5s - loss: 0.1570 - acc: 0.950 - ETA: 5s - loss: 0.1742 - acc: 0.949 - ETA: 5s - loss: 0.1735 - acc: 0.948 - ETA: 5s - loss: 0.1863 - acc: 0.946 - ETA: 5s - loss: 0.1854 - acc: 0.946 - ETA: 5s - loss: 0.1839 - acc: 0.945 - ETA: 5s - loss: 0.1845 - acc: 0.944 - ETA: 5s - loss: 0.1818 - acc: 0.945 - ETA: 5s - loss: 0.1809 - acc: 0.944 - ETA: 5s - loss: 0.1793 - acc: 0.944 - ETA: 5s - loss: 0.1802 - acc: 0.944 - ETA: 5s - loss: 0.1773 - acc: 0.946 - ETA: 5s - loss: 0.1801 - acc: 0.944 - ETA: 5s - loss: 0.1880 - acc: 0.944 - ETA: 4s - loss: 0.1858 - acc: 0.944 - ETA: 4s - loss: 0.1859 - acc: 0.944 - ETA: 4s - loss: 0.1822 - acc: 0.945 - ETA: 4s - loss: 0.1797 - acc: 0.944 - ETA: 4s - loss: 0.1991 - acc: 0.944 - ETA: 4s - loss: 0.1973 - acc: 0.943 - ETA: 4s - loss: 0.1932 - acc: 0.944 - ETA: 4s - loss: 0.1913 - acc: 0.944 - ETA: 4s - loss: 0.1902 - acc: 0.945 - ETA: 4s - loss: 0.1894 - acc: 0.945 - ETA: 4s - loss: 0.1874 - acc: 0.946 - ETA: 4s - loss: 0.1856 - acc: 0.946 - ETA: 4s - loss: 0.1927 - acc: 0.945 - ETA: 3s - loss: 0.1916 - acc: 0.945 - ETA: 3s - loss: 0.1919 - acc: 0.946 - ETA: 3s - loss: 0.1915 - acc: 0.945 - ETA: 3s - loss: 0.1919 - acc: 0.945 - ETA: 3s - loss: 0.1909 - acc: 0.945 - ETA: 3s - loss: 0.1961 - acc: 0.945 - ETA: 3s - loss: 0.1947 - acc: 0.945 - ETA: 3s - loss: 0.1979 - acc: 0.945 - ETA: 3s - loss: 0.1957 - acc: 0.945 - ETA: 3s - loss: 0.1939 - acc: 0.946 - ETA: 3s - loss: 0.1950 - acc: 0.946 - ETA: 3s - loss: 0.1956 - acc: 0.946 - ETA: 3s - loss: 0.1941 - acc: 0.946 - ETA: 3s - loss: 0.1922 - acc: 0.947 - ETA: 3s - loss: 0.1902 - acc: 0.947 - ETA: 3s - loss: 0.1895 - acc: 0.947 - ETA: 3s - loss: 0.1883 - acc: 0.948 - ETA: 2s - loss: 0.1864 - acc: 0.948 - ETA: 2s - loss: 0.1862 - acc: 0.948 - ETA: 2s - loss: 0.1848 - acc: 0.949 - ETA: 2s - loss: 0.1887 - acc: 0.948 - ETA: 2s - loss: 0.1872 - acc: 0.948 - ETA: 2s - loss: 0.1916 - acc: 0.948 - ETA: 2s - loss: 0.1914 - acc: 0.948 - ETA: 2s - loss: 0.1900 - acc: 0.948 - ETA: 2s - loss: 0.1944 - acc: 0.949 - ETA: 2s - loss: 0.1940 - acc: 0.948 - ETA: 2s - loss: 0.1923 - acc: 0.949 - ETA: 2s - loss: 0.1904 - acc: 0.949 - ETA: 2s - loss: 0.1942 - acc: 0.949 - ETA: 2s - loss: 0.1937 - acc: 0.949 - ETA: 2s - loss: 0.1920 - acc: 0.950 - ETA: 1s - loss: 0.1916 - acc: 0.950 - ETA: 1s - loss: 0.1907 - acc: 0.950 - ETA: 1s - loss: 0.1974 - acc: 0.949 - ETA: 1s - loss: 0.1964 - acc: 0.949 - ETA: 1s - loss: 0.1947 - acc: 0.949 - ETA: 1s - loss: 0.1932 - acc: 0.950 - ETA: 1s - loss: 0.1919 - acc: 0.950 - ETA: 1s - loss: 0.1952 - acc: 0.950 - ETA: 1s - loss: 0.1942 - acc: 0.950 - ETA: 1s - loss: 0.1920 - acc: 0.950 - ETA: 1s - loss: 0.1957 - acc: 0.950 - ETA: 1s - loss: 0.1942 - acc: 0.950 - ETA: 1s - loss: 0.1975 - acc: 0.949 - ETA: 0s - loss: 0.1959 - acc: 0.949 - ETA: 0s - loss: 0.1952 - acc: 0.949 - ETA: 0s - loss: 0.1982 - acc: 0.949 - ETA: 0s - loss: 0.2006 - acc: 0.949 - ETA: 0s - loss: 0.1983 - acc: 0.949 - ETA: 0s - loss: 0.1967 - acc: 0.949 - ETA: 0s - loss: 0.1954 - acc: 0.949 - ETA: 0s - loss: 0.1988 - acc: 0.949 - ETA: 0s - loss: 0.1966 - acc: 0.950 - ETA: 0s - loss: 0.1949 - acc: 0.950 - 8s 2ms/step - loss: 0.1941 - acc: 0.9506 - val_loss: 0.4132 - val_acc: 0.9397\n",
      "Epoch 73/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1978 - acc: 0.875 - ETA: 4s - loss: 0.0617 - acc: 0.953 - ETA: 4s - loss: 0.0598 - acc: 0.964 - ETA: 4s - loss: 0.0865 - acc: 0.943 - ETA: 5s - loss: 0.0778 - acc: 0.953 - ETA: 5s - loss: 0.0753 - acc: 0.955 - ETA: 5s - loss: 0.0682 - acc: 0.959 - ETA: 5s - loss: 0.0682 - acc: 0.960 - ETA: 5s - loss: 0.1158 - acc: 0.957 - ETA: 5s - loss: 0.1124 - acc: 0.957 - ETA: 5s - loss: 0.1069 - acc: 0.957 - ETA: 4s - loss: 0.1146 - acc: 0.955 - ETA: 4s - loss: 0.1351 - acc: 0.955 - ETA: 4s - loss: 0.1301 - acc: 0.956 - ETA: 4s - loss: 0.1284 - acc: 0.954 - ETA: 4s - loss: 0.1246 - acc: 0.955 - ETA: 4s - loss: 0.1289 - acc: 0.954 - ETA: 4s - loss: 0.1256 - acc: 0.954 - ETA: 4s - loss: 0.1219 - acc: 0.956 - ETA: 4s - loss: 0.1345 - acc: 0.957 - ETA: 4s - loss: 0.1289 - acc: 0.959 - ETA: 4s - loss: 0.1269 - acc: 0.959 - ETA: 4s - loss: 0.1216 - acc: 0.961 - ETA: 4s - loss: 0.1209 - acc: 0.962 - ETA: 4s - loss: 0.1354 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.961 - ETA: 4s - loss: 0.1296 - acc: 0.962 - ETA: 4s - loss: 0.1277 - acc: 0.962 - ETA: 3s - loss: 0.1251 - acc: 0.962 - ETA: 3s - loss: 0.1244 - acc: 0.962 - ETA: 3s - loss: 0.1211 - acc: 0.964 - ETA: 3s - loss: 0.1225 - acc: 0.963 - ETA: 3s - loss: 0.1204 - acc: 0.964 - ETA: 3s - loss: 0.1211 - acc: 0.962 - ETA: 3s - loss: 0.1197 - acc: 0.961 - ETA: 3s - loss: 0.1311 - acc: 0.960 - ETA: 3s - loss: 0.1288 - acc: 0.961 - ETA: 3s - loss: 0.1267 - acc: 0.962 - ETA: 3s - loss: 0.1250 - acc: 0.962 - ETA: 3s - loss: 0.1243 - acc: 0.962 - ETA: 3s - loss: 0.1229 - acc: 0.962 - ETA: 3s - loss: 0.1212 - acc: 0.962 - ETA: 3s - loss: 0.1205 - acc: 0.962 - ETA: 2s - loss: 0.1183 - acc: 0.963 - ETA: 2s - loss: 0.1248 - acc: 0.963 - ETA: 2s - loss: 0.1239 - acc: 0.963 - ETA: 2s - loss: 0.1224 - acc: 0.963 - ETA: 2s - loss: 0.1228 - acc: 0.963 - ETA: 2s - loss: 0.1220 - acc: 0.963 - ETA: 2s - loss: 0.1223 - acc: 0.962 - ETA: 2s - loss: 0.1212 - acc: 0.962 - ETA: 2s - loss: 0.1268 - acc: 0.962 - ETA: 2s - loss: 0.1260 - acc: 0.962 - ETA: 2s - loss: 0.1317 - acc: 0.961 - ETA: 2s - loss: 0.1298 - acc: 0.962 - ETA: 2s - loss: 0.1351 - acc: 0.961 - ETA: 2s - loss: 0.1341 - acc: 0.961 - ETA: 2s - loss: 0.1384 - acc: 0.962 - ETA: 1s - loss: 0.1361 - acc: 0.962 - ETA: 1s - loss: 0.1414 - acc: 0.961 - ETA: 1s - loss: 0.1392 - acc: 0.962 - ETA: 1s - loss: 0.1381 - acc: 0.962 - ETA: 1s - loss: 0.1429 - acc: 0.962 - ETA: 1s - loss: 0.1415 - acc: 0.962 - ETA: 1s - loss: 0.1462 - acc: 0.961 - ETA: 1s - loss: 0.1567 - acc: 0.960 - ETA: 1s - loss: 0.1558 - acc: 0.959 - ETA: 1s - loss: 0.1591 - acc: 0.959 - ETA: 1s - loss: 0.1580 - acc: 0.959 - ETA: 1s - loss: 0.1559 - acc: 0.960 - ETA: 1s - loss: 0.1544 - acc: 0.960 - ETA: 1s - loss: 0.1529 - acc: 0.960 - ETA: 1s - loss: 0.1512 - acc: 0.961 - ETA: 0s - loss: 0.1511 - acc: 0.961 - ETA: 0s - loss: 0.1540 - acc: 0.961 - ETA: 0s - loss: 0.1575 - acc: 0.961 - ETA: 0s - loss: 0.1578 - acc: 0.961 - ETA: 0s - loss: 0.1579 - acc: 0.961 - ETA: 0s - loss: 0.1566 - acc: 0.961 - ETA: 0s - loss: 0.1553 - acc: 0.961 - ETA: 0s - loss: 0.1541 - acc: 0.961 - ETA: 0s - loss: 0.1555 - acc: 0.960 - ETA: 0s - loss: 0.1588 - acc: 0.960 - ETA: 0s - loss: 0.1637 - acc: 0.959 - ETA: 0s - loss: 0.1620 - acc: 0.960 - ETA: 0s - loss: 0.1648 - acc: 0.959 - ETA: 0s - loss: 0.1640 - acc: 0.959 - ETA: 0s - loss: 0.1629 - acc: 0.959 - ETA: 0s - loss: 0.1661 - acc: 0.959 - 6s 1ms/step - loss: 0.1659 - acc: 0.9592 - val_loss: 0.3958 - val_acc: 0.9391\n",
      "Epoch 74/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0684 - acc: 0.937 - ETA: 5s - loss: 0.3336 - acc: 0.937 - ETA: 5s - loss: 0.3451 - acc: 0.955 - ETA: 5s - loss: 0.2846 - acc: 0.951 - ETA: 5s - loss: 0.2439 - acc: 0.954 - ETA: 6s - loss: 0.2162 - acc: 0.956 - ETA: 6s - loss: 0.1938 - acc: 0.958 - ETA: 6s - loss: 0.2119 - acc: 0.959 - ETA: 6s - loss: 0.1973 - acc: 0.960 - ETA: 5s - loss: 0.1744 - acc: 0.965 - ETA: 5s - loss: 0.1760 - acc: 0.958 - ETA: 5s - loss: 0.1625 - acc: 0.958 - ETA: 5s - loss: 0.1479 - acc: 0.962 - ETA: 5s - loss: 0.1484 - acc: 0.962 - ETA: 5s - loss: 0.1446 - acc: 0.958 - ETA: 5s - loss: 0.1420 - acc: 0.957 - ETA: 5s - loss: 0.1609 - acc: 0.957 - ETA: 5s - loss: 0.1568 - acc: 0.956 - ETA: 5s - loss: 0.1769 - acc: 0.955 - ETA: 4s - loss: 0.1715 - acc: 0.954 - ETA: 4s - loss: 0.1692 - acc: 0.954 - ETA: 4s - loss: 0.1813 - acc: 0.954 - ETA: 4s - loss: 0.2271 - acc: 0.953 - ETA: 4s - loss: 0.2187 - acc: 0.955 - ETA: 4s - loss: 0.2270 - acc: 0.954 - ETA: 4s - loss: 0.2193 - acc: 0.954 - ETA: 4s - loss: 0.2105 - acc: 0.956 - ETA: 4s - loss: 0.2067 - acc: 0.956 - ETA: 4s - loss: 0.2164 - acc: 0.955 - ETA: 4s - loss: 0.2117 - acc: 0.956 - ETA: 4s - loss: 0.2197 - acc: 0.957 - ETA: 4s - loss: 0.2162 - acc: 0.956 - ETA: 4s - loss: 0.2114 - acc: 0.957 - ETA: 4s - loss: 0.2071 - acc: 0.958 - ETA: 4s - loss: 0.2027 - acc: 0.959 - ETA: 4s - loss: 0.2020 - acc: 0.959 - ETA: 3s - loss: 0.1981 - acc: 0.959 - ETA: 3s - loss: 0.1925 - acc: 0.960 - ETA: 3s - loss: 0.1892 - acc: 0.960 - ETA: 3s - loss: 0.1857 - acc: 0.961 - ETA: 3s - loss: 0.1919 - acc: 0.961 - ETA: 3s - loss: 0.1869 - acc: 0.961 - ETA: 3s - loss: 0.1827 - acc: 0.962 - ETA: 3s - loss: 0.1838 - acc: 0.961 - ETA: 3s - loss: 0.1813 - acc: 0.961 - ETA: 3s - loss: 0.1792 - acc: 0.962 - ETA: 3s - loss: 0.1769 - acc: 0.962 - ETA: 3s - loss: 0.1792 - acc: 0.960 - ETA: 3s - loss: 0.1855 - acc: 0.960 - ETA: 3s - loss: 0.1907 - acc: 0.959 - ETA: 3s - loss: 0.1883 - acc: 0.959 - ETA: 3s - loss: 0.1852 - acc: 0.960 - ETA: 3s - loss: 0.1900 - acc: 0.960 - ETA: 2s - loss: 0.1870 - acc: 0.960 - ETA: 2s - loss: 0.1846 - acc: 0.960 - ETA: 2s - loss: 0.1832 - acc: 0.960 - ETA: 2s - loss: 0.1802 - acc: 0.961 - ETA: 2s - loss: 0.1802 - acc: 0.960 - ETA: 2s - loss: 0.1774 - acc: 0.960 - ETA: 2s - loss: 0.1881 - acc: 0.960 - ETA: 2s - loss: 0.1925 - acc: 0.960 - ETA: 2s - loss: 0.1894 - acc: 0.960 - ETA: 2s - loss: 0.1868 - acc: 0.961 - ETA: 2s - loss: 0.1914 - acc: 0.960 - ETA: 2s - loss: 0.1894 - acc: 0.960 - ETA: 2s - loss: 0.1872 - acc: 0.961 - ETA: 2s - loss: 0.1850 - acc: 0.961 - ETA: 2s - loss: 0.1958 - acc: 0.961 - ETA: 1s - loss: 0.1942 - acc: 0.961 - ETA: 1s - loss: 0.1922 - acc: 0.961 - ETA: 1s - loss: 0.1895 - acc: 0.962 - ETA: 1s - loss: 0.1864 - acc: 0.962 - ETA: 1s - loss: 0.1876 - acc: 0.962 - ETA: 1s - loss: 0.1890 - acc: 0.963 - ETA: 1s - loss: 0.1878 - acc: 0.963 - ETA: 1s - loss: 0.1926 - acc: 0.962 - ETA: 1s - loss: 0.1974 - acc: 0.962 - ETA: 1s - loss: 0.1955 - acc: 0.962 - ETA: 1s - loss: 0.1941 - acc: 0.962 - ETA: 1s - loss: 0.1933 - acc: 0.962 - ETA: 1s - loss: 0.1909 - acc: 0.963 - ETA: 1s - loss: 0.1893 - acc: 0.963 - ETA: 1s - loss: 0.1883 - acc: 0.964 - ETA: 1s - loss: 0.1866 - acc: 0.964 - ETA: 1s - loss: 0.1850 - acc: 0.964 - ETA: 0s - loss: 0.1827 - acc: 0.965 - ETA: 0s - loss: 0.1854 - acc: 0.965 - ETA: 0s - loss: 0.1936 - acc: 0.964 - ETA: 0s - loss: 0.1964 - acc: 0.964 - ETA: 0s - loss: 0.1948 - acc: 0.964 - ETA: 0s - loss: 0.1939 - acc: 0.964 - ETA: 0s - loss: 0.1930 - acc: 0.964 - ETA: 0s - loss: 0.1911 - acc: 0.964 - ETA: 0s - loss: 0.1900 - acc: 0.965 - ETA: 0s - loss: 0.1909 - acc: 0.964 - ETA: 0s - loss: 0.1938 - acc: 0.964 - ETA: 0s - loss: 0.1925 - acc: 0.964 - ETA: 0s - loss: 0.1915 - acc: 0.964 - ETA: 0s - loss: 0.1906 - acc: 0.964 - ETA: 0s - loss: 0.1894 - acc: 0.964 - ETA: 0s - loss: 0.1924 - acc: 0.964 - ETA: 0s - loss: 0.1913 - acc: 0.964 - 6s 2ms/step - loss: 0.1908 - acc: 0.9646 - val_loss: 0.4422 - val_acc: 0.9282\n",
      "Epoch 75/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0445 - acc: 1.000 - ETA: 5s - loss: 0.0488 - acc: 0.984 - ETA: 5s - loss: 0.2244 - acc: 0.968 - ETA: 5s - loss: 0.2435 - acc: 0.951 - ETA: 5s - loss: 0.2123 - acc: 0.954 - ETA: 5s - loss: 0.2088 - acc: 0.937 - ETA: 5s - loss: 0.1795 - acc: 0.945 - ETA: 5s - loss: 0.1538 - acc: 0.953 - ETA: 5s - loss: 0.1945 - acc: 0.946 - ETA: 5s - loss: 0.1761 - acc: 0.950 - ETA: 5s - loss: 0.1669 - acc: 0.950 - ETA: 5s - loss: 0.1552 - acc: 0.953 - ETA: 5s - loss: 0.1494 - acc: 0.954 - ETA: 5s - loss: 0.1462 - acc: 0.955 - ETA: 5s - loss: 0.1409 - acc: 0.957 - ETA: 5s - loss: 0.1600 - acc: 0.956 - ETA: 5s - loss: 0.1510 - acc: 0.957 - ETA: 5s - loss: 0.1457 - acc: 0.959 - ETA: 5s - loss: 0.1400 - acc: 0.961 - ETA: 4s - loss: 0.1360 - acc: 0.961 - ETA: 4s - loss: 0.1316 - acc: 0.962 - ETA: 4s - loss: 0.1285 - acc: 0.962 - ETA: 4s - loss: 0.1239 - acc: 0.963 - ETA: 4s - loss: 0.1219 - acc: 0.963 - ETA: 4s - loss: 0.1205 - acc: 0.962 - ETA: 4s - loss: 0.1186 - acc: 0.962 - ETA: 4s - loss: 0.1185 - acc: 0.961 - ETA: 4s - loss: 0.1182 - acc: 0.960 - ETA: 4s - loss: 0.1157 - acc: 0.961 - ETA: 4s - loss: 0.1123 - acc: 0.963 - ETA: 4s - loss: 0.1138 - acc: 0.963 - ETA: 4s - loss: 0.1134 - acc: 0.962 - ETA: 4s - loss: 0.1163 - acc: 0.959 - ETA: 4s - loss: 0.1262 - acc: 0.960 - ETA: 4s - loss: 0.1246 - acc: 0.960 - ETA: 4s - loss: 0.1226 - acc: 0.961 - ETA: 4s - loss: 0.1195 - acc: 0.962 - ETA: 4s - loss: 0.1175 - acc: 0.962 - ETA: 4s - loss: 0.1190 - acc: 0.960 - ETA: 3s - loss: 0.1267 - acc: 0.960 - ETA: 3s - loss: 0.1257 - acc: 0.960 - ETA: 3s - loss: 0.1243 - acc: 0.961 - ETA: 3s - loss: 0.1225 - acc: 0.962 - ETA: 3s - loss: 0.1213 - acc: 0.962 - ETA: 3s - loss: 0.1282 - acc: 0.962 - ETA: 3s - loss: 0.1273 - acc: 0.962 - ETA: 3s - loss: 0.1257 - acc: 0.963 - ETA: 3s - loss: 0.1245 - acc: 0.963 - ETA: 3s - loss: 0.1312 - acc: 0.963 - ETA: 3s - loss: 0.1321 - acc: 0.963 - ETA: 3s - loss: 0.1301 - acc: 0.963 - ETA: 3s - loss: 0.1378 - acc: 0.962 - ETA: 3s - loss: 0.1348 - acc: 0.963 - ETA: 3s - loss: 0.1348 - acc: 0.962 - ETA: 3s - loss: 0.1405 - acc: 0.962 - ETA: 2s - loss: 0.1386 - acc: 0.962 - ETA: 2s - loss: 0.1367 - acc: 0.963 - ETA: 2s - loss: 0.1355 - acc: 0.963 - ETA: 2s - loss: 0.1362 - acc: 0.961 - ETA: 2s - loss: 0.1362 - acc: 0.962 - ETA: 2s - loss: 0.1357 - acc: 0.962 - ETA: 2s - loss: 0.1344 - acc: 0.962 - ETA: 2s - loss: 0.1334 - acc: 0.963 - ETA: 2s - loss: 0.1323 - acc: 0.963 - ETA: 2s - loss: 0.1307 - acc: 0.963 - ETA: 2s - loss: 0.1300 - acc: 0.963 - ETA: 2s - loss: 0.1289 - acc: 0.964 - ETA: 2s - loss: 0.1344 - acc: 0.963 - ETA: 2s - loss: 0.1351 - acc: 0.963 - ETA: 2s - loss: 0.1336 - acc: 0.963 - ETA: 2s - loss: 0.1322 - acc: 0.964 - ETA: 2s - loss: 0.1307 - acc: 0.964 - ETA: 2s - loss: 0.1367 - acc: 0.963 - ETA: 1s - loss: 0.1350 - acc: 0.964 - ETA: 1s - loss: 0.1339 - acc: 0.964 - ETA: 1s - loss: 0.1326 - acc: 0.964 - ETA: 1s - loss: 0.1318 - acc: 0.964 - ETA: 1s - loss: 0.1362 - acc: 0.964 - ETA: 1s - loss: 0.1404 - acc: 0.964 - ETA: 1s - loss: 0.1393 - acc: 0.964 - ETA: 1s - loss: 0.1382 - acc: 0.964 - ETA: 1s - loss: 0.1367 - acc: 0.964 - ETA: 1s - loss: 0.1361 - acc: 0.964 - ETA: 1s - loss: 0.1398 - acc: 0.964 - ETA: 1s - loss: 0.1387 - acc: 0.964 - ETA: 1s - loss: 0.1380 - acc: 0.964 - ETA: 1s - loss: 0.1367 - acc: 0.965 - ETA: 1s - loss: 0.1353 - acc: 0.965 - ETA: 0s - loss: 0.1385 - acc: 0.965 - ETA: 0s - loss: 0.1404 - acc: 0.965 - ETA: 0s - loss: 0.1399 - acc: 0.964 - ETA: 0s - loss: 0.1389 - acc: 0.965 - ETA: 0s - loss: 0.1417 - acc: 0.965 - ETA: 0s - loss: 0.1411 - acc: 0.965 - ETA: 0s - loss: 0.1401 - acc: 0.965 - ETA: 0s - loss: 0.1386 - acc: 0.965 - ETA: 0s - loss: 0.1378 - acc: 0.965 - ETA: 0s - loss: 0.1414 - acc: 0.965 - ETA: 0s - loss: 0.1445 - acc: 0.965 - ETA: 0s - loss: 0.1433 - acc: 0.965 - ETA: 0s - loss: 0.1466 - acc: 0.965 - ETA: 0s - loss: 0.1456 - acc: 0.965 - ETA: 0s - loss: 0.1449 - acc: 0.965 - ETA: 0s - loss: 0.1438 - acc: 0.966 - 7s 2ms/step - loss: 0.1433 - acc: 0.9663 - val_loss: 0.3898 - val_acc: 0.9391\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0493 - acc: 0.937 - ETA: 4s - loss: 0.0994 - acc: 0.937 - ETA: 4s - loss: 0.0736 - acc: 0.964 - ETA: 4s - loss: 0.0643 - acc: 0.968 - ETA: 4s - loss: 0.0631 - acc: 0.971 - ETA: 5s - loss: 0.0612 - acc: 0.975 - ETA: 5s - loss: 0.0612 - acc: 0.974 - ETA: 5s - loss: 0.0644 - acc: 0.970 - ETA: 5s - loss: 0.1570 - acc: 0.967 - ETA: 5s - loss: 0.1505 - acc: 0.967 - ETA: 5s - loss: 0.1437 - acc: 0.967 - ETA: 5s - loss: 0.1397 - acc: 0.965 - ETA: 5s - loss: 0.1290 - acc: 0.968 - ETA: 5s - loss: 0.1189 - acc: 0.971 - ETA: 5s - loss: 0.1123 - acc: 0.972 - ETA: 5s - loss: 0.1054 - acc: 0.971 - ETA: 4s - loss: 0.1046 - acc: 0.969 - ETA: 4s - loss: 0.1039 - acc: 0.967 - ETA: 4s - loss: 0.1023 - acc: 0.966 - ETA: 4s - loss: 0.0952 - acc: 0.969 - ETA: 4s - loss: 0.0972 - acc: 0.969 - ETA: 4s - loss: 0.0943 - acc: 0.970 - ETA: 4s - loss: 0.0926 - acc: 0.969 - ETA: 4s - loss: 0.0902 - acc: 0.971 - ETA: 4s - loss: 0.0874 - acc: 0.972 - ETA: 4s - loss: 0.0867 - acc: 0.971 - ETA: 3s - loss: 0.1122 - acc: 0.970 - ETA: 3s - loss: 0.1112 - acc: 0.969 - ETA: 3s - loss: 0.1092 - acc: 0.968 - ETA: 3s - loss: 0.1119 - acc: 0.968 - ETA: 3s - loss: 0.1127 - acc: 0.968 - ETA: 3s - loss: 0.1103 - acc: 0.968 - ETA: 3s - loss: 0.1075 - acc: 0.968 - ETA: 3s - loss: 0.1045 - acc: 0.969 - ETA: 3s - loss: 0.1068 - acc: 0.968 - ETA: 3s - loss: 0.1053 - acc: 0.968 - ETA: 3s - loss: 0.1034 - acc: 0.968 - ETA: 3s - loss: 0.1027 - acc: 0.968 - ETA: 3s - loss: 0.1129 - acc: 0.967 - ETA: 3s - loss: 0.1114 - acc: 0.967 - ETA: 2s - loss: 0.1108 - acc: 0.967 - ETA: 2s - loss: 0.1184 - acc: 0.966 - ETA: 2s - loss: 0.1178 - acc: 0.965 - ETA: 2s - loss: 0.1166 - acc: 0.965 - ETA: 2s - loss: 0.1166 - acc: 0.965 - ETA: 2s - loss: 0.1142 - acc: 0.966 - ETA: 2s - loss: 0.1136 - acc: 0.966 - ETA: 2s - loss: 0.1197 - acc: 0.965 - ETA: 2s - loss: 0.1265 - acc: 0.965 - ETA: 2s - loss: 0.1313 - acc: 0.965 - ETA: 2s - loss: 0.1382 - acc: 0.964 - ETA: 2s - loss: 0.1364 - acc: 0.965 - ETA: 2s - loss: 0.1355 - acc: 0.965 - ETA: 2s - loss: 0.1341 - acc: 0.965 - ETA: 2s - loss: 0.1327 - acc: 0.965 - ETA: 2s - loss: 0.1310 - acc: 0.966 - ETA: 2s - loss: 0.1304 - acc: 0.966 - ETA: 2s - loss: 0.1300 - acc: 0.966 - ETA: 2s - loss: 0.1288 - acc: 0.966 - ETA: 2s - loss: 0.1280 - acc: 0.966 - ETA: 1s - loss: 0.1336 - acc: 0.966 - ETA: 1s - loss: 0.1383 - acc: 0.966 - ETA: 1s - loss: 0.1390 - acc: 0.965 - ETA: 1s - loss: 0.1500 - acc: 0.965 - ETA: 1s - loss: 0.1484 - acc: 0.965 - ETA: 1s - loss: 0.1472 - acc: 0.965 - ETA: 1s - loss: 0.1456 - acc: 0.966 - ETA: 1s - loss: 0.1442 - acc: 0.966 - ETA: 1s - loss: 0.1430 - acc: 0.967 - ETA: 1s - loss: 0.1424 - acc: 0.966 - ETA: 1s - loss: 0.1472 - acc: 0.966 - ETA: 1s - loss: 0.1457 - acc: 0.966 - ETA: 1s - loss: 0.1442 - acc: 0.966 - ETA: 1s - loss: 0.1483 - acc: 0.966 - ETA: 1s - loss: 0.1474 - acc: 0.966 - ETA: 1s - loss: 0.1461 - acc: 0.966 - ETA: 1s - loss: 0.1495 - acc: 0.966 - ETA: 1s - loss: 0.1532 - acc: 0.966 - ETA: 1s - loss: 0.1525 - acc: 0.965 - ETA: 1s - loss: 0.1533 - acc: 0.964 - ETA: 1s - loss: 0.1669 - acc: 0.964 - ETA: 1s - loss: 0.1656 - acc: 0.964 - ETA: 1s - loss: 0.1741 - acc: 0.963 - ETA: 1s - loss: 0.1733 - acc: 0.964 - ETA: 0s - loss: 0.1726 - acc: 0.964 - ETA: 0s - loss: 0.1718 - acc: 0.963 - ETA: 0s - loss: 0.1707 - acc: 0.964 - ETA: 0s - loss: 0.1685 - acc: 0.964 - ETA: 0s - loss: 0.1721 - acc: 0.964 - ETA: 0s - loss: 0.1747 - acc: 0.964 - ETA: 0s - loss: 0.1729 - acc: 0.964 - ETA: 0s - loss: 0.1720 - acc: 0.964 - ETA: 0s - loss: 0.1757 - acc: 0.964 - ETA: 0s - loss: 0.1756 - acc: 0.964 - ETA: 0s - loss: 0.1750 - acc: 0.963 - ETA: 0s - loss: 0.1796 - acc: 0.962 - ETA: 0s - loss: 0.1872 - acc: 0.962 - ETA: 0s - loss: 0.1896 - acc: 0.962 - ETA: 0s - loss: 0.1875 - acc: 0.962 - ETA: 0s - loss: 0.1860 - acc: 0.962 - 7s 2ms/step - loss: 0.1852 - acc: 0.9629 - val_loss: 0.4420 - val_acc: 0.9353\n",
      "Epoch 77/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0260 - acc: 1.000 - ETA: 5s - loss: 0.0589 - acc: 0.968 - ETA: 5s - loss: 0.0509 - acc: 0.973 - ETA: 5s - loss: 0.0870 - acc: 0.943 - ETA: 5s - loss: 0.0850 - acc: 0.947 - ETA: 5s - loss: 0.2179 - acc: 0.941 - ETA: 5s - loss: 0.2599 - acc: 0.937 - ETA: 5s - loss: 0.2953 - acc: 0.934 - ETA: 5s - loss: 0.2659 - acc: 0.937 - ETA: 5s - loss: 0.2369 - acc: 0.944 - ETA: 4s - loss: 0.2133 - acc: 0.950 - ETA: 4s - loss: 0.1945 - acc: 0.955 - ETA: 4s - loss: 0.2131 - acc: 0.951 - ETA: 4s - loss: 0.2267 - acc: 0.952 - ETA: 4s - loss: 0.2146 - acc: 0.952 - ETA: 4s - loss: 0.2257 - acc: 0.953 - ETA: 4s - loss: 0.2136 - acc: 0.954 - ETA: 4s - loss: 0.2079 - acc: 0.954 - ETA: 4s - loss: 0.2046 - acc: 0.952 - ETA: 4s - loss: 0.1934 - acc: 0.954 - ETA: 4s - loss: 0.1873 - acc: 0.956 - ETA: 4s - loss: 0.1986 - acc: 0.956 - ETA: 4s - loss: 0.1945 - acc: 0.956 - ETA: 4s - loss: 0.1894 - acc: 0.956 - ETA: 4s - loss: 0.2196 - acc: 0.954 - ETA: 4s - loss: 0.2321 - acc: 0.953 - ETA: 4s - loss: 0.2428 - acc: 0.952 - ETA: 4s - loss: 0.2395 - acc: 0.953 - ETA: 4s - loss: 0.2358 - acc: 0.952 - ETA: 4s - loss: 0.2436 - acc: 0.953 - ETA: 4s - loss: 0.2517 - acc: 0.953 - ETA: 4s - loss: 0.2453 - acc: 0.954 - ETA: 4s - loss: 0.2399 - acc: 0.955 - ETA: 4s - loss: 0.2359 - acc: 0.956 - ETA: 4s - loss: 0.2279 - acc: 0.957 - ETA: 4s - loss: 0.2205 - acc: 0.959 - ETA: 4s - loss: 0.2155 - acc: 0.960 - ETA: 4s - loss: 0.2143 - acc: 0.959 - ETA: 4s - loss: 0.2092 - acc: 0.960 - ETA: 4s - loss: 0.2063 - acc: 0.960 - ETA: 3s - loss: 0.2148 - acc: 0.959 - ETA: 3s - loss: 0.2086 - acc: 0.960 - ETA: 3s - loss: 0.2125 - acc: 0.960 - ETA: 3s - loss: 0.2067 - acc: 0.961 - ETA: 3s - loss: 0.2040 - acc: 0.959 - ETA: 3s - loss: 0.2004 - acc: 0.960 - ETA: 3s - loss: 0.2050 - acc: 0.960 - ETA: 3s - loss: 0.2103 - acc: 0.960 - ETA: 3s - loss: 0.2066 - acc: 0.960 - ETA: 3s - loss: 0.2025 - acc: 0.961 - ETA: 3s - loss: 0.2016 - acc: 0.960 - ETA: 3s - loss: 0.1981 - acc: 0.960 - ETA: 2s - loss: 0.2042 - acc: 0.960 - ETA: 2s - loss: 0.2013 - acc: 0.960 - ETA: 2s - loss: 0.1982 - acc: 0.960 - ETA: 2s - loss: 0.2018 - acc: 0.960 - ETA: 2s - loss: 0.1983 - acc: 0.961 - ETA: 2s - loss: 0.1959 - acc: 0.962 - ETA: 2s - loss: 0.1997 - acc: 0.961 - ETA: 2s - loss: 0.1977 - acc: 0.961 - ETA: 2s - loss: 0.1948 - acc: 0.962 - ETA: 2s - loss: 0.1917 - acc: 0.962 - ETA: 2s - loss: 0.1887 - acc: 0.963 - ETA: 2s - loss: 0.1936 - acc: 0.963 - ETA: 2s - loss: 0.1940 - acc: 0.963 - ETA: 1s - loss: 0.2028 - acc: 0.963 - ETA: 1s - loss: 0.1996 - acc: 0.964 - ETA: 1s - loss: 0.1972 - acc: 0.964 - ETA: 1s - loss: 0.1948 - acc: 0.964 - ETA: 1s - loss: 0.1922 - acc: 0.964 - ETA: 1s - loss: 0.1909 - acc: 0.964 - ETA: 1s - loss: 0.1888 - acc: 0.964 - ETA: 1s - loss: 0.1874 - acc: 0.964 - ETA: 1s - loss: 0.1951 - acc: 0.964 - ETA: 1s - loss: 0.1934 - acc: 0.963 - ETA: 1s - loss: 0.1957 - acc: 0.963 - ETA: 1s - loss: 0.1974 - acc: 0.963 - ETA: 1s - loss: 0.1958 - acc: 0.963 - ETA: 0s - loss: 0.1991 - acc: 0.963 - ETA: 0s - loss: 0.1966 - acc: 0.964 - ETA: 0s - loss: 0.1949 - acc: 0.964 - ETA: 0s - loss: 0.1973 - acc: 0.964 - ETA: 0s - loss: 0.1949 - acc: 0.964 - ETA: 0s - loss: 0.1931 - acc: 0.964 - ETA: 0s - loss: 0.1915 - acc: 0.964 - ETA: 0s - loss: 0.1900 - acc: 0.964 - ETA: 0s - loss: 0.1902 - acc: 0.964 - ETA: 0s - loss: 0.1941 - acc: 0.963 - ETA: 0s - loss: 0.2105 - acc: 0.961 - ETA: 0s - loss: 0.2088 - acc: 0.961 - ETA: 0s - loss: 0.2084 - acc: 0.961 - ETA: 0s - loss: 0.2110 - acc: 0.960 - ETA: 0s - loss: 0.2133 - acc: 0.960 - 6s 1ms/step - loss: 0.2123 - acc: 0.9604 - val_loss: 0.4623 - val_acc: 0.9128\n",
      "Epoch 78/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0953 - acc: 1.000 - ETA: 4s - loss: 0.0627 - acc: 0.984 - ETA: 4s - loss: 0.1153 - acc: 0.955 - ETA: 4s - loss: 0.0910 - acc: 0.962 - ETA: 4s - loss: 0.0835 - acc: 0.971 - ETA: 4s - loss: 0.0838 - acc: 0.968 - ETA: 4s - loss: 0.0884 - acc: 0.970 - ETA: 4s - loss: 0.0804 - acc: 0.974 - ETA: 4s - loss: 0.0737 - acc: 0.977 - ETA: 4s - loss: 0.0747 - acc: 0.973 - ETA: 4s - loss: 0.0738 - acc: 0.971 - ETA: 4s - loss: 0.0981 - acc: 0.972 - ETA: 4s - loss: 0.0961 - acc: 0.970 - ETA: 4s - loss: 0.0970 - acc: 0.968 - ETA: 4s - loss: 0.1172 - acc: 0.968 - ETA: 4s - loss: 0.1351 - acc: 0.967 - ETA: 4s - loss: 0.1285 - acc: 0.969 - ETA: 4s - loss: 0.1231 - acc: 0.971 - ETA: 4s - loss: 0.1173 - acc: 0.972 - ETA: 4s - loss: 0.1137 - acc: 0.973 - ETA: 4s - loss: 0.1256 - acc: 0.973 - ETA: 4s - loss: 0.1216 - acc: 0.973 - ETA: 4s - loss: 0.1173 - acc: 0.974 - ETA: 4s - loss: 0.1142 - acc: 0.974 - ETA: 4s - loss: 0.1282 - acc: 0.971 - ETA: 4s - loss: 0.1246 - acc: 0.972 - ETA: 4s - loss: 0.1352 - acc: 0.973 - ETA: 4s - loss: 0.1331 - acc: 0.972 - ETA: 4s - loss: 0.1294 - acc: 0.973 - ETA: 3s - loss: 0.1274 - acc: 0.973 - ETA: 3s - loss: 0.1428 - acc: 0.972 - ETA: 3s - loss: 0.1406 - acc: 0.972 - ETA: 3s - loss: 0.1410 - acc: 0.971 - ETA: 3s - loss: 0.1371 - acc: 0.972 - ETA: 3s - loss: 0.1344 - acc: 0.972 - ETA: 3s - loss: 0.1383 - acc: 0.973 - ETA: 3s - loss: 0.1346 - acc: 0.973 - ETA: 3s - loss: 0.1410 - acc: 0.972 - ETA: 3s - loss: 0.1590 - acc: 0.971 - ETA: 3s - loss: 0.1651 - acc: 0.971 - ETA: 3s - loss: 0.1626 - acc: 0.970 - ETA: 3s - loss: 0.1685 - acc: 0.970 - ETA: 2s - loss: 0.1691 - acc: 0.970 - ETA: 2s - loss: 0.1677 - acc: 0.970 - ETA: 2s - loss: 0.1723 - acc: 0.970 - ETA: 2s - loss: 0.1784 - acc: 0.969 - ETA: 2s - loss: 0.1751 - acc: 0.969 - ETA: 2s - loss: 0.1789 - acc: 0.969 - ETA: 2s - loss: 0.1804 - acc: 0.969 - ETA: 2s - loss: 0.1863 - acc: 0.968 - ETA: 2s - loss: 0.1830 - acc: 0.969 - ETA: 2s - loss: 0.1819 - acc: 0.968 - ETA: 2s - loss: 0.1789 - acc: 0.968 - ETA: 2s - loss: 0.1764 - acc: 0.968 - ETA: 2s - loss: 0.1811 - acc: 0.967 - ETA: 2s - loss: 0.1790 - acc: 0.967 - ETA: 2s - loss: 0.1769 - acc: 0.967 - ETA: 2s - loss: 0.1754 - acc: 0.967 - ETA: 2s - loss: 0.1797 - acc: 0.967 - ETA: 1s - loss: 0.1839 - acc: 0.966 - ETA: 1s - loss: 0.1835 - acc: 0.966 - ETA: 1s - loss: 0.1924 - acc: 0.966 - ETA: 1s - loss: 0.2020 - acc: 0.965 - ETA: 1s - loss: 0.2059 - acc: 0.965 - ETA: 1s - loss: 0.2028 - acc: 0.965 - ETA: 1s - loss: 0.2002 - acc: 0.966 - ETA: 1s - loss: 0.2030 - acc: 0.965 - ETA: 1s - loss: 0.2040 - acc: 0.965 - ETA: 1s - loss: 0.2011 - acc: 0.965 - ETA: 1s - loss: 0.1984 - acc: 0.966 - ETA: 1s - loss: 0.1959 - acc: 0.966 - ETA: 1s - loss: 0.1984 - acc: 0.966 - ETA: 1s - loss: 0.2013 - acc: 0.966 - ETA: 1s - loss: 0.2045 - acc: 0.966 - ETA: 0s - loss: 0.2064 - acc: 0.966 - ETA: 0s - loss: 0.2041 - acc: 0.966 - ETA: 0s - loss: 0.2069 - acc: 0.966 - ETA: 0s - loss: 0.2140 - acc: 0.965 - ETA: 0s - loss: 0.2140 - acc: 0.965 - ETA: 0s - loss: 0.2113 - acc: 0.966 - ETA: 0s - loss: 0.2183 - acc: 0.965 - ETA: 0s - loss: 0.2199 - acc: 0.965 - ETA: 0s - loss: 0.2174 - acc: 0.966 - ETA: 0s - loss: 0.2185 - acc: 0.965 - ETA: 0s - loss: 0.2205 - acc: 0.965 - ETA: 0s - loss: 0.2183 - acc: 0.965 - ETA: 0s - loss: 0.2223 - acc: 0.965 - ETA: 0s - loss: 0.2239 - acc: 0.965 - ETA: 0s - loss: 0.2258 - acc: 0.964 - 6s 1ms/step - loss: 0.2248 - acc: 0.9651 - val_loss: 0.4871 - val_acc: 0.9359\n",
      "Epoch 79/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0890 - acc: 0.937 - ETA: 4s - loss: 0.0657 - acc: 0.968 - ETA: 5s - loss: 0.3842 - acc: 0.958 - ETA: 5s - loss: 0.3745 - acc: 0.965 - ETA: 5s - loss: 0.3675 - acc: 0.968 - ETA: 4s - loss: 0.3760 - acc: 0.966 - ETA: 4s - loss: 0.3612 - acc: 0.965 - ETA: 4s - loss: 0.3181 - acc: 0.967 - ETA: 4s - loss: 0.2805 - acc: 0.971 - ETA: 4s - loss: 0.2947 - acc: 0.969 - ETA: 4s - loss: 0.3352 - acc: 0.962 - ETA: 4s - loss: 0.3203 - acc: 0.960 - ETA: 4s - loss: 0.3392 - acc: 0.957 - ETA: 4s - loss: 0.3362 - acc: 0.953 - ETA: 4s - loss: 0.3195 - acc: 0.956 - ETA: 4s - loss: 0.3059 - acc: 0.958 - ETA: 4s - loss: 0.2940 - acc: 0.958 - ETA: 4s - loss: 0.2817 - acc: 0.960 - ETA: 4s - loss: 0.3182 - acc: 0.956 - ETA: 4s - loss: 0.3048 - acc: 0.957 - ETA: 4s - loss: 0.3315 - acc: 0.954 - ETA: 4s - loss: 0.3211 - acc: 0.954 - ETA: 4s - loss: 0.3073 - acc: 0.956 - ETA: 4s - loss: 0.2932 - acc: 0.958 - ETA: 4s - loss: 0.3127 - acc: 0.957 - ETA: 4s - loss: 0.2989 - acc: 0.959 - ETA: 4s - loss: 0.2875 - acc: 0.961 - ETA: 4s - loss: 0.2897 - acc: 0.961 - ETA: 4s - loss: 0.2784 - acc: 0.963 - ETA: 3s - loss: 0.2723 - acc: 0.963 - ETA: 3s - loss: 0.2747 - acc: 0.964 - ETA: 3s - loss: 0.2649 - acc: 0.965 - ETA: 3s - loss: 0.2627 - acc: 0.966 - ETA: 3s - loss: 0.2577 - acc: 0.966 - ETA: 3s - loss: 0.2603 - acc: 0.966 - ETA: 3s - loss: 0.2566 - acc: 0.966 - ETA: 3s - loss: 0.2882 - acc: 0.964 - ETA: 3s - loss: 0.2983 - acc: 0.963 - ETA: 3s - loss: 0.2906 - acc: 0.964 - ETA: 3s - loss: 0.2838 - acc: 0.964 - ETA: 3s - loss: 0.3068 - acc: 0.961 - ETA: 3s - loss: 0.3121 - acc: 0.959 - ETA: 2s - loss: 0.3066 - acc: 0.958 - ETA: 2s - loss: 0.3159 - acc: 0.955 - ETA: 2s - loss: 0.3108 - acc: 0.954 - ETA: 2s - loss: 0.3073 - acc: 0.953 - ETA: 2s - loss: 0.3023 - acc: 0.953 - ETA: 2s - loss: 0.2958 - acc: 0.954 - ETA: 2s - loss: 0.2983 - acc: 0.953 - ETA: 2s - loss: 0.3005 - acc: 0.953 - ETA: 2s - loss: 0.2947 - acc: 0.954 - ETA: 2s - loss: 0.2889 - acc: 0.954 - ETA: 2s - loss: 0.2836 - acc: 0.955 - ETA: 2s - loss: 0.2788 - acc: 0.956 - ETA: 2s - loss: 0.2789 - acc: 0.956 - ETA: 2s - loss: 0.2810 - acc: 0.956 - ETA: 2s - loss: 0.2823 - acc: 0.956 - ETA: 1s - loss: 0.2828 - acc: 0.956 - ETA: 1s - loss: 0.2788 - acc: 0.956 - ETA: 1s - loss: 0.2743 - acc: 0.957 - ETA: 1s - loss: 0.2726 - acc: 0.957 - ETA: 1s - loss: 0.2693 - acc: 0.957 - ETA: 1s - loss: 0.2776 - acc: 0.956 - ETA: 1s - loss: 0.2772 - acc: 0.956 - ETA: 1s - loss: 0.2773 - acc: 0.957 - ETA: 1s - loss: 0.2806 - acc: 0.956 - ETA: 1s - loss: 0.2767 - acc: 0.957 - ETA: 1s - loss: 0.2728 - acc: 0.958 - ETA: 1s - loss: 0.2694 - acc: 0.958 - ETA: 1s - loss: 0.2705 - acc: 0.958 - ETA: 1s - loss: 0.2784 - acc: 0.957 - ETA: 1s - loss: 0.2762 - acc: 0.956 - ETA: 0s - loss: 0.2727 - acc: 0.956 - ETA: 0s - loss: 0.2785 - acc: 0.956 - ETA: 0s - loss: 0.2763 - acc: 0.956 - ETA: 0s - loss: 0.2731 - acc: 0.957 - ETA: 0s - loss: 0.2843 - acc: 0.956 - ETA: 0s - loss: 0.2851 - acc: 0.956 - ETA: 0s - loss: 0.2825 - acc: 0.956 - ETA: 0s - loss: 0.2971 - acc: 0.955 - ETA: 0s - loss: 0.2979 - acc: 0.956 - ETA: 0s - loss: 0.2988 - acc: 0.956 - ETA: 0s - loss: 0.2998 - acc: 0.956 - ETA: 0s - loss: 0.2969 - acc: 0.956 - ETA: 0s - loss: 0.2934 - acc: 0.956 - ETA: 0s - loss: 0.3029 - acc: 0.956 - ETA: 0s - loss: 0.3025 - acc: 0.956 - ETA: 0s - loss: 0.3001 - acc: 0.956 - ETA: 0s - loss: 0.2966 - acc: 0.957 - 6s 1ms/step - loss: 0.3012 - acc: 0.9567 - val_loss: 0.6400 - val_acc: 0.9340\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 4s - loss: 0.3228 - acc: 0.968 - ETA: 4s - loss: 0.1969 - acc: 0.973 - ETA: 5s - loss: 0.1606 - acc: 0.972 - ETA: 4s - loss: 0.2712 - acc: 0.958 - ETA: 4s - loss: 0.2361 - acc: 0.958 - ETA: 4s - loss: 0.2587 - acc: 0.958 - ETA: 4s - loss: 0.3231 - acc: 0.955 - ETA: 4s - loss: 0.3296 - acc: 0.955 - ETA: 4s - loss: 0.3095 - acc: 0.956 - ETA: 4s - loss: 0.2907 - acc: 0.956 - ETA: 4s - loss: 0.2724 - acc: 0.956 - ETA: 4s - loss: 0.2894 - acc: 0.953 - ETA: 4s - loss: 0.2696 - acc: 0.956 - ETA: 4s - loss: 0.2683 - acc: 0.956 - ETA: 4s - loss: 0.3081 - acc: 0.954 - ETA: 4s - loss: 0.3402 - acc: 0.953 - ETA: 4s - loss: 0.3566 - acc: 0.951 - ETA: 3s - loss: 0.3391 - acc: 0.952 - ETA: 3s - loss: 0.3247 - acc: 0.952 - ETA: 3s - loss: 0.3331 - acc: 0.951 - ETA: 3s - loss: 0.3239 - acc: 0.950 - ETA: 3s - loss: 0.3275 - acc: 0.950 - ETA: 3s - loss: 0.3317 - acc: 0.950 - ETA: 3s - loss: 0.3204 - acc: 0.951 - ETA: 3s - loss: 0.3106 - acc: 0.952 - ETA: 3s - loss: 0.3026 - acc: 0.953 - ETA: 3s - loss: 0.3059 - acc: 0.953 - ETA: 3s - loss: 0.2968 - acc: 0.954 - ETA: 3s - loss: 0.2871 - acc: 0.955 - ETA: 3s - loss: 0.2912 - acc: 0.954 - ETA: 3s - loss: 0.2984 - acc: 0.954 - ETA: 3s - loss: 0.3038 - acc: 0.954 - ETA: 3s - loss: 0.3095 - acc: 0.953 - ETA: 3s - loss: 0.3197 - acc: 0.947 - ETA: 3s - loss: 0.3241 - acc: 0.946 - ETA: 3s - loss: 0.3272 - acc: 0.945 - ETA: 3s - loss: 0.3236 - acc: 0.944 - ETA: 3s - loss: 0.3175 - acc: 0.944 - ETA: 2s - loss: 0.3194 - acc: 0.945 - ETA: 2s - loss: 0.3137 - acc: 0.945 - ETA: 2s - loss: 0.3079 - acc: 0.945 - ETA: 2s - loss: 0.3050 - acc: 0.946 - ETA: 2s - loss: 0.3077 - acc: 0.946 - ETA: 2s - loss: 0.3113 - acc: 0.946 - ETA: 2s - loss: 0.3082 - acc: 0.946 - ETA: 2s - loss: 0.3042 - acc: 0.946 - ETA: 2s - loss: 0.2995 - acc: 0.946 - ETA: 2s - loss: 0.3062 - acc: 0.945 - ETA: 2s - loss: 0.3019 - acc: 0.945 - ETA: 2s - loss: 0.3090 - acc: 0.944 - ETA: 2s - loss: 0.3050 - acc: 0.943 - ETA: 2s - loss: 0.3002 - acc: 0.944 - ETA: 2s - loss: 0.3055 - acc: 0.943 - ETA: 2s - loss: 0.3009 - acc: 0.943 - ETA: 2s - loss: 0.2973 - acc: 0.944 - ETA: 1s - loss: 0.2941 - acc: 0.944 - ETA: 1s - loss: 0.2910 - acc: 0.944 - ETA: 1s - loss: 0.3042 - acc: 0.943 - ETA: 1s - loss: 0.3007 - acc: 0.943 - ETA: 1s - loss: 0.2970 - acc: 0.942 - ETA: 1s - loss: 0.2952 - acc: 0.941 - ETA: 1s - loss: 0.2990 - acc: 0.941 - ETA: 1s - loss: 0.2956 - acc: 0.941 - ETA: 1s - loss: 0.2930 - acc: 0.941 - ETA: 1s - loss: 0.2899 - acc: 0.941 - ETA: 1s - loss: 0.2976 - acc: 0.941 - ETA: 1s - loss: 0.2948 - acc: 0.941 - ETA: 1s - loss: 0.2913 - acc: 0.942 - ETA: 1s - loss: 0.2936 - acc: 0.942 - ETA: 1s - loss: 0.2898 - acc: 0.943 - ETA: 0s - loss: 0.2919 - acc: 0.942 - ETA: 0s - loss: 0.2941 - acc: 0.942 - ETA: 0s - loss: 0.2945 - acc: 0.942 - ETA: 0s - loss: 0.2924 - acc: 0.942 - ETA: 0s - loss: 0.2892 - acc: 0.943 - ETA: 0s - loss: 0.2931 - acc: 0.942 - ETA: 0s - loss: 0.2913 - acc: 0.942 - ETA: 0s - loss: 0.2887 - acc: 0.942 - ETA: 0s - loss: 0.2857 - acc: 0.942 - ETA: 0s - loss: 0.2837 - acc: 0.942 - ETA: 0s - loss: 0.2855 - acc: 0.942 - ETA: 0s - loss: 0.2832 - acc: 0.943 - ETA: 0s - loss: 0.2815 - acc: 0.943 - ETA: 0s - loss: 0.2886 - acc: 0.942 - ETA: 0s - loss: 0.2914 - acc: 0.942 - ETA: 0s - loss: 0.2884 - acc: 0.942 - 6s 1ms/step - loss: 0.2863 - acc: 0.9432 - val_loss: 0.3787 - val_acc: 0.9340\n",
      "Epoch 81/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1283 - acc: 0.937 - ETA: 4s - loss: 0.3276 - acc: 0.968 - ETA: 4s - loss: 0.2414 - acc: 0.955 - ETA: 4s - loss: 0.1749 - acc: 0.968 - ETA: 4s - loss: 0.1544 - acc: 0.971 - ETA: 4s - loss: 0.1364 - acc: 0.972 - ETA: 4s - loss: 0.1249 - acc: 0.970 - ETA: 4s - loss: 0.1783 - acc: 0.960 - ETA: 4s - loss: 0.1660 - acc: 0.962 - ETA: 4s - loss: 0.1545 - acc: 0.964 - ETA: 4s - loss: 0.1487 - acc: 0.965 - ETA: 4s - loss: 0.1471 - acc: 0.963 - ETA: 4s - loss: 0.1659 - acc: 0.964 - ETA: 4s - loss: 0.1596 - acc: 0.964 - ETA: 4s - loss: 0.1546 - acc: 0.965 - ETA: 4s - loss: 0.1741 - acc: 0.962 - ETA: 4s - loss: 0.1924 - acc: 0.957 - ETA: 4s - loss: 0.1878 - acc: 0.956 - ETA: 4s - loss: 0.1804 - acc: 0.958 - ETA: 4s - loss: 0.1762 - acc: 0.959 - ETA: 4s - loss: 0.1761 - acc: 0.959 - ETA: 3s - loss: 0.1725 - acc: 0.958 - ETA: 3s - loss: 0.1746 - acc: 0.955 - ETA: 3s - loss: 0.1742 - acc: 0.953 - ETA: 3s - loss: 0.1717 - acc: 0.952 - ETA: 3s - loss: 0.1669 - acc: 0.953 - ETA: 3s - loss: 0.1632 - acc: 0.954 - ETA: 3s - loss: 0.1604 - acc: 0.953 - ETA: 3s - loss: 0.1681 - acc: 0.953 - ETA: 3s - loss: 0.1661 - acc: 0.954 - ETA: 3s - loss: 0.1621 - acc: 0.956 - ETA: 3s - loss: 0.1585 - acc: 0.956 - ETA: 3s - loss: 0.1669 - acc: 0.956 - ETA: 3s - loss: 0.1746 - acc: 0.955 - ETA: 3s - loss: 0.1723 - acc: 0.955 - ETA: 3s - loss: 0.1743 - acc: 0.954 - ETA: 3s - loss: 0.1709 - acc: 0.955 - ETA: 2s - loss: 0.1882 - acc: 0.952 - ETA: 2s - loss: 0.1853 - acc: 0.952 - ETA: 2s - loss: 0.2030 - acc: 0.951 - ETA: 2s - loss: 0.2009 - acc: 0.951 - ETA: 2s - loss: 0.1993 - acc: 0.950 - ETA: 2s - loss: 0.1966 - acc: 0.950 - ETA: 2s - loss: 0.1942 - acc: 0.950 - ETA: 2s - loss: 0.1999 - acc: 0.949 - ETA: 2s - loss: 0.1973 - acc: 0.949 - ETA: 2s - loss: 0.2017 - acc: 0.949 - ETA: 2s - loss: 0.2011 - acc: 0.949 - ETA: 2s - loss: 0.2061 - acc: 0.949 - ETA: 2s - loss: 0.2041 - acc: 0.949 - ETA: 2s - loss: 0.2010 - acc: 0.950 - ETA: 2s - loss: 0.1971 - acc: 0.951 - ETA: 2s - loss: 0.1957 - acc: 0.951 - ETA: 2s - loss: 0.1947 - acc: 0.950 - ETA: 1s - loss: 0.1989 - acc: 0.951 - ETA: 1s - loss: 0.1964 - acc: 0.951 - ETA: 1s - loss: 0.2024 - acc: 0.951 - ETA: 1s - loss: 0.2056 - acc: 0.951 - ETA: 1s - loss: 0.2117 - acc: 0.950 - ETA: 1s - loss: 0.2148 - acc: 0.951 - ETA: 1s - loss: 0.2120 - acc: 0.951 - ETA: 1s - loss: 0.2104 - acc: 0.951 - ETA: 1s - loss: 0.2079 - acc: 0.952 - ETA: 1s - loss: 0.2062 - acc: 0.952 - ETA: 1s - loss: 0.2037 - acc: 0.952 - ETA: 1s - loss: 0.2012 - acc: 0.953 - ETA: 1s - loss: 0.2046 - acc: 0.952 - ETA: 1s - loss: 0.2073 - acc: 0.952 - ETA: 1s - loss: 0.2075 - acc: 0.952 - ETA: 1s - loss: 0.2175 - acc: 0.950 - ETA: 0s - loss: 0.2152 - acc: 0.950 - ETA: 0s - loss: 0.2130 - acc: 0.950 - ETA: 0s - loss: 0.2130 - acc: 0.949 - ETA: 0s - loss: 0.2201 - acc: 0.949 - ETA: 0s - loss: 0.2248 - acc: 0.948 - ETA: 0s - loss: 0.2267 - acc: 0.948 - ETA: 0s - loss: 0.2246 - acc: 0.948 - ETA: 0s - loss: 0.2232 - acc: 0.947 - ETA: 0s - loss: 0.2204 - acc: 0.948 - ETA: 0s - loss: 0.2230 - acc: 0.947 - ETA: 0s - loss: 0.2254 - acc: 0.947 - ETA: 0s - loss: 0.2274 - acc: 0.947 - ETA: 0s - loss: 0.2312 - acc: 0.946 - ETA: 0s - loss: 0.2307 - acc: 0.946 - ETA: 0s - loss: 0.2375 - acc: 0.945 - ETA: 0s - loss: 0.2363 - acc: 0.944 - 6s 1ms/step - loss: 0.2361 - acc: 0.9447 - val_loss: 0.4003 - val_acc: 0.9128\n",
      "Epoch 82/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1084 - acc: 0.937 - ETA: 5s - loss: 0.1317 - acc: 0.906 - ETA: 5s - loss: 0.0943 - acc: 0.946 - ETA: 4s - loss: 0.0879 - acc: 0.943 - ETA: 4s - loss: 0.1521 - acc: 0.937 - ETA: 4s - loss: 0.1471 - acc: 0.933 - ETA: 4s - loss: 0.1375 - acc: 0.937 - ETA: 4s - loss: 0.1386 - acc: 0.934 - ETA: 4s - loss: 0.1459 - acc: 0.932 - ETA: 4s - loss: 0.1370 - acc: 0.939 - ETA: 4s - loss: 0.1421 - acc: 0.935 - ETA: 4s - loss: 0.1369 - acc: 0.939 - ETA: 4s - loss: 0.1365 - acc: 0.939 - ETA: 4s - loss: 0.1309 - acc: 0.942 - ETA: 4s - loss: 0.1261 - acc: 0.944 - ETA: 4s - loss: 0.1253 - acc: 0.946 - ETA: 4s - loss: 0.1361 - acc: 0.940 - ETA: 4s - loss: 0.1408 - acc: 0.938 - ETA: 4s - loss: 0.1385 - acc: 0.941 - ETA: 4s - loss: 0.1550 - acc: 0.942 - ETA: 4s - loss: 0.1688 - acc: 0.941 - ETA: 4s - loss: 0.1682 - acc: 0.939 - ETA: 4s - loss: 0.1835 - acc: 0.939 - ETA: 4s - loss: 0.1815 - acc: 0.940 - ETA: 4s - loss: 0.1820 - acc: 0.940 - ETA: 4s - loss: 0.2084 - acc: 0.940 - ETA: 4s - loss: 0.2429 - acc: 0.936 - ETA: 4s - loss: 0.2625 - acc: 0.935 - ETA: 3s - loss: 0.2800 - acc: 0.934 - ETA: 3s - loss: 0.2908 - acc: 0.933 - ETA: 3s - loss: 0.3238 - acc: 0.930 - ETA: 3s - loss: 0.3155 - acc: 0.933 - ETA: 3s - loss: 0.3167 - acc: 0.933 - ETA: 3s - loss: 0.3206 - acc: 0.933 - ETA: 3s - loss: 0.3164 - acc: 0.932 - ETA: 3s - loss: 0.3242 - acc: 0.931 - ETA: 3s - loss: 0.3300 - acc: 0.931 - ETA: 3s - loss: 0.3463 - acc: 0.929 - ETA: 3s - loss: 0.3514 - acc: 0.929 - ETA: 3s - loss: 0.3454 - acc: 0.929 - ETA: 3s - loss: 0.3564 - acc: 0.929 - ETA: 3s - loss: 0.3615 - acc: 0.927 - ETA: 3s - loss: 0.3573 - acc: 0.927 - ETA: 3s - loss: 0.3541 - acc: 0.926 - ETA: 3s - loss: 0.3491 - acc: 0.927 - ETA: 3s - loss: 0.3684 - acc: 0.925 - ETA: 2s - loss: 0.3782 - acc: 0.924 - ETA: 2s - loss: 0.3924 - acc: 0.922 - ETA: 2s - loss: 0.4042 - acc: 0.919 - ETA: 2s - loss: 0.4124 - acc: 0.920 - ETA: 2s - loss: 0.4222 - acc: 0.919 - ETA: 2s - loss: 0.4183 - acc: 0.919 - ETA: 2s - loss: 0.4224 - acc: 0.917 - ETA: 2s - loss: 0.4364 - acc: 0.916 - ETA: 2s - loss: 0.4581 - acc: 0.915 - ETA: 2s - loss: 0.4655 - acc: 0.913 - ETA: 2s - loss: 0.4712 - acc: 0.913 - ETA: 2s - loss: 0.4710 - acc: 0.914 - ETA: 2s - loss: 0.4668 - acc: 0.914 - ETA: 2s - loss: 0.4678 - acc: 0.914 - ETA: 2s - loss: 0.4702 - acc: 0.914 - ETA: 2s - loss: 0.4846 - acc: 0.913 - ETA: 1s - loss: 0.4867 - acc: 0.911 - ETA: 1s - loss: 0.4828 - acc: 0.910 - ETA: 1s - loss: 0.4771 - acc: 0.910 - ETA: 1s - loss: 0.4828 - acc: 0.910 - ETA: 1s - loss: 0.4788 - acc: 0.910 - ETA: 1s - loss: 0.4803 - acc: 0.910 - ETA: 1s - loss: 0.4813 - acc: 0.910 - ETA: 1s - loss: 0.4834 - acc: 0.909 - ETA: 1s - loss: 0.5039 - acc: 0.908 - ETA: 1s - loss: 0.4993 - acc: 0.909 - ETA: 1s - loss: 0.4970 - acc: 0.909 - ETA: 1s - loss: 0.4963 - acc: 0.909 - ETA: 1s - loss: 0.4969 - acc: 0.909 - ETA: 1s - loss: 0.5065 - acc: 0.908 - ETA: 1s - loss: 0.5109 - acc: 0.908 - ETA: 1s - loss: 0.5167 - acc: 0.906 - ETA: 1s - loss: 0.5112 - acc: 0.906 - ETA: 1s - loss: 0.5223 - acc: 0.905 - ETA: 0s - loss: 0.5206 - acc: 0.905 - ETA: 0s - loss: 0.5259 - acc: 0.905 - ETA: 0s - loss: 0.5224 - acc: 0.906 - ETA: 0s - loss: 0.5237 - acc: 0.906 - ETA: 0s - loss: 0.5240 - acc: 0.906 - ETA: 0s - loss: 0.5281 - acc: 0.906 - ETA: 0s - loss: 0.5350 - acc: 0.906 - ETA: 0s - loss: 0.5376 - acc: 0.906 - ETA: 0s - loss: 0.5524 - acc: 0.906 - ETA: 0s - loss: 0.5629 - acc: 0.906 - ETA: 0s - loss: 0.5817 - acc: 0.905 - ETA: 0s - loss: 0.5864 - acc: 0.905 - ETA: 0s - loss: 0.5943 - acc: 0.905 - ETA: 0s - loss: 0.5987 - acc: 0.905 - ETA: 0s - loss: 0.6082 - acc: 0.905 - ETA: 0s - loss: 0.6290 - acc: 0.905 - 6s 2ms/step - loss: 0.6263 - acc: 0.9056 - val_loss: 1.6853 - val_acc: 0.8872\n",
      "Epoch 83/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 2.0735 - acc: 0.812 - ETA: 6s - loss: 1.3628 - acc: 0.895 - ETA: 6s - loss: 1.4358 - acc: 0.900 - ETA: 6s - loss: 1.6644 - acc: 0.882 - ETA: 5s - loss: 1.4910 - acc: 0.897 - ETA: 5s - loss: 1.4941 - acc: 0.899 - ETA: 5s - loss: 1.4736 - acc: 0.898 - ETA: 5s - loss: 1.6309 - acc: 0.888 - ETA: 5s - loss: 1.6432 - acc: 0.889 - ETA: 5s - loss: 1.6109 - acc: 0.890 - ETA: 5s - loss: 1.4962 - acc: 0.897 - ETA: 5s - loss: 1.4362 - acc: 0.897 - ETA: 5s - loss: 1.3798 - acc: 0.900 - ETA: 5s - loss: 1.3310 - acc: 0.904 - ETA: 5s - loss: 1.3802 - acc: 0.897 - ETA: 4s - loss: 1.3325 - acc: 0.898 - ETA: 4s - loss: 1.3000 - acc: 0.892 - ETA: 4s - loss: 1.2630 - acc: 0.894 - ETA: 4s - loss: 1.2151 - acc: 0.894 - ETA: 4s - loss: 1.1708 - acc: 0.893 - ETA: 4s - loss: 1.1652 - acc: 0.892 - ETA: 4s - loss: 1.1086 - acc: 0.895 - ETA: 4s - loss: 1.1086 - acc: 0.893 - ETA: 4s - loss: 1.0759 - acc: 0.896 - ETA: 4s - loss: 1.0299 - acc: 0.899 - ETA: 3s - loss: 1.0019 - acc: 0.900 - ETA: 3s - loss: 1.0060 - acc: 0.900 - ETA: 3s - loss: 0.9964 - acc: 0.900 - ETA: 3s - loss: 0.9867 - acc: 0.900 - ETA: 3s - loss: 0.9654 - acc: 0.900 - ETA: 3s - loss: 0.9939 - acc: 0.899 - ETA: 3s - loss: 0.9860 - acc: 0.899 - ETA: 3s - loss: 1.0049 - acc: 0.897 - ETA: 3s - loss: 0.9957 - acc: 0.897 - ETA: 3s - loss: 0.9883 - acc: 0.896 - ETA: 3s - loss: 0.9652 - acc: 0.893 - ETA: 3s - loss: 0.9489 - acc: 0.894 - ETA: 3s - loss: 0.9340 - acc: 0.894 - ETA: 3s - loss: 0.9116 - acc: 0.894 - ETA: 3s - loss: 0.8908 - acc: 0.894 - ETA: 2s - loss: 0.8963 - acc: 0.893 - ETA: 2s - loss: 0.8768 - acc: 0.893 - ETA: 2s - loss: 0.8828 - acc: 0.892 - ETA: 2s - loss: 0.8714 - acc: 0.892 - ETA: 2s - loss: 0.8764 - acc: 0.892 - ETA: 2s - loss: 0.8579 - acc: 0.892 - ETA: 2s - loss: 0.8478 - acc: 0.893 - ETA: 2s - loss: 0.8526 - acc: 0.893 - ETA: 2s - loss: 0.8440 - acc: 0.892 - ETA: 2s - loss: 0.8726 - acc: 0.891 - ETA: 2s - loss: 0.8697 - acc: 0.889 - ETA: 2s - loss: 0.8718 - acc: 0.889 - ETA: 2s - loss: 0.8625 - acc: 0.888 - ETA: 2s - loss: 0.8557 - acc: 0.885 - ETA: 2s - loss: 0.8552 - acc: 0.884 - ETA: 2s - loss: 0.8537 - acc: 0.883 - ETA: 2s - loss: 0.8457 - acc: 0.883 - ETA: 1s - loss: 0.8345 - acc: 0.883 - ETA: 1s - loss: 0.8344 - acc: 0.882 - ETA: 1s - loss: 0.8217 - acc: 0.883 - ETA: 1s - loss: 0.8129 - acc: 0.884 - ETA: 1s - loss: 0.8045 - acc: 0.886 - ETA: 1s - loss: 0.7980 - acc: 0.886 - ETA: 1s - loss: 0.7907 - acc: 0.886 - ETA: 1s - loss: 0.7927 - acc: 0.885 - ETA: 1s - loss: 0.7863 - acc: 0.883 - ETA: 1s - loss: 0.7756 - acc: 0.884 - ETA: 1s - loss: 0.7690 - acc: 0.884 - ETA: 1s - loss: 0.7676 - acc: 0.884 - ETA: 1s - loss: 0.7607 - acc: 0.885 - ETA: 1s - loss: 0.7599 - acc: 0.884 - ETA: 1s - loss: 0.7597 - acc: 0.884 - ETA: 1s - loss: 0.7549 - acc: 0.885 - ETA: 1s - loss: 0.7449 - acc: 0.886 - ETA: 1s - loss: 0.7357 - acc: 0.886 - ETA: 1s - loss: 0.7271 - acc: 0.886 - ETA: 0s - loss: 0.7230 - acc: 0.886 - ETA: 0s - loss: 0.7266 - acc: 0.885 - ETA: 0s - loss: 0.7281 - acc: 0.884 - ETA: 0s - loss: 0.7200 - acc: 0.884 - ETA: 0s - loss: 0.7182 - acc: 0.885 - ETA: 0s - loss: 0.7171 - acc: 0.886 - ETA: 0s - loss: 0.7120 - acc: 0.886 - ETA: 0s - loss: 0.7116 - acc: 0.886 - ETA: 0s - loss: 0.7067 - acc: 0.886 - ETA: 0s - loss: 0.7024 - acc: 0.886 - ETA: 0s - loss: 0.6977 - acc: 0.886 - ETA: 0s - loss: 0.6994 - acc: 0.886 - ETA: 0s - loss: 0.6957 - acc: 0.887 - ETA: 0s - loss: 0.6952 - acc: 0.887 - ETA: 0s - loss: 0.6907 - acc: 0.887 - ETA: 0s - loss: 0.6868 - acc: 0.887 - ETA: 0s - loss: 0.6903 - acc: 0.887 - ETA: 0s - loss: 0.6859 - acc: 0.888 - ETA: 0s - loss: 0.6796 - acc: 0.888 - ETA: 0s - loss: 0.6796 - acc: 0.888 - 6s 2ms/step - loss: 0.6792 - acc: 0.8884 - val_loss: 0.4681 - val_acc: 0.8686\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.8440 - acc: 0.875 - ETA: 5s - loss: 0.4254 - acc: 0.859 - ETA: 5s - loss: 0.2981 - acc: 0.901 - ETA: 5s - loss: 0.3614 - acc: 0.912 - ETA: 5s - loss: 0.3261 - acc: 0.913 - ETA: 5s - loss: 0.3857 - acc: 0.902 - ETA: 4s - loss: 0.4112 - acc: 0.904 - ETA: 4s - loss: 0.4197 - acc: 0.906 - ETA: 4s - loss: 0.4714 - acc: 0.900 - ETA: 4s - loss: 0.4840 - acc: 0.897 - ETA: 4s - loss: 0.4633 - acc: 0.900 - ETA: 4s - loss: 0.5590 - acc: 0.892 - ETA: 4s - loss: 0.5328 - acc: 0.895 - ETA: 4s - loss: 0.5341 - acc: 0.892 - ETA: 4s - loss: 0.5679 - acc: 0.881 - ETA: 4s - loss: 0.5458 - acc: 0.881 - ETA: 4s - loss: 0.5259 - acc: 0.885 - ETA: 4s - loss: 0.5657 - acc: 0.882 - ETA: 4s - loss: 0.5720 - acc: 0.883 - ETA: 4s - loss: 0.5551 - acc: 0.884 - ETA: 4s - loss: 0.5431 - acc: 0.884 - ETA: 4s - loss: 0.5506 - acc: 0.882 - ETA: 4s - loss: 0.5396 - acc: 0.881 - ETA: 4s - loss: 0.5391 - acc: 0.882 - ETA: 4s - loss: 0.5512 - acc: 0.883 - ETA: 4s - loss: 0.5359 - acc: 0.883 - ETA: 4s - loss: 0.5189 - acc: 0.887 - ETA: 4s - loss: 0.5101 - acc: 0.887 - ETA: 3s - loss: 0.4964 - acc: 0.888 - ETA: 3s - loss: 0.4875 - acc: 0.890 - ETA: 3s - loss: 0.5038 - acc: 0.890 - ETA: 3s - loss: 0.4987 - acc: 0.889 - ETA: 3s - loss: 0.5119 - acc: 0.887 - ETA: 3s - loss: 0.5185 - acc: 0.886 - ETA: 3s - loss: 0.5301 - acc: 0.884 - ETA: 3s - loss: 0.5270 - acc: 0.886 - ETA: 3s - loss: 0.5153 - acc: 0.887 - ETA: 3s - loss: 0.5142 - acc: 0.887 - ETA: 3s - loss: 0.5173 - acc: 0.884 - ETA: 3s - loss: 0.5071 - acc: 0.885 - ETA: 3s - loss: 0.4987 - acc: 0.885 - ETA: 3s - loss: 0.5012 - acc: 0.886 - ETA: 3s - loss: 0.4934 - acc: 0.886 - ETA: 2s - loss: 0.5011 - acc: 0.887 - ETA: 2s - loss: 0.4938 - acc: 0.888 - ETA: 2s - loss: 0.4834 - acc: 0.890 - ETA: 2s - loss: 0.4831 - acc: 0.890 - ETA: 2s - loss: 0.4831 - acc: 0.892 - ETA: 2s - loss: 0.4846 - acc: 0.890 - ETA: 2s - loss: 0.4822 - acc: 0.889 - ETA: 2s - loss: 0.4747 - acc: 0.890 - ETA: 2s - loss: 0.4705 - acc: 0.890 - ETA: 2s - loss: 0.4647 - acc: 0.890 - ETA: 2s - loss: 0.4794 - acc: 0.888 - ETA: 2s - loss: 0.4729 - acc: 0.889 - ETA: 2s - loss: 0.4792 - acc: 0.889 - ETA: 2s - loss: 0.4847 - acc: 0.890 - ETA: 1s - loss: 0.4784 - acc: 0.890 - ETA: 1s - loss: 0.4896 - acc: 0.890 - ETA: 1s - loss: 0.4828 - acc: 0.891 - ETA: 1s - loss: 0.4828 - acc: 0.891 - ETA: 1s - loss: 0.4825 - acc: 0.891 - ETA: 1s - loss: 0.4771 - acc: 0.891 - ETA: 1s - loss: 0.4768 - acc: 0.892 - ETA: 1s - loss: 0.4714 - acc: 0.892 - ETA: 1s - loss: 0.4673 - acc: 0.892 - ETA: 1s - loss: 0.4736 - acc: 0.892 - ETA: 1s - loss: 0.4674 - acc: 0.893 - ETA: 1s - loss: 0.4670 - acc: 0.893 - ETA: 1s - loss: 0.4669 - acc: 0.893 - ETA: 1s - loss: 0.4634 - acc: 0.892 - ETA: 1s - loss: 0.4643 - acc: 0.893 - ETA: 1s - loss: 0.4590 - acc: 0.893 - ETA: 0s - loss: 0.4535 - acc: 0.894 - ETA: 0s - loss: 0.4490 - acc: 0.894 - ETA: 0s - loss: 0.4534 - acc: 0.894 - ETA: 0s - loss: 0.4538 - acc: 0.894 - ETA: 0s - loss: 0.4598 - acc: 0.892 - ETA: 0s - loss: 0.4607 - acc: 0.891 - ETA: 0s - loss: 0.4559 - acc: 0.892 - ETA: 0s - loss: 0.4551 - acc: 0.893 - ETA: 0s - loss: 0.4517 - acc: 0.893 - ETA: 0s - loss: 0.4467 - acc: 0.894 - ETA: 0s - loss: 0.4434 - acc: 0.894 - ETA: 0s - loss: 0.4469 - acc: 0.894 - ETA: 0s - loss: 0.4429 - acc: 0.894 - ETA: 0s - loss: 0.4389 - acc: 0.895 - ETA: 0s - loss: 0.4355 - acc: 0.895 - ETA: 0s - loss: 0.4358 - acc: 0.895 - 6s 1ms/step - loss: 0.4356 - acc: 0.8953 - val_loss: 0.4345 - val_acc: 0.8872\n",
      "Epoch 85/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0353 - acc: 1.000 - ETA: 5s - loss: 0.0906 - acc: 0.937 - ETA: 5s - loss: 0.2994 - acc: 0.910 - ETA: 4s - loss: 0.3374 - acc: 0.912 - ETA: 4s - loss: 0.3889 - acc: 0.894 - ETA: 4s - loss: 0.3378 - acc: 0.902 - ETA: 4s - loss: 0.2939 - acc: 0.911 - ETA: 4s - loss: 0.4116 - acc: 0.900 - ETA: 4s - loss: 0.3887 - acc: 0.895 - ETA: 4s - loss: 0.3535 - acc: 0.901 - ETA: 4s - loss: 0.3593 - acc: 0.907 - ETA: 4s - loss: 0.3328 - acc: 0.915 - ETA: 4s - loss: 0.3281 - acc: 0.915 - ETA: 4s - loss: 0.3597 - acc: 0.914 - ETA: 4s - loss: 0.3861 - acc: 0.912 - ETA: 4s - loss: 0.4101 - acc: 0.913 - ETA: 4s - loss: 0.3990 - acc: 0.909 - ETA: 4s - loss: 0.4231 - acc: 0.905 - ETA: 3s - loss: 0.4046 - acc: 0.908 - ETA: 3s - loss: 0.4229 - acc: 0.910 - ETA: 3s - loss: 0.4323 - acc: 0.906 - ETA: 3s - loss: 0.4308 - acc: 0.906 - ETA: 3s - loss: 0.4145 - acc: 0.909 - ETA: 3s - loss: 0.4148 - acc: 0.908 - ETA: 3s - loss: 0.4010 - acc: 0.910 - ETA: 3s - loss: 0.4050 - acc: 0.910 - ETA: 3s - loss: 0.4004 - acc: 0.911 - ETA: 3s - loss: 0.4309 - acc: 0.908 - ETA: 3s - loss: 0.4312 - acc: 0.908 - ETA: 3s - loss: 0.4221 - acc: 0.910 - ETA: 3s - loss: 0.4323 - acc: 0.909 - ETA: 3s - loss: 0.4331 - acc: 0.909 - ETA: 3s - loss: 0.4386 - acc: 0.907 - ETA: 3s - loss: 0.4411 - acc: 0.905 - ETA: 3s - loss: 0.4447 - acc: 0.902 - ETA: 3s - loss: 0.4454 - acc: 0.902 - ETA: 3s - loss: 0.4363 - acc: 0.901 - ETA: 3s - loss: 0.4265 - acc: 0.902 - ETA: 3s - loss: 0.4175 - acc: 0.903 - ETA: 2s - loss: 0.4172 - acc: 0.901 - ETA: 2s - loss: 0.4205 - acc: 0.900 - ETA: 2s - loss: 0.4236 - acc: 0.897 - ETA: 2s - loss: 0.4251 - acc: 0.897 - ETA: 2s - loss: 0.4197 - acc: 0.896 - ETA: 2s - loss: 0.4207 - acc: 0.896 - ETA: 2s - loss: 0.4180 - acc: 0.896 - ETA: 2s - loss: 0.4172 - acc: 0.897 - ETA: 2s - loss: 0.4248 - acc: 0.897 - ETA: 2s - loss: 0.4177 - acc: 0.898 - ETA: 2s - loss: 0.4187 - acc: 0.898 - ETA: 2s - loss: 0.4179 - acc: 0.897 - ETA: 2s - loss: 0.4110 - acc: 0.898 - ETA: 2s - loss: 0.4197 - acc: 0.896 - ETA: 2s - loss: 0.4242 - acc: 0.895 - ETA: 2s - loss: 0.4175 - acc: 0.896 - ETA: 1s - loss: 0.4123 - acc: 0.896 - ETA: 1s - loss: 0.4131 - acc: 0.896 - ETA: 1s - loss: 0.4151 - acc: 0.896 - ETA: 1s - loss: 0.4228 - acc: 0.895 - ETA: 1s - loss: 0.4288 - acc: 0.895 - ETA: 1s - loss: 0.4230 - acc: 0.895 - ETA: 1s - loss: 0.4225 - acc: 0.895 - ETA: 1s - loss: 0.4177 - acc: 0.895 - ETA: 1s - loss: 0.4195 - acc: 0.894 - ETA: 1s - loss: 0.4153 - acc: 0.895 - ETA: 1s - loss: 0.4151 - acc: 0.895 - ETA: 1s - loss: 0.4109 - acc: 0.896 - ETA: 1s - loss: 0.4139 - acc: 0.895 - ETA: 1s - loss: 0.4092 - acc: 0.896 - ETA: 1s - loss: 0.4049 - acc: 0.896 - ETA: 1s - loss: 0.4052 - acc: 0.896 - ETA: 0s - loss: 0.4034 - acc: 0.896 - ETA: 0s - loss: 0.4083 - acc: 0.896 - ETA: 0s - loss: 0.4109 - acc: 0.895 - ETA: 0s - loss: 0.4084 - acc: 0.895 - ETA: 0s - loss: 0.4052 - acc: 0.895 - ETA: 0s - loss: 0.4073 - acc: 0.895 - ETA: 0s - loss: 0.4061 - acc: 0.894 - ETA: 0s - loss: 0.4040 - acc: 0.895 - ETA: 0s - loss: 0.4021 - acc: 0.894 - ETA: 0s - loss: 0.3993 - acc: 0.895 - ETA: 0s - loss: 0.3974 - acc: 0.894 - ETA: 0s - loss: 0.3944 - acc: 0.895 - ETA: 0s - loss: 0.3993 - acc: 0.895 - ETA: 0s - loss: 0.4009 - acc: 0.894 - ETA: 0s - loss: 0.3969 - acc: 0.895 - ETA: 0s - loss: 0.3990 - acc: 0.894 - ETA: 0s - loss: 0.4006 - acc: 0.894 - ETA: 0s - loss: 0.3983 - acc: 0.894 - ETA: 0s - loss: 0.3944 - acc: 0.895 - ETA: 0s - loss: 0.3970 - acc: 0.895 - 6s 1ms/step - loss: 0.3955 - acc: 0.8962 - val_loss: 0.4314 - val_acc: 0.9276\n",
      "Epoch 86/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0425 - acc: 1.000 - ETA: 5s - loss: 0.5697 - acc: 0.921 - ETA: 5s - loss: 0.5215 - acc: 0.919 - ETA: 5s - loss: 0.4974 - acc: 0.906 - ETA: 5s - loss: 0.3898 - acc: 0.918 - ETA: 5s - loss: 0.3936 - acc: 0.921 - ETA: 5s - loss: 0.3986 - acc: 0.924 - ETA: 5s - loss: 0.3671 - acc: 0.928 - ETA: 5s - loss: 0.4010 - acc: 0.929 - ETA: 5s - loss: 0.3665 - acc: 0.930 - ETA: 5s - loss: 0.4417 - acc: 0.926 - ETA: 5s - loss: 0.4355 - acc: 0.931 - ETA: 5s - loss: 0.4176 - acc: 0.930 - ETA: 5s - loss: 0.3982 - acc: 0.932 - ETA: 5s - loss: 0.4084 - acc: 0.930 - ETA: 4s - loss: 0.4096 - acc: 0.932 - ETA: 4s - loss: 0.4102 - acc: 0.931 - ETA: 4s - loss: 0.4010 - acc: 0.930 - ETA: 4s - loss: 0.3877 - acc: 0.930 - ETA: 4s - loss: 0.3707 - acc: 0.932 - ETA: 4s - loss: 0.3927 - acc: 0.931 - ETA: 4s - loss: 0.3791 - acc: 0.930 - ETA: 4s - loss: 0.3696 - acc: 0.927 - ETA: 4s - loss: 0.3602 - acc: 0.924 - ETA: 4s - loss: 0.3501 - acc: 0.922 - ETA: 4s - loss: 0.3529 - acc: 0.920 - ETA: 3s - loss: 0.3579 - acc: 0.918 - ETA: 3s - loss: 0.3544 - acc: 0.912 - ETA: 3s - loss: 0.3416 - acc: 0.914 - ETA: 3s - loss: 0.3677 - acc: 0.914 - ETA: 3s - loss: 0.3623 - acc: 0.914 - ETA: 3s - loss: 0.3639 - acc: 0.913 - ETA: 3s - loss: 0.3647 - acc: 0.915 - ETA: 3s - loss: 0.3688 - acc: 0.915 - ETA: 3s - loss: 0.3815 - acc: 0.912 - ETA: 3s - loss: 0.3941 - acc: 0.912 - ETA: 3s - loss: 0.3856 - acc: 0.914 - ETA: 3s - loss: 0.3792 - acc: 0.913 - ETA: 3s - loss: 0.3796 - acc: 0.913 - ETA: 2s - loss: 0.3745 - acc: 0.914 - ETA: 2s - loss: 0.3842 - acc: 0.915 - ETA: 2s - loss: 0.3870 - acc: 0.914 - ETA: 2s - loss: 0.3886 - acc: 0.914 - ETA: 2s - loss: 0.3863 - acc: 0.913 - ETA: 2s - loss: 0.3919 - acc: 0.911 - ETA: 2s - loss: 0.4078 - acc: 0.910 - ETA: 2s - loss: 0.4082 - acc: 0.910 - ETA: 2s - loss: 0.4107 - acc: 0.910 - ETA: 2s - loss: 0.4039 - acc: 0.911 - ETA: 2s - loss: 0.3968 - acc: 0.912 - ETA: 2s - loss: 0.3964 - acc: 0.913 - ETA: 2s - loss: 0.3914 - acc: 0.914 - ETA: 2s - loss: 0.3869 - acc: 0.914 - ETA: 2s - loss: 0.3886 - acc: 0.914 - ETA: 1s - loss: 0.3968 - acc: 0.914 - ETA: 1s - loss: 0.3907 - acc: 0.915 - ETA: 1s - loss: 0.3850 - acc: 0.916 - ETA: 1s - loss: 0.3811 - acc: 0.916 - ETA: 1s - loss: 0.3812 - acc: 0.917 - ETA: 1s - loss: 0.3785 - acc: 0.917 - ETA: 1s - loss: 0.3817 - acc: 0.916 - ETA: 1s - loss: 0.3782 - acc: 0.917 - ETA: 1s - loss: 0.3730 - acc: 0.919 - ETA: 1s - loss: 0.3731 - acc: 0.919 - ETA: 1s - loss: 0.3706 - acc: 0.920 - ETA: 1s - loss: 0.3725 - acc: 0.920 - ETA: 1s - loss: 0.3701 - acc: 0.920 - ETA: 1s - loss: 0.3682 - acc: 0.920 - ETA: 1s - loss: 0.3680 - acc: 0.920 - ETA: 1s - loss: 0.3664 - acc: 0.921 - ETA: 1s - loss: 0.3668 - acc: 0.921 - ETA: 1s - loss: 0.3671 - acc: 0.922 - ETA: 1s - loss: 0.3673 - acc: 0.923 - ETA: 0s - loss: 0.3726 - acc: 0.922 - ETA: 0s - loss: 0.3697 - acc: 0.922 - ETA: 0s - loss: 0.3703 - acc: 0.923 - ETA: 0s - loss: 0.3712 - acc: 0.923 - ETA: 0s - loss: 0.3719 - acc: 0.924 - ETA: 0s - loss: 0.3677 - acc: 0.925 - ETA: 0s - loss: 0.3636 - acc: 0.926 - ETA: 0s - loss: 0.3667 - acc: 0.925 - ETA: 0s - loss: 0.3668 - acc: 0.925 - ETA: 0s - loss: 0.3692 - acc: 0.924 - ETA: 0s - loss: 0.3713 - acc: 0.924 - ETA: 0s - loss: 0.3715 - acc: 0.925 - ETA: 0s - loss: 0.3717 - acc: 0.926 - ETA: 0s - loss: 0.3685 - acc: 0.926 - ETA: 0s - loss: 0.3694 - acc: 0.926 - ETA: 0s - loss: 0.3658 - acc: 0.926 - ETA: 0s - loss: 0.3667 - acc: 0.926 - 6s 1ms/step - loss: 0.3665 - acc: 0.9270 - val_loss: 0.3859 - val_acc: 0.9391\n",
      "Epoch 87/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0652 - acc: 0.937 - ETA: 4s - loss: 1.0609 - acc: 0.921 - ETA: 4s - loss: 0.6351 - acc: 0.946 - ETA: 4s - loss: 0.5682 - acc: 0.950 - ETA: 4s - loss: 0.4396 - acc: 0.961 - ETA: 4s - loss: 0.4157 - acc: 0.953 - ETA: 4s - loss: 0.3789 - acc: 0.947 - ETA: 4s - loss: 0.3439 - acc: 0.948 - ETA: 4s - loss: 0.3117 - acc: 0.947 - ETA: 4s - loss: 0.3564 - acc: 0.944 - ETA: 4s - loss: 0.3655 - acc: 0.941 - ETA: 4s - loss: 0.3480 - acc: 0.941 - ETA: 4s - loss: 0.3368 - acc: 0.935 - ETA: 4s - loss: 0.3302 - acc: 0.939 - ETA: 4s - loss: 0.3127 - acc: 0.940 - ETA: 4s - loss: 0.3249 - acc: 0.938 - ETA: 4s - loss: 0.3524 - acc: 0.937 - ETA: 3s - loss: 0.3588 - acc: 0.936 - ETA: 3s - loss: 0.3824 - acc: 0.934 - ETA: 3s - loss: 0.4058 - acc: 0.933 - ETA: 3s - loss: 0.4081 - acc: 0.934 - ETA: 3s - loss: 0.3944 - acc: 0.934 - ETA: 3s - loss: 0.3945 - acc: 0.935 - ETA: 3s - loss: 0.4068 - acc: 0.936 - ETA: 3s - loss: 0.4057 - acc: 0.937 - ETA: 3s - loss: 0.3922 - acc: 0.938 - ETA: 3s - loss: 0.3789 - acc: 0.939 - ETA: 3s - loss: 0.3672 - acc: 0.941 - ETA: 3s - loss: 0.3588 - acc: 0.941 - ETA: 3s - loss: 0.3749 - acc: 0.940 - ETA: 3s - loss: 0.3639 - acc: 0.941 - ETA: 3s - loss: 0.3558 - acc: 0.940 - ETA: 3s - loss: 0.3473 - acc: 0.941 - ETA: 3s - loss: 0.3519 - acc: 0.939 - ETA: 2s - loss: 0.3539 - acc: 0.939 - ETA: 2s - loss: 0.3546 - acc: 0.939 - ETA: 2s - loss: 0.3576 - acc: 0.939 - ETA: 2s - loss: 0.3610 - acc: 0.939 - ETA: 2s - loss: 0.3747 - acc: 0.938 - ETA: 2s - loss: 0.3691 - acc: 0.938 - ETA: 2s - loss: 0.3646 - acc: 0.938 - ETA: 2s - loss: 0.3680 - acc: 0.938 - ETA: 2s - loss: 0.3628 - acc: 0.939 - ETA: 2s - loss: 0.3742 - acc: 0.938 - ETA: 2s - loss: 0.3704 - acc: 0.938 - ETA: 2s - loss: 0.3681 - acc: 0.938 - ETA: 2s - loss: 0.3710 - acc: 0.938 - ETA: 2s - loss: 0.3669 - acc: 0.938 - ETA: 2s - loss: 0.3627 - acc: 0.938 - ETA: 2s - loss: 0.3596 - acc: 0.937 - ETA: 2s - loss: 0.3555 - acc: 0.938 - ETA: 2s - loss: 0.3519 - acc: 0.938 - ETA: 2s - loss: 0.3474 - acc: 0.939 - ETA: 2s - loss: 0.3433 - acc: 0.940 - ETA: 2s - loss: 0.3466 - acc: 0.940 - ETA: 2s - loss: 0.3431 - acc: 0.940 - ETA: 2s - loss: 0.3464 - acc: 0.940 - ETA: 2s - loss: 0.3427 - acc: 0.940 - ETA: 2s - loss: 0.3368 - acc: 0.942 - ETA: 2s - loss: 0.3409 - acc: 0.940 - ETA: 2s - loss: 0.3428 - acc: 0.940 - ETA: 2s - loss: 0.3451 - acc: 0.941 - ETA: 2s - loss: 0.3393 - acc: 0.942 - ETA: 1s - loss: 0.3422 - acc: 0.941 - ETA: 1s - loss: 0.3431 - acc: 0.941 - ETA: 1s - loss: 0.3387 - acc: 0.942 - ETA: 1s - loss: 0.3405 - acc: 0.942 - ETA: 1s - loss: 0.3468 - acc: 0.942 - ETA: 1s - loss: 0.3471 - acc: 0.942 - ETA: 1s - loss: 0.3439 - acc: 0.942 - ETA: 1s - loss: 0.3411 - acc: 0.943 - ETA: 1s - loss: 0.3441 - acc: 0.943 - ETA: 1s - loss: 0.3411 - acc: 0.943 - ETA: 1s - loss: 0.3495 - acc: 0.942 - ETA: 1s - loss: 0.3671 - acc: 0.941 - ETA: 1s - loss: 0.3678 - acc: 0.941 - ETA: 1s - loss: 0.3650 - acc: 0.941 - ETA: 1s - loss: 0.3817 - acc: 0.940 - ETA: 1s - loss: 0.3866 - acc: 0.940 - ETA: 1s - loss: 0.3863 - acc: 0.941 - ETA: 0s - loss: 0.4194 - acc: 0.939 - ETA: 0s - loss: 0.4250 - acc: 0.939 - ETA: 0s - loss: 0.4387 - acc: 0.938 - ETA: 0s - loss: 0.4521 - acc: 0.938 - ETA: 0s - loss: 0.4644 - acc: 0.937 - ETA: 0s - loss: 0.4814 - acc: 0.936 - ETA: 0s - loss: 0.4856 - acc: 0.935 - ETA: 0s - loss: 0.5026 - acc: 0.934 - ETA: 0s - loss: 0.5305 - acc: 0.932 - ETA: 0s - loss: 0.5330 - acc: 0.932 - ETA: 0s - loss: 0.5437 - acc: 0.932 - ETA: 0s - loss: 0.5671 - acc: 0.930 - ETA: 0s - loss: 0.5688 - acc: 0.930 - ETA: 0s - loss: 0.5706 - acc: 0.931 - ETA: 0s - loss: 0.5807 - acc: 0.930 - 6s 2ms/step - loss: 0.5803 - acc: 0.9309 - val_loss: 1.2782 - val_acc: 0.8859\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 1.0595 - acc: 0.937 - ETA: 6s - loss: 1.0507 - acc: 0.937 - ETA: 5s - loss: 1.6596 - acc: 0.854 - ETA: 5s - loss: 1.5696 - acc: 0.875 - ETA: 5s - loss: 1.4599 - acc: 0.880 - ETA: 5s - loss: 1.3343 - acc: 0.891 - ETA: 5s - loss: 1.3278 - acc: 0.892 - ETA: 5s - loss: 1.2919 - acc: 0.892 - ETA: 4s - loss: 1.3075 - acc: 0.893 - ETA: 4s - loss: 1.2423 - acc: 0.900 - ETA: 4s - loss: 1.2711 - acc: 0.897 - ETA: 4s - loss: 1.2199 - acc: 0.901 - ETA: 4s - loss: 1.2063 - acc: 0.902 - ETA: 4s - loss: 1.2048 - acc: 0.899 - ETA: 4s - loss: 1.2006 - acc: 0.894 - ETA: 4s - loss: 1.2845 - acc: 0.887 - ETA: 4s - loss: 1.2913 - acc: 0.888 - ETA: 4s - loss: 1.3406 - acc: 0.883 - ETA: 4s - loss: 1.3476 - acc: 0.880 - ETA: 4s - loss: 1.3163 - acc: 0.882 - ETA: 4s - loss: 1.2926 - acc: 0.884 - ETA: 4s - loss: 1.3324 - acc: 0.880 - ETA: 3s - loss: 1.4013 - acc: 0.876 - ETA: 3s - loss: 1.3896 - acc: 0.876 - ETA: 3s - loss: 1.4184 - acc: 0.876 - ETA: 3s - loss: 1.4626 - acc: 0.875 - ETA: 3s - loss: 1.4351 - acc: 0.878 - ETA: 3s - loss: 1.4410 - acc: 0.874 - ETA: 3s - loss: 1.4690 - acc: 0.872 - ETA: 3s - loss: 1.4401 - acc: 0.875 - ETA: 3s - loss: 1.4536 - acc: 0.875 - ETA: 3s - loss: 1.4891 - acc: 0.873 - ETA: 3s - loss: 1.5289 - acc: 0.871 - ETA: 3s - loss: 1.5192 - acc: 0.872 - ETA: 3s - loss: 1.5187 - acc: 0.872 - ETA: 3s - loss: 1.5177 - acc: 0.872 - ETA: 3s - loss: 1.5202 - acc: 0.871 - ETA: 3s - loss: 1.5220 - acc: 0.871 - ETA: 3s - loss: 1.5252 - acc: 0.869 - ETA: 3s - loss: 1.5107 - acc: 0.867 - ETA: 3s - loss: 1.4960 - acc: 0.866 - ETA: 3s - loss: 1.5110 - acc: 0.862 - ETA: 3s - loss: 1.5515 - acc: 0.854 - ETA: 3s - loss: 1.5563 - acc: 0.850 - ETA: 3s - loss: 1.5430 - acc: 0.848 - ETA: 3s - loss: 1.5299 - acc: 0.844 - ETA: 3s - loss: 1.5426 - acc: 0.840 - ETA: 3s - loss: 1.5311 - acc: 0.837 - ETA: 3s - loss: 1.5255 - acc: 0.837 - ETA: 3s - loss: 1.5222 - acc: 0.833 - ETA: 3s - loss: 1.5233 - acc: 0.829 - ETA: 2s - loss: 1.5239 - acc: 0.827 - ETA: 2s - loss: 1.5122 - acc: 0.822 - ETA: 2s - loss: 1.4961 - acc: 0.819 - ETA: 2s - loss: 1.4781 - acc: 0.815 - ETA: 2s - loss: 1.4698 - acc: 0.812 - ETA: 2s - loss: 1.4566 - acc: 0.812 - ETA: 2s - loss: 1.4468 - acc: 0.810 - ETA: 2s - loss: 1.4373 - acc: 0.808 - ETA: 2s - loss: 1.4285 - acc: 0.806 - ETA: 2s - loss: 1.4182 - acc: 0.807 - ETA: 2s - loss: 1.4092 - acc: 0.808 - ETA: 2s - loss: 1.4015 - acc: 0.809 - ETA: 2s - loss: 1.3916 - acc: 0.809 - ETA: 2s - loss: 1.3878 - acc: 0.810 - ETA: 2s - loss: 1.3868 - acc: 0.811 - ETA: 1s - loss: 1.3725 - acc: 0.812 - ETA: 1s - loss: 1.3588 - acc: 0.813 - ETA: 1s - loss: 1.3447 - acc: 0.815 - ETA: 1s - loss: 1.3492 - acc: 0.815 - ETA: 1s - loss: 1.3521 - acc: 0.816 - ETA: 1s - loss: 1.3436 - acc: 0.817 - ETA: 1s - loss: 1.3426 - acc: 0.817 - ETA: 1s - loss: 1.3312 - acc: 0.818 - ETA: 1s - loss: 1.3235 - acc: 0.819 - ETA: 1s - loss: 1.3271 - acc: 0.819 - ETA: 1s - loss: 1.3363 - acc: 0.819 - ETA: 1s - loss: 1.3386 - acc: 0.820 - ETA: 1s - loss: 1.3379 - acc: 0.820 - ETA: 1s - loss: 1.3319 - acc: 0.820 - ETA: 1s - loss: 1.3160 - acc: 0.821 - ETA: 0s - loss: 1.3285 - acc: 0.821 - ETA: 0s - loss: 1.3332 - acc: 0.820 - ETA: 0s - loss: 1.3268 - acc: 0.820 - ETA: 0s - loss: 1.3332 - acc: 0.821 - ETA: 0s - loss: 1.3284 - acc: 0.820 - ETA: 0s - loss: 1.3311 - acc: 0.820 - ETA: 0s - loss: 1.3256 - acc: 0.821 - ETA: 0s - loss: 1.3246 - acc: 0.821 - ETA: 0s - loss: 1.3188 - acc: 0.822 - ETA: 0s - loss: 1.3175 - acc: 0.822 - ETA: 0s - loss: 1.3029 - acc: 0.823 - ETA: 0s - loss: 1.2943 - acc: 0.823 - ETA: 0s - loss: 1.2818 - acc: 0.824 - ETA: 0s - loss: 1.2734 - acc: 0.825 - ETA: 0s - loss: 1.2649 - acc: 0.826 - 6s 2ms/step - loss: 1.2596 - acc: 0.8269 - val_loss: 0.4688 - val_acc: 0.8795\n",
      "Epoch 89/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.2818 - acc: 0.812 - ETA: 5s - loss: 0.4061 - acc: 0.921 - ETA: 5s - loss: 0.3085 - acc: 0.937 - ETA: 5s - loss: 0.4731 - acc: 0.931 - ETA: 5s - loss: 0.5077 - acc: 0.932 - ETA: 5s - loss: 0.4308 - acc: 0.937 - ETA: 5s - loss: 0.3822 - acc: 0.941 - ETA: 5s - loss: 0.4041 - acc: 0.940 - ETA: 5s - loss: 0.3947 - acc: 0.929 - ETA: 5s - loss: 0.3759 - acc: 0.927 - ETA: 5s - loss: 0.3601 - acc: 0.930 - ETA: 5s - loss: 0.4244 - acc: 0.921 - ETA: 5s - loss: 0.4523 - acc: 0.912 - ETA: 5s - loss: 0.4675 - acc: 0.912 - ETA: 5s - loss: 0.4516 - acc: 0.908 - ETA: 5s - loss: 0.4615 - acc: 0.904 - ETA: 5s - loss: 0.4720 - acc: 0.904 - ETA: 5s - loss: 0.4675 - acc: 0.898 - ETA: 5s - loss: 0.4735 - acc: 0.900 - ETA: 5s - loss: 0.4645 - acc: 0.898 - ETA: 5s - loss: 0.4583 - acc: 0.894 - ETA: 5s - loss: 0.4474 - acc: 0.893 - ETA: 5s - loss: 0.4369 - acc: 0.895 - ETA: 4s - loss: 0.4605 - acc: 0.895 - ETA: 4s - loss: 0.4598 - acc: 0.897 - ETA: 4s - loss: 0.4616 - acc: 0.898 - ETA: 4s - loss: 0.4976 - acc: 0.894 - ETA: 4s - loss: 0.4825 - acc: 0.897 - ETA: 4s - loss: 0.4741 - acc: 0.895 - ETA: 4s - loss: 0.4834 - acc: 0.893 - ETA: 4s - loss: 0.4847 - acc: 0.894 - ETA: 4s - loss: 0.5183 - acc: 0.893 - ETA: 4s - loss: 0.5184 - acc: 0.894 - ETA: 4s - loss: 0.5205 - acc: 0.892 - ETA: 4s - loss: 0.5105 - acc: 0.892 - ETA: 3s - loss: 0.5089 - acc: 0.893 - ETA: 3s - loss: 0.5095 - acc: 0.893 - ETA: 3s - loss: 0.5105 - acc: 0.894 - ETA: 3s - loss: 0.5030 - acc: 0.894 - ETA: 3s - loss: 0.5119 - acc: 0.894 - ETA: 3s - loss: 0.5200 - acc: 0.894 - ETA: 3s - loss: 0.5484 - acc: 0.893 - ETA: 3s - loss: 0.5553 - acc: 0.893 - ETA: 3s - loss: 0.5612 - acc: 0.894 - ETA: 3s - loss: 0.5546 - acc: 0.895 - ETA: 3s - loss: 0.5492 - acc: 0.895 - ETA: 3s - loss: 0.5426 - acc: 0.896 - ETA: 3s - loss: 0.5348 - acc: 0.898 - ETA: 3s - loss: 0.5294 - acc: 0.898 - ETA: 3s - loss: 0.5229 - acc: 0.899 - ETA: 3s - loss: 0.5222 - acc: 0.899 - ETA: 2s - loss: 0.5274 - acc: 0.900 - ETA: 2s - loss: 0.5264 - acc: 0.901 - ETA: 2s - loss: 0.5187 - acc: 0.902 - ETA: 2s - loss: 0.5139 - acc: 0.902 - ETA: 2s - loss: 0.5114 - acc: 0.900 - ETA: 2s - loss: 0.5134 - acc: 0.900 - ETA: 2s - loss: 0.5236 - acc: 0.897 - ETA: 2s - loss: 0.5224 - acc: 0.895 - ETA: 2s - loss: 0.5218 - acc: 0.895 - ETA: 2s - loss: 0.5165 - acc: 0.894 - ETA: 2s - loss: 0.5188 - acc: 0.892 - ETA: 2s - loss: 0.5151 - acc: 0.891 - ETA: 2s - loss: 0.5298 - acc: 0.889 - ETA: 2s - loss: 0.5248 - acc: 0.888 - ETA: 2s - loss: 0.5190 - acc: 0.888 - ETA: 1s - loss: 0.5199 - acc: 0.888 - ETA: 1s - loss: 0.5196 - acc: 0.888 - ETA: 1s - loss: 0.5220 - acc: 0.886 - ETA: 1s - loss: 0.5190 - acc: 0.885 - ETA: 1s - loss: 0.5155 - acc: 0.884 - ETA: 1s - loss: 0.5141 - acc: 0.883 - ETA: 1s - loss: 0.5153 - acc: 0.883 - ETA: 1s - loss: 0.5170 - acc: 0.883 - ETA: 1s - loss: 0.5195 - acc: 0.882 - ETA: 1s - loss: 0.5222 - acc: 0.881 - ETA: 1s - loss: 0.5193 - acc: 0.882 - ETA: 1s - loss: 0.5218 - acc: 0.881 - ETA: 1s - loss: 0.5206 - acc: 0.880 - ETA: 1s - loss: 0.5173 - acc: 0.881 - ETA: 1s - loss: 0.5191 - acc: 0.881 - ETA: 1s - loss: 0.5242 - acc: 0.881 - ETA: 1s - loss: 0.5198 - acc: 0.881 - ETA: 0s - loss: 0.5201 - acc: 0.881 - ETA: 0s - loss: 0.5215 - acc: 0.882 - ETA: 0s - loss: 0.5235 - acc: 0.881 - ETA: 0s - loss: 0.5193 - acc: 0.882 - ETA: 0s - loss: 0.5240 - acc: 0.882 - ETA: 0s - loss: 0.5288 - acc: 0.882 - ETA: 0s - loss: 0.5287 - acc: 0.882 - ETA: 0s - loss: 0.5321 - acc: 0.882 - ETA: 0s - loss: 0.5455 - acc: 0.881 - ETA: 0s - loss: 0.5540 - acc: 0.880 - ETA: 0s - loss: 0.5636 - acc: 0.879 - ETA: 0s - loss: 0.5604 - acc: 0.879 - ETA: 0s - loss: 0.5587 - acc: 0.878 - ETA: 0s - loss: 0.5538 - acc: 0.878 - ETA: 0s - loss: 0.5532 - acc: 0.879 - ETA: 0s - loss: 0.5504 - acc: 0.879 - 6s 2ms/step - loss: 0.5501 - acc: 0.8793 - val_loss: 0.4671 - val_acc: 0.8545\n",
      "Epoch 90/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0953 - acc: 1.000 - ETA: 5s - loss: 1.6798 - acc: 0.843 - ETA: 5s - loss: 1.2140 - acc: 0.848 - ETA: 5s - loss: 0.9323 - acc: 0.843 - ETA: 5s - loss: 0.9245 - acc: 0.831 - ETA: 5s - loss: 0.8153 - acc: 0.820 - ETA: 5s - loss: 0.8344 - acc: 0.815 - ETA: 4s - loss: 0.7491 - acc: 0.826 - ETA: 4s - loss: 0.6841 - acc: 0.832 - ETA: 4s - loss: 0.6403 - acc: 0.837 - ETA: 4s - loss: 0.6282 - acc: 0.844 - ETA: 4s - loss: 0.6201 - acc: 0.847 - ETA: 4s - loss: 0.5805 - acc: 0.856 - ETA: 4s - loss: 0.6282 - acc: 0.854 - ETA: 4s - loss: 0.5963 - acc: 0.860 - ETA: 4s - loss: 0.6443 - acc: 0.857 - ETA: 4s - loss: 0.6372 - acc: 0.858 - ETA: 4s - loss: 0.6277 - acc: 0.861 - ETA: 4s - loss: 0.6245 - acc: 0.861 - ETA: 4s - loss: 0.6370 - acc: 0.862 - ETA: 4s - loss: 0.6313 - acc: 0.864 - ETA: 4s - loss: 0.6111 - acc: 0.866 - ETA: 3s - loss: 0.6140 - acc: 0.863 - ETA: 3s - loss: 0.6408 - acc: 0.859 - ETA: 3s - loss: 0.6350 - acc: 0.863 - ETA: 3s - loss: 0.6293 - acc: 0.866 - ETA: 3s - loss: 0.6304 - acc: 0.866 - ETA: 3s - loss: 0.6347 - acc: 0.864 - ETA: 3s - loss: 0.6374 - acc: 0.863 - ETA: 3s - loss: 0.6367 - acc: 0.861 - ETA: 3s - loss: 0.6210 - acc: 0.864 - ETA: 3s - loss: 0.6085 - acc: 0.864 - ETA: 3s - loss: 0.6065 - acc: 0.865 - ETA: 3s - loss: 0.5937 - acc: 0.866 - ETA: 3s - loss: 0.5939 - acc: 0.865 - ETA: 3s - loss: 0.5813 - acc: 0.867 - ETA: 3s - loss: 0.5900 - acc: 0.866 - ETA: 3s - loss: 0.5881 - acc: 0.868 - ETA: 3s - loss: 0.5771 - acc: 0.868 - ETA: 2s - loss: 0.5842 - acc: 0.868 - ETA: 2s - loss: 0.5744 - acc: 0.869 - ETA: 2s - loss: 0.5646 - acc: 0.870 - ETA: 2s - loss: 0.5607 - acc: 0.870 - ETA: 2s - loss: 0.5572 - acc: 0.871 - ETA: 2s - loss: 0.5596 - acc: 0.871 - ETA: 2s - loss: 0.5625 - acc: 0.871 - ETA: 2s - loss: 0.5579 - acc: 0.871 - ETA: 2s - loss: 0.5524 - acc: 0.872 - ETA: 2s - loss: 0.5555 - acc: 0.871 - ETA: 2s - loss: 0.5527 - acc: 0.869 - ETA: 2s - loss: 0.5466 - acc: 0.870 - ETA: 2s - loss: 0.5416 - acc: 0.871 - ETA: 2s - loss: 0.5448 - acc: 0.870 - ETA: 2s - loss: 0.5467 - acc: 0.870 - ETA: 2s - loss: 0.5414 - acc: 0.871 - ETA: 2s - loss: 0.5437 - acc: 0.871 - ETA: 2s - loss: 0.5449 - acc: 0.871 - ETA: 2s - loss: 0.5464 - acc: 0.872 - ETA: 2s - loss: 0.5410 - acc: 0.873 - ETA: 2s - loss: 0.5383 - acc: 0.872 - ETA: 2s - loss: 0.5454 - acc: 0.873 - ETA: 2s - loss: 0.5413 - acc: 0.873 - ETA: 2s - loss: 0.5421 - acc: 0.873 - ETA: 2s - loss: 0.5387 - acc: 0.873 - ETA: 2s - loss: 0.5403 - acc: 0.873 - ETA: 2s - loss: 0.5351 - acc: 0.872 - ETA: 1s - loss: 0.5393 - acc: 0.873 - ETA: 1s - loss: 0.5324 - acc: 0.875 - ETA: 1s - loss: 0.5436 - acc: 0.874 - ETA: 1s - loss: 0.5476 - acc: 0.875 - ETA: 1s - loss: 0.5461 - acc: 0.876 - ETA: 1s - loss: 0.5439 - acc: 0.878 - ETA: 1s - loss: 0.5433 - acc: 0.878 - ETA: 1s - loss: 0.5364 - acc: 0.879 - ETA: 1s - loss: 0.5403 - acc: 0.879 - ETA: 1s - loss: 0.5351 - acc: 0.880 - ETA: 1s - loss: 0.5291 - acc: 0.880 - ETA: 1s - loss: 0.5334 - acc: 0.881 - ETA: 1s - loss: 0.5340 - acc: 0.881 - ETA: 1s - loss: 0.5303 - acc: 0.882 - ETA: 1s - loss: 0.5272 - acc: 0.882 - ETA: 0s - loss: 0.5223 - acc: 0.882 - ETA: 0s - loss: 0.5309 - acc: 0.882 - ETA: 0s - loss: 0.5346 - acc: 0.882 - ETA: 0s - loss: 0.5376 - acc: 0.883 - ETA: 0s - loss: 0.5369 - acc: 0.883 - ETA: 0s - loss: 0.5321 - acc: 0.884 - ETA: 0s - loss: 0.5291 - acc: 0.884 - ETA: 0s - loss: 0.5286 - acc: 0.885 - ETA: 0s - loss: 0.5245 - acc: 0.884 - ETA: 0s - loss: 0.5280 - acc: 0.884 - ETA: 0s - loss: 0.5333 - acc: 0.884 - ETA: 0s - loss: 0.5346 - acc: 0.883 - ETA: 0s - loss: 0.5337 - acc: 0.884 - ETA: 0s - loss: 0.5299 - acc: 0.884 - ETA: 0s - loss: 0.5312 - acc: 0.883 - 6s 2ms/step - loss: 0.5308 - acc: 0.8839 - val_loss: 0.4392 - val_acc: 0.8776\n",
      "Epoch 91/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0941 - acc: 1.000 - ETA: 5s - loss: 0.4286 - acc: 0.875 - ETA: 5s - loss: 0.3274 - acc: 0.892 - ETA: 4s - loss: 0.6897 - acc: 0.887 - ETA: 4s - loss: 0.7318 - acc: 0.884 - ETA: 4s - loss: 0.7551 - acc: 0.882 - ETA: 4s - loss: 0.7079 - acc: 0.888 - ETA: 4s - loss: 0.6822 - acc: 0.892 - ETA: 4s - loss: 0.6988 - acc: 0.897 - ETA: 4s - loss: 0.7603 - acc: 0.888 - ETA: 4s - loss: 0.7250 - acc: 0.887 - ETA: 4s - loss: 0.7082 - acc: 0.887 - ETA: 4s - loss: 0.7115 - acc: 0.891 - ETA: 4s - loss: 0.6750 - acc: 0.892 - ETA: 4s - loss: 0.7089 - acc: 0.892 - ETA: 4s - loss: 0.7001 - acc: 0.891 - ETA: 4s - loss: 0.6845 - acc: 0.894 - ETA: 4s - loss: 0.6915 - acc: 0.896 - ETA: 4s - loss: 0.6774 - acc: 0.900 - ETA: 4s - loss: 0.6702 - acc: 0.899 - ETA: 4s - loss: 0.6783 - acc: 0.899 - ETA: 3s - loss: 0.6567 - acc: 0.899 - ETA: 3s - loss: 0.6505 - acc: 0.899 - ETA: 3s - loss: 0.6586 - acc: 0.899 - ETA: 3s - loss: 0.6388 - acc: 0.900 - ETA: 3s - loss: 0.6183 - acc: 0.904 - ETA: 3s - loss: 0.5991 - acc: 0.907 - ETA: 3s - loss: 0.5957 - acc: 0.907 - ETA: 3s - loss: 0.6044 - acc: 0.905 - ETA: 3s - loss: 0.6019 - acc: 0.905 - ETA: 3s - loss: 0.5976 - acc: 0.905 - ETA: 3s - loss: 0.5818 - acc: 0.908 - ETA: 3s - loss: 0.5682 - acc: 0.909 - ETA: 3s - loss: 0.5778 - acc: 0.908 - ETA: 3s - loss: 0.5662 - acc: 0.908 - ETA: 3s - loss: 0.5549 - acc: 0.908 - ETA: 3s - loss: 0.5544 - acc: 0.908 - ETA: 2s - loss: 0.5690 - acc: 0.909 - ETA: 2s - loss: 0.5697 - acc: 0.907 - ETA: 2s - loss: 0.5707 - acc: 0.907 - ETA: 2s - loss: 0.5700 - acc: 0.907 - ETA: 2s - loss: 0.5840 - acc: 0.906 - ETA: 2s - loss: 0.5924 - acc: 0.906 - ETA: 2s - loss: 0.5946 - acc: 0.905 - ETA: 2s - loss: 0.5980 - acc: 0.903 - ETA: 2s - loss: 0.5943 - acc: 0.901 - ETA: 2s - loss: 0.5966 - acc: 0.900 - ETA: 2s - loss: 0.5932 - acc: 0.900 - ETA: 2s - loss: 0.5865 - acc: 0.902 - ETA: 2s - loss: 0.5884 - acc: 0.901 - ETA: 2s - loss: 0.5979 - acc: 0.900 - ETA: 2s - loss: 0.6070 - acc: 0.899 - ETA: 2s - loss: 0.5999 - acc: 0.900 - ETA: 2s - loss: 0.5944 - acc: 0.899 - ETA: 2s - loss: 0.5910 - acc: 0.898 - ETA: 2s - loss: 0.5923 - acc: 0.898 - ETA: 2s - loss: 0.5943 - acc: 0.897 - ETA: 2s - loss: 0.5880 - acc: 0.898 - ETA: 2s - loss: 0.5897 - acc: 0.897 - ETA: 2s - loss: 0.5848 - acc: 0.897 - ETA: 2s - loss: 0.5851 - acc: 0.898 - ETA: 2s - loss: 0.5867 - acc: 0.898 - ETA: 2s - loss: 0.5793 - acc: 0.899 - ETA: 2s - loss: 0.5780 - acc: 0.899 - ETA: 2s - loss: 0.5809 - acc: 0.898 - ETA: 2s - loss: 0.5734 - acc: 0.899 - ETA: 1s - loss: 0.5844 - acc: 0.897 - ETA: 1s - loss: 0.5831 - acc: 0.897 - ETA: 1s - loss: 0.5814 - acc: 0.898 - ETA: 1s - loss: 0.5809 - acc: 0.897 - ETA: 1s - loss: 0.5773 - acc: 0.896 - ETA: 1s - loss: 0.5765 - acc: 0.896 - ETA: 1s - loss: 0.5918 - acc: 0.895 - ETA: 1s - loss: 0.5935 - acc: 0.895 - ETA: 1s - loss: 0.5866 - acc: 0.895 - ETA: 1s - loss: 0.5962 - acc: 0.894 - ETA: 1s - loss: 0.5957 - acc: 0.893 - ETA: 1s - loss: 0.5905 - acc: 0.893 - ETA: 1s - loss: 0.5911 - acc: 0.892 - ETA: 0s - loss: 0.5861 - acc: 0.892 - ETA: 0s - loss: 0.5950 - acc: 0.891 - ETA: 0s - loss: 0.5890 - acc: 0.892 - ETA: 0s - loss: 0.5896 - acc: 0.891 - ETA: 0s - loss: 0.5845 - acc: 0.891 - ETA: 0s - loss: 0.5851 - acc: 0.890 - ETA: 0s - loss: 0.5823 - acc: 0.889 - ETA: 0s - loss: 0.5860 - acc: 0.888 - ETA: 0s - loss: 0.5797 - acc: 0.890 - ETA: 0s - loss: 0.5847 - acc: 0.889 - ETA: 0s - loss: 0.5875 - acc: 0.889 - ETA: 0s - loss: 0.5814 - acc: 0.890 - ETA: 0s - loss: 0.5824 - acc: 0.889 - ETA: 0s - loss: 0.5790 - acc: 0.888 - ETA: 0s - loss: 0.5746 - acc: 0.889 - 6s 1ms/step - loss: 0.5783 - acc: 0.8891 - val_loss: 0.5150 - val_acc: 0.8724\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 5s - loss: 0.2013 - acc: 0.875 - ETA: 5s - loss: 0.5140 - acc: 0.828 - ETA: 6s - loss: 0.5614 - acc: 0.854 - ETA: 6s - loss: 0.5039 - acc: 0.835 - ETA: 6s - loss: 0.4273 - acc: 0.852 - ETA: 5s - loss: 0.4540 - acc: 0.857 - ETA: 5s - loss: 0.4806 - acc: 0.852 - ETA: 5s - loss: 0.4423 - acc: 0.862 - ETA: 5s - loss: 0.4565 - acc: 0.864 - ETA: 5s - loss: 0.4686 - acc: 0.867 - ETA: 5s - loss: 0.4827 - acc: 0.864 - ETA: 5s - loss: 0.5597 - acc: 0.857 - ETA: 4s - loss: 0.5855 - acc: 0.858 - ETA: 4s - loss: 0.5617 - acc: 0.858 - ETA: 4s - loss: 0.5565 - acc: 0.861 - ETA: 4s - loss: 0.5391 - acc: 0.859 - ETA: 4s - loss: 0.5453 - acc: 0.862 - ETA: 4s - loss: 0.5620 - acc: 0.866 - ETA: 4s - loss: 0.5641 - acc: 0.865 - ETA: 4s - loss: 0.5651 - acc: 0.863 - ETA: 4s - loss: 0.5656 - acc: 0.863 - ETA: 4s - loss: 0.5632 - acc: 0.866 - ETA: 4s - loss: 0.5456 - acc: 0.871 - ETA: 4s - loss: 0.5271 - acc: 0.875 - ETA: 4s - loss: 0.5303 - acc: 0.877 - ETA: 4s - loss: 0.5372 - acc: 0.876 - ETA: 4s - loss: 0.5259 - acc: 0.880 - ETA: 4s - loss: 0.5208 - acc: 0.877 - ETA: 3s - loss: 0.5060 - acc: 0.881 - ETA: 3s - loss: 0.5171 - acc: 0.882 - ETA: 3s - loss: 0.5193 - acc: 0.882 - ETA: 3s - loss: 0.5259 - acc: 0.883 - ETA: 3s - loss: 0.5315 - acc: 0.881 - ETA: 3s - loss: 0.5346 - acc: 0.880 - ETA: 3s - loss: 0.5335 - acc: 0.880 - ETA: 3s - loss: 0.5356 - acc: 0.879 - ETA: 3s - loss: 0.5229 - acc: 0.882 - ETA: 3s - loss: 0.5175 - acc: 0.880 - ETA: 3s - loss: 0.5353 - acc: 0.880 - ETA: 3s - loss: 0.5606 - acc: 0.879 - ETA: 3s - loss: 0.5604 - acc: 0.879 - ETA: 2s - loss: 0.5610 - acc: 0.878 - ETA: 2s - loss: 0.5639 - acc: 0.880 - ETA: 2s - loss: 0.5732 - acc: 0.878 - ETA: 2s - loss: 0.5641 - acc: 0.879 - ETA: 2s - loss: 0.5545 - acc: 0.879 - ETA: 2s - loss: 0.5634 - acc: 0.879 - ETA: 2s - loss: 0.5596 - acc: 0.880 - ETA: 2s - loss: 0.5529 - acc: 0.880 - ETA: 2s - loss: 0.5596 - acc: 0.880 - ETA: 2s - loss: 0.5599 - acc: 0.880 - ETA: 2s - loss: 0.5655 - acc: 0.880 - ETA: 2s - loss: 0.5658 - acc: 0.880 - ETA: 2s - loss: 0.5730 - acc: 0.879 - ETA: 2s - loss: 0.5712 - acc: 0.881 - ETA: 1s - loss: 0.5768 - acc: 0.881 - ETA: 1s - loss: 0.5814 - acc: 0.882 - ETA: 1s - loss: 0.5787 - acc: 0.884 - ETA: 1s - loss: 0.5734 - acc: 0.884 - ETA: 1s - loss: 0.5713 - acc: 0.885 - ETA: 1s - loss: 0.5649 - acc: 0.885 - ETA: 1s - loss: 0.5803 - acc: 0.884 - ETA: 1s - loss: 0.5781 - acc: 0.886 - ETA: 1s - loss: 0.5782 - acc: 0.885 - ETA: 1s - loss: 0.5779 - acc: 0.885 - ETA: 1s - loss: 0.5896 - acc: 0.883 - ETA: 1s - loss: 0.5945 - acc: 0.883 - ETA: 1s - loss: 0.5935 - acc: 0.883 - ETA: 1s - loss: 0.5947 - acc: 0.882 - ETA: 1s - loss: 0.5884 - acc: 0.883 - ETA: 1s - loss: 0.5901 - acc: 0.883 - ETA: 1s - loss: 0.5859 - acc: 0.883 - ETA: 1s - loss: 0.5823 - acc: 0.884 - ETA: 0s - loss: 0.5838 - acc: 0.883 - ETA: 0s - loss: 0.5797 - acc: 0.884 - ETA: 0s - loss: 0.5811 - acc: 0.884 - ETA: 0s - loss: 0.5871 - acc: 0.883 - ETA: 0s - loss: 0.5836 - acc: 0.883 - ETA: 0s - loss: 0.5794 - acc: 0.884 - ETA: 0s - loss: 0.5752 - acc: 0.884 - ETA: 0s - loss: 0.5723 - acc: 0.885 - ETA: 0s - loss: 0.5702 - acc: 0.885 - ETA: 0s - loss: 0.5706 - acc: 0.886 - ETA: 0s - loss: 0.5662 - acc: 0.887 - ETA: 0s - loss: 0.5632 - acc: 0.887 - ETA: 0s - loss: 0.5594 - acc: 0.887 - ETA: 0s - loss: 0.5567 - acc: 0.887 - ETA: 0s - loss: 0.5577 - acc: 0.887 - ETA: 0s - loss: 0.5533 - acc: 0.888 - ETA: 0s - loss: 0.5494 - acc: 0.888 - ETA: 0s - loss: 0.5487 - acc: 0.888 - ETA: 0s - loss: 0.5481 - acc: 0.888 - ETA: 0s - loss: 0.5474 - acc: 0.888 - ETA: 0s - loss: 0.5460 - acc: 0.887 - ETA: 0s - loss: 0.5470 - acc: 0.888 - 6s 2ms/step - loss: 0.5467 - acc: 0.8881 - val_loss: 0.5242 - val_acc: 0.8897\n",
      "Epoch 93/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1414 - acc: 0.937 - ETA: 6s - loss: 0.4948 - acc: 0.916 - ETA: 7s - loss: 0.7493 - acc: 0.912 - ETA: 7s - loss: 0.5958 - acc: 0.910 - ETA: 8s - loss: 0.6826 - acc: 0.898 - ETA: 7s - loss: 0.6232 - acc: 0.914 - ETA: 7s - loss: 0.5545 - acc: 0.918 - ETA: 7s - loss: 0.4994 - acc: 0.920 - ETA: 7s - loss: 0.4666 - acc: 0.919 - ETA: 6s - loss: 0.4673 - acc: 0.918 - ETA: 6s - loss: 0.4833 - acc: 0.910 - ETA: 6s - loss: 0.5164 - acc: 0.913 - ETA: 6s - loss: 0.5093 - acc: 0.915 - ETA: 6s - loss: 0.4896 - acc: 0.915 - ETA: 5s - loss: 0.4695 - acc: 0.916 - ETA: 5s - loss: 0.4459 - acc: 0.918 - ETA: 5s - loss: 0.5182 - acc: 0.913 - ETA: 5s - loss: 0.5161 - acc: 0.913 - ETA: 5s - loss: 0.5144 - acc: 0.915 - ETA: 5s - loss: 0.5203 - acc: 0.914 - ETA: 5s - loss: 0.5174 - acc: 0.906 - ETA: 5s - loss: 0.5255 - acc: 0.905 - ETA: 5s - loss: 0.5243 - acc: 0.906 - ETA: 5s - loss: 0.5086 - acc: 0.905 - ETA: 4s - loss: 0.5087 - acc: 0.906 - ETA: 4s - loss: 0.4987 - acc: 0.907 - ETA: 4s - loss: 0.4935 - acc: 0.904 - ETA: 4s - loss: 0.5031 - acc: 0.901 - ETA: 4s - loss: 0.4950 - acc: 0.901 - ETA: 4s - loss: 0.5022 - acc: 0.899 - ETA: 4s - loss: 0.4997 - acc: 0.900 - ETA: 4s - loss: 0.5005 - acc: 0.901 - ETA: 4s - loss: 0.5025 - acc: 0.901 - ETA: 4s - loss: 0.5152 - acc: 0.900 - ETA: 4s - loss: 0.5077 - acc: 0.897 - ETA: 4s - loss: 0.5191 - acc: 0.897 - ETA: 4s - loss: 0.5126 - acc: 0.897 - ETA: 4s - loss: 0.5044 - acc: 0.899 - ETA: 3s - loss: 0.5067 - acc: 0.898 - ETA: 3s - loss: 0.5011 - acc: 0.898 - ETA: 3s - loss: 0.4956 - acc: 0.898 - ETA: 3s - loss: 0.4914 - acc: 0.897 - ETA: 3s - loss: 0.4870 - acc: 0.896 - ETA: 3s - loss: 0.4796 - acc: 0.896 - ETA: 3s - loss: 0.4912 - acc: 0.896 - ETA: 3s - loss: 0.5043 - acc: 0.895 - ETA: 3s - loss: 0.4982 - acc: 0.896 - ETA: 3s - loss: 0.4928 - acc: 0.895 - ETA: 3s - loss: 0.5042 - acc: 0.893 - ETA: 3s - loss: 0.5127 - acc: 0.893 - ETA: 3s - loss: 0.5098 - acc: 0.893 - ETA: 3s - loss: 0.5044 - acc: 0.894 - ETA: 3s - loss: 0.5040 - acc: 0.894 - ETA: 3s - loss: 0.4958 - acc: 0.895 - ETA: 2s - loss: 0.5046 - acc: 0.894 - ETA: 2s - loss: 0.4969 - acc: 0.895 - ETA: 2s - loss: 0.4916 - acc: 0.896 - ETA: 2s - loss: 0.4876 - acc: 0.897 - ETA: 2s - loss: 0.4893 - acc: 0.897 - ETA: 2s - loss: 0.4954 - acc: 0.898 - ETA: 2s - loss: 0.4959 - acc: 0.898 - ETA: 2s - loss: 0.4958 - acc: 0.898 - ETA: 2s - loss: 0.4969 - acc: 0.897 - ETA: 2s - loss: 0.4992 - acc: 0.897 - ETA: 2s - loss: 0.5022 - acc: 0.897 - ETA: 2s - loss: 0.5091 - acc: 0.896 - ETA: 2s - loss: 0.5162 - acc: 0.895 - ETA: 2s - loss: 0.5155 - acc: 0.896 - ETA: 2s - loss: 0.5191 - acc: 0.897 - ETA: 1s - loss: 0.5183 - acc: 0.897 - ETA: 1s - loss: 0.5137 - acc: 0.897 - ETA: 1s - loss: 0.5111 - acc: 0.897 - ETA: 1s - loss: 0.5073 - acc: 0.897 - ETA: 1s - loss: 0.5127 - acc: 0.896 - ETA: 1s - loss: 0.5233 - acc: 0.896 - ETA: 1s - loss: 0.5306 - acc: 0.895 - ETA: 1s - loss: 0.5263 - acc: 0.895 - ETA: 1s - loss: 0.5232 - acc: 0.895 - ETA: 1s - loss: 0.5196 - acc: 0.896 - ETA: 1s - loss: 0.5208 - acc: 0.896 - ETA: 1s - loss: 0.5231 - acc: 0.895 - ETA: 1s - loss: 0.5242 - acc: 0.896 - ETA: 1s - loss: 0.5246 - acc: 0.896 - ETA: 1s - loss: 0.5205 - acc: 0.897 - ETA: 1s - loss: 0.5276 - acc: 0.896 - ETA: 1s - loss: 0.5288 - acc: 0.896 - ETA: 1s - loss: 0.5292 - acc: 0.897 - ETA: 1s - loss: 0.5303 - acc: 0.897 - ETA: 0s - loss: 0.5369 - acc: 0.896 - ETA: 0s - loss: 0.5307 - acc: 0.898 - ETA: 0s - loss: 0.5255 - acc: 0.898 - ETA: 0s - loss: 0.5246 - acc: 0.899 - ETA: 0s - loss: 0.5208 - acc: 0.898 - ETA: 0s - loss: 0.5208 - acc: 0.898 - ETA: 0s - loss: 0.5213 - acc: 0.898 - ETA: 0s - loss: 0.5189 - acc: 0.897 - ETA: 0s - loss: 0.5195 - acc: 0.898 - ETA: 0s - loss: 0.5254 - acc: 0.897 - ETA: 0s - loss: 0.5284 - acc: 0.896 - ETA: 0s - loss: 0.5292 - acc: 0.896 - ETA: 0s - loss: 0.5338 - acc: 0.896 - ETA: 0s - loss: 0.5349 - acc: 0.897 - ETA: 0s - loss: 0.5357 - acc: 0.897 - ETA: 0s - loss: 0.5327 - acc: 0.897 - ETA: 0s - loss: 0.5292 - acc: 0.898 - 7s 2ms/step - loss: 0.5290 - acc: 0.8977 - val_loss: 0.4954 - val_acc: 0.8897\n",
      "Epoch 94/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.1230 - acc: 0.937 - ETA: 5s - loss: 0.2086 - acc: 0.921 - ETA: 5s - loss: 0.4702 - acc: 0.910 - ETA: 4s - loss: 0.4568 - acc: 0.925 - ETA: 4s - loss: 0.4736 - acc: 0.913 - ETA: 4s - loss: 0.5295 - acc: 0.914 - ETA: 4s - loss: 0.4618 - acc: 0.921 - ETA: 4s - loss: 0.4172 - acc: 0.923 - ETA: 4s - loss: 0.4307 - acc: 0.917 - ETA: 4s - loss: 0.4338 - acc: 0.919 - ETA: 4s - loss: 0.4655 - acc: 0.921 - ETA: 4s - loss: 0.4420 - acc: 0.919 - ETA: 4s - loss: 0.4816 - acc: 0.912 - ETA: 4s - loss: 0.4869 - acc: 0.907 - ETA: 4s - loss: 0.4633 - acc: 0.909 - ETA: 4s - loss: 0.4925 - acc: 0.909 - ETA: 4s - loss: 0.5153 - acc: 0.905 - ETA: 4s - loss: 0.5027 - acc: 0.905 - ETA: 4s - loss: 0.5068 - acc: 0.906 - ETA: 4s - loss: 0.4962 - acc: 0.902 - ETA: 4s - loss: 0.5132 - acc: 0.902 - ETA: 4s - loss: 0.4966 - acc: 0.903 - ETA: 4s - loss: 0.4855 - acc: 0.901 - ETA: 4s - loss: 0.4869 - acc: 0.900 - ETA: 3s - loss: 0.4726 - acc: 0.902 - ETA: 3s - loss: 0.4867 - acc: 0.900 - ETA: 3s - loss: 0.4767 - acc: 0.899 - ETA: 3s - loss: 0.4786 - acc: 0.898 - ETA: 3s - loss: 0.4717 - acc: 0.896 - ETA: 3s - loss: 0.4604 - acc: 0.896 - ETA: 3s - loss: 0.4544 - acc: 0.896 - ETA: 3s - loss: 0.4463 - acc: 0.897 - ETA: 3s - loss: 0.4469 - acc: 0.898 - ETA: 3s - loss: 0.4466 - acc: 0.900 - ETA: 3s - loss: 0.4491 - acc: 0.899 - ETA: 3s - loss: 0.4510 - acc: 0.898 - ETA: 3s - loss: 0.4423 - acc: 0.899 - ETA: 3s - loss: 0.4547 - acc: 0.898 - ETA: 3s - loss: 0.4453 - acc: 0.899 - ETA: 3s - loss: 0.4634 - acc: 0.898 - ETA: 2s - loss: 0.4647 - acc: 0.898 - ETA: 2s - loss: 0.4816 - acc: 0.897 - ETA: 2s - loss: 0.4746 - acc: 0.897 - ETA: 2s - loss: 0.4745 - acc: 0.898 - ETA: 2s - loss: 0.4659 - acc: 0.900 - ETA: 2s - loss: 0.4744 - acc: 0.899 - ETA: 2s - loss: 0.4763 - acc: 0.898 - ETA: 2s - loss: 0.4763 - acc: 0.898 - ETA: 2s - loss: 0.4779 - acc: 0.897 - ETA: 2s - loss: 0.4839 - acc: 0.898 - ETA: 2s - loss: 0.4902 - acc: 0.898 - ETA: 2s - loss: 0.4891 - acc: 0.899 - ETA: 2s - loss: 0.5077 - acc: 0.899 - ETA: 2s - loss: 0.5183 - acc: 0.899 - ETA: 2s - loss: 0.5229 - acc: 0.900 - ETA: 1s - loss: 0.5406 - acc: 0.899 - ETA: 1s - loss: 0.5693 - acc: 0.898 - ETA: 1s - loss: 0.5789 - acc: 0.898 - ETA: 1s - loss: 0.5948 - acc: 0.896 - ETA: 1s - loss: 0.6028 - acc: 0.897 - ETA: 1s - loss: 0.5998 - acc: 0.898 - ETA: 1s - loss: 0.6182 - acc: 0.897 - ETA: 1s - loss: 0.6102 - acc: 0.898 - ETA: 1s - loss: 0.6138 - acc: 0.899 - ETA: 1s - loss: 0.6372 - acc: 0.898 - ETA: 1s - loss: 0.6443 - acc: 0.898 - ETA: 1s - loss: 0.6407 - acc: 0.899 - ETA: 1s - loss: 0.6379 - acc: 0.899 - ETA: 1s - loss: 0.6346 - acc: 0.900 - ETA: 1s - loss: 0.6323 - acc: 0.901 - ETA: 1s - loss: 0.6293 - acc: 0.901 - ETA: 0s - loss: 0.6266 - acc: 0.902 - ETA: 0s - loss: 0.6214 - acc: 0.901 - ETA: 0s - loss: 0.6335 - acc: 0.900 - ETA: 0s - loss: 0.6309 - acc: 0.901 - ETA: 0s - loss: 0.6378 - acc: 0.900 - ETA: 0s - loss: 0.6347 - acc: 0.901 - ETA: 0s - loss: 0.6282 - acc: 0.901 - ETA: 0s - loss: 0.6472 - acc: 0.900 - ETA: 0s - loss: 0.6464 - acc: 0.901 - ETA: 0s - loss: 0.6420 - acc: 0.901 - ETA: 0s - loss: 0.6426 - acc: 0.900 - ETA: 0s - loss: 0.6381 - acc: 0.900 - ETA: 0s - loss: 0.6333 - acc: 0.901 - ETA: 0s - loss: 0.6287 - acc: 0.902 - ETA: 0s - loss: 0.6245 - acc: 0.902 - ETA: 0s - loss: 0.6285 - acc: 0.902 - ETA: 0s - loss: 0.6289 - acc: 0.902 - ETA: 0s - loss: 0.6365 - acc: 0.901 - ETA: 0s - loss: 0.6403 - acc: 0.901 - 6s 2ms/step - loss: 0.6377 - acc: 0.9019 - val_loss: 0.7253 - val_acc: 0.9128\n",
      "Epoch 95/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 1.0491 - acc: 0.937 - ETA: 5s - loss: 0.8108 - acc: 0.921 - ETA: 5s - loss: 0.6532 - acc: 0.919 - ETA: 5s - loss: 0.7715 - acc: 0.918 - ETA: 5s - loss: 0.7715 - acc: 0.913 - ETA: 5s - loss: 0.7134 - acc: 0.910 - ETA: 5s - loss: 0.7184 - acc: 0.911 - ETA: 4s - loss: 0.7140 - acc: 0.917 - ETA: 4s - loss: 0.7685 - acc: 0.910 - ETA: 4s - loss: 0.6932 - acc: 0.915 - ETA: 4s - loss: 0.7021 - acc: 0.913 - ETA: 4s - loss: 0.7059 - acc: 0.913 - ETA: 4s - loss: 0.6823 - acc: 0.913 - ETA: 4s - loss: 0.6870 - acc: 0.914 - ETA: 4s - loss: 0.7399 - acc: 0.911 - ETA: 4s - loss: 0.7169 - acc: 0.913 - ETA: 4s - loss: 0.7203 - acc: 0.910 - ETA: 4s - loss: 0.7244 - acc: 0.909 - ETA: 4s - loss: 0.7600 - acc: 0.909 - ETA: 4s - loss: 0.7754 - acc: 0.908 - ETA: 4s - loss: 0.8019 - acc: 0.907 - ETA: 4s - loss: 0.7939 - acc: 0.908 - ETA: 4s - loss: 0.8089 - acc: 0.906 - ETA: 4s - loss: 0.7990 - acc: 0.904 - ETA: 4s - loss: 0.7973 - acc: 0.905 - ETA: 3s - loss: 0.7817 - acc: 0.905 - ETA: 3s - loss: 0.8073 - acc: 0.904 - ETA: 3s - loss: 0.7940 - acc: 0.905 - ETA: 3s - loss: 0.8137 - acc: 0.904 - ETA: 3s - loss: 0.8227 - acc: 0.904 - ETA: 3s - loss: 0.8055 - acc: 0.905 - ETA: 3s - loss: 0.8011 - acc: 0.905 - ETA: 3s - loss: 0.7778 - acc: 0.906 - ETA: 3s - loss: 0.7744 - acc: 0.906 - ETA: 3s - loss: 0.7601 - acc: 0.907 - ETA: 3s - loss: 0.7667 - acc: 0.907 - ETA: 3s - loss: 0.7549 - acc: 0.907 - ETA: 3s - loss: 0.7717 - acc: 0.907 - ETA: 3s - loss: 0.7714 - acc: 0.907 - ETA: 3s - loss: 0.7696 - acc: 0.906 - ETA: 3s - loss: 0.7583 - acc: 0.906 - ETA: 3s - loss: 0.7458 - acc: 0.908 - ETA: 3s - loss: 0.7357 - acc: 0.908 - ETA: 3s - loss: 0.7338 - acc: 0.908 - ETA: 3s - loss: 0.7407 - acc: 0.907 - ETA: 3s - loss: 0.7364 - acc: 0.907 - ETA: 3s - loss: 0.7218 - acc: 0.907 - ETA: 3s - loss: 0.7316 - acc: 0.906 - ETA: 3s - loss: 0.7234 - acc: 0.907 - ETA: 2s - loss: 0.7093 - acc: 0.908 - ETA: 2s - loss: 0.7036 - acc: 0.908 - ETA: 2s - loss: 0.7127 - acc: 0.907 - ETA: 2s - loss: 0.7008 - acc: 0.907 - ETA: 2s - loss: 0.6980 - acc: 0.906 - ETA: 2s - loss: 0.7004 - acc: 0.905 - ETA: 2s - loss: 0.6883 - acc: 0.906 - ETA: 2s - loss: 0.6808 - acc: 0.907 - ETA: 2s - loss: 0.6734 - acc: 0.907 - ETA: 2s - loss: 0.6684 - acc: 0.907 - ETA: 2s - loss: 0.6713 - acc: 0.907 - ETA: 2s - loss: 0.6783 - acc: 0.906 - ETA: 2s - loss: 0.6784 - acc: 0.905 - ETA: 2s - loss: 0.6719 - acc: 0.906 - ETA: 2s - loss: 0.6662 - acc: 0.906 - ETA: 2s - loss: 0.6683 - acc: 0.905 - ETA: 2s - loss: 0.6695 - acc: 0.905 - ETA: 1s - loss: 0.6599 - acc: 0.905 - ETA: 1s - loss: 0.6506 - acc: 0.906 - ETA: 1s - loss: 0.6456 - acc: 0.906 - ETA: 1s - loss: 0.6408 - acc: 0.903 - ETA: 1s - loss: 0.6445 - acc: 0.902 - ETA: 1s - loss: 0.6484 - acc: 0.901 - ETA: 1s - loss: 0.6409 - acc: 0.901 - ETA: 1s - loss: 0.6447 - acc: 0.901 - ETA: 1s - loss: 0.6438 - acc: 0.900 - ETA: 1s - loss: 0.6430 - acc: 0.899 - ETA: 1s - loss: 0.6423 - acc: 0.899 - ETA: 1s - loss: 0.6424 - acc: 0.897 - ETA: 1s - loss: 0.6462 - acc: 0.897 - ETA: 1s - loss: 0.6451 - acc: 0.897 - ETA: 0s - loss: 0.6538 - acc: 0.896 - ETA: 0s - loss: 0.6487 - acc: 0.896 - ETA: 0s - loss: 0.6430 - acc: 0.896 - ETA: 0s - loss: 0.6418 - acc: 0.896 - ETA: 0s - loss: 0.6362 - acc: 0.896 - ETA: 0s - loss: 0.6366 - acc: 0.896 - ETA: 0s - loss: 0.6352 - acc: 0.896 - ETA: 0s - loss: 0.6320 - acc: 0.896 - ETA: 0s - loss: 0.6300 - acc: 0.895 - ETA: 0s - loss: 0.6270 - acc: 0.894 - ETA: 0s - loss: 0.6287 - acc: 0.893 - ETA: 0s - loss: 0.6260 - acc: 0.893 - ETA: 0s - loss: 0.6234 - acc: 0.892 - ETA: 0s - loss: 0.6247 - acc: 0.892 - ETA: 0s - loss: 0.6214 - acc: 0.892 - ETA: 0s - loss: 0.6303 - acc: 0.892 - ETA: 0s - loss: 0.6265 - acc: 0.892 - ETA: 0s - loss: 0.6276 - acc: 0.892 - ETA: 0s - loss: 0.6281 - acc: 0.892 - 7s 2ms/step - loss: 0.6296 - acc: 0.8928 - val_loss: 0.5519 - val_acc: 0.9160\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067/4067 [==============================] - ETA: 6s - loss: 0.0655 - acc: 1.000 - ETA: 5s - loss: 0.8737 - acc: 0.890 - ETA: 5s - loss: 0.7846 - acc: 0.857 - ETA: 5s - loss: 0.8856 - acc: 0.847 - ETA: 5s - loss: 0.7437 - acc: 0.843 - ETA: 5s - loss: 0.6515 - acc: 0.845 - ETA: 5s - loss: 0.6371 - acc: 0.850 - ETA: 5s - loss: 0.5833 - acc: 0.854 - ETA: 5s - loss: 0.5861 - acc: 0.854 - ETA: 4s - loss: 0.5566 - acc: 0.851 - ETA: 4s - loss: 0.5278 - acc: 0.852 - ETA: 4s - loss: 0.5081 - acc: 0.856 - ETA: 4s - loss: 0.4799 - acc: 0.862 - ETA: 4s - loss: 0.5181 - acc: 0.859 - ETA: 4s - loss: 0.5021 - acc: 0.857 - ETA: 4s - loss: 0.4826 - acc: 0.859 - ETA: 4s - loss: 0.4845 - acc: 0.863 - ETA: 4s - loss: 0.4920 - acc: 0.862 - ETA: 4s - loss: 0.4777 - acc: 0.863 - ETA: 4s - loss: 0.4711 - acc: 0.862 - ETA: 4s - loss: 0.4612 - acc: 0.865 - ETA: 4s - loss: 0.4707 - acc: 0.864 - ETA: 4s - loss: 0.4943 - acc: 0.864 - ETA: 4s - loss: 0.4881 - acc: 0.865 - ETA: 4s - loss: 0.5228 - acc: 0.862 - ETA: 4s - loss: 0.5278 - acc: 0.863 - ETA: 4s - loss: 0.5331 - acc: 0.863 - ETA: 4s - loss: 0.5251 - acc: 0.863 - ETA: 4s - loss: 0.5160 - acc: 0.865 - ETA: 4s - loss: 0.5095 - acc: 0.866 - ETA: 4s - loss: 0.5260 - acc: 0.867 - ETA: 4s - loss: 0.5181 - acc: 0.869 - ETA: 4s - loss: 0.5130 - acc: 0.869 - ETA: 4s - loss: 0.5187 - acc: 0.869 - ETA: 4s - loss: 0.5145 - acc: 0.867 - ETA: 4s - loss: 0.5083 - acc: 0.868 - ETA: 3s - loss: 0.5107 - acc: 0.870 - ETA: 3s - loss: 0.5041 - acc: 0.871 - ETA: 3s - loss: 0.4994 - acc: 0.871 - ETA: 3s - loss: 0.4921 - acc: 0.871 - ETA: 3s - loss: 0.4870 - acc: 0.871 - ETA: 3s - loss: 0.4816 - acc: 0.872 - ETA: 3s - loss: 0.4876 - acc: 0.872 - ETA: 3s - loss: 0.4817 - acc: 0.873 - ETA: 3s - loss: 0.4769 - acc: 0.873 - ETA: 3s - loss: 0.4990 - acc: 0.873 - ETA: 3s - loss: 0.5031 - acc: 0.873 - ETA: 3s - loss: 0.5064 - acc: 0.873 - ETA: 3s - loss: 0.5036 - acc: 0.873 - ETA: 3s - loss: 0.4987 - acc: 0.873 - ETA: 3s - loss: 0.5044 - acc: 0.872 - ETA: 3s - loss: 0.5002 - acc: 0.873 - ETA: 3s - loss: 0.5030 - acc: 0.873 - ETA: 3s - loss: 0.5085 - acc: 0.873 - ETA: 3s - loss: 0.5108 - acc: 0.874 - ETA: 3s - loss: 0.5141 - acc: 0.874 - ETA: 3s - loss: 0.5253 - acc: 0.873 - ETA: 3s - loss: 0.5278 - acc: 0.873 - ETA: 3s - loss: 0.5240 - acc: 0.872 - ETA: 2s - loss: 0.5266 - acc: 0.873 - ETA: 2s - loss: 0.5221 - acc: 0.873 - ETA: 2s - loss: 0.5249 - acc: 0.873 - ETA: 2s - loss: 0.5282 - acc: 0.872 - ETA: 2s - loss: 0.5302 - acc: 0.873 - ETA: 2s - loss: 0.5265 - acc: 0.873 - ETA: 2s - loss: 0.5355 - acc: 0.872 - ETA: 2s - loss: 0.5375 - acc: 0.873 - ETA: 2s - loss: 0.5379 - acc: 0.874 - ETA: 2s - loss: 0.5390 - acc: 0.874 - ETA: 2s - loss: 0.5333 - acc: 0.874 - ETA: 2s - loss: 0.5286 - acc: 0.875 - ETA: 2s - loss: 0.5373 - acc: 0.875 - ETA: 2s - loss: 0.5398 - acc: 0.874 - ETA: 2s - loss: 0.5412 - acc: 0.875 - ETA: 2s - loss: 0.5408 - acc: 0.875 - ETA: 2s - loss: 0.5364 - acc: 0.876 - ETA: 2s - loss: 0.5342 - acc: 0.875 - ETA: 2s - loss: 0.5375 - acc: 0.874 - ETA: 1s - loss: 0.5397 - acc: 0.874 - ETA: 1s - loss: 0.5406 - acc: 0.874 - ETA: 1s - loss: 0.5341 - acc: 0.876 - ETA: 1s - loss: 0.5339 - acc: 0.877 - ETA: 1s - loss: 0.5295 - acc: 0.877 - ETA: 1s - loss: 0.5299 - acc: 0.878 - ETA: 1s - loss: 0.5252 - acc: 0.878 - ETA: 1s - loss: 0.5274 - acc: 0.878 - ETA: 1s - loss: 0.5322 - acc: 0.878 - ETA: 1s - loss: 0.5289 - acc: 0.879 - ETA: 1s - loss: 0.5251 - acc: 0.880 - ETA: 1s - loss: 0.5235 - acc: 0.880 - ETA: 1s - loss: 0.5201 - acc: 0.880 - ETA: 1s - loss: 0.5161 - acc: 0.881 - ETA: 1s - loss: 0.5169 - acc: 0.882 - ETA: 1s - loss: 0.5175 - acc: 0.882 - ETA: 0s - loss: 0.5196 - acc: 0.882 - ETA: 0s - loss: 0.5255 - acc: 0.882 - ETA: 0s - loss: 0.5240 - acc: 0.883 - ETA: 0s - loss: 0.5254 - acc: 0.883 - ETA: 0s - loss: 0.5242 - acc: 0.883 - ETA: 0s - loss: 0.5231 - acc: 0.883 - ETA: 0s - loss: 0.5198 - acc: 0.883 - ETA: 0s - loss: 0.5214 - acc: 0.883 - ETA: 0s - loss: 0.5184 - acc: 0.883 - ETA: 0s - loss: 0.5153 - acc: 0.884 - ETA: 0s - loss: 0.5129 - acc: 0.884 - ETA: 0s - loss: 0.5148 - acc: 0.883 - ETA: 0s - loss: 0.5153 - acc: 0.884 - ETA: 0s - loss: 0.5156 - acc: 0.885 - ETA: 0s - loss: 0.5171 - acc: 0.885 - ETA: 0s - loss: 0.5140 - acc: 0.885 - ETA: 0s - loss: 0.5156 - acc: 0.885 - ETA: 0s - loss: 0.5118 - acc: 0.886 - ETA: 0s - loss: 0.5068 - acc: 0.887 - 7s 2ms/step - loss: 0.5068 - acc: 0.8876 - val_loss: 0.4543 - val_acc: 0.8955\n",
      "Epoch 97/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1249 - acc: 0.937 - ETA: 5s - loss: 0.1939 - acc: 0.890 - ETA: 5s - loss: 0.3480 - acc: 0.883 - ETA: 5s - loss: 0.6001 - acc: 0.881 - ETA: 5s - loss: 0.6550 - acc: 0.879 - ETA: 5s - loss: 0.7089 - acc: 0.878 - ETA: 4s - loss: 0.6232 - acc: 0.888 - ETA: 5s - loss: 0.6061 - acc: 0.889 - ETA: 4s - loss: 0.5493 - acc: 0.901 - ETA: 4s - loss: 0.5029 - acc: 0.909 - ETA: 4s - loss: 0.5095 - acc: 0.906 - ETA: 4s - loss: 0.4850 - acc: 0.908 - ETA: 4s - loss: 0.5035 - acc: 0.902 - ETA: 5s - loss: 0.5386 - acc: 0.902 - ETA: 4s - loss: 0.5324 - acc: 0.905 - ETA: 4s - loss: 0.5089 - acc: 0.904 - ETA: 4s - loss: 0.4983 - acc: 0.902 - ETA: 4s - loss: 0.4894 - acc: 0.900 - ETA: 4s - loss: 0.4685 - acc: 0.903 - ETA: 4s - loss: 0.4590 - acc: 0.903 - ETA: 4s - loss: 0.4398 - acc: 0.905 - ETA: 4s - loss: 0.4447 - acc: 0.903 - ETA: 4s - loss: 0.4489 - acc: 0.903 - ETA: 4s - loss: 0.4389 - acc: 0.903 - ETA: 4s - loss: 0.4586 - acc: 0.901 - ETA: 4s - loss: 0.4648 - acc: 0.898 - ETA: 4s - loss: 0.4557 - acc: 0.898 - ETA: 4s - loss: 0.4480 - acc: 0.899 - ETA: 4s - loss: 0.4657 - acc: 0.900 - ETA: 4s - loss: 0.4753 - acc: 0.902 - ETA: 4s - loss: 0.4755 - acc: 0.902 - ETA: 4s - loss: 0.4609 - acc: 0.905 - ETA: 3s - loss: 0.4734 - acc: 0.904 - ETA: 3s - loss: 0.4954 - acc: 0.903 - ETA: 3s - loss: 0.4935 - acc: 0.905 - ETA: 3s - loss: 0.4807 - acc: 0.907 - ETA: 3s - loss: 0.4793 - acc: 0.909 - ETA: 3s - loss: 0.4802 - acc: 0.908 - ETA: 3s - loss: 0.4714 - acc: 0.908 - ETA: 3s - loss: 0.4724 - acc: 0.908 - ETA: 3s - loss: 0.4739 - acc: 0.908 - ETA: 3s - loss: 0.4702 - acc: 0.906 - ETA: 3s - loss: 0.4907 - acc: 0.903 - ETA: 3s - loss: 0.4919 - acc: 0.902 - ETA: 2s - loss: 0.4989 - acc: 0.902 - ETA: 2s - loss: 0.4901 - acc: 0.903 - ETA: 2s - loss: 0.4839 - acc: 0.904 - ETA: 2s - loss: 0.4844 - acc: 0.904 - ETA: 2s - loss: 0.4757 - acc: 0.905 - ETA: 2s - loss: 0.4827 - acc: 0.905 - ETA: 2s - loss: 0.4746 - acc: 0.906 - ETA: 2s - loss: 0.4736 - acc: 0.907 - ETA: 2s - loss: 0.4659 - acc: 0.909 - ETA: 2s - loss: 0.4651 - acc: 0.910 - ETA: 2s - loss: 0.4600 - acc: 0.910 - ETA: 2s - loss: 0.4730 - acc: 0.910 - ETA: 2s - loss: 0.4745 - acc: 0.909 - ETA: 2s - loss: 0.4805 - acc: 0.909 - ETA: 1s - loss: 0.4742 - acc: 0.909 - ETA: 1s - loss: 0.4685 - acc: 0.910 - ETA: 1s - loss: 0.4744 - acc: 0.909 - ETA: 1s - loss: 0.4699 - acc: 0.908 - ETA: 1s - loss: 0.4635 - acc: 0.909 - ETA: 1s - loss: 0.4589 - acc: 0.909 - ETA: 1s - loss: 0.4542 - acc: 0.909 - ETA: 1s - loss: 0.4531 - acc: 0.909 - ETA: 1s - loss: 0.4597 - acc: 0.908 - ETA: 1s - loss: 0.4646 - acc: 0.908 - ETA: 1s - loss: 0.4604 - acc: 0.908 - ETA: 1s - loss: 0.4545 - acc: 0.908 - ETA: 1s - loss: 0.4557 - acc: 0.908 - ETA: 1s - loss: 0.4541 - acc: 0.907 - ETA: 1s - loss: 0.4611 - acc: 0.907 - ETA: 0s - loss: 0.4564 - acc: 0.907 - ETA: 0s - loss: 0.4557 - acc: 0.907 - ETA: 0s - loss: 0.4585 - acc: 0.907 - ETA: 0s - loss: 0.4614 - acc: 0.907 - ETA: 0s - loss: 0.4643 - acc: 0.906 - ETA: 0s - loss: 0.4669 - acc: 0.905 - ETA: 0s - loss: 0.4734 - acc: 0.905 - ETA: 0s - loss: 0.4749 - acc: 0.905 - ETA: 0s - loss: 0.4775 - acc: 0.904 - ETA: 0s - loss: 0.4749 - acc: 0.905 - ETA: 0s - loss: 0.4763 - acc: 0.905 - ETA: 0s - loss: 0.4762 - acc: 0.904 - ETA: 0s - loss: 0.4744 - acc: 0.904 - ETA: 0s - loss: 0.4719 - acc: 0.904 - ETA: 0s - loss: 0.4700 - acc: 0.904 - ETA: 0s - loss: 0.4681 - acc: 0.904 - ETA: 0s - loss: 0.4706 - acc: 0.903 - ETA: 0s - loss: 0.4684 - acc: 0.903 - ETA: 0s - loss: 0.4662 - acc: 0.903 - ETA: 0s - loss: 0.4677 - acc: 0.903 - ETA: 0s - loss: 0.4694 - acc: 0.903 - ETA: 0s - loss: 0.4677 - acc: 0.903 - ETA: 0s - loss: 0.4660 - acc: 0.903 - 6s 2ms/step - loss: 0.4693 - acc: 0.9026 - val_loss: 0.4657 - val_acc: 0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.2230 - acc: 0.937 - ETA: 4s - loss: 0.4282 - acc: 0.887 - ETA: 5s - loss: 0.5190 - acc: 0.875 - ETA: 4s - loss: 0.5028 - acc: 0.875 - ETA: 4s - loss: 0.6039 - acc: 0.875 - ETA: 4s - loss: 0.5352 - acc: 0.882 - ETA: 4s - loss: 0.4807 - acc: 0.891 - ETA: 4s - loss: 0.4402 - acc: 0.900 - ETA: 4s - loss: 0.4947 - acc: 0.895 - ETA: 4s - loss: 0.4677 - acc: 0.892 - ETA: 4s - loss: 0.4507 - acc: 0.891 - ETA: 4s - loss: 0.4655 - acc: 0.882 - ETA: 4s - loss: 0.4555 - acc: 0.883 - ETA: 4s - loss: 0.4618 - acc: 0.884 - ETA: 4s - loss: 0.4721 - acc: 0.881 - ETA: 4s - loss: 0.4837 - acc: 0.877 - ETA: 4s - loss: 0.4666 - acc: 0.880 - ETA: 4s - loss: 0.4479 - acc: 0.884 - ETA: 3s - loss: 0.4522 - acc: 0.886 - ETA: 3s - loss: 0.4571 - acc: 0.888 - ETA: 3s - loss: 0.4665 - acc: 0.888 - ETA: 3s - loss: 0.4779 - acc: 0.887 - ETA: 3s - loss: 0.4899 - acc: 0.886 - ETA: 3s - loss: 0.4765 - acc: 0.888 - ETA: 3s - loss: 0.4671 - acc: 0.887 - ETA: 3s - loss: 0.4560 - acc: 0.888 - ETA: 3s - loss: 0.4574 - acc: 0.890 - ETA: 3s - loss: 0.4510 - acc: 0.889 - ETA: 3s - loss: 0.4408 - acc: 0.891 - ETA: 3s - loss: 0.4428 - acc: 0.893 - ETA: 3s - loss: 0.4811 - acc: 0.890 - ETA: 3s - loss: 0.4812 - acc: 0.891 - ETA: 3s - loss: 0.4725 - acc: 0.893 - ETA: 3s - loss: 0.4844 - acc: 0.893 - ETA: 3s - loss: 0.4982 - acc: 0.890 - ETA: 2s - loss: 0.5201 - acc: 0.887 - ETA: 2s - loss: 0.5112 - acc: 0.888 - ETA: 2s - loss: 0.5116 - acc: 0.889 - ETA: 2s - loss: 0.5026 - acc: 0.889 - ETA: 2s - loss: 0.5042 - acc: 0.889 - ETA: 2s - loss: 0.5198 - acc: 0.889 - ETA: 2s - loss: 0.5193 - acc: 0.890 - ETA: 2s - loss: 0.5185 - acc: 0.891 - ETA: 2s - loss: 0.5133 - acc: 0.892 - ETA: 2s - loss: 0.5126 - acc: 0.892 - ETA: 2s - loss: 0.5119 - acc: 0.893 - ETA: 2s - loss: 0.5142 - acc: 0.893 - ETA: 2s - loss: 0.5091 - acc: 0.894 - ETA: 2s - loss: 0.5007 - acc: 0.896 - ETA: 2s - loss: 0.5075 - acc: 0.897 - ETA: 2s - loss: 0.5070 - acc: 0.897 - ETA: 2s - loss: 0.5007 - acc: 0.898 - ETA: 2s - loss: 0.4942 - acc: 0.899 - ETA: 1s - loss: 0.4880 - acc: 0.900 - ETA: 1s - loss: 0.4872 - acc: 0.901 - ETA: 1s - loss: 0.4816 - acc: 0.901 - ETA: 1s - loss: 0.4826 - acc: 0.901 - ETA: 1s - loss: 0.4895 - acc: 0.901 - ETA: 1s - loss: 0.5018 - acc: 0.900 - ETA: 1s - loss: 0.5029 - acc: 0.901 - ETA: 1s - loss: 0.4994 - acc: 0.901 - ETA: 1s - loss: 0.4991 - acc: 0.901 - ETA: 1s - loss: 0.4940 - acc: 0.901 - ETA: 1s - loss: 0.4869 - acc: 0.903 - ETA: 1s - loss: 0.4897 - acc: 0.903 - ETA: 1s - loss: 0.4834 - acc: 0.904 - ETA: 1s - loss: 0.4776 - acc: 0.905 - ETA: 1s - loss: 0.4729 - acc: 0.906 - ETA: 1s - loss: 0.4681 - acc: 0.907 - ETA: 1s - loss: 0.4673 - acc: 0.907 - ETA: 0s - loss: 0.4671 - acc: 0.908 - ETA: 0s - loss: 0.4668 - acc: 0.908 - ETA: 0s - loss: 0.4717 - acc: 0.908 - ETA: 0s - loss: 0.4731 - acc: 0.908 - ETA: 0s - loss: 0.4760 - acc: 0.908 - ETA: 0s - loss: 0.4723 - acc: 0.908 - ETA: 0s - loss: 0.4775 - acc: 0.907 - ETA: 0s - loss: 0.4800 - acc: 0.906 - ETA: 0s - loss: 0.4753 - acc: 0.907 - ETA: 0s - loss: 0.4720 - acc: 0.906 - ETA: 0s - loss: 0.4691 - acc: 0.907 - ETA: 0s - loss: 0.4651 - acc: 0.907 - ETA: 0s - loss: 0.4650 - acc: 0.908 - ETA: 0s - loss: 0.4655 - acc: 0.908 - ETA: 0s - loss: 0.4709 - acc: 0.907 - ETA: 0s - loss: 0.4717 - acc: 0.907 - ETA: 0s - loss: 0.4691 - acc: 0.907 - 6s 1ms/step - loss: 0.4688 - acc: 0.9073 - val_loss: 0.4635 - val_acc: 0.8949\n",
      "Epoch 99/100\n",
      "4067/4067 [==============================] - ETA: 5s - loss: 0.0987 - acc: 1.000 - ETA: 5s - loss: 0.4120 - acc: 0.937 - ETA: 5s - loss: 0.2804 - acc: 0.946 - ETA: 4s - loss: 0.2305 - acc: 0.950 - ETA: 4s - loss: 0.2249 - acc: 0.937 - ETA: 4s - loss: 0.2976 - acc: 0.921 - ETA: 4s - loss: 0.2768 - acc: 0.927 - ETA: 4s - loss: 0.3658 - acc: 0.914 - ETA: 4s - loss: 0.3517 - acc: 0.912 - ETA: 4s - loss: 0.3344 - acc: 0.912 - ETA: 4s - loss: 0.3843 - acc: 0.911 - ETA: 4s - loss: 0.3677 - acc: 0.911 - ETA: 4s - loss: 0.3499 - acc: 0.913 - ETA: 4s - loss: 0.3603 - acc: 0.915 - ETA: 4s - loss: 0.3712 - acc: 0.914 - ETA: 4s - loss: 0.3798 - acc: 0.914 - ETA: 4s - loss: 0.3911 - acc: 0.912 - ETA: 4s - loss: 0.3777 - acc: 0.913 - ETA: 4s - loss: 0.3674 - acc: 0.915 - ETA: 4s - loss: 0.3785 - acc: 0.915 - ETA: 4s - loss: 0.3721 - acc: 0.914 - ETA: 4s - loss: 0.3649 - acc: 0.914 - ETA: 4s - loss: 0.3581 - acc: 0.916 - ETA: 4s - loss: 0.3525 - acc: 0.917 - ETA: 4s - loss: 0.3592 - acc: 0.918 - ETA: 4s - loss: 0.3563 - acc: 0.917 - ETA: 4s - loss: 0.3792 - acc: 0.917 - ETA: 4s - loss: 0.3699 - acc: 0.919 - ETA: 4s - loss: 0.3677 - acc: 0.920 - ETA: 4s - loss: 0.3787 - acc: 0.917 - ETA: 4s - loss: 0.3734 - acc: 0.918 - ETA: 4s - loss: 0.3711 - acc: 0.918 - ETA: 4s - loss: 0.3802 - acc: 0.917 - ETA: 4s - loss: 0.3791 - acc: 0.915 - ETA: 4s - loss: 0.3732 - acc: 0.916 - ETA: 4s - loss: 0.3674 - acc: 0.918 - ETA: 4s - loss: 0.3771 - acc: 0.917 - ETA: 4s - loss: 0.3743 - acc: 0.917 - ETA: 4s - loss: 0.3682 - acc: 0.919 - ETA: 3s - loss: 0.3619 - acc: 0.921 - ETA: 3s - loss: 0.3644 - acc: 0.922 - ETA: 3s - loss: 0.3671 - acc: 0.923 - ETA: 3s - loss: 0.3765 - acc: 0.923 - ETA: 3s - loss: 0.3702 - acc: 0.924 - ETA: 3s - loss: 0.3638 - acc: 0.925 - ETA: 3s - loss: 0.3666 - acc: 0.925 - ETA: 3s - loss: 0.3785 - acc: 0.924 - ETA: 3s - loss: 0.3723 - acc: 0.925 - ETA: 3s - loss: 0.3713 - acc: 0.923 - ETA: 3s - loss: 0.3848 - acc: 0.921 - ETA: 3s - loss: 0.3893 - acc: 0.919 - ETA: 2s - loss: 0.3840 - acc: 0.920 - ETA: 2s - loss: 0.3814 - acc: 0.919 - ETA: 2s - loss: 0.3918 - acc: 0.918 - ETA: 2s - loss: 0.3881 - acc: 0.918 - ETA: 2s - loss: 0.3851 - acc: 0.919 - ETA: 2s - loss: 0.3824 - acc: 0.918 - ETA: 2s - loss: 0.3955 - acc: 0.916 - ETA: 2s - loss: 0.4028 - acc: 0.916 - ETA: 2s - loss: 0.4040 - acc: 0.917 - ETA: 2s - loss: 0.3968 - acc: 0.918 - ETA: 2s - loss: 0.3934 - acc: 0.918 - ETA: 2s - loss: 0.3897 - acc: 0.918 - ETA: 2s - loss: 0.3902 - acc: 0.919 - ETA: 1s - loss: 0.3944 - acc: 0.917 - ETA: 1s - loss: 0.4008 - acc: 0.915 - ETA: 1s - loss: 0.3972 - acc: 0.915 - ETA: 1s - loss: 0.3997 - acc: 0.914 - ETA: 1s - loss: 0.4073 - acc: 0.913 - ETA: 1s - loss: 0.4095 - acc: 0.913 - ETA: 1s - loss: 0.4125 - acc: 0.912 - ETA: 1s - loss: 0.4086 - acc: 0.912 - ETA: 1s - loss: 0.4078 - acc: 0.911 - ETA: 1s - loss: 0.4108 - acc: 0.911 - ETA: 1s - loss: 0.4176 - acc: 0.911 - ETA: 1s - loss: 0.4174 - acc: 0.911 - ETA: 1s - loss: 0.4197 - acc: 0.910 - ETA: 1s - loss: 0.4158 - acc: 0.911 - ETA: 1s - loss: 0.4165 - acc: 0.911 - ETA: 1s - loss: 0.4133 - acc: 0.911 - ETA: 0s - loss: 0.4199 - acc: 0.911 - ETA: 0s - loss: 0.4172 - acc: 0.911 - ETA: 0s - loss: 0.4185 - acc: 0.912 - ETA: 0s - loss: 0.4243 - acc: 0.911 - ETA: 0s - loss: 0.4212 - acc: 0.911 - ETA: 0s - loss: 0.4182 - acc: 0.911 - ETA: 0s - loss: 0.4279 - acc: 0.911 - ETA: 0s - loss: 0.4284 - acc: 0.911 - ETA: 0s - loss: 0.4336 - acc: 0.911 - ETA: 0s - loss: 0.4313 - acc: 0.911 - ETA: 0s - loss: 0.4409 - acc: 0.911 - ETA: 0s - loss: 0.4378 - acc: 0.911 - ETA: 0s - loss: 0.4338 - acc: 0.911 - ETA: 0s - loss: 0.4404 - acc: 0.911 - ETA: 0s - loss: 0.4398 - acc: 0.911 - 6s 2ms/step - loss: 0.4423 - acc: 0.9115 - val_loss: 0.4349 - val_acc: 0.9013\n",
      "Epoch 100/100\n",
      "4067/4067 [==============================] - ETA: 6s - loss: 0.1773 - acc: 0.875 - ETA: 6s - loss: 0.2038 - acc: 0.895 - ETA: 7s - loss: 0.3536 - acc: 0.912 - ETA: 6s - loss: 0.4095 - acc: 0.914 - ETA: 6s - loss: 0.4529 - acc: 0.918 - ETA: 6s - loss: 0.4688 - acc: 0.927 - ETA: 6s - loss: 0.4492 - acc: 0.915 - ETA: 6s - loss: 0.4095 - acc: 0.921 - ETA: 6s - loss: 0.4333 - acc: 0.923 - ETA: 6s - loss: 0.4472 - acc: 0.919 - ETA: 6s - loss: 0.4482 - acc: 0.921 - ETA: 6s - loss: 0.4652 - acc: 0.920 - ETA: 6s - loss: 0.4441 - acc: 0.924 - ETA: 5s - loss: 0.4199 - acc: 0.925 - ETA: 5s - loss: 0.4071 - acc: 0.926 - ETA: 5s - loss: 0.3900 - acc: 0.928 - ETA: 5s - loss: 0.3762 - acc: 0.930 - ETA: 5s - loss: 0.3846 - acc: 0.929 - ETA: 5s - loss: 0.3741 - acc: 0.930 - ETA: 5s - loss: 0.4008 - acc: 0.924 - ETA: 5s - loss: 0.4042 - acc: 0.925 - ETA: 5s - loss: 0.4145 - acc: 0.925 - ETA: 5s - loss: 0.4175 - acc: 0.925 - ETA: 5s - loss: 0.4218 - acc: 0.925 - ETA: 4s - loss: 0.4439 - acc: 0.923 - ETA: 4s - loss: 0.4531 - acc: 0.922 - ETA: 4s - loss: 0.4498 - acc: 0.919 - ETA: 4s - loss: 0.4388 - acc: 0.919 - ETA: 4s - loss: 0.4379 - acc: 0.918 - ETA: 4s - loss: 0.4456 - acc: 0.917 - ETA: 4s - loss: 0.4380 - acc: 0.916 - ETA: 4s - loss: 0.4246 - acc: 0.919 - ETA: 4s - loss: 0.4434 - acc: 0.915 - ETA: 4s - loss: 0.4351 - acc: 0.917 - ETA: 4s - loss: 0.4293 - acc: 0.914 - ETA: 4s - loss: 0.4480 - acc: 0.913 - ETA: 4s - loss: 0.4371 - acc: 0.914 - ETA: 4s - loss: 0.4318 - acc: 0.914 - ETA: 4s - loss: 0.4229 - acc: 0.915 - ETA: 3s - loss: 0.4149 - acc: 0.916 - ETA: 3s - loss: 0.4091 - acc: 0.915 - ETA: 3s - loss: 0.4220 - acc: 0.914 - ETA: 3s - loss: 0.4189 - acc: 0.911 - ETA: 3s - loss: 0.4102 - acc: 0.913 - ETA: 3s - loss: 0.4167 - acc: 0.912 - ETA: 3s - loss: 0.4103 - acc: 0.914 - ETA: 3s - loss: 0.4134 - acc: 0.913 - ETA: 3s - loss: 0.4069 - acc: 0.913 - ETA: 3s - loss: 0.4105 - acc: 0.912 - ETA: 3s - loss: 0.4122 - acc: 0.911 - ETA: 3s - loss: 0.4163 - acc: 0.910 - ETA: 3s - loss: 0.4103 - acc: 0.911 - ETA: 2s - loss: 0.4216 - acc: 0.910 - ETA: 2s - loss: 0.4158 - acc: 0.911 - ETA: 2s - loss: 0.4164 - acc: 0.911 - ETA: 2s - loss: 0.4120 - acc: 0.911 - ETA: 2s - loss: 0.4164 - acc: 0.910 - ETA: 2s - loss: 0.4167 - acc: 0.911 - ETA: 2s - loss: 0.4139 - acc: 0.911 - ETA: 2s - loss: 0.4100 - acc: 0.911 - ETA: 2s - loss: 0.4074 - acc: 0.911 - ETA: 2s - loss: 0.4115 - acc: 0.910 - ETA: 2s - loss: 0.4076 - acc: 0.910 - ETA: 2s - loss: 0.4041 - acc: 0.910 - ETA: 2s - loss: 0.4230 - acc: 0.909 - ETA: 2s - loss: 0.4264 - acc: 0.908 - ETA: 2s - loss: 0.4366 - acc: 0.906 - ETA: 1s - loss: 0.4367 - acc: 0.906 - ETA: 1s - loss: 0.4432 - acc: 0.906 - ETA: 1s - loss: 0.4380 - acc: 0.907 - ETA: 1s - loss: 0.4406 - acc: 0.906 - ETA: 1s - loss: 0.4420 - acc: 0.907 - ETA: 1s - loss: 0.4384 - acc: 0.908 - ETA: 1s - loss: 0.4335 - acc: 0.909 - ETA: 1s - loss: 0.4303 - acc: 0.908 - ETA: 1s - loss: 0.4369 - acc: 0.908 - ETA: 1s - loss: 0.4326 - acc: 0.908 - ETA: 1s - loss: 0.4291 - acc: 0.909 - ETA: 1s - loss: 0.4309 - acc: 0.908 - ETA: 1s - loss: 0.4315 - acc: 0.909 - ETA: 1s - loss: 0.4286 - acc: 0.909 - ETA: 0s - loss: 0.4355 - acc: 0.908 - ETA: 0s - loss: 0.4327 - acc: 0.907 - ETA: 0s - loss: 0.4399 - acc: 0.907 - ETA: 0s - loss: 0.4356 - acc: 0.907 - ETA: 0s - loss: 0.4430 - acc: 0.907 - ETA: 0s - loss: 0.4435 - acc: 0.908 - ETA: 0s - loss: 0.4436 - acc: 0.908 - ETA: 0s - loss: 0.4402 - acc: 0.908 - ETA: 0s - loss: 0.4458 - acc: 0.908 - ETA: 0s - loss: 0.4478 - acc: 0.908 - ETA: 0s - loss: 0.4511 - acc: 0.908 - ETA: 0s - loss: 0.4496 - acc: 0.907 - ETA: 0s - loss: 0.4540 - acc: 0.907 - ETA: 0s - loss: 0.4519 - acc: 0.907 - ETA: 0s - loss: 0.4558 - acc: 0.906 - ETA: 0s - loss: 0.4588 - acc: 0.906 - ETA: 0s - loss: 0.4591 - acc: 0.906 - 6s 2ms/step - loss: 0.4566 - acc: 0.9071 - val_loss: 0.4318 - val_acc: 0.8987\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "adam = keras.optimizers.Adam(lr=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.fit(X_train_s,Y_train_s, epochs=100, batch_size=16,validation_data=(X_val_s, Y_val_s), verbose=1)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d, Y_train_d, X_val_d, Y_val_d = data_scaled_dynamic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3285 samples, validate on 1387 samples\n",
      "Epoch 1/40\n",
      "3285/3285 [==============================] - ETA: 7:49 - loss: 2.0778 - acc: 0.312 - ETA: 2:40 - loss: 1.5070 - acc: 0.458 - ETA: 1:38 - loss: 1.6841 - acc: 0.412 - ETA: 1:12 - loss: 1.8303 - acc: 0.366 - ETA: 57s - loss: 1.7413 - acc: 0.381 - ETA: 47s - loss: 1.7483 - acc: 0.36 - ETA: 40s - loss: 1.7444 - acc: 0.36 - ETA: 36s - loss: 1.7777 - acc: 0.36 - ETA: 32s - loss: 1.7455 - acc: 0.36 - ETA: 29s - loss: 1.7407 - acc: 0.37 - ETA: 26s - loss: 1.7432 - acc: 0.36 - ETA: 24s - loss: 1.7441 - acc: 0.36 - ETA: 23s - loss: 1.7319 - acc: 0.36 - ETA: 22s - loss: 1.7000 - acc: 0.37 - ETA: 21s - loss: 1.6991 - acc: 0.37 - ETA: 20s - loss: 1.6753 - acc: 0.37 - ETA: 19s - loss: 1.6495 - acc: 0.37 - ETA: 18s - loss: 1.6184 - acc: 0.38 - ETA: 17s - loss: 1.6328 - acc: 0.38 - ETA: 16s - loss: 1.6416 - acc: 0.38 - ETA: 15s - loss: 1.6538 - acc: 0.37 - ETA: 15s - loss: 1.6380 - acc: 0.37 - ETA: 14s - loss: 1.6267 - acc: 0.37 - ETA: 14s - loss: 1.6193 - acc: 0.37 - ETA: 14s - loss: 1.6059 - acc: 0.37 - ETA: 13s - loss: 1.6084 - acc: 0.37 - ETA: 13s - loss: 1.6062 - acc: 0.37 - ETA: 13s - loss: 1.5882 - acc: 0.38 - ETA: 12s - loss: 1.5818 - acc: 0.38 - ETA: 12s - loss: 1.5749 - acc: 0.38 - ETA: 12s - loss: 1.5638 - acc: 0.39 - ETA: 11s - loss: 1.5599 - acc: 0.39 - ETA: 11s - loss: 1.5498 - acc: 0.39 - ETA: 10s - loss: 1.5484 - acc: 0.39 - ETA: 10s - loss: 1.5426 - acc: 0.39 - ETA: 10s - loss: 1.5411 - acc: 0.39 - ETA: 10s - loss: 1.5312 - acc: 0.39 - ETA: 9s - loss: 1.5157 - acc: 0.4058 - ETA: 9s - loss: 1.5046 - acc: 0.410 - ETA: 9s - loss: 1.4975 - acc: 0.414 - ETA: 8s - loss: 1.4825 - acc: 0.421 - ETA: 8s - loss: 1.4878 - acc: 0.420 - ETA: 8s - loss: 1.4857 - acc: 0.420 - ETA: 8s - loss: 1.4786 - acc: 0.420 - ETA: 8s - loss: 1.4788 - acc: 0.419 - ETA: 7s - loss: 1.4705 - acc: 0.419 - ETA: 7s - loss: 1.4641 - acc: 0.418 - ETA: 7s - loss: 1.4636 - acc: 0.416 - ETA: 7s - loss: 1.4605 - acc: 0.417 - ETA: 7s - loss: 1.4594 - acc: 0.415 - ETA: 6s - loss: 1.4492 - acc: 0.418 - ETA: 6s - loss: 1.4389 - acc: 0.421 - ETA: 6s - loss: 1.4326 - acc: 0.419 - ETA: 6s - loss: 1.4249 - acc: 0.419 - ETA: 6s - loss: 1.4249 - acc: 0.423 - ETA: 5s - loss: 1.4210 - acc: 0.427 - ETA: 5s - loss: 1.4215 - acc: 0.427 - ETA: 5s - loss: 1.4192 - acc: 0.428 - ETA: 5s - loss: 1.4140 - acc: 0.431 - ETA: 5s - loss: 1.4132 - acc: 0.432 - ETA: 5s - loss: 1.4021 - acc: 0.437 - ETA: 5s - loss: 1.3918 - acc: 0.441 - ETA: 4s - loss: 1.3921 - acc: 0.442 - ETA: 4s - loss: 1.3828 - acc: 0.446 - ETA: 4s - loss: 1.3837 - acc: 0.448 - ETA: 4s - loss: 1.3758 - acc: 0.450 - ETA: 4s - loss: 1.3776 - acc: 0.451 - ETA: 4s - loss: 1.3702 - acc: 0.454 - ETA: 4s - loss: 1.3639 - acc: 0.455 - ETA: 3s - loss: 1.3553 - acc: 0.458 - ETA: 3s - loss: 1.3495 - acc: 0.461 - ETA: 3s - loss: 1.3411 - acc: 0.464 - ETA: 3s - loss: 1.3370 - acc: 0.465 - ETA: 3s - loss: 1.3353 - acc: 0.466 - ETA: 3s - loss: 1.3285 - acc: 0.468 - ETA: 3s - loss: 1.3223 - acc: 0.469 - ETA: 3s - loss: 1.3183 - acc: 0.471 - ETA: 3s - loss: 1.3134 - acc: 0.473 - ETA: 2s - loss: 1.3071 - acc: 0.475 - ETA: 2s - loss: 1.3027 - acc: 0.476 - ETA: 2s - loss: 1.2993 - acc: 0.478 - ETA: 2s - loss: 1.2962 - acc: 0.480 - ETA: 2s - loss: 1.2907 - acc: 0.482 - ETA: 2s - loss: 1.2862 - acc: 0.483 - ETA: 2s - loss: 1.2787 - acc: 0.486 - ETA: 2s - loss: 1.2744 - acc: 0.487 - ETA: 1s - loss: 1.2706 - acc: 0.488 - ETA: 1s - loss: 1.2674 - acc: 0.488 - ETA: 1s - loss: 1.2663 - acc: 0.489 - ETA: 1s - loss: 1.2615 - acc: 0.491 - ETA: 1s - loss: 1.2555 - acc: 0.492 - ETA: 1s - loss: 1.2523 - acc: 0.493 - ETA: 1s - loss: 1.2490 - acc: 0.494 - ETA: 1s - loss: 1.2472 - acc: 0.495 - ETA: 1s - loss: 1.2428 - acc: 0.495 - ETA: 0s - loss: 1.2416 - acc: 0.495 - ETA: 0s - loss: 1.2390 - acc: 0.495 - ETA: 0s - loss: 1.2350 - acc: 0.496 - ETA: 0s - loss: 1.2288 - acc: 0.499 - ETA: 0s - loss: 1.2246 - acc: 0.500 - ETA: 0s - loss: 1.2211 - acc: 0.500 - ETA: 0s - loss: 1.2163 - acc: 0.502 - ETA: 0s - loss: 1.2105 - acc: 0.503 - ETA: 0s - loss: 1.2084 - acc: 0.503 - ETA: 0s - loss: 1.2076 - acc: 0.503 - 11s 3ms/step - loss: 1.2067 - acc: 0.5041 - val_loss: 1.0325 - val_acc: 0.6294\n",
      "Epoch 2/40\n",
      "3285/3285 [==============================] - ETA: 8s - loss: 0.7313 - acc: 0.500 - ETA: 7s - loss: 0.8495 - acc: 0.541 - ETA: 7s - loss: 0.9625 - acc: 0.537 - ETA: 7s - loss: 0.8512 - acc: 0.589 - ETA: 7s - loss: 0.8278 - acc: 0.609 - ETA: 7s - loss: 0.8791 - acc: 0.600 - ETA: 7s - loss: 0.8519 - acc: 0.619 - ETA: 7s - loss: 0.8602 - acc: 0.620 - ETA: 7s - loss: 0.8820 - acc: 0.621 - ETA: 7s - loss: 0.8731 - acc: 0.621 - ETA: 7s - loss: 0.8529 - acc: 0.631 - ETA: 7s - loss: 0.8778 - acc: 0.625 - ETA: 7s - loss: 0.8609 - acc: 0.632 - ETA: 7s - loss: 0.8472 - acc: 0.641 - ETA: 6s - loss: 0.8349 - acc: 0.651 - ETA: 6s - loss: 0.8353 - acc: 0.650 - ETA: 6s - loss: 0.8397 - acc: 0.652 - ETA: 6s - loss: 0.8245 - acc: 0.663 - ETA: 6s - loss: 0.8323 - acc: 0.654 - ETA: 6s - loss: 0.8208 - acc: 0.654 - ETA: 6s - loss: 0.8282 - acc: 0.651 - ETA: 6s - loss: 0.8307 - acc: 0.651 - ETA: 6s - loss: 0.8291 - acc: 0.653 - ETA: 6s - loss: 0.8226 - acc: 0.652 - ETA: 6s - loss: 0.8355 - acc: 0.648 - ETA: 6s - loss: 0.8282 - acc: 0.652 - ETA: 6s - loss: 0.8161 - acc: 0.661 - ETA: 6s - loss: 0.8052 - acc: 0.665 - ETA: 5s - loss: 0.8083 - acc: 0.662 - ETA: 5s - loss: 0.8042 - acc: 0.664 - ETA: 5s - loss: 0.7990 - acc: 0.665 - ETA: 5s - loss: 0.7929 - acc: 0.670 - ETA: 5s - loss: 0.7885 - acc: 0.670 - ETA: 5s - loss: 0.7804 - acc: 0.674 - ETA: 5s - loss: 0.7693 - acc: 0.679 - ETA: 5s - loss: 0.7613 - acc: 0.683 - ETA: 5s - loss: 0.7516 - acc: 0.688 - ETA: 5s - loss: 0.7472 - acc: 0.691 - ETA: 5s - loss: 0.7459 - acc: 0.693 - ETA: 4s - loss: 0.7419 - acc: 0.694 - ETA: 4s - loss: 0.7419 - acc: 0.694 - ETA: 4s - loss: 0.7394 - acc: 0.692 - ETA: 4s - loss: 0.7344 - acc: 0.693 - ETA: 4s - loss: 0.7353 - acc: 0.693 - ETA: 4s - loss: 0.7343 - acc: 0.693 - ETA: 4s - loss: 0.7339 - acc: 0.696 - ETA: 4s - loss: 0.7292 - acc: 0.699 - ETA: 4s - loss: 0.7294 - acc: 0.700 - ETA: 4s - loss: 0.7282 - acc: 0.700 - ETA: 4s - loss: 0.7274 - acc: 0.699 - ETA: 4s - loss: 0.7257 - acc: 0.700 - ETA: 4s - loss: 0.7251 - acc: 0.699 - ETA: 3s - loss: 0.7269 - acc: 0.697 - ETA: 3s - loss: 0.7289 - acc: 0.698 - ETA: 3s - loss: 0.7301 - acc: 0.699 - ETA: 3s - loss: 0.7276 - acc: 0.701 - ETA: 3s - loss: 0.7250 - acc: 0.702 - ETA: 3s - loss: 0.7233 - acc: 0.703 - ETA: 3s - loss: 0.7203 - acc: 0.703 - ETA: 3s - loss: 0.7204 - acc: 0.702 - ETA: 3s - loss: 0.7177 - acc: 0.703 - ETA: 3s - loss: 0.7161 - acc: 0.702 - ETA: 3s - loss: 0.7170 - acc: 0.701 - ETA: 3s - loss: 0.7147 - acc: 0.701 - ETA: 3s - loss: 0.7124 - acc: 0.703 - ETA: 2s - loss: 0.7139 - acc: 0.704 - ETA: 2s - loss: 0.7150 - acc: 0.704 - ETA: 2s - loss: 0.7146 - acc: 0.704 - ETA: 2s - loss: 0.7111 - acc: 0.705 - ETA: 2s - loss: 0.7078 - acc: 0.706 - ETA: 2s - loss: 0.7042 - acc: 0.707 - ETA: 2s - loss: 0.7053 - acc: 0.706 - ETA: 2s - loss: 0.7050 - acc: 0.705 - ETA: 2s - loss: 0.7086 - acc: 0.703 - ETA: 2s - loss: 0.7098 - acc: 0.702 - ETA: 2s - loss: 0.7048 - acc: 0.704 - ETA: 2s - loss: 0.7016 - acc: 0.706 - ETA: 1s - loss: 0.6972 - acc: 0.707 - ETA: 1s - loss: 0.6954 - acc: 0.709 - ETA: 1s - loss: 0.6906 - acc: 0.711 - ETA: 1s - loss: 0.6883 - acc: 0.712 - ETA: 1s - loss: 0.6869 - acc: 0.713 - ETA: 1s - loss: 0.6834 - acc: 0.714 - ETA: 1s - loss: 0.6801 - acc: 0.714 - ETA: 1s - loss: 0.6785 - acc: 0.715 - ETA: 1s - loss: 0.6754 - acc: 0.717 - ETA: 1s - loss: 0.6722 - acc: 0.719 - ETA: 1s - loss: 0.6719 - acc: 0.720 - ETA: 1s - loss: 0.6715 - acc: 0.720 - ETA: 1s - loss: 0.6692 - acc: 0.721 - ETA: 1s - loss: 0.6669 - acc: 0.723 - ETA: 0s - loss: 0.6648 - acc: 0.723 - ETA: 0s - loss: 0.6628 - acc: 0.724 - ETA: 0s - loss: 0.6635 - acc: 0.724 - ETA: 0s - loss: 0.6598 - acc: 0.726 - ETA: 0s - loss: 0.6577 - acc: 0.727 - ETA: 0s - loss: 0.6566 - acc: 0.728 - ETA: 0s - loss: 0.6535 - acc: 0.729 - ETA: 0s - loss: 0.6509 - acc: 0.731 - ETA: 0s - loss: 0.6506 - acc: 0.731 - ETA: 0s - loss: 0.6491 - acc: 0.732 - ETA: 0s - loss: 0.6472 - acc: 0.733 - ETA: 0s - loss: 0.6459 - acc: 0.733 - ETA: 0s - loss: 0.6446 - acc: 0.734 - 8s 3ms/step - loss: 0.6438 - acc: 0.7349 - val_loss: 0.5180 - val_acc: 0.8017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.2576 - acc: 0.875 - ETA: 7s - loss: 0.3494 - acc: 0.895 - ETA: 7s - loss: 0.3980 - acc: 0.862 - ETA: 7s - loss: 0.4207 - acc: 0.812 - ETA: 7s - loss: 0.4208 - acc: 0.819 - ETA: 7s - loss: 0.4673 - acc: 0.812 - ETA: 7s - loss: 0.4439 - acc: 0.826 - ETA: 7s - loss: 0.4305 - acc: 0.825 - ETA: 7s - loss: 0.4547 - acc: 0.816 - ETA: 7s - loss: 0.4562 - acc: 0.819 - ETA: 6s - loss: 0.4594 - acc: 0.812 - ETA: 6s - loss: 0.4570 - acc: 0.812 - ETA: 6s - loss: 0.4565 - acc: 0.815 - ETA: 6s - loss: 0.4616 - acc: 0.814 - ETA: 6s - loss: 0.4721 - acc: 0.808 - ETA: 6s - loss: 0.4689 - acc: 0.812 - ETA: 6s - loss: 0.4756 - acc: 0.808 - ETA: 6s - loss: 0.4725 - acc: 0.812 - ETA: 6s - loss: 0.4649 - acc: 0.819 - ETA: 6s - loss: 0.4631 - acc: 0.820 - ETA: 6s - loss: 0.4586 - acc: 0.820 - ETA: 6s - loss: 0.4598 - acc: 0.819 - ETA: 6s - loss: 0.4505 - acc: 0.826 - ETA: 6s - loss: 0.4525 - acc: 0.825 - ETA: 6s - loss: 0.4562 - acc: 0.822 - ETA: 6s - loss: 0.4632 - acc: 0.818 - ETA: 6s - loss: 0.4649 - acc: 0.816 - ETA: 6s - loss: 0.4633 - acc: 0.818 - ETA: 5s - loss: 0.4677 - acc: 0.814 - ETA: 5s - loss: 0.4664 - acc: 0.813 - ETA: 5s - loss: 0.4611 - acc: 0.815 - ETA: 5s - loss: 0.4562 - acc: 0.817 - ETA: 5s - loss: 0.4507 - acc: 0.820 - ETA: 5s - loss: 0.4538 - acc: 0.819 - ETA: 5s - loss: 0.4477 - acc: 0.822 - ETA: 5s - loss: 0.4465 - acc: 0.825 - ETA: 5s - loss: 0.4405 - acc: 0.827 - ETA: 5s - loss: 0.4373 - acc: 0.828 - ETA: 5s - loss: 0.4318 - acc: 0.831 - ETA: 5s - loss: 0.4289 - acc: 0.832 - ETA: 5s - loss: 0.4284 - acc: 0.832 - ETA: 4s - loss: 0.4285 - acc: 0.833 - ETA: 4s - loss: 0.4263 - acc: 0.834 - ETA: 4s - loss: 0.4232 - acc: 0.836 - ETA: 4s - loss: 0.4238 - acc: 0.835 - ETA: 4s - loss: 0.4225 - acc: 0.835 - ETA: 4s - loss: 0.4252 - acc: 0.833 - ETA: 4s - loss: 0.4270 - acc: 0.834 - ETA: 4s - loss: 0.4314 - acc: 0.835 - ETA: 4s - loss: 0.4270 - acc: 0.837 - ETA: 4s - loss: 0.4253 - acc: 0.837 - ETA: 4s - loss: 0.4213 - acc: 0.839 - ETA: 4s - loss: 0.4205 - acc: 0.840 - ETA: 3s - loss: 0.4173 - acc: 0.842 - ETA: 3s - loss: 0.4173 - acc: 0.843 - ETA: 3s - loss: 0.4178 - acc: 0.842 - ETA: 3s - loss: 0.4147 - acc: 0.844 - ETA: 3s - loss: 0.4112 - acc: 0.845 - ETA: 3s - loss: 0.4090 - acc: 0.846 - ETA: 3s - loss: 0.4062 - acc: 0.848 - ETA: 3s - loss: 0.4048 - acc: 0.848 - ETA: 3s - loss: 0.4045 - acc: 0.849 - ETA: 3s - loss: 0.4019 - acc: 0.851 - ETA: 3s - loss: 0.3996 - acc: 0.852 - ETA: 3s - loss: 0.3981 - acc: 0.852 - ETA: 2s - loss: 0.3956 - acc: 0.854 - ETA: 2s - loss: 0.3923 - acc: 0.855 - ETA: 2s - loss: 0.3936 - acc: 0.853 - ETA: 2s - loss: 0.3979 - acc: 0.850 - ETA: 2s - loss: 0.3973 - acc: 0.850 - ETA: 2s - loss: 0.3953 - acc: 0.852 - ETA: 2s - loss: 0.3921 - acc: 0.853 - ETA: 2s - loss: 0.3942 - acc: 0.852 - ETA: 2s - loss: 0.3928 - acc: 0.853 - ETA: 2s - loss: 0.3911 - acc: 0.853 - ETA: 2s - loss: 0.3903 - acc: 0.854 - ETA: 2s - loss: 0.3895 - acc: 0.853 - ETA: 2s - loss: 0.3893 - acc: 0.854 - ETA: 1s - loss: 0.3889 - acc: 0.853 - ETA: 1s - loss: 0.3890 - acc: 0.853 - ETA: 1s - loss: 0.3909 - acc: 0.852 - ETA: 1s - loss: 0.3935 - acc: 0.851 - ETA: 1s - loss: 0.3941 - acc: 0.850 - ETA: 1s - loss: 0.3930 - acc: 0.850 - ETA: 1s - loss: 0.3932 - acc: 0.850 - ETA: 1s - loss: 0.3921 - acc: 0.850 - ETA: 1s - loss: 0.3928 - acc: 0.850 - ETA: 1s - loss: 0.3913 - acc: 0.851 - ETA: 1s - loss: 0.3914 - acc: 0.851 - ETA: 1s - loss: 0.3906 - acc: 0.852 - ETA: 1s - loss: 0.3901 - acc: 0.852 - ETA: 0s - loss: 0.3884 - acc: 0.853 - ETA: 0s - loss: 0.3887 - acc: 0.852 - ETA: 0s - loss: 0.3870 - acc: 0.853 - ETA: 0s - loss: 0.3861 - acc: 0.853 - ETA: 0s - loss: 0.3889 - acc: 0.853 - ETA: 0s - loss: 0.3886 - acc: 0.853 - ETA: 0s - loss: 0.3892 - acc: 0.853 - ETA: 0s - loss: 0.3891 - acc: 0.853 - ETA: 0s - loss: 0.3873 - acc: 0.855 - ETA: 0s - loss: 0.3854 - acc: 0.856 - ETA: 0s - loss: 0.3845 - acc: 0.857 - ETA: 0s - loss: 0.3830 - acc: 0.858 - ETA: 0s - loss: 0.3821 - acc: 0.858 - 8s 3ms/step - loss: 0.3817 - acc: 0.8588 - val_loss: 0.2767 - val_acc: 0.9243\n",
      "Epoch 4/40\n",
      "3285/3285 [==============================] - ETA: 8s - loss: 0.1959 - acc: 1.000 - ETA: 7s - loss: 0.1750 - acc: 0.979 - ETA: 7s - loss: 0.2272 - acc: 0.937 - ETA: 7s - loss: 0.2127 - acc: 0.937 - ETA: 7s - loss: 0.2004 - acc: 0.944 - ETA: 7s - loss: 0.2080 - acc: 0.943 - ETA: 7s - loss: 0.2309 - acc: 0.923 - ETA: 7s - loss: 0.2365 - acc: 0.925 - ETA: 7s - loss: 0.2479 - acc: 0.919 - ETA: 7s - loss: 0.2528 - acc: 0.914 - ETA: 7s - loss: 0.2935 - acc: 0.895 - ETA: 7s - loss: 0.2975 - acc: 0.891 - ETA: 6s - loss: 0.2922 - acc: 0.892 - ETA: 6s - loss: 0.2900 - acc: 0.895 - ETA: 6s - loss: 0.2900 - acc: 0.890 - ETA: 6s - loss: 0.2801 - acc: 0.895 - ETA: 6s - loss: 0.2786 - acc: 0.895 - ETA: 6s - loss: 0.2780 - acc: 0.898 - ETA: 6s - loss: 0.2830 - acc: 0.895 - ETA: 6s - loss: 0.2854 - acc: 0.891 - ETA: 6s - loss: 0.2846 - acc: 0.890 - ETA: 6s - loss: 0.2779 - acc: 0.893 - ETA: 5s - loss: 0.2744 - acc: 0.897 - ETA: 5s - loss: 0.2941 - acc: 0.893 - ETA: 5s - loss: 0.2913 - acc: 0.895 - ETA: 5s - loss: 0.2985 - acc: 0.893 - ETA: 5s - loss: 0.2961 - acc: 0.893 - ETA: 5s - loss: 0.2928 - acc: 0.893 - ETA: 5s - loss: 0.2893 - acc: 0.893 - ETA: 5s - loss: 0.2841 - acc: 0.897 - ETA: 5s - loss: 0.2808 - acc: 0.899 - ETA: 5s - loss: 0.2816 - acc: 0.899 - ETA: 5s - loss: 0.2792 - acc: 0.900 - ETA: 5s - loss: 0.2802 - acc: 0.899 - ETA: 5s - loss: 0.2778 - acc: 0.901 - ETA: 5s - loss: 0.2786 - acc: 0.900 - ETA: 5s - loss: 0.2781 - acc: 0.900 - ETA: 5s - loss: 0.2767 - acc: 0.901 - ETA: 5s - loss: 0.2734 - acc: 0.902 - ETA: 4s - loss: 0.2704 - acc: 0.903 - ETA: 4s - loss: 0.2696 - acc: 0.903 - ETA: 4s - loss: 0.2680 - acc: 0.904 - ETA: 4s - loss: 0.2687 - acc: 0.903 - ETA: 4s - loss: 0.2643 - acc: 0.905 - ETA: 4s - loss: 0.2626 - acc: 0.905 - ETA: 4s - loss: 0.2621 - acc: 0.906 - ETA: 4s - loss: 0.2655 - acc: 0.905 - ETA: 4s - loss: 0.2642 - acc: 0.905 - ETA: 4s - loss: 0.2649 - acc: 0.905 - ETA: 4s - loss: 0.2639 - acc: 0.906 - ETA: 4s - loss: 0.2624 - acc: 0.907 - ETA: 4s - loss: 0.2602 - acc: 0.908 - ETA: 3s - loss: 0.2587 - acc: 0.909 - ETA: 3s - loss: 0.2587 - acc: 0.909 - ETA: 3s - loss: 0.2583 - acc: 0.908 - ETA: 3s - loss: 0.2553 - acc: 0.910 - ETA: 3s - loss: 0.2547 - acc: 0.910 - ETA: 3s - loss: 0.2539 - acc: 0.910 - ETA: 3s - loss: 0.2536 - acc: 0.910 - ETA: 3s - loss: 0.2522 - acc: 0.910 - ETA: 3s - loss: 0.2513 - acc: 0.910 - ETA: 3s - loss: 0.2503 - acc: 0.911 - ETA: 3s - loss: 0.2478 - acc: 0.912 - ETA: 3s - loss: 0.2473 - acc: 0.912 - ETA: 3s - loss: 0.2467 - acc: 0.912 - ETA: 2s - loss: 0.2476 - acc: 0.912 - ETA: 2s - loss: 0.2466 - acc: 0.912 - ETA: 2s - loss: 0.2452 - acc: 0.913 - ETA: 2s - loss: 0.2445 - acc: 0.913 - ETA: 2s - loss: 0.2430 - acc: 0.914 - ETA: 2s - loss: 0.2412 - acc: 0.915 - ETA: 2s - loss: 0.2436 - acc: 0.914 - ETA: 2s - loss: 0.2421 - acc: 0.915 - ETA: 2s - loss: 0.2414 - acc: 0.916 - ETA: 2s - loss: 0.2398 - acc: 0.916 - ETA: 2s - loss: 0.2391 - acc: 0.917 - ETA: 2s - loss: 0.2396 - acc: 0.917 - ETA: 2s - loss: 0.2387 - acc: 0.918 - ETA: 1s - loss: 0.2409 - acc: 0.917 - ETA: 1s - loss: 0.2394 - acc: 0.918 - ETA: 1s - loss: 0.2377 - acc: 0.919 - ETA: 1s - loss: 0.2372 - acc: 0.919 - ETA: 1s - loss: 0.2365 - acc: 0.919 - ETA: 1s - loss: 0.2351 - acc: 0.919 - ETA: 1s - loss: 0.2348 - acc: 0.920 - ETA: 1s - loss: 0.2335 - acc: 0.921 - ETA: 1s - loss: 0.2319 - acc: 0.922 - ETA: 1s - loss: 0.2317 - acc: 0.922 - ETA: 1s - loss: 0.2312 - acc: 0.922 - ETA: 1s - loss: 0.2304 - acc: 0.922 - ETA: 1s - loss: 0.2298 - acc: 0.922 - ETA: 1s - loss: 0.2290 - acc: 0.922 - ETA: 1s - loss: 0.2291 - acc: 0.922 - ETA: 0s - loss: 0.2302 - acc: 0.922 - ETA: 0s - loss: 0.2304 - acc: 0.922 - ETA: 0s - loss: 0.2303 - acc: 0.922 - ETA: 0s - loss: 0.2314 - acc: 0.922 - ETA: 0s - loss: 0.2316 - acc: 0.921 - ETA: 0s - loss: 0.2335 - acc: 0.921 - ETA: 0s - loss: 0.2323 - acc: 0.922 - ETA: 0s - loss: 0.2320 - acc: 0.922 - ETA: 0s - loss: 0.2323 - acc: 0.922 - ETA: 0s - loss: 0.2313 - acc: 0.922 - ETA: 0s - loss: 0.2303 - acc: 0.923 - ETA: 0s - loss: 0.2303 - acc: 0.922 - ETA: 0s - loss: 0.2319 - acc: 0.922 - ETA: 0s - loss: 0.2318 - acc: 0.922 - ETA: 0s - loss: 0.2311 - acc: 0.922 - 8s 3ms/step - loss: 0.2302 - acc: 0.9230 - val_loss: 0.2534 - val_acc: 0.9200\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.3791 - acc: 0.875 - ETA: 7s - loss: 0.2227 - acc: 0.937 - ETA: 7s - loss: 0.2104 - acc: 0.937 - ETA: 7s - loss: 0.2162 - acc: 0.937 - ETA: 7s - loss: 0.2030 - acc: 0.945 - ETA: 8s - loss: 0.1917 - acc: 0.951 - ETA: 7s - loss: 0.1834 - acc: 0.960 - ETA: 8s - loss: 0.1751 - acc: 0.961 - ETA: 8s - loss: 0.1664 - acc: 0.964 - ETA: 8s - loss: 0.1832 - acc: 0.953 - ETA: 8s - loss: 0.1751 - acc: 0.954 - ETA: 8s - loss: 0.1749 - acc: 0.953 - ETA: 8s - loss: 0.1667 - acc: 0.958 - ETA: 8s - loss: 0.1828 - acc: 0.953 - ETA: 8s - loss: 0.1987 - acc: 0.947 - ETA: 8s - loss: 0.1976 - acc: 0.947 - ETA: 8s - loss: 0.1901 - acc: 0.948 - ETA: 8s - loss: 0.1872 - acc: 0.948 - ETA: 8s - loss: 0.1830 - acc: 0.950 - ETA: 8s - loss: 0.1824 - acc: 0.949 - ETA: 7s - loss: 0.1874 - acc: 0.945 - ETA: 7s - loss: 0.1895 - acc: 0.942 - ETA: 7s - loss: 0.1854 - acc: 0.944 - ETA: 7s - loss: 0.1904 - acc: 0.944 - ETA: 7s - loss: 0.2067 - acc: 0.935 - ETA: 7s - loss: 0.2000 - acc: 0.939 - ETA: 7s - loss: 0.2013 - acc: 0.937 - ETA: 7s - loss: 0.2051 - acc: 0.937 - ETA: 7s - loss: 0.2063 - acc: 0.937 - ETA: 7s - loss: 0.2073 - acc: 0.936 - ETA: 6s - loss: 0.2046 - acc: 0.937 - ETA: 6s - loss: 0.2042 - acc: 0.937 - ETA: 6s - loss: 0.2026 - acc: 0.937 - ETA: 6s - loss: 0.2024 - acc: 0.936 - ETA: 6s - loss: 0.2026 - acc: 0.935 - ETA: 6s - loss: 0.2064 - acc: 0.933 - ETA: 6s - loss: 0.2089 - acc: 0.933 - ETA: 6s - loss: 0.2078 - acc: 0.934 - ETA: 6s - loss: 0.2106 - acc: 0.931 - ETA: 6s - loss: 0.2083 - acc: 0.932 - ETA: 6s - loss: 0.2063 - acc: 0.933 - ETA: 6s - loss: 0.2052 - acc: 0.933 - ETA: 6s - loss: 0.2014 - acc: 0.935 - ETA: 6s - loss: 0.1981 - acc: 0.937 - ETA: 6s - loss: 0.1961 - acc: 0.938 - ETA: 5s - loss: 0.1918 - acc: 0.940 - ETA: 5s - loss: 0.1893 - acc: 0.941 - ETA: 5s - loss: 0.1898 - acc: 0.940 - ETA: 5s - loss: 0.1866 - acc: 0.942 - ETA: 5s - loss: 0.1851 - acc: 0.943 - ETA: 5s - loss: 0.1856 - acc: 0.943 - ETA: 5s - loss: 0.1843 - acc: 0.944 - ETA: 5s - loss: 0.1825 - acc: 0.944 - ETA: 5s - loss: 0.1824 - acc: 0.943 - ETA: 4s - loss: 0.1802 - acc: 0.944 - ETA: 4s - loss: 0.1843 - acc: 0.942 - ETA: 4s - loss: 0.1832 - acc: 0.942 - ETA: 4s - loss: 0.1866 - acc: 0.940 - ETA: 4s - loss: 0.1864 - acc: 0.940 - ETA: 4s - loss: 0.1845 - acc: 0.941 - ETA: 4s - loss: 0.1835 - acc: 0.941 - ETA: 4s - loss: 0.1812 - acc: 0.942 - ETA: 4s - loss: 0.1808 - acc: 0.942 - ETA: 4s - loss: 0.1790 - acc: 0.943 - ETA: 3s - loss: 0.1810 - acc: 0.943 - ETA: 3s - loss: 0.1792 - acc: 0.943 - ETA: 3s - loss: 0.1821 - acc: 0.942 - ETA: 3s - loss: 0.1846 - acc: 0.941 - ETA: 3s - loss: 0.1871 - acc: 0.941 - ETA: 3s - loss: 0.1877 - acc: 0.941 - ETA: 3s - loss: 0.1912 - acc: 0.940 - ETA: 3s - loss: 0.1887 - acc: 0.941 - ETA: 3s - loss: 0.1877 - acc: 0.941 - ETA: 3s - loss: 0.1901 - acc: 0.940 - ETA: 3s - loss: 0.1884 - acc: 0.941 - ETA: 2s - loss: 0.1884 - acc: 0.941 - ETA: 2s - loss: 0.1880 - acc: 0.941 - ETA: 2s - loss: 0.1873 - acc: 0.942 - ETA: 2s - loss: 0.1860 - acc: 0.942 - ETA: 2s - loss: 0.1856 - acc: 0.942 - ETA: 2s - loss: 0.1843 - acc: 0.942 - ETA: 2s - loss: 0.1834 - acc: 0.942 - ETA: 2s - loss: 0.1817 - acc: 0.943 - ETA: 2s - loss: 0.1812 - acc: 0.943 - ETA: 2s - loss: 0.1822 - acc: 0.943 - ETA: 2s - loss: 0.1836 - acc: 0.943 - ETA: 2s - loss: 0.1833 - acc: 0.943 - ETA: 1s - loss: 0.1865 - acc: 0.943 - ETA: 1s - loss: 0.1848 - acc: 0.944 - ETA: 1s - loss: 0.1839 - acc: 0.944 - ETA: 1s - loss: 0.1857 - acc: 0.944 - ETA: 1s - loss: 0.1845 - acc: 0.944 - ETA: 1s - loss: 0.1838 - acc: 0.944 - ETA: 1s - loss: 0.1824 - acc: 0.944 - ETA: 1s - loss: 0.1814 - acc: 0.945 - ETA: 1s - loss: 0.1807 - acc: 0.946 - ETA: 1s - loss: 0.1817 - acc: 0.945 - ETA: 1s - loss: 0.1811 - acc: 0.945 - ETA: 1s - loss: 0.1804 - acc: 0.945 - ETA: 0s - loss: 0.1793 - acc: 0.945 - ETA: 0s - loss: 0.1795 - acc: 0.945 - ETA: 0s - loss: 0.1797 - acc: 0.945 - ETA: 0s - loss: 0.1787 - acc: 0.945 - ETA: 0s - loss: 0.1782 - acc: 0.945 - ETA: 0s - loss: 0.1781 - acc: 0.945 - ETA: 0s - loss: 0.1773 - acc: 0.945 - ETA: 0s - loss: 0.1762 - acc: 0.946 - ETA: 0s - loss: 0.1751 - acc: 0.946 - ETA: 0s - loss: 0.1744 - acc: 0.947 - ETA: 0s - loss: 0.1733 - acc: 0.947 - ETA: 0s - loss: 0.1730 - acc: 0.947 - ETA: 0s - loss: 0.1724 - acc: 0.947 - 8s 3ms/step - loss: 0.1727 - acc: 0.9476 - val_loss: 0.2436 - val_acc: 0.9164\n",
      "Epoch 6/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0241 - acc: 1.000 - ETA: 7s - loss: 0.0603 - acc: 0.979 - ETA: 7s - loss: 0.1008 - acc: 0.975 - ETA: 7s - loss: 0.0890 - acc: 0.982 - ETA: 7s - loss: 0.0964 - acc: 0.986 - ETA: 6s - loss: 0.1368 - acc: 0.977 - ETA: 6s - loss: 0.1282 - acc: 0.976 - ETA: 7s - loss: 0.1185 - acc: 0.979 - ETA: 7s - loss: 0.1139 - acc: 0.980 - ETA: 7s - loss: 0.1146 - acc: 0.977 - ETA: 7s - loss: 0.1143 - acc: 0.977 - ETA: 7s - loss: 0.1098 - acc: 0.979 - ETA: 7s - loss: 0.1113 - acc: 0.978 - ETA: 7s - loss: 0.1098 - acc: 0.977 - ETA: 7s - loss: 0.1153 - acc: 0.976 - ETA: 7s - loss: 0.1214 - acc: 0.969 - ETA: 6s - loss: 0.1207 - acc: 0.969 - ETA: 6s - loss: 0.1494 - acc: 0.958 - ETA: 6s - loss: 0.1448 - acc: 0.960 - ETA: 6s - loss: 0.1487 - acc: 0.957 - ETA: 6s - loss: 0.1495 - acc: 0.957 - ETA: 6s - loss: 0.1474 - acc: 0.957 - ETA: 6s - loss: 0.1448 - acc: 0.958 - ETA: 6s - loss: 0.1406 - acc: 0.960 - ETA: 6s - loss: 0.1431 - acc: 0.958 - ETA: 6s - loss: 0.1407 - acc: 0.959 - ETA: 6s - loss: 0.1395 - acc: 0.960 - ETA: 6s - loss: 0.1373 - acc: 0.960 - ETA: 6s - loss: 0.1346 - acc: 0.961 - ETA: 6s - loss: 0.1390 - acc: 0.960 - ETA: 6s - loss: 0.1395 - acc: 0.959 - ETA: 6s - loss: 0.1411 - acc: 0.958 - ETA: 6s - loss: 0.1401 - acc: 0.958 - ETA: 6s - loss: 0.1382 - acc: 0.959 - ETA: 6s - loss: 0.1362 - acc: 0.960 - ETA: 6s - loss: 0.1342 - acc: 0.961 - ETA: 5s - loss: 0.1337 - acc: 0.960 - ETA: 5s - loss: 0.1319 - acc: 0.961 - ETA: 5s - loss: 0.1328 - acc: 0.961 - ETA: 5s - loss: 0.1416 - acc: 0.959 - ETA: 5s - loss: 0.1401 - acc: 0.959 - ETA: 5s - loss: 0.1408 - acc: 0.960 - ETA: 5s - loss: 0.1396 - acc: 0.960 - ETA: 5s - loss: 0.1392 - acc: 0.960 - ETA: 5s - loss: 0.1406 - acc: 0.959 - ETA: 5s - loss: 0.1390 - acc: 0.960 - ETA: 4s - loss: 0.1378 - acc: 0.961 - ETA: 4s - loss: 0.1362 - acc: 0.962 - ETA: 4s - loss: 0.1350 - acc: 0.962 - ETA: 4s - loss: 0.1339 - acc: 0.962 - ETA: 4s - loss: 0.1333 - acc: 0.962 - ETA: 4s - loss: 0.1324 - acc: 0.962 - ETA: 4s - loss: 0.1315 - acc: 0.962 - ETA: 4s - loss: 0.1349 - acc: 0.960 - ETA: 4s - loss: 0.1343 - acc: 0.961 - ETA: 4s - loss: 0.1333 - acc: 0.961 - ETA: 4s - loss: 0.1362 - acc: 0.961 - ETA: 4s - loss: 0.1347 - acc: 0.962 - ETA: 3s - loss: 0.1347 - acc: 0.962 - ETA: 3s - loss: 0.1368 - acc: 0.960 - ETA: 3s - loss: 0.1359 - acc: 0.961 - ETA: 3s - loss: 0.1345 - acc: 0.961 - ETA: 3s - loss: 0.1368 - acc: 0.960 - ETA: 3s - loss: 0.1362 - acc: 0.960 - ETA: 3s - loss: 0.1385 - acc: 0.961 - ETA: 3s - loss: 0.1379 - acc: 0.960 - ETA: 3s - loss: 0.1375 - acc: 0.960 - ETA: 3s - loss: 0.1376 - acc: 0.960 - ETA: 3s - loss: 0.1371 - acc: 0.960 - ETA: 3s - loss: 0.1364 - acc: 0.960 - ETA: 3s - loss: 0.1367 - acc: 0.960 - ETA: 2s - loss: 0.1356 - acc: 0.960 - ETA: 2s - loss: 0.1350 - acc: 0.961 - ETA: 2s - loss: 0.1348 - acc: 0.961 - ETA: 2s - loss: 0.1337 - acc: 0.961 - ETA: 2s - loss: 0.1325 - acc: 0.962 - ETA: 2s - loss: 0.1316 - acc: 0.962 - ETA: 2s - loss: 0.1325 - acc: 0.962 - ETA: 2s - loss: 0.1337 - acc: 0.961 - ETA: 2s - loss: 0.1323 - acc: 0.962 - ETA: 2s - loss: 0.1314 - acc: 0.962 - ETA: 2s - loss: 0.1312 - acc: 0.962 - ETA: 2s - loss: 0.1311 - acc: 0.962 - ETA: 2s - loss: 0.1306 - acc: 0.963 - ETA: 1s - loss: 0.1302 - acc: 0.963 - ETA: 1s - loss: 0.1293 - acc: 0.963 - ETA: 1s - loss: 0.1289 - acc: 0.963 - ETA: 1s - loss: 0.1297 - acc: 0.963 - ETA: 1s - loss: 0.1307 - acc: 0.962 - ETA: 1s - loss: 0.1308 - acc: 0.962 - ETA: 1s - loss: 0.1301 - acc: 0.963 - ETA: 1s - loss: 0.1291 - acc: 0.963 - ETA: 1s - loss: 0.1304 - acc: 0.962 - ETA: 1s - loss: 0.1315 - acc: 0.961 - ETA: 1s - loss: 0.1308 - acc: 0.962 - ETA: 1s - loss: 0.1313 - acc: 0.961 - ETA: 1s - loss: 0.1305 - acc: 0.961 - ETA: 0s - loss: 0.1319 - acc: 0.960 - ETA: 0s - loss: 0.1321 - acc: 0.960 - ETA: 0s - loss: 0.1317 - acc: 0.961 - ETA: 0s - loss: 0.1315 - acc: 0.960 - ETA: 0s - loss: 0.1326 - acc: 0.960 - ETA: 0s - loss: 0.1331 - acc: 0.960 - ETA: 0s - loss: 0.1338 - acc: 0.959 - ETA: 0s - loss: 0.1332 - acc: 0.959 - ETA: 0s - loss: 0.1328 - acc: 0.959 - ETA: 0s - loss: 0.1321 - acc: 0.960 - ETA: 0s - loss: 0.1315 - acc: 0.960 - ETA: 0s - loss: 0.1317 - acc: 0.960 - 8s 3ms/step - loss: 0.1312 - acc: 0.9604 - val_loss: 0.1507 - val_acc: 0.9668\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.1086 - acc: 0.937 - ETA: 7s - loss: 0.0725 - acc: 0.979 - ETA: 7s - loss: 0.0706 - acc: 0.987 - ETA: 7s - loss: 0.0649 - acc: 0.991 - ETA: 7s - loss: 0.0913 - acc: 0.984 - ETA: 7s - loss: 0.0965 - acc: 0.981 - ETA: 8s - loss: 0.0894 - acc: 0.983 - ETA: 8s - loss: 0.0840 - acc: 0.985 - ETA: 7s - loss: 0.0937 - acc: 0.979 - ETA: 7s - loss: 0.0912 - acc: 0.981 - ETA: 7s - loss: 0.0961 - acc: 0.980 - ETA: 7s - loss: 0.0908 - acc: 0.982 - ETA: 7s - loss: 0.0901 - acc: 0.981 - ETA: 7s - loss: 0.0986 - acc: 0.975 - ETA: 7s - loss: 0.0994 - acc: 0.974 - ETA: 7s - loss: 0.0972 - acc: 0.976 - ETA: 7s - loss: 0.0942 - acc: 0.977 - ETA: 7s - loss: 0.0920 - acc: 0.978 - ETA: 7s - loss: 0.0901 - acc: 0.979 - ETA: 7s - loss: 0.0889 - acc: 0.979 - ETA: 7s - loss: 0.0925 - acc: 0.978 - ETA: 7s - loss: 0.0906 - acc: 0.979 - ETA: 7s - loss: 0.0872 - acc: 0.980 - ETA: 6s - loss: 0.0896 - acc: 0.977 - ETA: 6s - loss: 0.0867 - acc: 0.978 - ETA: 6s - loss: 0.0892 - acc: 0.976 - ETA: 6s - loss: 0.0900 - acc: 0.976 - ETA: 6s - loss: 0.0884 - acc: 0.977 - ETA: 6s - loss: 0.0866 - acc: 0.977 - ETA: 6s - loss: 0.0857 - acc: 0.977 - ETA: 6s - loss: 0.0849 - acc: 0.977 - ETA: 6s - loss: 0.0833 - acc: 0.978 - ETA: 6s - loss: 0.0877 - acc: 0.975 - ETA: 5s - loss: 0.0860 - acc: 0.976 - ETA: 5s - loss: 0.0865 - acc: 0.976 - ETA: 5s - loss: 0.0885 - acc: 0.976 - ETA: 5s - loss: 0.0878 - acc: 0.975 - ETA: 5s - loss: 0.0866 - acc: 0.976 - ETA: 5s - loss: 0.0896 - acc: 0.975 - ETA: 5s - loss: 0.0891 - acc: 0.976 - ETA: 5s - loss: 0.0895 - acc: 0.975 - ETA: 5s - loss: 0.0905 - acc: 0.975 - ETA: 5s - loss: 0.0964 - acc: 0.973 - ETA: 5s - loss: 0.0949 - acc: 0.974 - ETA: 5s - loss: 0.0971 - acc: 0.972 - ETA: 5s - loss: 0.0972 - acc: 0.972 - ETA: 5s - loss: 0.0962 - acc: 0.972 - ETA: 5s - loss: 0.0950 - acc: 0.972 - ETA: 4s - loss: 0.0944 - acc: 0.972 - ETA: 4s - loss: 0.0945 - acc: 0.972 - ETA: 4s - loss: 0.0934 - acc: 0.973 - ETA: 4s - loss: 0.0925 - acc: 0.973 - ETA: 4s - loss: 0.0929 - acc: 0.973 - ETA: 4s - loss: 0.0913 - acc: 0.973 - ETA: 4s - loss: 0.0920 - acc: 0.972 - ETA: 4s - loss: 0.0910 - acc: 0.973 - ETA: 4s - loss: 0.0907 - acc: 0.973 - ETA: 4s - loss: 0.0899 - acc: 0.974 - ETA: 4s - loss: 0.0898 - acc: 0.974 - ETA: 4s - loss: 0.0901 - acc: 0.973 - ETA: 4s - loss: 0.0938 - acc: 0.972 - ETA: 3s - loss: 0.0935 - acc: 0.972 - ETA: 3s - loss: 0.0938 - acc: 0.972 - ETA: 3s - loss: 0.0933 - acc: 0.972 - ETA: 3s - loss: 0.0926 - acc: 0.972 - ETA: 3s - loss: 0.0914 - acc: 0.973 - ETA: 3s - loss: 0.0916 - acc: 0.972 - ETA: 3s - loss: 0.0913 - acc: 0.972 - ETA: 3s - loss: 0.0904 - acc: 0.972 - ETA: 3s - loss: 0.0899 - acc: 0.973 - ETA: 3s - loss: 0.0906 - acc: 0.972 - ETA: 3s - loss: 0.0946 - acc: 0.971 - ETA: 3s - loss: 0.0951 - acc: 0.971 - ETA: 2s - loss: 0.0946 - acc: 0.971 - ETA: 2s - loss: 0.0937 - acc: 0.971 - ETA: 2s - loss: 0.0944 - acc: 0.971 - ETA: 2s - loss: 0.0940 - acc: 0.971 - ETA: 2s - loss: 0.0935 - acc: 0.971 - ETA: 2s - loss: 0.0933 - acc: 0.971 - ETA: 2s - loss: 0.0928 - acc: 0.971 - ETA: 2s - loss: 0.0927 - acc: 0.971 - ETA: 2s - loss: 0.0944 - acc: 0.971 - ETA: 2s - loss: 0.0958 - acc: 0.971 - ETA: 2s - loss: 0.0971 - acc: 0.970 - ETA: 2s - loss: 0.0985 - acc: 0.969 - ETA: 1s - loss: 0.0978 - acc: 0.969 - ETA: 1s - loss: 0.0972 - acc: 0.970 - ETA: 1s - loss: 0.0965 - acc: 0.970 - ETA: 1s - loss: 0.0955 - acc: 0.970 - ETA: 1s - loss: 0.0948 - acc: 0.971 - ETA: 1s - loss: 0.0940 - acc: 0.971 - ETA: 1s - loss: 0.0953 - acc: 0.971 - ETA: 1s - loss: 0.0951 - acc: 0.971 - ETA: 1s - loss: 0.0952 - acc: 0.971 - ETA: 1s - loss: 0.0947 - acc: 0.971 - ETA: 1s - loss: 0.0940 - acc: 0.971 - ETA: 1s - loss: 0.0939 - acc: 0.972 - ETA: 0s - loss: 0.0932 - acc: 0.972 - ETA: 0s - loss: 0.0929 - acc: 0.972 - ETA: 0s - loss: 0.0922 - acc: 0.973 - ETA: 0s - loss: 0.0920 - acc: 0.973 - ETA: 0s - loss: 0.0921 - acc: 0.973 - ETA: 0s - loss: 0.0915 - acc: 0.973 - ETA: 0s - loss: 0.0918 - acc: 0.973 - ETA: 0s - loss: 0.0920 - acc: 0.973 - ETA: 0s - loss: 0.0918 - acc: 0.973 - ETA: 0s - loss: 0.0912 - acc: 0.973 - ETA: 0s - loss: 0.0907 - acc: 0.973 - ETA: 0s - loss: 0.0913 - acc: 0.973 - ETA: 0s - loss: 0.0922 - acc: 0.973 - 9s 3ms/step - loss: 0.0926 - acc: 0.9723 - val_loss: 0.1359 - val_acc: 0.9575\n",
      "Epoch 8/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.2137 - acc: 0.875 - ETA: 7s - loss: 0.1828 - acc: 0.937 - ETA: 7s - loss: 0.1323 - acc: 0.950 - ETA: 7s - loss: 0.1004 - acc: 0.964 - ETA: 7s - loss: 0.0874 - acc: 0.972 - ETA: 7s - loss: 0.0871 - acc: 0.971 - ETA: 7s - loss: 0.0846 - acc: 0.971 - ETA: 7s - loss: 0.0768 - acc: 0.975 - ETA: 7s - loss: 0.0732 - acc: 0.977 - ETA: 6s - loss: 0.0776 - acc: 0.977 - ETA: 6s - loss: 0.0748 - acc: 0.979 - ETA: 6s - loss: 0.0752 - acc: 0.978 - ETA: 6s - loss: 0.0809 - acc: 0.977 - ETA: 6s - loss: 0.0774 - acc: 0.979 - ETA: 6s - loss: 0.0839 - acc: 0.978 - ETA: 6s - loss: 0.0828 - acc: 0.979 - ETA: 6s - loss: 0.0835 - acc: 0.979 - ETA: 6s - loss: 0.0823 - acc: 0.980 - ETA: 6s - loss: 0.0851 - acc: 0.976 - ETA: 6s - loss: 0.0869 - acc: 0.976 - ETA: 5s - loss: 0.0847 - acc: 0.977 - ETA: 5s - loss: 0.0881 - acc: 0.976 - ETA: 5s - loss: 0.0886 - acc: 0.976 - ETA: 5s - loss: 0.0866 - acc: 0.977 - ETA: 5s - loss: 0.0856 - acc: 0.977 - ETA: 5s - loss: 0.0830 - acc: 0.977 - ETA: 5s - loss: 0.0823 - acc: 0.976 - ETA: 5s - loss: 0.0809 - acc: 0.977 - ETA: 5s - loss: 0.0789 - acc: 0.978 - ETA: 5s - loss: 0.0773 - acc: 0.978 - ETA: 5s - loss: 0.0780 - acc: 0.978 - ETA: 5s - loss: 0.0815 - acc: 0.976 - ETA: 5s - loss: 0.0805 - acc: 0.976 - ETA: 5s - loss: 0.0801 - acc: 0.975 - ETA: 4s - loss: 0.0785 - acc: 0.976 - ETA: 4s - loss: 0.0779 - acc: 0.976 - ETA: 4s - loss: 0.0846 - acc: 0.975 - ETA: 4s - loss: 0.0840 - acc: 0.975 - ETA: 4s - loss: 0.0831 - acc: 0.975 - ETA: 4s - loss: 0.0824 - acc: 0.975 - ETA: 4s - loss: 0.0850 - acc: 0.974 - ETA: 4s - loss: 0.0836 - acc: 0.975 - ETA: 4s - loss: 0.0829 - acc: 0.975 - ETA: 4s - loss: 0.0866 - acc: 0.972 - ETA: 4s - loss: 0.0867 - acc: 0.972 - ETA: 4s - loss: 0.0894 - acc: 0.971 - ETA: 4s - loss: 0.0880 - acc: 0.972 - ETA: 4s - loss: 0.0865 - acc: 0.973 - ETA: 3s - loss: 0.0859 - acc: 0.973 - ETA: 3s - loss: 0.0886 - acc: 0.972 - ETA: 3s - loss: 0.0876 - acc: 0.973 - ETA: 3s - loss: 0.0867 - acc: 0.973 - ETA: 3s - loss: 0.0859 - acc: 0.973 - ETA: 3s - loss: 0.0880 - acc: 0.973 - ETA: 3s - loss: 0.0868 - acc: 0.974 - ETA: 3s - loss: 0.0860 - acc: 0.974 - ETA: 3s - loss: 0.0850 - acc: 0.975 - ETA: 3s - loss: 0.0845 - acc: 0.975 - ETA: 3s - loss: 0.0845 - acc: 0.975 - ETA: 3s - loss: 0.0843 - acc: 0.975 - ETA: 3s - loss: 0.0878 - acc: 0.974 - ETA: 3s - loss: 0.0880 - acc: 0.974 - ETA: 2s - loss: 0.0875 - acc: 0.974 - ETA: 2s - loss: 0.0875 - acc: 0.974 - ETA: 2s - loss: 0.0880 - acc: 0.973 - ETA: 2s - loss: 0.0871 - acc: 0.973 - ETA: 2s - loss: 0.0861 - acc: 0.974 - ETA: 2s - loss: 0.0856 - acc: 0.974 - ETA: 2s - loss: 0.0847 - acc: 0.974 - ETA: 2s - loss: 0.0837 - acc: 0.975 - ETA: 2s - loss: 0.0844 - acc: 0.975 - ETA: 2s - loss: 0.0840 - acc: 0.975 - ETA: 2s - loss: 0.0837 - acc: 0.975 - ETA: 2s - loss: 0.0849 - acc: 0.974 - ETA: 2s - loss: 0.0843 - acc: 0.974 - ETA: 2s - loss: 0.0839 - acc: 0.974 - ETA: 1s - loss: 0.0830 - acc: 0.975 - ETA: 1s - loss: 0.0835 - acc: 0.975 - ETA: 1s - loss: 0.0864 - acc: 0.974 - ETA: 1s - loss: 0.0863 - acc: 0.973 - ETA: 1s - loss: 0.0858 - acc: 0.973 - ETA: 1s - loss: 0.0860 - acc: 0.973 - ETA: 1s - loss: 0.0852 - acc: 0.973 - ETA: 1s - loss: 0.0847 - acc: 0.973 - ETA: 1s - loss: 0.0844 - acc: 0.974 - ETA: 1s - loss: 0.0887 - acc: 0.973 - ETA: 1s - loss: 0.0882 - acc: 0.972 - ETA: 1s - loss: 0.0878 - acc: 0.973 - ETA: 1s - loss: 0.0873 - acc: 0.973 - ETA: 0s - loss: 0.0873 - acc: 0.973 - ETA: 0s - loss: 0.0871 - acc: 0.973 - ETA: 0s - loss: 0.0865 - acc: 0.973 - ETA: 0s - loss: 0.0860 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.973 - ETA: 0s - loss: 0.0874 - acc: 0.973 - ETA: 0s - loss: 0.0870 - acc: 0.974 - ETA: 0s - loss: 0.0864 - acc: 0.974 - ETA: 0s - loss: 0.0858 - acc: 0.974 - ETA: 0s - loss: 0.0862 - acc: 0.974 - ETA: 0s - loss: 0.0863 - acc: 0.974 - ETA: 0s - loss: 0.0860 - acc: 0.974 - ETA: 0s - loss: 0.0864 - acc: 0.974 - ETA: 0s - loss: 0.0857 - acc: 0.974 - 8s 2ms/step - loss: 0.0879 - acc: 0.9741 - val_loss: 0.1170 - val_acc: 0.9719\n",
      "Epoch 9/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0570 - acc: 1.000 - ETA: 7s - loss: 0.1232 - acc: 0.937 - ETA: 7s - loss: 0.0961 - acc: 0.962 - ETA: 7s - loss: 0.0837 - acc: 0.973 - ETA: 6s - loss: 0.0680 - acc: 0.979 - ETA: 6s - loss: 0.0642 - acc: 0.983 - ETA: 6s - loss: 0.0576 - acc: 0.985 - ETA: 6s - loss: 0.0536 - acc: 0.987 - ETA: 6s - loss: 0.0586 - acc: 0.985 - ETA: 6s - loss: 0.0558 - acc: 0.986 - ETA: 6s - loss: 0.0540 - acc: 0.988 - ETA: 6s - loss: 0.0580 - acc: 0.983 - ETA: 6s - loss: 0.0546 - acc: 0.985 - ETA: 6s - loss: 0.0607 - acc: 0.983 - ETA: 6s - loss: 0.0593 - acc: 0.984 - ETA: 6s - loss: 0.0611 - acc: 0.983 - ETA: 6s - loss: 0.0621 - acc: 0.983 - ETA: 6s - loss: 0.0612 - acc: 0.982 - ETA: 6s - loss: 0.0615 - acc: 0.981 - ETA: 5s - loss: 0.0604 - acc: 0.982 - ETA: 5s - loss: 0.0601 - acc: 0.983 - ETA: 5s - loss: 0.0579 - acc: 0.984 - ETA: 5s - loss: 0.0623 - acc: 0.980 - ETA: 5s - loss: 0.0611 - acc: 0.980 - ETA: 5s - loss: 0.0629 - acc: 0.979 - ETA: 5s - loss: 0.0643 - acc: 0.977 - ETA: 5s - loss: 0.0670 - acc: 0.977 - ETA: 5s - loss: 0.0659 - acc: 0.978 - ETA: 5s - loss: 0.0641 - acc: 0.979 - ETA: 5s - loss: 0.0651 - acc: 0.978 - ETA: 5s - loss: 0.0663 - acc: 0.977 - ETA: 5s - loss: 0.0664 - acc: 0.977 - ETA: 5s - loss: 0.0674 - acc: 0.976 - ETA: 4s - loss: 0.0670 - acc: 0.976 - ETA: 4s - loss: 0.0704 - acc: 0.975 - ETA: 4s - loss: 0.0694 - acc: 0.976 - ETA: 4s - loss: 0.0680 - acc: 0.976 - ETA: 4s - loss: 0.0693 - acc: 0.976 - ETA: 4s - loss: 0.0687 - acc: 0.977 - ETA: 4s - loss: 0.0674 - acc: 0.977 - ETA: 4s - loss: 0.0662 - acc: 0.978 - ETA: 4s - loss: 0.0665 - acc: 0.978 - ETA: 4s - loss: 0.0656 - acc: 0.978 - ETA: 4s - loss: 0.0649 - acc: 0.979 - ETA: 4s - loss: 0.0673 - acc: 0.977 - ETA: 4s - loss: 0.0679 - acc: 0.977 - ETA: 4s - loss: 0.0695 - acc: 0.976 - ETA: 3s - loss: 0.0695 - acc: 0.976 - ETA: 3s - loss: 0.0698 - acc: 0.976 - ETA: 3s - loss: 0.0707 - acc: 0.976 - ETA: 3s - loss: 0.0699 - acc: 0.976 - ETA: 3s - loss: 0.0701 - acc: 0.976 - ETA: 3s - loss: 0.0698 - acc: 0.976 - ETA: 3s - loss: 0.0710 - acc: 0.976 - ETA: 3s - loss: 0.0721 - acc: 0.975 - ETA: 3s - loss: 0.0716 - acc: 0.975 - ETA: 3s - loss: 0.0715 - acc: 0.975 - ETA: 3s - loss: 0.0755 - acc: 0.974 - ETA: 3s - loss: 0.0769 - acc: 0.974 - ETA: 3s - loss: 0.0762 - acc: 0.974 - ETA: 3s - loss: 0.0774 - acc: 0.974 - ETA: 2s - loss: 0.0775 - acc: 0.974 - ETA: 2s - loss: 0.0785 - acc: 0.974 - ETA: 2s - loss: 0.0873 - acc: 0.972 - ETA: 2s - loss: 0.0891 - acc: 0.971 - ETA: 2s - loss: 0.0902 - acc: 0.970 - ETA: 2s - loss: 0.0891 - acc: 0.971 - ETA: 2s - loss: 0.0897 - acc: 0.970 - ETA: 2s - loss: 0.0892 - acc: 0.971 - ETA: 2s - loss: 0.0906 - acc: 0.970 - ETA: 2s - loss: 0.0901 - acc: 0.970 - ETA: 2s - loss: 0.0901 - acc: 0.970 - ETA: 2s - loss: 0.0896 - acc: 0.970 - ETA: 2s - loss: 0.0897 - acc: 0.970 - ETA: 2s - loss: 0.0911 - acc: 0.970 - ETA: 2s - loss: 0.0902 - acc: 0.970 - ETA: 2s - loss: 0.0897 - acc: 0.970 - ETA: 1s - loss: 0.0889 - acc: 0.971 - ETA: 1s - loss: 0.0879 - acc: 0.971 - ETA: 1s - loss: 0.0872 - acc: 0.972 - ETA: 1s - loss: 0.0864 - acc: 0.972 - ETA: 1s - loss: 0.0858 - acc: 0.972 - ETA: 1s - loss: 0.0852 - acc: 0.973 - ETA: 1s - loss: 0.0845 - acc: 0.973 - ETA: 1s - loss: 0.0845 - acc: 0.973 - ETA: 1s - loss: 0.0836 - acc: 0.973 - ETA: 1s - loss: 0.0841 - acc: 0.973 - ETA: 1s - loss: 0.0838 - acc: 0.973 - ETA: 1s - loss: 0.0830 - acc: 0.973 - ETA: 1s - loss: 0.0826 - acc: 0.973 - ETA: 0s - loss: 0.0818 - acc: 0.974 - ETA: 0s - loss: 0.0811 - acc: 0.974 - ETA: 0s - loss: 0.0806 - acc: 0.974 - ETA: 0s - loss: 0.0807 - acc: 0.974 - ETA: 0s - loss: 0.0810 - acc: 0.974 - ETA: 0s - loss: 0.0808 - acc: 0.974 - ETA: 0s - loss: 0.0806 - acc: 0.974 - ETA: 0s - loss: 0.0801 - acc: 0.974 - ETA: 0s - loss: 0.0794 - acc: 0.974 - ETA: 0s - loss: 0.0793 - acc: 0.974 - ETA: 0s - loss: 0.0787 - acc: 0.975 - ETA: 0s - loss: 0.0782 - acc: 0.975 - ETA: 0s - loss: 0.0803 - acc: 0.974 - ETA: 0s - loss: 0.0798 - acc: 0.974 - 8s 2ms/step - loss: 0.0830 - acc: 0.9744 - val_loss: 0.1384 - val_acc: 0.9611\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.2065 - acc: 0.937 - ETA: 7s - loss: 0.1873 - acc: 0.937 - ETA: 7s - loss: 0.1197 - acc: 0.962 - ETA: 7s - loss: 0.1326 - acc: 0.955 - ETA: 7s - loss: 0.1148 - acc: 0.965 - ETA: 6s - loss: 0.2177 - acc: 0.943 - ETA: 6s - loss: 0.1874 - acc: 0.951 - ETA: 6s - loss: 0.1703 - acc: 0.958 - ETA: 6s - loss: 0.1637 - acc: 0.952 - ETA: 6s - loss: 0.1502 - acc: 0.957 - ETA: 6s - loss: 0.1369 - acc: 0.961 - ETA: 6s - loss: 0.1277 - acc: 0.964 - ETA: 6s - loss: 0.1195 - acc: 0.967 - ETA: 6s - loss: 0.1115 - acc: 0.969 - ETA: 6s - loss: 0.1066 - acc: 0.972 - ETA: 6s - loss: 0.1020 - acc: 0.973 - ETA: 6s - loss: 0.1094 - acc: 0.971 - ETA: 6s - loss: 0.1184 - acc: 0.969 - ETA: 6s - loss: 0.1127 - acc: 0.971 - ETA: 5s - loss: 0.1100 - acc: 0.972 - ETA: 5s - loss: 0.1107 - acc: 0.972 - ETA: 5s - loss: 0.1064 - acc: 0.973 - ETA: 5s - loss: 0.1022 - acc: 0.975 - ETA: 5s - loss: 0.0993 - acc: 0.976 - ETA: 5s - loss: 0.1169 - acc: 0.971 - ETA: 5s - loss: 0.1138 - acc: 0.973 - ETA: 5s - loss: 0.1165 - acc: 0.972 - ETA: 5s - loss: 0.1144 - acc: 0.973 - ETA: 5s - loss: 0.1139 - acc: 0.972 - ETA: 5s - loss: 0.1105 - acc: 0.973 - ETA: 5s - loss: 0.1100 - acc: 0.972 - ETA: 5s - loss: 0.1067 - acc: 0.973 - ETA: 5s - loss: 0.1057 - acc: 0.973 - ETA: 5s - loss: 0.1027 - acc: 0.974 - ETA: 5s - loss: 0.1027 - acc: 0.974 - ETA: 5s - loss: 0.1015 - acc: 0.974 - ETA: 4s - loss: 0.1030 - acc: 0.972 - ETA: 4s - loss: 0.1011 - acc: 0.973 - ETA: 4s - loss: 0.1011 - acc: 0.972 - ETA: 4s - loss: 0.0991 - acc: 0.973 - ETA: 4s - loss: 0.1029 - acc: 0.972 - ETA: 4s - loss: 0.1012 - acc: 0.973 - ETA: 4s - loss: 0.1003 - acc: 0.973 - ETA: 4s - loss: 0.1007 - acc: 0.973 - ETA: 4s - loss: 0.0991 - acc: 0.973 - ETA: 4s - loss: 0.0977 - acc: 0.974 - ETA: 4s - loss: 0.0963 - acc: 0.974 - ETA: 4s - loss: 0.0950 - acc: 0.975 - ETA: 4s - loss: 0.0934 - acc: 0.975 - ETA: 3s - loss: 0.0931 - acc: 0.975 - ETA: 3s - loss: 0.0920 - acc: 0.975 - ETA: 3s - loss: 0.0906 - acc: 0.976 - ETA: 3s - loss: 0.0892 - acc: 0.976 - ETA: 3s - loss: 0.0880 - acc: 0.977 - ETA: 3s - loss: 0.0870 - acc: 0.977 - ETA: 3s - loss: 0.0859 - acc: 0.977 - ETA: 3s - loss: 0.0848 - acc: 0.978 - ETA: 3s - loss: 0.0835 - acc: 0.978 - ETA: 3s - loss: 0.0824 - acc: 0.979 - ETA: 3s - loss: 0.0812 - acc: 0.979 - ETA: 3s - loss: 0.0801 - acc: 0.979 - ETA: 3s - loss: 0.0804 - acc: 0.979 - ETA: 2s - loss: 0.0794 - acc: 0.979 - ETA: 2s - loss: 0.0783 - acc: 0.980 - ETA: 2s - loss: 0.0775 - acc: 0.980 - ETA: 2s - loss: 0.0811 - acc: 0.979 - ETA: 2s - loss: 0.0801 - acc: 0.979 - ETA: 2s - loss: 0.0791 - acc: 0.979 - ETA: 2s - loss: 0.0782 - acc: 0.980 - ETA: 2s - loss: 0.0774 - acc: 0.980 - ETA: 2s - loss: 0.0770 - acc: 0.980 - ETA: 2s - loss: 0.0760 - acc: 0.980 - ETA: 2s - loss: 0.0754 - acc: 0.980 - ETA: 2s - loss: 0.0752 - acc: 0.981 - ETA: 2s - loss: 0.0746 - acc: 0.981 - ETA: 2s - loss: 0.0741 - acc: 0.980 - ETA: 1s - loss: 0.0746 - acc: 0.980 - ETA: 1s - loss: 0.0753 - acc: 0.980 - ETA: 1s - loss: 0.0745 - acc: 0.980 - ETA: 1s - loss: 0.0745 - acc: 0.980 - ETA: 1s - loss: 0.0741 - acc: 0.980 - ETA: 1s - loss: 0.0733 - acc: 0.981 - ETA: 1s - loss: 0.0725 - acc: 0.981 - ETA: 1s - loss: 0.0718 - acc: 0.981 - ETA: 1s - loss: 0.0711 - acc: 0.981 - ETA: 1s - loss: 0.0704 - acc: 0.982 - ETA: 1s - loss: 0.0702 - acc: 0.982 - ETA: 1s - loss: 0.0696 - acc: 0.982 - ETA: 1s - loss: 0.0692 - acc: 0.982 - ETA: 0s - loss: 0.0700 - acc: 0.982 - ETA: 0s - loss: 0.0695 - acc: 0.982 - ETA: 0s - loss: 0.0690 - acc: 0.982 - ETA: 0s - loss: 0.0688 - acc: 0.982 - ETA: 0s - loss: 0.0689 - acc: 0.982 - ETA: 0s - loss: 0.0687 - acc: 0.983 - ETA: 0s - loss: 0.0690 - acc: 0.982 - ETA: 0s - loss: 0.0699 - acc: 0.982 - ETA: 0s - loss: 0.0695 - acc: 0.982 - ETA: 0s - loss: 0.0692 - acc: 0.982 - ETA: 0s - loss: 0.0691 - acc: 0.982 - ETA: 0s - loss: 0.0688 - acc: 0.982 - ETA: 0s - loss: 0.0684 - acc: 0.983 - ETA: 0s - loss: 0.0686 - acc: 0.982 - 8s 2ms/step - loss: 0.0709 - acc: 0.9823 - val_loss: 0.1916 - val_acc: 0.9322\n",
      "Epoch 11/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0208 - acc: 1.000 - ETA: 7s - loss: 0.0173 - acc: 1.000 - ETA: 7s - loss: 0.0395 - acc: 0.987 - ETA: 7s - loss: 0.0538 - acc: 0.982 - ETA: 7s - loss: 0.1183 - acc: 0.965 - ETA: 6s - loss: 0.1404 - acc: 0.960 - ETA: 6s - loss: 0.1877 - acc: 0.947 - ETA: 6s - loss: 0.1896 - acc: 0.945 - ETA: 6s - loss: 0.1787 - acc: 0.944 - ETA: 6s - loss: 0.1862 - acc: 0.934 - ETA: 6s - loss: 0.1919 - acc: 0.931 - ETA: 6s - loss: 0.2038 - acc: 0.923 - ETA: 6s - loss: 0.1938 - acc: 0.930 - ETA: 6s - loss: 0.1932 - acc: 0.932 - ETA: 6s - loss: 0.1824 - acc: 0.937 - ETA: 6s - loss: 0.1837 - acc: 0.935 - ETA: 6s - loss: 0.1752 - acc: 0.937 - ETA: 6s - loss: 0.1662 - acc: 0.941 - ETA: 6s - loss: 0.1658 - acc: 0.939 - ETA: 5s - loss: 0.1588 - acc: 0.942 - ETA: 5s - loss: 0.1544 - acc: 0.945 - ETA: 5s - loss: 0.1506 - acc: 0.946 - ETA: 5s - loss: 0.1452 - acc: 0.948 - ETA: 5s - loss: 0.1487 - acc: 0.948 - ETA: 5s - loss: 0.1449 - acc: 0.949 - ETA: 5s - loss: 0.1443 - acc: 0.947 - ETA: 5s - loss: 0.1411 - acc: 0.948 - ETA: 5s - loss: 0.1419 - acc: 0.946 - ETA: 5s - loss: 0.1384 - acc: 0.947 - ETA: 5s - loss: 0.1395 - acc: 0.948 - ETA: 5s - loss: 0.1381 - acc: 0.947 - ETA: 5s - loss: 0.1342 - acc: 0.949 - ETA: 5s - loss: 0.1327 - acc: 0.949 - ETA: 4s - loss: 0.1293 - acc: 0.950 - ETA: 4s - loss: 0.1257 - acc: 0.952 - ETA: 4s - loss: 0.1232 - acc: 0.953 - ETA: 4s - loss: 0.1205 - acc: 0.954 - ETA: 4s - loss: 0.1180 - acc: 0.955 - ETA: 4s - loss: 0.1175 - acc: 0.956 - ETA: 4s - loss: 0.1150 - acc: 0.957 - ETA: 4s - loss: 0.1127 - acc: 0.958 - ETA: 4s - loss: 0.1105 - acc: 0.959 - ETA: 4s - loss: 0.1082 - acc: 0.960 - ETA: 4s - loss: 0.1064 - acc: 0.961 - ETA: 4s - loss: 0.1049 - acc: 0.961 - ETA: 4s - loss: 0.1030 - acc: 0.962 - ETA: 4s - loss: 0.1034 - acc: 0.962 - ETA: 4s - loss: 0.1018 - acc: 0.963 - ETA: 3s - loss: 0.1002 - acc: 0.963 - ETA: 3s - loss: 0.0987 - acc: 0.964 - ETA: 3s - loss: 0.0986 - acc: 0.964 - ETA: 3s - loss: 0.0979 - acc: 0.964 - ETA: 3s - loss: 0.0970 - acc: 0.964 - ETA: 3s - loss: 0.0962 - acc: 0.965 - ETA: 3s - loss: 0.0956 - acc: 0.965 - ETA: 3s - loss: 0.0941 - acc: 0.965 - ETA: 3s - loss: 0.0935 - acc: 0.966 - ETA: 3s - loss: 0.0935 - acc: 0.966 - ETA: 3s - loss: 0.0924 - acc: 0.966 - ETA: 3s - loss: 0.0916 - acc: 0.967 - ETA: 3s - loss: 0.0903 - acc: 0.968 - ETA: 2s - loss: 0.0898 - acc: 0.968 - ETA: 2s - loss: 0.0886 - acc: 0.969 - ETA: 2s - loss: 0.0891 - acc: 0.969 - ETA: 2s - loss: 0.0879 - acc: 0.969 - ETA: 2s - loss: 0.0866 - acc: 0.969 - ETA: 2s - loss: 0.0874 - acc: 0.969 - ETA: 2s - loss: 0.0864 - acc: 0.970 - ETA: 2s - loss: 0.0862 - acc: 0.970 - ETA: 2s - loss: 0.0853 - acc: 0.970 - ETA: 2s - loss: 0.0842 - acc: 0.971 - ETA: 2s - loss: 0.0943 - acc: 0.968 - ETA: 2s - loss: 0.0943 - acc: 0.968 - ETA: 2s - loss: 0.0996 - acc: 0.967 - ETA: 2s - loss: 0.0993 - acc: 0.968 - ETA: 2s - loss: 0.0990 - acc: 0.968 - ETA: 2s - loss: 0.0984 - acc: 0.968 - ETA: 2s - loss: 0.0980 - acc: 0.968 - ETA: 2s - loss: 0.0973 - acc: 0.969 - ETA: 1s - loss: 0.1007 - acc: 0.967 - ETA: 1s - loss: 0.0998 - acc: 0.968 - ETA: 1s - loss: 0.0995 - acc: 0.968 - ETA: 1s - loss: 0.0987 - acc: 0.968 - ETA: 1s - loss: 0.0993 - acc: 0.967 - ETA: 1s - loss: 0.0986 - acc: 0.967 - ETA: 1s - loss: 0.0979 - acc: 0.967 - ETA: 1s - loss: 0.0971 - acc: 0.968 - ETA: 1s - loss: 0.0963 - acc: 0.968 - ETA: 1s - loss: 0.0955 - acc: 0.968 - ETA: 1s - loss: 0.0949 - acc: 0.969 - ETA: 1s - loss: 0.0947 - acc: 0.969 - ETA: 1s - loss: 0.0942 - acc: 0.969 - ETA: 1s - loss: 0.0937 - acc: 0.970 - ETA: 1s - loss: 0.0936 - acc: 0.970 - ETA: 0s - loss: 0.0931 - acc: 0.970 - ETA: 0s - loss: 0.0928 - acc: 0.970 - ETA: 0s - loss: 0.0930 - acc: 0.970 - ETA: 0s - loss: 0.0921 - acc: 0.970 - ETA: 0s - loss: 0.0914 - acc: 0.970 - ETA: 0s - loss: 0.0915 - acc: 0.969 - ETA: 0s - loss: 0.0908 - acc: 0.970 - ETA: 0s - loss: 0.0900 - acc: 0.970 - ETA: 0s - loss: 0.0907 - acc: 0.970 - ETA: 0s - loss: 0.0900 - acc: 0.970 - ETA: 0s - loss: 0.0895 - acc: 0.970 - ETA: 0s - loss: 0.0892 - acc: 0.971 - ETA: 0s - loss: 0.0895 - acc: 0.971 - ETA: 0s - loss: 0.0894 - acc: 0.971 - ETA: 0s - loss: 0.0888 - acc: 0.971 - 8s 2ms/step - loss: 0.0892 - acc: 0.9711 - val_loss: 0.1163 - val_acc: 0.9625\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0152 - acc: 1.000 - ETA: 7s - loss: 0.0662 - acc: 0.979 - ETA: 7s - loss: 0.0454 - acc: 0.987 - ETA: 7s - loss: 0.0450 - acc: 0.991 - ETA: 7s - loss: 0.0475 - acc: 0.986 - ETA: 7s - loss: 0.0586 - acc: 0.977 - ETA: 7s - loss: 0.0542 - acc: 0.980 - ETA: 7s - loss: 0.0523 - acc: 0.979 - ETA: 7s - loss: 0.0491 - acc: 0.981 - ETA: 7s - loss: 0.0464 - acc: 0.983 - ETA: 7s - loss: 0.0451 - acc: 0.985 - ETA: 7s - loss: 0.0416 - acc: 0.986 - ETA: 7s - loss: 0.0439 - acc: 0.985 - ETA: 6s - loss: 0.0441 - acc: 0.986 - ETA: 6s - loss: 0.0431 - acc: 0.987 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0397 - acc: 0.988 - ETA: 6s - loss: 0.0381 - acc: 0.989 - ETA: 6s - loss: 0.0381 - acc: 0.989 - ETA: 6s - loss: 0.0442 - acc: 0.985 - ETA: 6s - loss: 0.0444 - acc: 0.984 - ETA: 6s - loss: 0.0428 - acc: 0.985 - ETA: 6s - loss: 0.0431 - acc: 0.986 - ETA: 6s - loss: 0.0416 - acc: 0.986 - ETA: 6s - loss: 0.0428 - acc: 0.986 - ETA: 5s - loss: 0.0432 - acc: 0.985 - ETA: 5s - loss: 0.0421 - acc: 0.985 - ETA: 5s - loss: 0.0424 - acc: 0.986 - ETA: 5s - loss: 0.0459 - acc: 0.985 - ETA: 5s - loss: 0.0447 - acc: 0.986 - ETA: 5s - loss: 0.0446 - acc: 0.986 - ETA: 5s - loss: 0.0461 - acc: 0.986 - ETA: 5s - loss: 0.0475 - acc: 0.985 - ETA: 5s - loss: 0.0483 - acc: 0.985 - ETA: 5s - loss: 0.0480 - acc: 0.985 - ETA: 5s - loss: 0.0469 - acc: 0.985 - ETA: 5s - loss: 0.0474 - acc: 0.985 - ETA: 4s - loss: 0.0465 - acc: 0.985 - ETA: 4s - loss: 0.0533 - acc: 0.982 - ETA: 4s - loss: 0.0540 - acc: 0.981 - ETA: 4s - loss: 0.0645 - acc: 0.979 - ETA: 4s - loss: 0.0635 - acc: 0.979 - ETA: 4s - loss: 0.0625 - acc: 0.980 - ETA: 4s - loss: 0.0624 - acc: 0.980 - ETA: 4s - loss: 0.0611 - acc: 0.981 - ETA: 4s - loss: 0.0603 - acc: 0.981 - ETA: 4s - loss: 0.0594 - acc: 0.981 - ETA: 4s - loss: 0.0630 - acc: 0.980 - ETA: 4s - loss: 0.0627 - acc: 0.981 - ETA: 4s - loss: 0.0618 - acc: 0.981 - ETA: 3s - loss: 0.0653 - acc: 0.980 - ETA: 3s - loss: 0.0641 - acc: 0.981 - ETA: 3s - loss: 0.0633 - acc: 0.981 - ETA: 3s - loss: 0.0624 - acc: 0.981 - ETA: 3s - loss: 0.0627 - acc: 0.981 - ETA: 3s - loss: 0.0619 - acc: 0.982 - ETA: 3s - loss: 0.0612 - acc: 0.982 - ETA: 3s - loss: 0.0614 - acc: 0.982 - ETA: 3s - loss: 0.0611 - acc: 0.982 - ETA: 3s - loss: 0.0608 - acc: 0.982 - ETA: 3s - loss: 0.0599 - acc: 0.983 - ETA: 3s - loss: 0.0600 - acc: 0.982 - ETA: 3s - loss: 0.0593 - acc: 0.983 - ETA: 2s - loss: 0.0588 - acc: 0.983 - ETA: 2s - loss: 0.0601 - acc: 0.982 - ETA: 2s - loss: 0.0597 - acc: 0.982 - ETA: 2s - loss: 0.0592 - acc: 0.982 - ETA: 2s - loss: 0.0594 - acc: 0.982 - ETA: 2s - loss: 0.0590 - acc: 0.982 - ETA: 2s - loss: 0.0586 - acc: 0.982 - ETA: 2s - loss: 0.0597 - acc: 0.982 - ETA: 2s - loss: 0.0597 - acc: 0.982 - ETA: 2s - loss: 0.0593 - acc: 0.982 - ETA: 2s - loss: 0.0596 - acc: 0.982 - ETA: 2s - loss: 0.0592 - acc: 0.982 - ETA: 2s - loss: 0.0589 - acc: 0.982 - ETA: 2s - loss: 0.0595 - acc: 0.982 - ETA: 2s - loss: 0.0589 - acc: 0.982 - ETA: 2s - loss: 0.0720 - acc: 0.980 - ETA: 2s - loss: 0.0711 - acc: 0.980 - ETA: 1s - loss: 0.0703 - acc: 0.980 - ETA: 1s - loss: 0.0708 - acc: 0.980 - ETA: 1s - loss: 0.0701 - acc: 0.980 - ETA: 1s - loss: 0.0696 - acc: 0.981 - ETA: 1s - loss: 0.0693 - acc: 0.981 - ETA: 1s - loss: 0.0686 - acc: 0.981 - ETA: 1s - loss: 0.0700 - acc: 0.980 - ETA: 1s - loss: 0.0693 - acc: 0.980 - ETA: 1s - loss: 0.0691 - acc: 0.980 - ETA: 1s - loss: 0.0696 - acc: 0.980 - ETA: 1s - loss: 0.0693 - acc: 0.980 - ETA: 1s - loss: 0.0716 - acc: 0.980 - ETA: 1s - loss: 0.0710 - acc: 0.980 - ETA: 0s - loss: 0.0709 - acc: 0.980 - ETA: 0s - loss: 0.0708 - acc: 0.981 - ETA: 0s - loss: 0.0704 - acc: 0.981 - ETA: 0s - loss: 0.0698 - acc: 0.981 - ETA: 0s - loss: 0.0691 - acc: 0.981 - ETA: 0s - loss: 0.0688 - acc: 0.981 - ETA: 0s - loss: 0.0693 - acc: 0.981 - ETA: 0s - loss: 0.0687 - acc: 0.981 - ETA: 0s - loss: 0.0685 - acc: 0.981 - ETA: 0s - loss: 0.0687 - acc: 0.981 - ETA: 0s - loss: 0.0686 - acc: 0.981 - ETA: 0s - loss: 0.0684 - acc: 0.981 - ETA: 0s - loss: 0.0682 - acc: 0.981 - ETA: 0s - loss: 0.0679 - acc: 0.981 - ETA: 0s - loss: 0.0679 - acc: 0.981 - ETA: 0s - loss: 0.0676 - acc: 0.981 - ETA: 0s - loss: 0.0674 - acc: 0.982 - ETA: 0s - loss: 0.0670 - acc: 0.982 - ETA: 0s - loss: 0.0665 - acc: 0.982 - 9s 3ms/step - loss: 0.0664 - acc: 0.9823 - val_loss: 0.0940 - val_acc: 0.9805\n",
      "Epoch 13/40\n",
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0059 - acc: 1.000 - ETA: 7s - loss: 0.0114 - acc: 1.000 - ETA: 7s - loss: 0.0259 - acc: 0.987 - ETA: 7s - loss: 0.0201 - acc: 0.991 - ETA: 7s - loss: 0.0198 - acc: 0.993 - ETA: 7s - loss: 0.0550 - acc: 0.977 - ETA: 7s - loss: 0.0923 - acc: 0.971 - ETA: 7s - loss: 0.0833 - acc: 0.975 - ETA: 7s - loss: 0.0800 - acc: 0.976 - ETA: 7s - loss: 0.0732 - acc: 0.979 - ETA: 7s - loss: 0.0799 - acc: 0.978 - ETA: 7s - loss: 0.1113 - acc: 0.968 - ETA: 7s - loss: 0.1097 - acc: 0.968 - ETA: 7s - loss: 0.1078 - acc: 0.970 - ETA: 7s - loss: 0.1074 - acc: 0.971 - ETA: 7s - loss: 0.1096 - acc: 0.968 - ETA: 7s - loss: 0.1082 - acc: 0.967 - ETA: 7s - loss: 0.1018 - acc: 0.969 - ETA: 7s - loss: 0.1143 - acc: 0.969 - ETA: 6s - loss: 0.1112 - acc: 0.969 - ETA: 6s - loss: 0.1071 - acc: 0.971 - ETA: 6s - loss: 0.1047 - acc: 0.971 - ETA: 6s - loss: 0.1090 - acc: 0.969 - ETA: 6s - loss: 0.1108 - acc: 0.968 - ETA: 6s - loss: 0.1070 - acc: 0.969 - ETA: 6s - loss: 0.1037 - acc: 0.970 - ETA: 6s - loss: 0.1005 - acc: 0.971 - ETA: 6s - loss: 0.0966 - acc: 0.973 - ETA: 6s - loss: 0.0971 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.973 - ETA: 5s - loss: 0.0916 - acc: 0.974 - ETA: 5s - loss: 0.0906 - acc: 0.975 - ETA: 5s - loss: 0.0878 - acc: 0.976 - ETA: 5s - loss: 0.0858 - acc: 0.976 - ETA: 5s - loss: 0.0888 - acc: 0.974 - ETA: 5s - loss: 0.0865 - acc: 0.975 - ETA: 5s - loss: 0.0845 - acc: 0.976 - ETA: 5s - loss: 0.0895 - acc: 0.975 - ETA: 5s - loss: 0.0878 - acc: 0.975 - ETA: 5s - loss: 0.0863 - acc: 0.976 - ETA: 4s - loss: 0.0903 - acc: 0.975 - ETA: 4s - loss: 0.0886 - acc: 0.976 - ETA: 4s - loss: 0.0867 - acc: 0.976 - ETA: 4s - loss: 0.0850 - acc: 0.977 - ETA: 4s - loss: 0.0846 - acc: 0.976 - ETA: 4s - loss: 0.0830 - acc: 0.977 - ETA: 4s - loss: 0.0815 - acc: 0.978 - ETA: 4s - loss: 0.0799 - acc: 0.978 - ETA: 4s - loss: 0.0789 - acc: 0.978 - ETA: 4s - loss: 0.0775 - acc: 0.979 - ETA: 4s - loss: 0.0767 - acc: 0.979 - ETA: 4s - loss: 0.0756 - acc: 0.980 - ETA: 3s - loss: 0.0741 - acc: 0.980 - ETA: 3s - loss: 0.0729 - acc: 0.981 - ETA: 3s - loss: 0.0721 - acc: 0.981 - ETA: 3s - loss: 0.0717 - acc: 0.981 - ETA: 3s - loss: 0.0705 - acc: 0.981 - ETA: 3s - loss: 0.0700 - acc: 0.981 - ETA: 3s - loss: 0.0690 - acc: 0.982 - ETA: 3s - loss: 0.0681 - acc: 0.982 - ETA: 3s - loss: 0.0674 - acc: 0.982 - ETA: 3s - loss: 0.0679 - acc: 0.982 - ETA: 3s - loss: 0.0672 - acc: 0.982 - ETA: 3s - loss: 0.0665 - acc: 0.983 - ETA: 2s - loss: 0.0666 - acc: 0.982 - ETA: 2s - loss: 0.0658 - acc: 0.983 - ETA: 2s - loss: 0.0651 - acc: 0.983 - ETA: 2s - loss: 0.0643 - acc: 0.983 - ETA: 2s - loss: 0.0635 - acc: 0.983 - ETA: 2s - loss: 0.0628 - acc: 0.984 - ETA: 2s - loss: 0.0627 - acc: 0.983 - ETA: 2s - loss: 0.0620 - acc: 0.984 - ETA: 2s - loss: 0.0636 - acc: 0.983 - ETA: 2s - loss: 0.0641 - acc: 0.983 - ETA: 2s - loss: 0.0632 - acc: 0.983 - ETA: 2s - loss: 0.0669 - acc: 0.981 - ETA: 2s - loss: 0.0666 - acc: 0.981 - ETA: 1s - loss: 0.0667 - acc: 0.981 - ETA: 1s - loss: 0.0664 - acc: 0.981 - ETA: 1s - loss: 0.0659 - acc: 0.981 - ETA: 1s - loss: 0.0652 - acc: 0.982 - ETA: 1s - loss: 0.0651 - acc: 0.981 - ETA: 1s - loss: 0.0644 - acc: 0.982 - ETA: 1s - loss: 0.0644 - acc: 0.981 - ETA: 1s - loss: 0.0648 - acc: 0.981 - ETA: 1s - loss: 0.0644 - acc: 0.981 - ETA: 1s - loss: 0.0639 - acc: 0.982 - ETA: 1s - loss: 0.0633 - acc: 0.982 - ETA: 1s - loss: 0.0627 - acc: 0.982 - ETA: 1s - loss: 0.0625 - acc: 0.982 - ETA: 1s - loss: 0.0621 - acc: 0.982 - ETA: 0s - loss: 0.0621 - acc: 0.982 - ETA: 0s - loss: 0.0618 - acc: 0.982 - ETA: 0s - loss: 0.0612 - acc: 0.982 - ETA: 0s - loss: 0.0607 - acc: 0.983 - ETA: 0s - loss: 0.0601 - acc: 0.983 - ETA: 0s - loss: 0.0595 - acc: 0.983 - ETA: 0s - loss: 0.0594 - acc: 0.983 - ETA: 0s - loss: 0.0593 - acc: 0.983 - ETA: 0s - loss: 0.0589 - acc: 0.983 - ETA: 0s - loss: 0.0599 - acc: 0.983 - ETA: 0s - loss: 0.0613 - acc: 0.982 - ETA: 0s - loss: 0.0613 - acc: 0.982 - ETA: 0s - loss: 0.0608 - acc: 0.983 - ETA: 0s - loss: 0.0608 - acc: 0.982 - 8s 2ms/step - loss: 0.0604 - acc: 0.9830 - val_loss: 0.1306 - val_acc: 0.9640\n",
      "Epoch 14/40\n",
      "3285/3285 [==============================] - ETA: 6s - loss: 0.0041 - acc: 1.000 - ETA: 6s - loss: 0.0076 - acc: 1.000 - ETA: 6s - loss: 0.0068 - acc: 1.000 - ETA: 6s - loss: 0.0232 - acc: 0.991 - ETA: 6s - loss: 0.0253 - acc: 0.986 - ETA: 6s - loss: 0.0228 - acc: 0.988 - ETA: 6s - loss: 0.0209 - acc: 0.990 - ETA: 6s - loss: 0.0301 - acc: 0.983 - ETA: 6s - loss: 0.0333 - acc: 0.981 - ETA: 6s - loss: 0.0307 - acc: 0.983 - ETA: 5s - loss: 0.0738 - acc: 0.970 - ETA: 5s - loss: 0.0697 - acc: 0.972 - ETA: 5s - loss: 0.0658 - acc: 0.975 - ETA: 5s - loss: 0.0653 - acc: 0.976 - ETA: 5s - loss: 0.0620 - acc: 0.978 - ETA: 5s - loss: 0.0605 - acc: 0.979 - ETA: 5s - loss: 0.0616 - acc: 0.979 - ETA: 5s - loss: 0.0588 - acc: 0.980 - ETA: 5s - loss: 0.0560 - acc: 0.981 - ETA: 5s - loss: 0.0544 - acc: 0.982 - ETA: 5s - loss: 0.0528 - acc: 0.983 - ETA: 5s - loss: 0.0537 - acc: 0.984 - ETA: 5s - loss: 0.0520 - acc: 0.984 - ETA: 5s - loss: 0.0565 - acc: 0.982 - ETA: 5s - loss: 0.0544 - acc: 0.983 - ETA: 5s - loss: 0.0605 - acc: 0.981 - ETA: 4s - loss: 0.0587 - acc: 0.982 - ETA: 4s - loss: 0.0572 - acc: 0.983 - ETA: 4s - loss: 0.0570 - acc: 0.982 - ETA: 4s - loss: 0.0554 - acc: 0.983 - ETA: 4s - loss: 0.0541 - acc: 0.983 - ETA: 4s - loss: 0.0527 - acc: 0.984 - ETA: 4s - loss: 0.0723 - acc: 0.979 - ETA: 4s - loss: 0.0705 - acc: 0.980 - ETA: 4s - loss: 0.0692 - acc: 0.981 - ETA: 4s - loss: 0.0685 - acc: 0.981 - ETA: 4s - loss: 0.0671 - acc: 0.982 - ETA: 4s - loss: 0.0673 - acc: 0.981 - ETA: 4s - loss: 0.0657 - acc: 0.982 - ETA: 4s - loss: 0.0646 - acc: 0.982 - ETA: 4s - loss: 0.0636 - acc: 0.983 - ETA: 4s - loss: 0.0631 - acc: 0.983 - ETA: 3s - loss: 0.0617 - acc: 0.983 - ETA: 3s - loss: 0.0608 - acc: 0.984 - ETA: 3s - loss: 0.0610 - acc: 0.984 - ETA: 3s - loss: 0.0605 - acc: 0.984 - ETA: 3s - loss: 0.0594 - acc: 0.984 - ETA: 3s - loss: 0.0600 - acc: 0.984 - ETA: 3s - loss: 0.0591 - acc: 0.984 - ETA: 3s - loss: 0.0581 - acc: 0.984 - ETA: 3s - loss: 0.0608 - acc: 0.983 - ETA: 3s - loss: 0.0603 - acc: 0.983 - ETA: 3s - loss: 0.0597 - acc: 0.983 - ETA: 3s - loss: 0.0753 - acc: 0.981 - ETA: 3s - loss: 0.0747 - acc: 0.982 - ETA: 3s - loss: 0.0748 - acc: 0.981 - ETA: 3s - loss: 0.0737 - acc: 0.981 - ETA: 3s - loss: 0.0727 - acc: 0.982 - ETA: 2s - loss: 0.0717 - acc: 0.982 - ETA: 2s - loss: 0.0708 - acc: 0.982 - ETA: 2s - loss: 0.0701 - acc: 0.983 - ETA: 2s - loss: 0.0710 - acc: 0.982 - ETA: 2s - loss: 0.0703 - acc: 0.983 - ETA: 2s - loss: 0.0693 - acc: 0.983 - ETA: 2s - loss: 0.0683 - acc: 0.983 - ETA: 2s - loss: 0.0679 - acc: 0.983 - ETA: 2s - loss: 0.0670 - acc: 0.984 - ETA: 2s - loss: 0.0667 - acc: 0.984 - ETA: 2s - loss: 0.0658 - acc: 0.984 - ETA: 2s - loss: 0.0649 - acc: 0.984 - ETA: 2s - loss: 0.0644 - acc: 0.984 - ETA: 2s - loss: 0.0637 - acc: 0.985 - ETA: 2s - loss: 0.0634 - acc: 0.984 - ETA: 2s - loss: 0.0627 - acc: 0.985 - ETA: 1s - loss: 0.0620 - acc: 0.985 - ETA: 1s - loss: 0.0614 - acc: 0.985 - ETA: 1s - loss: 0.0612 - acc: 0.985 - ETA: 1s - loss: 0.0606 - acc: 0.985 - ETA: 1s - loss: 0.0600 - acc: 0.985 - ETA: 1s - loss: 0.0594 - acc: 0.985 - ETA: 1s - loss: 0.0615 - acc: 0.984 - ETA: 1s - loss: 0.0612 - acc: 0.985 - ETA: 1s - loss: 0.0614 - acc: 0.984 - ETA: 1s - loss: 0.0700 - acc: 0.983 - ETA: 1s - loss: 0.0694 - acc: 0.984 - ETA: 1s - loss: 0.0687 - acc: 0.984 - ETA: 1s - loss: 0.0704 - acc: 0.983 - ETA: 1s - loss: 0.0697 - acc: 0.983 - ETA: 1s - loss: 0.0695 - acc: 0.984 - ETA: 0s - loss: 0.0694 - acc: 0.983 - ETA: 0s - loss: 0.0687 - acc: 0.984 - ETA: 0s - loss: 0.0682 - acc: 0.984 - ETA: 0s - loss: 0.0677 - acc: 0.984 - ETA: 0s - loss: 0.0672 - acc: 0.984 - ETA: 0s - loss: 0.0667 - acc: 0.984 - ETA: 0s - loss: 0.0677 - acc: 0.984 - ETA: 0s - loss: 0.0676 - acc: 0.984 - ETA: 0s - loss: 0.0669 - acc: 0.984 - ETA: 0s - loss: 0.0663 - acc: 0.984 - ETA: 0s - loss: 0.0657 - acc: 0.984 - ETA: 0s - loss: 0.0652 - acc: 0.984 - ETA: 0s - loss: 0.0647 - acc: 0.984 - ETA: 0s - loss: 0.0641 - acc: 0.985 - 8s 2ms/step - loss: 0.0659 - acc: 0.9845 - val_loss: 0.1408 - val_acc: 0.9690\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0372 - acc: 1.000 - ETA: 7s - loss: 0.0171 - acc: 1.000 - ETA: 7s - loss: 0.0138 - acc: 1.000 - ETA: 6s - loss: 0.0214 - acc: 1.000 - ETA: 6s - loss: 0.0191 - acc: 1.000 - ETA: 6s - loss: 0.0173 - acc: 1.000 - ETA: 6s - loss: 0.0617 - acc: 0.990 - ETA: 6s - loss: 0.0669 - acc: 0.983 - ETA: 6s - loss: 0.0602 - acc: 0.985 - ETA: 6s - loss: 0.0548 - acc: 0.986 - ETA: 6s - loss: 0.0599 - acc: 0.982 - ETA: 6s - loss: 0.0563 - acc: 0.983 - ETA: 6s - loss: 0.0758 - acc: 0.980 - ETA: 6s - loss: 0.0707 - acc: 0.981 - ETA: 5s - loss: 0.0731 - acc: 0.980 - ETA: 5s - loss: 0.0693 - acc: 0.981 - ETA: 5s - loss: 0.0659 - acc: 0.983 - ETA: 5s - loss: 0.0649 - acc: 0.983 - ETA: 5s - loss: 0.0620 - acc: 0.984 - ETA: 5s - loss: 0.0617 - acc: 0.984 - ETA: 5s - loss: 0.0590 - acc: 0.984 - ETA: 5s - loss: 0.0589 - acc: 0.984 - ETA: 5s - loss: 0.0580 - acc: 0.984 - ETA: 5s - loss: 0.0563 - acc: 0.985 - ETA: 5s - loss: 0.0545 - acc: 0.986 - ETA: 5s - loss: 0.0675 - acc: 0.980 - ETA: 5s - loss: 0.0651 - acc: 0.981 - ETA: 5s - loss: 0.0632 - acc: 0.981 - ETA: 4s - loss: 0.0632 - acc: 0.982 - ETA: 4s - loss: 0.0618 - acc: 0.983 - ETA: 4s - loss: 0.0602 - acc: 0.983 - ETA: 4s - loss: 0.0601 - acc: 0.983 - ETA: 4s - loss: 0.0584 - acc: 0.983 - ETA: 4s - loss: 0.0572 - acc: 0.984 - ETA: 4s - loss: 0.0593 - acc: 0.982 - ETA: 4s - loss: 0.0581 - acc: 0.983 - ETA: 4s - loss: 0.0595 - acc: 0.982 - ETA: 4s - loss: 0.0580 - acc: 0.982 - ETA: 4s - loss: 0.0586 - acc: 0.982 - ETA: 4s - loss: 0.0580 - acc: 0.982 - ETA: 4s - loss: 0.0578 - acc: 0.983 - ETA: 4s - loss: 0.0565 - acc: 0.983 - ETA: 3s - loss: 0.0553 - acc: 0.983 - ETA: 3s - loss: 0.0558 - acc: 0.983 - ETA: 3s - loss: 0.0553 - acc: 0.983 - ETA: 3s - loss: 0.0549 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.983 - ETA: 3s - loss: 0.0540 - acc: 0.984 - ETA: 3s - loss: 0.0530 - acc: 0.984 - ETA: 3s - loss: 0.0522 - acc: 0.984 - ETA: 3s - loss: 0.0514 - acc: 0.985 - ETA: 3s - loss: 0.0506 - acc: 0.985 - ETA: 3s - loss: 0.0497 - acc: 0.985 - ETA: 3s - loss: 0.0489 - acc: 0.986 - ETA: 3s - loss: 0.0495 - acc: 0.985 - ETA: 3s - loss: 0.0493 - acc: 0.985 - ETA: 3s - loss: 0.0485 - acc: 0.985 - ETA: 2s - loss: 0.0490 - acc: 0.984 - ETA: 2s - loss: 0.0483 - acc: 0.985 - ETA: 2s - loss: 0.0503 - acc: 0.983 - ETA: 2s - loss: 0.0505 - acc: 0.983 - ETA: 2s - loss: 0.0506 - acc: 0.983 - ETA: 2s - loss: 0.0499 - acc: 0.983 - ETA: 2s - loss: 0.0493 - acc: 0.983 - ETA: 2s - loss: 0.0490 - acc: 0.983 - ETA: 2s - loss: 0.0485 - acc: 0.983 - ETA: 2s - loss: 0.0480 - acc: 0.984 - ETA: 2s - loss: 0.0477 - acc: 0.984 - ETA: 2s - loss: 0.0472 - acc: 0.984 - ETA: 2s - loss: 0.0497 - acc: 0.984 - ETA: 2s - loss: 0.0493 - acc: 0.984 - ETA: 2s - loss: 0.0490 - acc: 0.984 - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0482 - acc: 0.985 - ETA: 1s - loss: 0.0477 - acc: 0.985 - ETA: 1s - loss: 0.0472 - acc: 0.985 - ETA: 1s - loss: 0.0499 - acc: 0.984 - ETA: 1s - loss: 0.0502 - acc: 0.985 - ETA: 1s - loss: 0.0498 - acc: 0.985 - ETA: 1s - loss: 0.0493 - acc: 0.985 - ETA: 1s - loss: 0.0488 - acc: 0.985 - ETA: 1s - loss: 0.0484 - acc: 0.985 - ETA: 1s - loss: 0.0482 - acc: 0.986 - ETA: 1s - loss: 0.0503 - acc: 0.985 - ETA: 1s - loss: 0.0497 - acc: 0.985 - ETA: 1s - loss: 0.0493 - acc: 0.985 - ETA: 1s - loss: 0.0488 - acc: 0.985 - ETA: 1s - loss: 0.0486 - acc: 0.986 - ETA: 0s - loss: 0.0482 - acc: 0.986 - ETA: 0s - loss: 0.0496 - acc: 0.985 - ETA: 0s - loss: 0.0491 - acc: 0.985 - ETA: 0s - loss: 0.0486 - acc: 0.986 - ETA: 0s - loss: 0.0481 - acc: 0.986 - ETA: 0s - loss: 0.0480 - acc: 0.986 - ETA: 0s - loss: 0.0477 - acc: 0.986 - ETA: 0s - loss: 0.0474 - acc: 0.986 - ETA: 0s - loss: 0.0470 - acc: 0.986 - ETA: 0s - loss: 0.0467 - acc: 0.986 - ETA: 0s - loss: 0.0468 - acc: 0.986 - ETA: 0s - loss: 0.0464 - acc: 0.986 - ETA: 0s - loss: 0.0462 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - 7s 2ms/step - loss: 0.0458 - acc: 0.9866 - val_loss: 0.0874 - val_acc: 0.9748\n",
      "Epoch 16/40\n",
      "3285/3285 [==============================] - ETA: 6s - loss: 0.0084 - acc: 1.000 - ETA: 6s - loss: 0.0073 - acc: 1.000 - ETA: 6s - loss: 0.0201 - acc: 1.000 - ETA: 6s - loss: 0.0205 - acc: 1.000 - ETA: 6s - loss: 0.0321 - acc: 0.993 - ETA: 6s - loss: 0.0294 - acc: 0.994 - ETA: 6s - loss: 0.0258 - acc: 0.995 - ETA: 6s - loss: 0.0254 - acc: 0.995 - ETA: 6s - loss: 0.0280 - acc: 0.992 - ETA: 6s - loss: 0.0360 - acc: 0.990 - ETA: 6s - loss: 0.0356 - acc: 0.991 - ETA: 6s - loss: 0.0337 - acc: 0.991 - ETA: 5s - loss: 0.0357 - acc: 0.990 - ETA: 6s - loss: 0.0345 - acc: 0.990 - ETA: 5s - loss: 0.0335 - acc: 0.991 - ETA: 5s - loss: 0.0325 - acc: 0.991 - ETA: 5s - loss: 0.0307 - acc: 0.992 - ETA: 5s - loss: 0.0308 - acc: 0.992 - ETA: 5s - loss: 0.0312 - acc: 0.991 - ETA: 5s - loss: 0.0301 - acc: 0.992 - ETA: 5s - loss: 0.0291 - acc: 0.992 - ETA: 5s - loss: 0.0282 - acc: 0.992 - ETA: 5s - loss: 0.0274 - acc: 0.993 - ETA: 5s - loss: 0.0268 - acc: 0.993 - ETA: 5s - loss: 0.0262 - acc: 0.993 - ETA: 5s - loss: 0.0257 - acc: 0.993 - ETA: 5s - loss: 0.0249 - acc: 0.994 - ETA: 5s - loss: 0.0257 - acc: 0.994 - ETA: 5s - loss: 0.0255 - acc: 0.994 - ETA: 4s - loss: 0.0247 - acc: 0.994 - ETA: 4s - loss: 0.0241 - acc: 0.994 - ETA: 4s - loss: 0.0253 - acc: 0.994 - ETA: 4s - loss: 0.0251 - acc: 0.994 - ETA: 4s - loss: 0.0246 - acc: 0.994 - ETA: 4s - loss: 0.0241 - acc: 0.994 - ETA: 4s - loss: 0.0235 - acc: 0.994 - ETA: 4s - loss: 0.0232 - acc: 0.994 - ETA: 4s - loss: 0.0284 - acc: 0.993 - ETA: 4s - loss: 0.0288 - acc: 0.993 - ETA: 4s - loss: 0.0282 - acc: 0.993 - ETA: 4s - loss: 0.0302 - acc: 0.993 - ETA: 4s - loss: 0.0315 - acc: 0.992 - ETA: 4s - loss: 0.0308 - acc: 0.992 - ETA: 3s - loss: 0.0305 - acc: 0.992 - ETA: 3s - loss: 0.0316 - acc: 0.992 - ETA: 3s - loss: 0.0311 - acc: 0.992 - ETA: 3s - loss: 0.0306 - acc: 0.992 - ETA: 3s - loss: 0.0322 - acc: 0.992 - ETA: 3s - loss: 0.0318 - acc: 0.992 - ETA: 3s - loss: 0.0315 - acc: 0.992 - ETA: 3s - loss: 0.0313 - acc: 0.992 - ETA: 3s - loss: 0.0317 - acc: 0.992 - ETA: 3s - loss: 0.0312 - acc: 0.992 - ETA: 3s - loss: 0.0308 - acc: 0.993 - ETA: 3s - loss: 0.0305 - acc: 0.993 - ETA: 3s - loss: 0.0300 - acc: 0.993 - ETA: 3s - loss: 0.0297 - acc: 0.993 - ETA: 3s - loss: 0.0293 - acc: 0.993 - ETA: 2s - loss: 0.0290 - acc: 0.993 - ETA: 2s - loss: 0.0287 - acc: 0.993 - ETA: 2s - loss: 0.0284 - acc: 0.993 - ETA: 2s - loss: 0.0281 - acc: 0.993 - ETA: 2s - loss: 0.0278 - acc: 0.994 - ETA: 2s - loss: 0.0275 - acc: 0.994 - ETA: 2s - loss: 0.0274 - acc: 0.994 - ETA: 2s - loss: 0.0271 - acc: 0.994 - ETA: 2s - loss: 0.0269 - acc: 0.994 - ETA: 2s - loss: 0.0266 - acc: 0.994 - ETA: 2s - loss: 0.0265 - acc: 0.994 - ETA: 2s - loss: 0.0263 - acc: 0.994 - ETA: 2s - loss: 0.0265 - acc: 0.994 - ETA: 2s - loss: 0.0266 - acc: 0.994 - ETA: 2s - loss: 0.0272 - acc: 0.994 - ETA: 1s - loss: 0.0269 - acc: 0.994 - ETA: 1s - loss: 0.0267 - acc: 0.994 - ETA: 1s - loss: 0.0265 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 1s - loss: 0.0264 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0261 - acc: 0.994 - ETA: 1s - loss: 0.0259 - acc: 0.994 - ETA: 1s - loss: 0.0257 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 0s - loss: 0.0258 - acc: 0.994 - ETA: 0s - loss: 0.0261 - acc: 0.994 - ETA: 0s - loss: 0.0262 - acc: 0.993 - ETA: 0s - loss: 0.0259 - acc: 0.993 - ETA: 0s - loss: 0.0257 - acc: 0.993 - ETA: 0s - loss: 0.0256 - acc: 0.994 - ETA: 0s - loss: 0.0260 - acc: 0.993 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0256 - acc: 0.993 - ETA: 0s - loss: 0.0255 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0265 - acc: 0.993 - ETA: 0s - loss: 0.0306 - acc: 0.992 - ETA: 0s - loss: 0.0304 - acc: 0.993 - 7s 2ms/step - loss: 0.0304 - acc: 0.9930 - val_loss: 0.1932 - val_acc: 0.9539\n",
      "Epoch 17/40\n",
      "3285/3285 [==============================] - ETA: 6s - loss: 0.0081 - acc: 1.000 - ETA: 6s - loss: 0.0085 - acc: 1.000 - ETA: 6s - loss: 0.0086 - acc: 1.000 - ETA: 6s - loss: 0.0096 - acc: 1.000 - ETA: 6s - loss: 0.1161 - acc: 0.986 - ETA: 6s - loss: 0.0970 - acc: 0.988 - ETA: 6s - loss: 0.0876 - acc: 0.985 - ETA: 6s - loss: 0.0772 - acc: 0.987 - ETA: 6s - loss: 0.0683 - acc: 0.989 - ETA: 6s - loss: 0.0645 - acc: 0.990 - ETA: 6s - loss: 0.0608 - acc: 0.991 - ETA: 6s - loss: 0.0563 - acc: 0.991 - ETA: 6s - loss: 0.0523 - acc: 0.992 - ETA: 6s - loss: 0.0488 - acc: 0.993 - ETA: 6s - loss: 0.0462 - acc: 0.993 - ETA: 6s - loss: 0.0434 - acc: 0.994 - ETA: 5s - loss: 0.0436 - acc: 0.994 - ETA: 5s - loss: 0.0415 - acc: 0.994 - ETA: 5s - loss: 0.0395 - acc: 0.994 - ETA: 5s - loss: 0.0386 - acc: 0.995 - ETA: 5s - loss: 0.0390 - acc: 0.993 - ETA: 5s - loss: 0.0378 - acc: 0.994 - ETA: 5s - loss: 0.0364 - acc: 0.994 - ETA: 5s - loss: 0.0364 - acc: 0.993 - ETA: 5s - loss: 0.0359 - acc: 0.993 - ETA: 5s - loss: 0.0358 - acc: 0.992 - ETA: 5s - loss: 0.0345 - acc: 0.992 - ETA: 5s - loss: 0.0334 - acc: 0.993 - ETA: 5s - loss: 0.0356 - acc: 0.991 - ETA: 5s - loss: 0.0346 - acc: 0.991 - ETA: 4s - loss: 0.0356 - acc: 0.990 - ETA: 4s - loss: 0.0346 - acc: 0.991 - ETA: 4s - loss: 0.0339 - acc: 0.991 - ETA: 4s - loss: 0.0333 - acc: 0.991 - ETA: 4s - loss: 0.0325 - acc: 0.991 - ETA: 4s - loss: 0.0329 - acc: 0.992 - ETA: 4s - loss: 0.0321 - acc: 0.992 - ETA: 4s - loss: 0.0375 - acc: 0.990 - ETA: 4s - loss: 0.0374 - acc: 0.990 - ETA: 4s - loss: 0.0389 - acc: 0.989 - ETA: 4s - loss: 0.0381 - acc: 0.990 - ETA: 4s - loss: 0.0380 - acc: 0.989 - ETA: 4s - loss: 0.0372 - acc: 0.989 - ETA: 4s - loss: 0.0368 - acc: 0.989 - ETA: 3s - loss: 0.0362 - acc: 0.990 - ETA: 3s - loss: 0.0355 - acc: 0.990 - ETA: 3s - loss: 0.0359 - acc: 0.989 - ETA: 3s - loss: 0.0352 - acc: 0.990 - ETA: 3s - loss: 0.0347 - acc: 0.990 - ETA: 3s - loss: 0.0341 - acc: 0.990 - ETA: 3s - loss: 0.0335 - acc: 0.990 - ETA: 3s - loss: 0.0362 - acc: 0.990 - ETA: 3s - loss: 0.0357 - acc: 0.990 - ETA: 3s - loss: 0.0352 - acc: 0.990 - ETA: 3s - loss: 0.0348 - acc: 0.990 - ETA: 3s - loss: 0.0343 - acc: 0.991 - ETA: 3s - loss: 0.0340 - acc: 0.991 - ETA: 3s - loss: 0.0336 - acc: 0.991 - ETA: 3s - loss: 0.0331 - acc: 0.991 - ETA: 2s - loss: 0.0330 - acc: 0.991 - ETA: 2s - loss: 0.0325 - acc: 0.991 - ETA: 2s - loss: 0.0321 - acc: 0.991 - ETA: 2s - loss: 0.0317 - acc: 0.991 - ETA: 2s - loss: 0.0313 - acc: 0.991 - ETA: 2s - loss: 0.0308 - acc: 0.991 - ETA: 2s - loss: 0.0311 - acc: 0.991 - ETA: 2s - loss: 0.0315 - acc: 0.991 - ETA: 2s - loss: 0.0313 - acc: 0.991 - ETA: 2s - loss: 0.0346 - acc: 0.990 - ETA: 2s - loss: 0.0342 - acc: 0.990 - ETA: 2s - loss: 0.0339 - acc: 0.990 - ETA: 2s - loss: 0.0335 - acc: 0.990 - ETA: 2s - loss: 0.0331 - acc: 0.990 - ETA: 1s - loss: 0.0327 - acc: 0.991 - ETA: 1s - loss: 0.0326 - acc: 0.991 - ETA: 1s - loss: 0.0322 - acc: 0.991 - ETA: 1s - loss: 0.0322 - acc: 0.991 - ETA: 1s - loss: 0.0321 - acc: 0.991 - ETA: 1s - loss: 0.0322 - acc: 0.990 - ETA: 1s - loss: 0.0345 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.990 - ETA: 1s - loss: 0.0338 - acc: 0.990 - ETA: 1s - loss: 0.0337 - acc: 0.990 - ETA: 1s - loss: 0.0333 - acc: 0.990 - ETA: 1s - loss: 0.0330 - acc: 0.990 - ETA: 1s - loss: 0.0326 - acc: 0.990 - ETA: 1s - loss: 0.0325 - acc: 0.990 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0320 - acc: 0.990 - ETA: 0s - loss: 0.0319 - acc: 0.991 - ETA: 0s - loss: 0.0350 - acc: 0.989 - ETA: 0s - loss: 0.0347 - acc: 0.989 - ETA: 0s - loss: 0.0370 - acc: 0.989 - ETA: 0s - loss: 0.0367 - acc: 0.989 - ETA: 0s - loss: 0.0375 - acc: 0.989 - ETA: 0s - loss: 0.0386 - acc: 0.989 - ETA: 0s - loss: 0.0384 - acc: 0.989 - ETA: 0s - loss: 0.0382 - acc: 0.989 - ETA: 0s - loss: 0.0378 - acc: 0.989 - ETA: 0s - loss: 0.0375 - acc: 0.989 - ETA: 0s - loss: 0.0374 - acc: 0.989 - ETA: 0s - loss: 0.0372 - acc: 0.989 - 7s 2ms/step - loss: 0.0371 - acc: 0.9900 - val_loss: 0.1121 - val_acc: 0.9748\n",
      "Epoch 18/40\n",
      "3285/3285 [==============================] - ETA: 6s - loss: 0.0011 - acc: 1.000 - ETA: 6s - loss: 0.0032 - acc: 1.000 - ETA: 6s - loss: 0.0086 - acc: 1.000 - ETA: 6s - loss: 0.2303 - acc: 0.955 - ETA: 6s - loss: 0.1797 - acc: 0.965 - ETA: 6s - loss: 0.1519 - acc: 0.971 - ETA: 6s - loss: 0.1385 - acc: 0.971 - ETA: 6s - loss: 0.1362 - acc: 0.970 - ETA: 6s - loss: 0.1221 - acc: 0.974 - ETA: 6s - loss: 0.1105 - acc: 0.977 - ETA: 5s - loss: 0.1005 - acc: 0.979 - ETA: 5s - loss: 0.0931 - acc: 0.981 - ETA: 5s - loss: 0.1162 - acc: 0.977 - ETA: 5s - loss: 0.1088 - acc: 0.979 - ETA: 5s - loss: 0.1041 - acc: 0.978 - ETA: 5s - loss: 0.0982 - acc: 0.979 - ETA: 5s - loss: 0.0930 - acc: 0.981 - ETA: 5s - loss: 0.0897 - acc: 0.980 - ETA: 5s - loss: 0.1015 - acc: 0.979 - ETA: 5s - loss: 0.0993 - acc: 0.979 - ETA: 5s - loss: 0.0970 - acc: 0.978 - ETA: 5s - loss: 0.0933 - acc: 0.979 - ETA: 5s - loss: 0.1097 - acc: 0.977 - ETA: 5s - loss: 0.1071 - acc: 0.978 - ETA: 5s - loss: 0.1032 - acc: 0.979 - ETA: 5s - loss: 0.1018 - acc: 0.979 - ETA: 4s - loss: 0.1039 - acc: 0.976 - ETA: 4s - loss: 0.1043 - acc: 0.975 - ETA: 4s - loss: 0.1010 - acc: 0.975 - ETA: 4s - loss: 0.1011 - acc: 0.975 - ETA: 4s - loss: 0.0980 - acc: 0.976 - ETA: 4s - loss: 0.0998 - acc: 0.975 - ETA: 4s - loss: 0.0988 - acc: 0.976 - ETA: 4s - loss: 0.0960 - acc: 0.976 - ETA: 4s - loss: 0.0935 - acc: 0.977 - ETA: 4s - loss: 0.0911 - acc: 0.978 - ETA: 4s - loss: 0.0889 - acc: 0.978 - ETA: 4s - loss: 0.0878 - acc: 0.978 - ETA: 4s - loss: 0.0856 - acc: 0.978 - ETA: 4s - loss: 0.0837 - acc: 0.979 - ETA: 4s - loss: 0.0818 - acc: 0.979 - ETA: 3s - loss: 0.0799 - acc: 0.980 - ETA: 3s - loss: 0.0802 - acc: 0.980 - ETA: 3s - loss: 0.0863 - acc: 0.978 - ETA: 3s - loss: 0.0850 - acc: 0.978 - ETA: 3s - loss: 0.0832 - acc: 0.979 - ETA: 3s - loss: 0.0821 - acc: 0.979 - ETA: 3s - loss: 0.0806 - acc: 0.980 - ETA: 3s - loss: 0.0796 - acc: 0.980 - ETA: 3s - loss: 0.0783 - acc: 0.980 - ETA: 3s - loss: 0.0796 - acc: 0.979 - ETA: 3s - loss: 0.0815 - acc: 0.978 - ETA: 3s - loss: 0.0803 - acc: 0.979 - ETA: 3s - loss: 0.0832 - acc: 0.978 - ETA: 3s - loss: 0.0844 - acc: 0.978 - ETA: 3s - loss: 0.0839 - acc: 0.978 - ETA: 3s - loss: 0.0829 - acc: 0.979 - ETA: 2s - loss: 0.0815 - acc: 0.979 - ETA: 2s - loss: 0.0813 - acc: 0.979 - ETA: 2s - loss: 0.0804 - acc: 0.979 - ETA: 2s - loss: 0.0791 - acc: 0.979 - ETA: 2s - loss: 0.0779 - acc: 0.980 - ETA: 2s - loss: 0.0767 - acc: 0.980 - ETA: 2s - loss: 0.0759 - acc: 0.980 - ETA: 2s - loss: 0.0755 - acc: 0.980 - ETA: 2s - loss: 0.0748 - acc: 0.980 - ETA: 2s - loss: 0.0738 - acc: 0.981 - ETA: 2s - loss: 0.0729 - acc: 0.981 - ETA: 2s - loss: 0.0719 - acc: 0.981 - ETA: 2s - loss: 0.0712 - acc: 0.982 - ETA: 2s - loss: 0.0703 - acc: 0.982 - ETA: 2s - loss: 0.0703 - acc: 0.982 - ETA: 1s - loss: 0.0695 - acc: 0.982 - ETA: 1s - loss: 0.0706 - acc: 0.982 - ETA: 1s - loss: 0.0706 - acc: 0.982 - ETA: 1s - loss: 0.0699 - acc: 0.982 - ETA: 1s - loss: 0.0707 - acc: 0.982 - ETA: 1s - loss: 0.0698 - acc: 0.982 - ETA: 1s - loss: 0.0698 - acc: 0.981 - ETA: 1s - loss: 0.0690 - acc: 0.981 - ETA: 1s - loss: 0.0693 - acc: 0.981 - ETA: 1s - loss: 0.0685 - acc: 0.981 - ETA: 1s - loss: 0.0777 - acc: 0.979 - ETA: 1s - loss: 0.0770 - acc: 0.980 - ETA: 1s - loss: 0.0765 - acc: 0.980 - ETA: 1s - loss: 0.0757 - acc: 0.980 - ETA: 1s - loss: 0.0750 - acc: 0.980 - ETA: 0s - loss: 0.0742 - acc: 0.980 - ETA: 0s - loss: 0.0741 - acc: 0.980 - ETA: 0s - loss: 0.0736 - acc: 0.980 - ETA: 0s - loss: 0.0728 - acc: 0.981 - ETA: 0s - loss: 0.0720 - acc: 0.981 - ETA: 0s - loss: 0.0715 - acc: 0.981 - ETA: 0s - loss: 0.0712 - acc: 0.981 - ETA: 0s - loss: 0.0708 - acc: 0.981 - ETA: 0s - loss: 0.0720 - acc: 0.981 - ETA: 0s - loss: 0.0713 - acc: 0.981 - ETA: 0s - loss: 0.0712 - acc: 0.981 - ETA: 0s - loss: 0.0709 - acc: 0.981 - ETA: 0s - loss: 0.0702 - acc: 0.981 - ETA: 0s - loss: 0.0695 - acc: 0.981 - ETA: 0s - loss: 0.0693 - acc: 0.981 - ETA: 0s - loss: 0.0687 - acc: 0.982 - 7s 2ms/step - loss: 0.0697 - acc: 0.9817 - val_loss: 0.0756 - val_acc: 0.9733\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 6s - loss: 0.0095 - acc: 1.000 - ETA: 6s - loss: 0.0057 - acc: 1.000 - ETA: 6s - loss: 0.0139 - acc: 1.000 - ETA: 6s - loss: 0.0115 - acc: 1.000 - ETA: 6s - loss: 0.0122 - acc: 1.000 - ETA: 6s - loss: 0.0114 - acc: 1.000 - ETA: 6s - loss: 0.0143 - acc: 0.995 - ETA: 6s - loss: 0.0417 - acc: 0.991 - ETA: 6s - loss: 0.0384 - acc: 0.992 - ETA: 6s - loss: 0.0369 - acc: 0.993 - ETA: 6s - loss: 0.0376 - acc: 0.994 - ETA: 6s - loss: 0.0352 - acc: 0.994 - ETA: 6s - loss: 0.0634 - acc: 0.990 - ETA: 6s - loss: 0.0609 - acc: 0.990 - ETA: 5s - loss: 0.0580 - acc: 0.991 - ETA: 5s - loss: 0.0609 - acc: 0.989 - ETA: 5s - loss: 0.0792 - acc: 0.986 - ETA: 5s - loss: 0.0750 - acc: 0.987 - ETA: 5s - loss: 0.0811 - acc: 0.986 - ETA: 5s - loss: 0.0839 - acc: 0.984 - ETA: 5s - loss: 0.0815 - acc: 0.983 - ETA: 5s - loss: 0.0786 - acc: 0.984 - ETA: 5s - loss: 0.0773 - acc: 0.983 - ETA: 5s - loss: 0.0745 - acc: 0.984 - ETA: 5s - loss: 0.0733 - acc: 0.984 - ETA: 5s - loss: 0.0711 - acc: 0.985 - ETA: 5s - loss: 0.0690 - acc: 0.985 - ETA: 5s - loss: 0.0672 - acc: 0.986 - ETA: 4s - loss: 0.0702 - acc: 0.984 - ETA: 4s - loss: 0.0683 - acc: 0.985 - ETA: 4s - loss: 0.0673 - acc: 0.985 - ETA: 4s - loss: 0.0657 - acc: 0.986 - ETA: 4s - loss: 0.0656 - acc: 0.985 - ETA: 4s - loss: 0.0691 - acc: 0.983 - ETA: 4s - loss: 0.0678 - acc: 0.983 - ETA: 4s - loss: 0.0660 - acc: 0.984 - ETA: 4s - loss: 0.0647 - acc: 0.984 - ETA: 4s - loss: 0.0631 - acc: 0.985 - ETA: 4s - loss: 0.0617 - acc: 0.985 - ETA: 4s - loss: 0.0602 - acc: 0.985 - ETA: 4s - loss: 0.0597 - acc: 0.985 - ETA: 4s - loss: 0.0584 - acc: 0.985 - ETA: 4s - loss: 0.0575 - acc: 0.986 - ETA: 3s - loss: 0.0567 - acc: 0.986 - ETA: 3s - loss: 0.0558 - acc: 0.986 - ETA: 3s - loss: 0.0552 - acc: 0.987 - ETA: 3s - loss: 0.0554 - acc: 0.985 - ETA: 3s - loss: 0.0584 - acc: 0.984 - ETA: 3s - loss: 0.0617 - acc: 0.983 - ETA: 3s - loss: 0.0610 - acc: 0.984 - ETA: 3s - loss: 0.0607 - acc: 0.983 - ETA: 3s - loss: 0.0612 - acc: 0.983 - ETA: 3s - loss: 0.0612 - acc: 0.982 - ETA: 3s - loss: 0.0610 - acc: 0.982 - ETA: 3s - loss: 0.0599 - acc: 0.982 - ETA: 3s - loss: 0.0590 - acc: 0.983 - ETA: 3s - loss: 0.0580 - acc: 0.983 - ETA: 3s - loss: 0.0572 - acc: 0.983 - ETA: 2s - loss: 0.0564 - acc: 0.984 - ETA: 2s - loss: 0.0556 - acc: 0.984 - ETA: 2s - loss: 0.0559 - acc: 0.984 - ETA: 2s - loss: 0.0558 - acc: 0.984 - ETA: 2s - loss: 0.0715 - acc: 0.982 - ETA: 2s - loss: 0.0705 - acc: 0.982 - ETA: 2s - loss: 0.0695 - acc: 0.982 - ETA: 2s - loss: 0.0685 - acc: 0.982 - ETA: 2s - loss: 0.0675 - acc: 0.983 - ETA: 2s - loss: 0.0688 - acc: 0.982 - ETA: 2s - loss: 0.0687 - acc: 0.982 - ETA: 2s - loss: 0.0683 - acc: 0.982 - ETA: 2s - loss: 0.0675 - acc: 0.983 - ETA: 2s - loss: 0.0666 - acc: 0.983 - ETA: 2s - loss: 0.0657 - acc: 0.983 - ETA: 1s - loss: 0.0649 - acc: 0.983 - ETA: 1s - loss: 0.0646 - acc: 0.983 - ETA: 1s - loss: 0.0639 - acc: 0.983 - ETA: 1s - loss: 0.0647 - acc: 0.983 - ETA: 1s - loss: 0.0642 - acc: 0.983 - ETA: 1s - loss: 0.0634 - acc: 0.983 - ETA: 1s - loss: 0.0629 - acc: 0.983 - ETA: 1s - loss: 0.0626 - acc: 0.983 - ETA: 1s - loss: 0.0619 - acc: 0.983 - ETA: 1s - loss: 0.0612 - acc: 0.984 - ETA: 1s - loss: 0.0611 - acc: 0.983 - ETA: 1s - loss: 0.0605 - acc: 0.984 - ETA: 1s - loss: 0.0598 - acc: 0.984 - ETA: 1s - loss: 0.0598 - acc: 0.984 - ETA: 1s - loss: 0.0610 - acc: 0.983 - ETA: 0s - loss: 0.0605 - acc: 0.984 - ETA: 0s - loss: 0.0599 - acc: 0.984 - ETA: 0s - loss: 0.0610 - acc: 0.984 - ETA: 0s - loss: 0.0609 - acc: 0.983 - ETA: 0s - loss: 0.0605 - acc: 0.984 - ETA: 0s - loss: 0.0601 - acc: 0.984 - ETA: 0s - loss: 0.0599 - acc: 0.984 - ETA: 0s - loss: 0.0602 - acc: 0.984 - ETA: 0s - loss: 0.0601 - acc: 0.984 - ETA: 0s - loss: 0.0597 - acc: 0.984 - ETA: 0s - loss: 0.0592 - acc: 0.984 - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0585 - acc: 0.984 - ETA: 0s - loss: 0.0582 - acc: 0.984 - ETA: 0s - loss: 0.0580 - acc: 0.984 - ETA: 0s - loss: 0.0577 - acc: 0.984 - ETA: 0s - loss: 0.0575 - acc: 0.984 - 8s 2ms/step - loss: 0.0575 - acc: 0.9848 - val_loss: 0.1154 - val_acc: 0.9784\n",
      "Epoch 20/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0090 - acc: 1.000 - ETA: 8s - loss: 0.0067 - acc: 1.000 - ETA: 18s - loss: 0.0076 - acc: 1.00 - ETA: 17s - loss: 0.0107 - acc: 1.00 - ETA: 16s - loss: 0.0101 - acc: 1.00 - ETA: 15s - loss: 0.0099 - acc: 1.00 - ETA: 15s - loss: 0.0088 - acc: 1.00 - ETA: 15s - loss: 0.0084 - acc: 1.00 - ETA: 14s - loss: 0.0123 - acc: 1.00 - ETA: 13s - loss: 0.0158 - acc: 0.99 - ETA: 12s - loss: 0.0150 - acc: 0.99 - ETA: 12s - loss: 0.0146 - acc: 0.99 - ETA: 12s - loss: 0.0167 - acc: 0.99 - ETA: 12s - loss: 0.0158 - acc: 0.99 - ETA: 12s - loss: 0.0155 - acc: 0.99 - ETA: 12s - loss: 0.0147 - acc: 0.99 - ETA: 11s - loss: 0.0153 - acc: 0.99 - ETA: 11s - loss: 0.0146 - acc: 0.99 - ETA: 11s - loss: 0.0191 - acc: 0.99 - ETA: 10s - loss: 0.0185 - acc: 0.99 - ETA: 10s - loss: 0.0286 - acc: 0.99 - ETA: 10s - loss: 0.0312 - acc: 0.99 - ETA: 10s - loss: 0.0399 - acc: 0.98 - ETA: 10s - loss: 0.0389 - acc: 0.98 - ETA: 10s - loss: 0.0380 - acc: 0.98 - ETA: 10s - loss: 0.0373 - acc: 0.98 - ETA: 10s - loss: 0.0376 - acc: 0.99 - ETA: 10s - loss: 0.0404 - acc: 0.98 - ETA: 9s - loss: 0.0382 - acc: 0.9893 - ETA: 9s - loss: 0.0371 - acc: 0.989 - ETA: 9s - loss: 0.0357 - acc: 0.990 - ETA: 9s - loss: 0.0360 - acc: 0.989 - ETA: 8s - loss: 0.0377 - acc: 0.988 - ETA: 8s - loss: 0.0389 - acc: 0.987 - ETA: 8s - loss: 0.0387 - acc: 0.986 - ETA: 8s - loss: 0.0379 - acc: 0.987 - ETA: 8s - loss: 0.0402 - acc: 0.985 - ETA: 7s - loss: 0.0396 - acc: 0.985 - ETA: 7s - loss: 0.0387 - acc: 0.986 - ETA: 7s - loss: 0.0383 - acc: 0.985 - ETA: 7s - loss: 0.0471 - acc: 0.980 - ETA: 7s - loss: 0.0456 - acc: 0.981 - ETA: 6s - loss: 0.0446 - acc: 0.982 - ETA: 6s - loss: 0.0446 - acc: 0.982 - ETA: 6s - loss: 0.0434 - acc: 0.983 - ETA: 6s - loss: 0.0423 - acc: 0.983 - ETA: 6s - loss: 0.0420 - acc: 0.983 - ETA: 6s - loss: 0.0409 - acc: 0.984 - ETA: 6s - loss: 0.0405 - acc: 0.984 - ETA: 6s - loss: 0.0401 - acc: 0.985 - ETA: 6s - loss: 0.0392 - acc: 0.985 - ETA: 6s - loss: 0.0391 - acc: 0.985 - ETA: 5s - loss: 0.0385 - acc: 0.986 - ETA: 5s - loss: 0.0378 - acc: 0.986 - ETA: 5s - loss: 0.0374 - acc: 0.986 - ETA: 5s - loss: 0.0378 - acc: 0.986 - ETA: 5s - loss: 0.0375 - acc: 0.986 - ETA: 5s - loss: 0.0371 - acc: 0.986 - ETA: 5s - loss: 0.0370 - acc: 0.986 - ETA: 5s - loss: 0.0366 - acc: 0.986 - ETA: 5s - loss: 0.0360 - acc: 0.987 - ETA: 5s - loss: 0.0354 - acc: 0.987 - ETA: 5s - loss: 0.0351 - acc: 0.987 - ETA: 5s - loss: 0.0347 - acc: 0.987 - ETA: 5s - loss: 0.0349 - acc: 0.987 - ETA: 5s - loss: 0.0346 - acc: 0.987 - ETA: 5s - loss: 0.0358 - acc: 0.987 - ETA: 5s - loss: 0.0352 - acc: 0.987 - ETA: 4s - loss: 0.0358 - acc: 0.987 - ETA: 4s - loss: 0.0363 - acc: 0.986 - ETA: 4s - loss: 0.0364 - acc: 0.986 - ETA: 4s - loss: 0.0358 - acc: 0.986 - ETA: 4s - loss: 0.0354 - acc: 0.987 - ETA: 4s - loss: 0.0351 - acc: 0.987 - ETA: 4s - loss: 0.0351 - acc: 0.987 - ETA: 4s - loss: 0.0388 - acc: 0.986 - ETA: 4s - loss: 0.0386 - acc: 0.986 - ETA: 4s - loss: 0.0382 - acc: 0.986 - ETA: 4s - loss: 0.0380 - acc: 0.987 - ETA: 3s - loss: 0.0381 - acc: 0.986 - ETA: 3s - loss: 0.0379 - acc: 0.986 - ETA: 3s - loss: 0.0375 - acc: 0.987 - ETA: 3s - loss: 0.0372 - acc: 0.987 - ETA: 3s - loss: 0.0369 - acc: 0.987 - ETA: 3s - loss: 0.0366 - acc: 0.987 - ETA: 3s - loss: 0.0363 - acc: 0.987 - ETA: 3s - loss: 0.0360 - acc: 0.987 - ETA: 3s - loss: 0.0356 - acc: 0.988 - ETA: 3s - loss: 0.0353 - acc: 0.988 - ETA: 3s - loss: 0.0349 - acc: 0.988 - ETA: 3s - loss: 0.0355 - acc: 0.987 - ETA: 3s - loss: 0.0353 - acc: 0.988 - ETA: 3s - loss: 0.0351 - acc: 0.988 - ETA: 3s - loss: 0.0348 - acc: 0.988 - ETA: 2s - loss: 0.0346 - acc: 0.988 - ETA: 2s - loss: 0.0341 - acc: 0.988 - ETA: 2s - loss: 0.0346 - acc: 0.988 - ETA: 2s - loss: 0.0342 - acc: 0.988 - ETA: 2s - loss: 0.0342 - acc: 0.988 - ETA: 2s - loss: 0.0338 - acc: 0.988 - ETA: 2s - loss: 0.0336 - acc: 0.988 - ETA: 2s - loss: 0.0334 - acc: 0.988 - ETA: 2s - loss: 0.0330 - acc: 0.989 - ETA: 2s - loss: 0.0327 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.988 - ETA: 1s - loss: 0.0334 - acc: 0.989 - ETA: 1s - loss: 0.0333 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0330 - acc: 0.989 - ETA: 1s - loss: 0.0329 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0328 - acc: 0.989 - ETA: 1s - loss: 0.0327 - acc: 0.989 - ETA: 1s - loss: 0.0326 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.988 - ETA: 1s - loss: 0.0356 - acc: 0.988 - ETA: 1s - loss: 0.0355 - acc: 0.989 - ETA: 1s - loss: 0.0353 - acc: 0.989 - ETA: 1s - loss: 0.0352 - acc: 0.989 - ETA: 1s - loss: 0.0350 - acc: 0.989 - ETA: 0s - loss: 0.0349 - acc: 0.989 - ETA: 0s - loss: 0.0349 - acc: 0.989 - ETA: 0s - loss: 0.0347 - acc: 0.989 - ETA: 0s - loss: 0.0345 - acc: 0.989 - ETA: 0s - loss: 0.0343 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0336 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0345 - acc: 0.989 - ETA: 0s - loss: 0.0344 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - 10s 3ms/step - loss: 0.0365 - acc: 0.9890 - val_loss: 0.1397 - val_acc: 0.9632\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 10s - loss: 0.0045 - acc: 1.00 - ETA: 10s - loss: 0.0072 - acc: 1.00 - ETA: 10s - loss: 0.0137 - acc: 1.00 - ETA: 9s - loss: 0.0107 - acc: 1.0000 - ETA: 9s - loss: 0.0113 - acc: 1.000 - ETA: 9s - loss: 0.0101 - acc: 1.000 - ETA: 8s - loss: 0.0091 - acc: 1.000 - ETA: 8s - loss: 0.0127 - acc: 1.000 - ETA: 9s - loss: 0.0119 - acc: 1.000 - ETA: 9s - loss: 0.0142 - acc: 1.000 - ETA: 8s - loss: 0.0131 - acc: 1.000 - ETA: 8s - loss: 0.0161 - acc: 0.996 - ETA: 8s - loss: 0.0172 - acc: 0.993 - ETA: 8s - loss: 0.0165 - acc: 0.994 - ETA: 8s - loss: 0.0164 - acc: 0.994 - ETA: 8s - loss: 0.0170 - acc: 0.994 - ETA: 8s - loss: 0.0173 - acc: 0.995 - ETA: 8s - loss: 0.0264 - acc: 0.993 - ETA: 8s - loss: 0.0255 - acc: 0.993 - ETA: 8s - loss: 0.0241 - acc: 0.994 - ETA: 7s - loss: 0.0229 - acc: 0.994 - ETA: 7s - loss: 0.0229 - acc: 0.994 - ETA: 7s - loss: 0.0223 - acc: 0.994 - ETA: 7s - loss: 0.0217 - acc: 0.994 - ETA: 7s - loss: 0.0211 - acc: 0.995 - ETA: 7s - loss: 0.0207 - acc: 0.995 - ETA: 7s - loss: 0.0204 - acc: 0.995 - ETA: 7s - loss: 0.0204 - acc: 0.995 - ETA: 7s - loss: 0.0202 - acc: 0.995 - ETA: 7s - loss: 0.0402 - acc: 0.992 - ETA: 7s - loss: 0.0393 - acc: 0.992 - ETA: 7s - loss: 0.0388 - acc: 0.993 - ETA: 7s - loss: 0.0380 - acc: 0.993 - ETA: 7s - loss: 0.0372 - acc: 0.993 - ETA: 7s - loss: 0.0365 - acc: 0.993 - ETA: 7s - loss: 0.0359 - acc: 0.993 - ETA: 7s - loss: 0.0358 - acc: 0.993 - ETA: 7s - loss: 0.0359 - acc: 0.993 - ETA: 7s - loss: 0.0368 - acc: 0.992 - ETA: 7s - loss: 0.0362 - acc: 0.992 - ETA: 7s - loss: 0.0369 - acc: 0.992 - ETA: 7s - loss: 0.0363 - acc: 0.992 - ETA: 7s - loss: 0.0357 - acc: 0.992 - ETA: 7s - loss: 0.0353 - acc: 0.992 - ETA: 7s - loss: 0.0347 - acc: 0.992 - ETA: 7s - loss: 0.0343 - acc: 0.992 - ETA: 7s - loss: 0.0338 - acc: 0.992 - ETA: 7s - loss: 0.0365 - acc: 0.991 - ETA: 7s - loss: 0.0359 - acc: 0.992 - ETA: 7s - loss: 0.0354 - acc: 0.992 - ETA: 7s - loss: 0.0354 - acc: 0.992 - ETA: 7s - loss: 0.0351 - acc: 0.992 - ETA: 7s - loss: 0.0344 - acc: 0.992 - ETA: 6s - loss: 0.0375 - acc: 0.990 - ETA: 6s - loss: 0.0372 - acc: 0.990 - ETA: 6s - loss: 0.0379 - acc: 0.989 - ETA: 6s - loss: 0.0373 - acc: 0.989 - ETA: 6s - loss: 0.0373 - acc: 0.989 - ETA: 6s - loss: 0.0365 - acc: 0.990 - ETA: 6s - loss: 0.0361 - acc: 0.990 - ETA: 6s - loss: 0.0357 - acc: 0.990 - ETA: 6s - loss: 0.0354 - acc: 0.990 - ETA: 6s - loss: 0.0352 - acc: 0.990 - ETA: 6s - loss: 0.0345 - acc: 0.991 - ETA: 6s - loss: 0.0341 - acc: 0.991 - ETA: 6s - loss: 0.0338 - acc: 0.991 - ETA: 6s - loss: 0.0334 - acc: 0.991 - ETA: 6s - loss: 0.0335 - acc: 0.991 - ETA: 6s - loss: 0.0335 - acc: 0.991 - ETA: 5s - loss: 0.0330 - acc: 0.991 - ETA: 5s - loss: 0.0327 - acc: 0.991 - ETA: 5s - loss: 0.0323 - acc: 0.991 - ETA: 5s - loss: 0.0320 - acc: 0.992 - ETA: 5s - loss: 0.0317 - acc: 0.992 - ETA: 5s - loss: 0.0316 - acc: 0.992 - ETA: 5s - loss: 0.0311 - acc: 0.992 - ETA: 5s - loss: 0.0310 - acc: 0.992 - ETA: 5s - loss: 0.0309 - acc: 0.992 - ETA: 5s - loss: 0.0308 - acc: 0.992 - ETA: 5s - loss: 0.0306 - acc: 0.992 - ETA: 5s - loss: 0.0317 - acc: 0.992 - ETA: 5s - loss: 0.0314 - acc: 0.992 - ETA: 5s - loss: 0.0314 - acc: 0.992 - ETA: 5s - loss: 0.0312 - acc: 0.992 - ETA: 5s - loss: 0.0309 - acc: 0.992 - ETA: 4s - loss: 0.0312 - acc: 0.992 - ETA: 4s - loss: 0.0310 - acc: 0.992 - ETA: 4s - loss: 0.0308 - acc: 0.992 - ETA: 4s - loss: 0.0305 - acc: 0.992 - ETA: 4s - loss: 0.0303 - acc: 0.992 - ETA: 4s - loss: 0.0310 - acc: 0.991 - ETA: 4s - loss: 0.0308 - acc: 0.992 - ETA: 4s - loss: 0.0306 - acc: 0.992 - ETA: 4s - loss: 0.0306 - acc: 0.992 - ETA: 4s - loss: 0.0301 - acc: 0.992 - ETA: 4s - loss: 0.0300 - acc: 0.992 - ETA: 4s - loss: 0.0302 - acc: 0.992 - ETA: 4s - loss: 0.0300 - acc: 0.992 - ETA: 4s - loss: 0.0296 - acc: 0.992 - ETA: 3s - loss: 0.0295 - acc: 0.992 - ETA: 3s - loss: 0.0295 - acc: 0.992 - ETA: 3s - loss: 0.0292 - acc: 0.992 - ETA: 3s - loss: 0.0291 - acc: 0.993 - ETA: 3s - loss: 0.0289 - acc: 0.993 - ETA: 3s - loss: 0.0286 - acc: 0.993 - ETA: 3s - loss: 0.0283 - acc: 0.993 - ETA: 3s - loss: 0.0282 - acc: 0.993 - ETA: 3s - loss: 0.0280 - acc: 0.993 - ETA: 3s - loss: 0.0278 - acc: 0.993 - ETA: 3s - loss: 0.0277 - acc: 0.993 - ETA: 3s - loss: 0.0280 - acc: 0.993 - ETA: 2s - loss: 0.0277 - acc: 0.993 - ETA: 2s - loss: 0.0275 - acc: 0.993 - ETA: 2s - loss: 0.0282 - acc: 0.993 - ETA: 2s - loss: 0.0281 - acc: 0.993 - ETA: 2s - loss: 0.0280 - acc: 0.993 - ETA: 2s - loss: 0.0278 - acc: 0.993 - ETA: 2s - loss: 0.0275 - acc: 0.993 - ETA: 2s - loss: 0.0282 - acc: 0.992 - ETA: 2s - loss: 0.0282 - acc: 0.992 - ETA: 2s - loss: 0.0283 - acc: 0.992 - ETA: 1s - loss: 0.0280 - acc: 0.992 - ETA: 1s - loss: 0.0277 - acc: 0.992 - ETA: 1s - loss: 0.0276 - acc: 0.992 - ETA: 1s - loss: 0.0273 - acc: 0.992 - ETA: 1s - loss: 0.0271 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 1s - loss: 0.0272 - acc: 0.992 - ETA: 1s - loss: 0.0270 - acc: 0.992 - ETA: 1s - loss: 0.0269 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 1s - loss: 0.0269 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - ETA: 0s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0266 - acc: 0.992 - ETA: 0s - loss: 0.0264 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.992 - ETA: 0s - loss: 0.0268 - acc: 0.992 - ETA: 0s - loss: 0.0276 - acc: 0.991 - ETA: 0s - loss: 0.0273 - acc: 0.991 - 10s 3ms/step - loss: 0.0272 - acc: 0.9918 - val_loss: 0.1259 - val_acc: 0.9748\n",
      "Epoch 22/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0180 - acc: 1.000 - ETA: 7s - loss: 0.0336 - acc: 1.000 - ETA: 7s - loss: 0.0215 - acc: 1.000 - ETA: 7s - loss: 0.0170 - acc: 1.000 - ETA: 7s - loss: 0.0147 - acc: 1.000 - ETA: 7s - loss: 0.0137 - acc: 1.000 - ETA: 7s - loss: 0.0126 - acc: 1.000 - ETA: 7s - loss: 0.0118 - acc: 1.000 - ETA: 7s - loss: 0.0195 - acc: 0.996 - ETA: 7s - loss: 0.0186 - acc: 0.996 - ETA: 7s - loss: 0.0202 - acc: 0.993 - ETA: 7s - loss: 0.0220 - acc: 0.991 - ETA: 7s - loss: 0.0237 - acc: 0.989 - ETA: 7s - loss: 0.0235 - acc: 0.990 - ETA: 7s - loss: 0.0221 - acc: 0.990 - ETA: 7s - loss: 0.0219 - acc: 0.991 - ETA: 7s - loss: 0.0215 - acc: 0.991 - ETA: 7s - loss: 0.0229 - acc: 0.990 - ETA: 7s - loss: 0.0228 - acc: 0.991 - ETA: 6s - loss: 0.0217 - acc: 0.991 - ETA: 6s - loss: 0.0211 - acc: 0.992 - ETA: 6s - loss: 0.0212 - acc: 0.992 - ETA: 6s - loss: 0.0284 - acc: 0.991 - ETA: 6s - loss: 0.0278 - acc: 0.991 - ETA: 6s - loss: 0.0269 - acc: 0.992 - ETA: 6s - loss: 0.0262 - acc: 0.992 - ETA: 6s - loss: 0.0257 - acc: 0.992 - ETA: 6s - loss: 0.0250 - acc: 0.992 - ETA: 6s - loss: 0.0244 - acc: 0.993 - ETA: 6s - loss: 0.0247 - acc: 0.993 - ETA: 6s - loss: 0.0245 - acc: 0.993 - ETA: 6s - loss: 0.0238 - acc: 0.993 - ETA: 5s - loss: 0.0234 - acc: 0.993 - ETA: 5s - loss: 0.0228 - acc: 0.994 - ETA: 5s - loss: 0.0227 - acc: 0.994 - ETA: 5s - loss: 0.0233 - acc: 0.993 - ETA: 5s - loss: 0.0228 - acc: 0.993 - ETA: 5s - loss: 0.0225 - acc: 0.993 - ETA: 5s - loss: 0.0221 - acc: 0.994 - ETA: 5s - loss: 0.0218 - acc: 0.994 - ETA: 5s - loss: 0.0216 - acc: 0.994 - ETA: 5s - loss: 0.0212 - acc: 0.994 - ETA: 5s - loss: 0.0209 - acc: 0.994 - ETA: 5s - loss: 0.0214 - acc: 0.993 - ETA: 5s - loss: 0.0210 - acc: 0.993 - ETA: 5s - loss: 0.0208 - acc: 0.994 - ETA: 5s - loss: 0.0208 - acc: 0.994 - ETA: 4s - loss: 0.0205 - acc: 0.994 - ETA: 4s - loss: 0.0211 - acc: 0.993 - ETA: 4s - loss: 0.0207 - acc: 0.993 - ETA: 4s - loss: 0.0204 - acc: 0.994 - ETA: 4s - loss: 0.0204 - acc: 0.994 - ETA: 4s - loss: 0.0231 - acc: 0.992 - ETA: 4s - loss: 0.0227 - acc: 0.993 - ETA: 4s - loss: 0.0230 - acc: 0.993 - ETA: 4s - loss: 0.0253 - acc: 0.991 - ETA: 4s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0248 - acc: 0.992 - ETA: 4s - loss: 0.0246 - acc: 0.992 - ETA: 4s - loss: 0.0267 - acc: 0.991 - ETA: 4s - loss: 0.0263 - acc: 0.991 - ETA: 4s - loss: 0.0261 - acc: 0.992 - ETA: 3s - loss: 0.0258 - acc: 0.992 - ETA: 3s - loss: 0.0262 - acc: 0.991 - ETA: 3s - loss: 0.0261 - acc: 0.991 - ETA: 3s - loss: 0.0260 - acc: 0.991 - ETA: 3s - loss: 0.0256 - acc: 0.992 - ETA: 3s - loss: 0.0255 - acc: 0.992 - ETA: 3s - loss: 0.0255 - acc: 0.992 - ETA: 3s - loss: 0.0253 - acc: 0.992 - ETA: 3s - loss: 0.0251 - acc: 0.992 - ETA: 3s - loss: 0.0248 - acc: 0.992 - ETA: 3s - loss: 0.0245 - acc: 0.992 - ETA: 3s - loss: 0.0243 - acc: 0.992 - ETA: 3s - loss: 0.0240 - acc: 0.992 - ETA: 3s - loss: 0.0237 - acc: 0.993 - ETA: 3s - loss: 0.0237 - acc: 0.993 - ETA: 3s - loss: 0.0235 - acc: 0.993 - ETA: 2s - loss: 0.0233 - acc: 0.993 - ETA: 2s - loss: 0.0231 - acc: 0.993 - ETA: 2s - loss: 0.0241 - acc: 0.992 - ETA: 2s - loss: 0.0270 - acc: 0.992 - ETA: 2s - loss: 0.0275 - acc: 0.991 - ETA: 2s - loss: 0.0273 - acc: 0.991 - ETA: 2s - loss: 0.0272 - acc: 0.991 - ETA: 2s - loss: 0.0274 - acc: 0.991 - ETA: 2s - loss: 0.0272 - acc: 0.992 - ETA: 2s - loss: 0.0272 - acc: 0.992 - ETA: 2s - loss: 0.0269 - acc: 0.992 - ETA: 2s - loss: 0.0266 - acc: 0.992 - ETA: 2s - loss: 0.0265 - acc: 0.992 - ETA: 2s - loss: 0.0264 - acc: 0.992 - ETA: 2s - loss: 0.0266 - acc: 0.992 - ETA: 2s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0268 - acc: 0.991 - ETA: 1s - loss: 0.0266 - acc: 0.991 - ETA: 1s - loss: 0.0263 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0268 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0265 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.991 - ETA: 1s - loss: 0.0263 - acc: 0.991 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - 9s 3ms/step - loss: 0.0245 - acc: 0.9927 - val_loss: 0.1371 - val_acc: 0.9755\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0454 - acc: 1.000 - ETA: 8s - loss: 0.0183 - acc: 1.000 - ETA: 8s - loss: 0.0327 - acc: 0.987 - ETA: 8s - loss: 0.0273 - acc: 0.989 - ETA: 8s - loss: 0.0240 - acc: 0.992 - ETA: 8s - loss: 0.0205 - acc: 0.993 - ETA: 8s - loss: 0.0177 - acc: 0.994 - ETA: 8s - loss: 0.0176 - acc: 0.995 - ETA: 7s - loss: 0.0364 - acc: 0.991 - ETA: 7s - loss: 0.0327 - acc: 0.992 - ETA: 7s - loss: 0.0331 - acc: 0.993 - ETA: 7s - loss: 0.0302 - acc: 0.994 - ETA: 7s - loss: 0.0282 - acc: 0.994 - ETA: 7s - loss: 0.0287 - acc: 0.992 - ETA: 7s - loss: 0.0298 - acc: 0.990 - ETA: 7s - loss: 0.0283 - acc: 0.991 - ETA: 7s - loss: 0.0272 - acc: 0.991 - ETA: 7s - loss: 0.0256 - acc: 0.992 - ETA: 7s - loss: 0.0253 - acc: 0.992 - ETA: 6s - loss: 0.0252 - acc: 0.993 - ETA: 6s - loss: 0.0240 - acc: 0.993 - ETA: 6s - loss: 0.0250 - acc: 0.992 - ETA: 6s - loss: 0.0245 - acc: 0.992 - ETA: 6s - loss: 0.0234 - acc: 0.992 - ETA: 6s - loss: 0.0223 - acc: 0.992 - ETA: 6s - loss: 0.0214 - acc: 0.993 - ETA: 6s - loss: 0.0211 - acc: 0.993 - ETA: 6s - loss: 0.0235 - acc: 0.992 - ETA: 6s - loss: 0.0228 - acc: 0.992 - ETA: 6s - loss: 0.0313 - acc: 0.989 - ETA: 6s - loss: 0.0309 - acc: 0.989 - ETA: 6s - loss: 0.0300 - acc: 0.990 - ETA: 6s - loss: 0.0292 - acc: 0.990 - ETA: 6s - loss: 0.0282 - acc: 0.990 - ETA: 6s - loss: 0.0277 - acc: 0.990 - ETA: 6s - loss: 0.0269 - acc: 0.991 - ETA: 5s - loss: 0.0282 - acc: 0.989 - ETA: 5s - loss: 0.0279 - acc: 0.989 - ETA: 5s - loss: 0.0272 - acc: 0.990 - ETA: 5s - loss: 0.0267 - acc: 0.990 - ETA: 5s - loss: 0.0264 - acc: 0.990 - ETA: 5s - loss: 0.0259 - acc: 0.990 - ETA: 5s - loss: 0.0259 - acc: 0.991 - ETA: 5s - loss: 0.0261 - acc: 0.990 - ETA: 5s - loss: 0.0263 - acc: 0.990 - ETA: 5s - loss: 0.0259 - acc: 0.990 - ETA: 5s - loss: 0.0263 - acc: 0.989 - ETA: 4s - loss: 0.0303 - acc: 0.989 - ETA: 4s - loss: 0.0297 - acc: 0.989 - ETA: 4s - loss: 0.0294 - acc: 0.989 - ETA: 4s - loss: 0.0291 - acc: 0.989 - ETA: 4s - loss: 0.0288 - acc: 0.989 - ETA: 4s - loss: 0.0282 - acc: 0.990 - ETA: 4s - loss: 0.0279 - acc: 0.990 - ETA: 4s - loss: 0.0277 - acc: 0.990 - ETA: 4s - loss: 0.0274 - acc: 0.990 - ETA: 4s - loss: 0.0272 - acc: 0.990 - ETA: 4s - loss: 0.0269 - acc: 0.990 - ETA: 4s - loss: 0.0302 - acc: 0.989 - ETA: 4s - loss: 0.0300 - acc: 0.989 - ETA: 4s - loss: 0.0300 - acc: 0.989 - ETA: 4s - loss: 0.0297 - acc: 0.989 - ETA: 4s - loss: 0.0294 - acc: 0.989 - ETA: 4s - loss: 0.0291 - acc: 0.990 - ETA: 4s - loss: 0.0286 - acc: 0.990 - ETA: 4s - loss: 0.0283 - acc: 0.990 - ETA: 4s - loss: 0.0280 - acc: 0.990 - ETA: 4s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0284 - acc: 0.990 - ETA: 3s - loss: 0.0281 - acc: 0.990 - ETA: 3s - loss: 0.0284 - acc: 0.990 - ETA: 3s - loss: 0.0280 - acc: 0.990 - ETA: 3s - loss: 0.0276 - acc: 0.990 - ETA: 3s - loss: 0.0273 - acc: 0.990 - ETA: 3s - loss: 0.0272 - acc: 0.990 - ETA: 3s - loss: 0.0268 - acc: 0.990 - ETA: 3s - loss: 0.0265 - acc: 0.991 - ETA: 3s - loss: 0.0262 - acc: 0.991 - ETA: 2s - loss: 0.0261 - acc: 0.991 - ETA: 2s - loss: 0.0266 - acc: 0.991 - ETA: 2s - loss: 0.0263 - acc: 0.991 - ETA: 2s - loss: 0.0260 - acc: 0.991 - ETA: 2s - loss: 0.0257 - acc: 0.991 - ETA: 2s - loss: 0.0254 - acc: 0.991 - ETA: 2s - loss: 0.0252 - acc: 0.991 - ETA: 2s - loss: 0.0251 - acc: 0.991 - ETA: 2s - loss: 0.0250 - acc: 0.991 - ETA: 2s - loss: 0.0248 - acc: 0.991 - ETA: 2s - loss: 0.0253 - acc: 0.991 - ETA: 2s - loss: 0.0250 - acc: 0.992 - ETA: 2s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 1s - loss: 0.0245 - acc: 0.992 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0241 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.992 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0234 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0232 - acc: 0.993 - ETA: 1s - loss: 0.0229 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.993 - ETA: 0s - loss: 0.0216 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - 9s 3ms/step - loss: 0.0209 - acc: 0.9939 - val_loss: 0.1155 - val_acc: 0.9813\n",
      "Epoch 24/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 4.3099e-04 - acc: 1.000 - ETA: 7s - loss: 0.0059 - acc: 1.0000    - ETA: 7s - loss: 0.0065 - acc: 1.000 - ETA: 7s - loss: 0.0048 - acc: 1.000 - ETA: 7s - loss: 0.0082 - acc: 1.000 - ETA: 7s - loss: 0.0363 - acc: 0.988 - ETA: 7s - loss: 0.0312 - acc: 0.990 - ETA: 7s - loss: 0.0290 - acc: 0.991 - ETA: 7s - loss: 0.0476 - acc: 0.985 - ETA: 7s - loss: 0.0451 - acc: 0.986 - ETA: 7s - loss: 0.0409 - acc: 0.987 - ETA: 7s - loss: 0.0375 - acc: 0.988 - ETA: 7s - loss: 0.0346 - acc: 0.989 - ETA: 7s - loss: 0.0326 - acc: 0.990 - ETA: 7s - loss: 0.0349 - acc: 0.988 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0316 - acc: 0.990 - ETA: 6s - loss: 0.0299 - acc: 0.990 - ETA: 6s - loss: 0.0329 - acc: 0.987 - ETA: 6s - loss: 0.0313 - acc: 0.988 - ETA: 6s - loss: 0.0313 - acc: 0.987 - ETA: 6s - loss: 0.0441 - acc: 0.985 - ETA: 6s - loss: 0.0423 - acc: 0.985 - ETA: 6s - loss: 0.0407 - acc: 0.986 - ETA: 6s - loss: 0.0392 - acc: 0.987 - ETA: 6s - loss: 0.0377 - acc: 0.987 - ETA: 6s - loss: 0.0367 - acc: 0.988 - ETA: 6s - loss: 0.0365 - acc: 0.987 - ETA: 5s - loss: 0.0392 - acc: 0.986 - ETA: 5s - loss: 0.0391 - acc: 0.986 - ETA: 5s - loss: 0.0408 - acc: 0.985 - ETA: 5s - loss: 0.0395 - acc: 0.985 - ETA: 5s - loss: 0.0383 - acc: 0.986 - ETA: 5s - loss: 0.0376 - acc: 0.986 - ETA: 5s - loss: 0.0366 - acc: 0.987 - ETA: 5s - loss: 0.0375 - acc: 0.986 - ETA: 5s - loss: 0.0366 - acc: 0.987 - ETA: 5s - loss: 0.0360 - acc: 0.987 - ETA: 5s - loss: 0.0351 - acc: 0.987 - ETA: 5s - loss: 0.0342 - acc: 0.988 - ETA: 4s - loss: 0.0334 - acc: 0.988 - ETA: 4s - loss: 0.0329 - acc: 0.988 - ETA: 4s - loss: 0.0368 - acc: 0.987 - ETA: 4s - loss: 0.0369 - acc: 0.986 - ETA: 4s - loss: 0.0362 - acc: 0.987 - ETA: 4s - loss: 0.0356 - acc: 0.987 - ETA: 4s - loss: 0.0351 - acc: 0.987 - ETA: 4s - loss: 0.0344 - acc: 0.988 - ETA: 4s - loss: 0.0337 - acc: 0.988 - ETA: 4s - loss: 0.0333 - acc: 0.988 - ETA: 4s - loss: 0.0327 - acc: 0.988 - ETA: 4s - loss: 0.0323 - acc: 0.989 - ETA: 4s - loss: 0.0317 - acc: 0.989 - ETA: 3s - loss: 0.0313 - acc: 0.989 - ETA: 3s - loss: 0.0307 - acc: 0.989 - ETA: 3s - loss: 0.0303 - acc: 0.989 - ETA: 3s - loss: 0.0301 - acc: 0.989 - ETA: 3s - loss: 0.0297 - acc: 0.990 - ETA: 3s - loss: 0.0295 - acc: 0.990 - ETA: 3s - loss: 0.0291 - acc: 0.990 - ETA: 3s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0284 - acc: 0.990 - ETA: 3s - loss: 0.0281 - acc: 0.990 - ETA: 3s - loss: 0.0285 - acc: 0.990 - ETA: 3s - loss: 0.0283 - acc: 0.990 - ETA: 3s - loss: 0.0278 - acc: 0.990 - ETA: 3s - loss: 0.0276 - acc: 0.990 - ETA: 2s - loss: 0.0277 - acc: 0.990 - ETA: 2s - loss: 0.0275 - acc: 0.991 - ETA: 2s - loss: 0.0274 - acc: 0.991 - ETA: 2s - loss: 0.0272 - acc: 0.991 - ETA: 2s - loss: 0.0270 - acc: 0.991 - ETA: 2s - loss: 0.0266 - acc: 0.991 - ETA: 2s - loss: 0.0275 - acc: 0.990 - ETA: 2s - loss: 0.0271 - acc: 0.990 - ETA: 2s - loss: 0.0268 - acc: 0.990 - ETA: 2s - loss: 0.0264 - acc: 0.990 - ETA: 2s - loss: 0.0268 - acc: 0.990 - ETA: 2s - loss: 0.0266 - acc: 0.990 - ETA: 2s - loss: 0.0264 - acc: 0.990 - ETA: 2s - loss: 0.0265 - acc: 0.991 - ETA: 2s - loss: 0.0266 - acc: 0.991 - ETA: 2s - loss: 0.0266 - acc: 0.991 - ETA: 1s - loss: 0.0264 - acc: 0.991 - ETA: 1s - loss: 0.0262 - acc: 0.991 - ETA: 1s - loss: 0.0260 - acc: 0.991 - ETA: 1s - loss: 0.0257 - acc: 0.991 - ETA: 1s - loss: 0.0255 - acc: 0.991 - ETA: 1s - loss: 0.0252 - acc: 0.991 - ETA: 1s - loss: 0.0250 - acc: 0.991 - ETA: 1s - loss: 0.0273 - acc: 0.991 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0269 - acc: 0.991 - ETA: 1s - loss: 0.0267 - acc: 0.991 - ETA: 1s - loss: 0.0273 - acc: 0.991 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 0s - loss: 0.0270 - acc: 0.991 - ETA: 0s - loss: 0.0285 - acc: 0.990 - ETA: 0s - loss: 0.0283 - acc: 0.990 - ETA: 0s - loss: 0.0282 - acc: 0.990 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0274 - acc: 0.991 - ETA: 0s - loss: 0.0272 - acc: 0.991 - ETA: 0s - loss: 0.0272 - acc: 0.991 - ETA: 0s - loss: 0.0269 - acc: 0.991 - ETA: 0s - loss: 0.0267 - acc: 0.991 - ETA: 0s - loss: 0.0264 - acc: 0.991 - ETA: 0s - loss: 0.0262 - acc: 0.991 - 9s 3ms/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.1294 - val_acc: 0.9618\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0115 - acc: 1.000 - ETA: 9s - loss: 0.0074 - acc: 1.000 - ETA: 8s - loss: 0.1456 - acc: 0.968 - ETA: 8s - loss: 0.1066 - acc: 0.968 - ETA: 8s - loss: 0.0900 - acc: 0.976 - ETA: 8s - loss: 0.0763 - acc: 0.981 - ETA: 8s - loss: 0.0643 - acc: 0.984 - ETA: 7s - loss: 0.0569 - acc: 0.986 - ETA: 7s - loss: 0.0554 - acc: 0.984 - ETA: 8s - loss: 0.0523 - acc: 0.985 - ETA: 8s - loss: 0.0495 - acc: 0.986 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0484 - acc: 0.985 - ETA: 8s - loss: 0.0473 - acc: 0.985 - ETA: 8s - loss: 0.0436 - acc: 0.987 - ETA: 7s - loss: 0.0414 - acc: 0.988 - ETA: 7s - loss: 0.0404 - acc: 0.986 - ETA: 7s - loss: 0.0391 - acc: 0.987 - ETA: 7s - loss: 0.0383 - acc: 0.987 - ETA: 7s - loss: 0.0373 - acc: 0.988 - ETA: 7s - loss: 0.0379 - acc: 0.987 - ETA: 7s - loss: 0.0361 - acc: 0.987 - ETA: 7s - loss: 0.0345 - acc: 0.988 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 7s - loss: 0.0318 - acc: 0.989 - ETA: 7s - loss: 0.0307 - acc: 0.990 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0321 - acc: 0.989 - ETA: 6s - loss: 0.0316 - acc: 0.990 - ETA: 6s - loss: 0.0305 - acc: 0.990 - ETA: 6s - loss: 0.0295 - acc: 0.990 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0312 - acc: 0.990 - ETA: 6s - loss: 0.0307 - acc: 0.990 - ETA: 6s - loss: 0.0303 - acc: 0.990 - ETA: 6s - loss: 0.0294 - acc: 0.990 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0298 - acc: 0.990 - ETA: 5s - loss: 0.0291 - acc: 0.990 - ETA: 5s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0280 - acc: 0.991 - ETA: 5s - loss: 0.0287 - acc: 0.990 - ETA: 5s - loss: 0.0284 - acc: 0.990 - ETA: 5s - loss: 0.0280 - acc: 0.990 - ETA: 5s - loss: 0.0459 - acc: 0.988 - ETA: 5s - loss: 0.0449 - acc: 0.988 - ETA: 5s - loss: 0.0439 - acc: 0.989 - ETA: 5s - loss: 0.0441 - acc: 0.988 - ETA: 5s - loss: 0.0435 - acc: 0.988 - ETA: 5s - loss: 0.0431 - acc: 0.989 - ETA: 5s - loss: 0.0432 - acc: 0.989 - ETA: 5s - loss: 0.0442 - acc: 0.988 - ETA: 5s - loss: 0.0441 - acc: 0.988 - ETA: 5s - loss: 0.0469 - acc: 0.988 - ETA: 5s - loss: 0.0476 - acc: 0.987 - ETA: 5s - loss: 0.0471 - acc: 0.987 - ETA: 5s - loss: 0.0467 - acc: 0.987 - ETA: 5s - loss: 0.0459 - acc: 0.988 - ETA: 4s - loss: 0.0455 - acc: 0.988 - ETA: 4s - loss: 0.0466 - acc: 0.987 - ETA: 4s - loss: 0.0463 - acc: 0.987 - ETA: 4s - loss: 0.0458 - acc: 0.988 - ETA: 4s - loss: 0.0454 - acc: 0.988 - ETA: 4s - loss: 0.0450 - acc: 0.988 - ETA: 4s - loss: 0.0446 - acc: 0.988 - ETA: 4s - loss: 0.0442 - acc: 0.988 - ETA: 4s - loss: 0.0434 - acc: 0.988 - ETA: 4s - loss: 0.0433 - acc: 0.988 - ETA: 4s - loss: 0.0426 - acc: 0.988 - ETA: 4s - loss: 0.0424 - acc: 0.988 - ETA: 4s - loss: 0.0418 - acc: 0.988 - ETA: 4s - loss: 0.0447 - acc: 0.987 - ETA: 4s - loss: 0.0445 - acc: 0.986 - ETA: 3s - loss: 0.0440 - acc: 0.987 - ETA: 3s - loss: 0.0434 - acc: 0.987 - ETA: 3s - loss: 0.0428 - acc: 0.987 - ETA: 3s - loss: 0.0429 - acc: 0.987 - ETA: 3s - loss: 0.0431 - acc: 0.986 - ETA: 3s - loss: 0.0456 - acc: 0.985 - ETA: 3s - loss: 0.0453 - acc: 0.985 - ETA: 3s - loss: 0.0447 - acc: 0.986 - ETA: 3s - loss: 0.0440 - acc: 0.986 - ETA: 3s - loss: 0.0434 - acc: 0.986 - ETA: 3s - loss: 0.0430 - acc: 0.986 - ETA: 3s - loss: 0.0425 - acc: 0.986 - ETA: 2s - loss: 0.0419 - acc: 0.987 - ETA: 2s - loss: 0.0415 - acc: 0.987 - ETA: 2s - loss: 0.0410 - acc: 0.987 - ETA: 2s - loss: 0.0407 - acc: 0.987 - ETA: 2s - loss: 0.0405 - acc: 0.987 - ETA: 2s - loss: 0.0400 - acc: 0.987 - ETA: 2s - loss: 0.0398 - acc: 0.987 - ETA: 2s - loss: 0.0400 - acc: 0.987 - ETA: 2s - loss: 0.0398 - acc: 0.987 - ETA: 2s - loss: 0.0397 - acc: 0.987 - ETA: 2s - loss: 0.0395 - acc: 0.987 - ETA: 2s - loss: 0.0393 - acc: 0.987 - ETA: 2s - loss: 0.0388 - acc: 0.988 - ETA: 2s - loss: 0.0474 - acc: 0.986 - ETA: 2s - loss: 0.0470 - acc: 0.986 - ETA: 1s - loss: 0.0474 - acc: 0.986 - ETA: 1s - loss: 0.0469 - acc: 0.986 - ETA: 1s - loss: 0.0468 - acc: 0.986 - ETA: 1s - loss: 0.0479 - acc: 0.986 - ETA: 1s - loss: 0.0474 - acc: 0.986 - ETA: 1s - loss: 0.0471 - acc: 0.986 - ETA: 1s - loss: 0.0469 - acc: 0.986 - ETA: 1s - loss: 0.0467 - acc: 0.986 - ETA: 1s - loss: 0.0469 - acc: 0.986 - ETA: 1s - loss: 0.0469 - acc: 0.986 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.986 - ETA: 1s - loss: 0.0460 - acc: 0.986 - ETA: 1s - loss: 0.0458 - acc: 0.986 - ETA: 0s - loss: 0.0455 - acc: 0.986 - ETA: 0s - loss: 0.0453 - acc: 0.986 - ETA: 0s - loss: 0.0450 - acc: 0.987 - ETA: 0s - loss: 0.0446 - acc: 0.987 - ETA: 0s - loss: 0.0442 - acc: 0.987 - ETA: 0s - loss: 0.0439 - acc: 0.987 - ETA: 0s - loss: 0.0437 - acc: 0.987 - ETA: 0s - loss: 0.0435 - acc: 0.987 - ETA: 0s - loss: 0.0433 - acc: 0.987 - ETA: 0s - loss: 0.0431 - acc: 0.987 - ETA: 0s - loss: 0.0429 - acc: 0.987 - ETA: 0s - loss: 0.0425 - acc: 0.987 - ETA: 0s - loss: 0.0423 - acc: 0.987 - ETA: 0s - loss: 0.0425 - acc: 0.987 - ETA: 0s - loss: 0.0423 - acc: 0.987 - ETA: 0s - loss: 0.0421 - acc: 0.987 - 10s 3ms/step - loss: 0.0421 - acc: 0.9878 - val_loss: 0.0635 - val_acc: 0.9791\n",
      "Epoch 26/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 7.6143e-04 - acc: 1.000 - ETA: 7s - loss: 0.0051 - acc: 1.0000    - ETA: 7s - loss: 0.0033 - acc: 1.000 - ETA: 7s - loss: 0.0034 - acc: 1.000 - ETA: 7s - loss: 0.0029 - acc: 1.000 - ETA: 7s - loss: 0.0106 - acc: 0.993 - ETA: 8s - loss: 0.0102 - acc: 0.994 - ETA: 8s - loss: 0.0136 - acc: 0.995 - ETA: 8s - loss: 0.0132 - acc: 0.995 - ETA: 8s - loss: 0.0126 - acc: 0.995 - ETA: 8s - loss: 0.0176 - acc: 0.992 - ETA: 8s - loss: 0.0168 - acc: 0.992 - ETA: 8s - loss: 0.0153 - acc: 0.993 - ETA: 8s - loss: 0.0168 - acc: 0.991 - ETA: 8s - loss: 0.0166 - acc: 0.991 - ETA: 8s - loss: 0.0176 - acc: 0.991 - ETA: 8s - loss: 0.0175 - acc: 0.992 - ETA: 8s - loss: 0.0171 - acc: 0.992 - ETA: 8s - loss: 0.0164 - acc: 0.992 - ETA: 8s - loss: 0.0166 - acc: 0.993 - ETA: 8s - loss: 0.0160 - acc: 0.993 - ETA: 8s - loss: 0.0155 - acc: 0.993 - ETA: 8s - loss: 0.0158 - acc: 0.993 - ETA: 8s - loss: 0.0189 - acc: 0.992 - ETA: 8s - loss: 0.0184 - acc: 0.992 - ETA: 8s - loss: 0.0186 - acc: 0.992 - ETA: 8s - loss: 0.0180 - acc: 0.993 - ETA: 8s - loss: 0.0176 - acc: 0.993 - ETA: 8s - loss: 0.0172 - acc: 0.993 - ETA: 8s - loss: 0.0168 - acc: 0.993 - ETA: 8s - loss: 0.0164 - acc: 0.993 - ETA: 7s - loss: 0.0160 - acc: 0.994 - ETA: 7s - loss: 0.0156 - acc: 0.994 - ETA: 7s - loss: 0.0159 - acc: 0.994 - ETA: 7s - loss: 0.0154 - acc: 0.994 - ETA: 7s - loss: 0.0150 - acc: 0.995 - ETA: 7s - loss: 0.0146 - acc: 0.995 - ETA: 7s - loss: 0.0143 - acc: 0.995 - ETA: 7s - loss: 0.0148 - acc: 0.995 - ETA: 7s - loss: 0.0146 - acc: 0.995 - ETA: 7s - loss: 0.0142 - acc: 0.995 - ETA: 7s - loss: 0.0138 - acc: 0.995 - ETA: 7s - loss: 0.0136 - acc: 0.995 - ETA: 6s - loss: 0.0135 - acc: 0.996 - ETA: 6s - loss: 0.0135 - acc: 0.996 - ETA: 6s - loss: 0.0137 - acc: 0.996 - ETA: 6s - loss: 0.0136 - acc: 0.996 - ETA: 6s - loss: 0.0134 - acc: 0.996 - ETA: 6s - loss: 0.0131 - acc: 0.996 - ETA: 6s - loss: 0.0129 - acc: 0.996 - ETA: 6s - loss: 0.0128 - acc: 0.996 - ETA: 6s - loss: 0.0126 - acc: 0.996 - ETA: 6s - loss: 0.0125 - acc: 0.996 - ETA: 6s - loss: 0.0124 - acc: 0.996 - ETA: 6s - loss: 0.0123 - acc: 0.996 - ETA: 6s - loss: 0.0121 - acc: 0.996 - ETA: 5s - loss: 0.0120 - acc: 0.997 - ETA: 5s - loss: 0.0118 - acc: 0.997 - ETA: 5s - loss: 0.0117 - acc: 0.997 - ETA: 5s - loss: 0.0121 - acc: 0.996 - ETA: 5s - loss: 0.0119 - acc: 0.996 - ETA: 5s - loss: 0.0121 - acc: 0.996 - ETA: 5s - loss: 0.0119 - acc: 0.996 - ETA: 5s - loss: 0.0116 - acc: 0.996 - ETA: 5s - loss: 0.0114 - acc: 0.996 - ETA: 5s - loss: 0.0121 - acc: 0.996 - ETA: 4s - loss: 0.0119 - acc: 0.996 - ETA: 4s - loss: 0.0118 - acc: 0.997 - ETA: 4s - loss: 0.0116 - acc: 0.997 - ETA: 4s - loss: 0.0114 - acc: 0.997 - ETA: 4s - loss: 0.0113 - acc: 0.997 - ETA: 4s - loss: 0.0112 - acc: 0.997 - ETA: 4s - loss: 0.0110 - acc: 0.997 - ETA: 4s - loss: 0.0110 - acc: 0.997 - ETA: 4s - loss: 0.0111 - acc: 0.997 - ETA: 4s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0110 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0108 - acc: 0.997 - ETA: 3s - loss: 0.0120 - acc: 0.997 - ETA: 3s - loss: 0.0142 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0140 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0142 - acc: 0.997 - ETA: 2s - loss: 0.0151 - acc: 0.996 - ETA: 2s - loss: 0.0149 - acc: 0.996 - ETA: 2s - loss: 0.0148 - acc: 0.996 - ETA: 2s - loss: 0.0147 - acc: 0.996 - ETA: 2s - loss: 0.0176 - acc: 0.996 - ETA: 2s - loss: 0.0176 - acc: 0.996 - ETA: 2s - loss: 0.0177 - acc: 0.996 - ETA: 2s - loss: 0.0179 - acc: 0.996 - ETA: 1s - loss: 0.0177 - acc: 0.996 - ETA: 1s - loss: 0.0177 - acc: 0.996 - ETA: 1s - loss: 0.0175 - acc: 0.996 - ETA: 1s - loss: 0.0175 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.996 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0174 - acc: 0.996 - ETA: 1s - loss: 0.0174 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.996 - ETA: 1s - loss: 0.0172 - acc: 0.996 - ETA: 1s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - ETA: 0s - loss: 0.0209 - acc: 0.994 - 10s 3ms/step - loss: 0.0211 - acc: 0.9942 - val_loss: 0.1016 - val_acc: 0.9704\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 8s - loss: 7.8895e-04 - acc: 1.000 - ETA: 8s - loss: 0.0049 - acc: 1.0000    - ETA: 8s - loss: 0.0042 - acc: 1.000 - ETA: 8s - loss: 0.0056 - acc: 1.000 - ETA: 8s - loss: 0.0075 - acc: 1.000 - ETA: 8s - loss: 0.0066 - acc: 1.000 - ETA: 8s - loss: 0.0096 - acc: 0.994 - ETA: 8s - loss: 0.0235 - acc: 0.991 - ETA: 8s - loss: 0.0212 - acc: 0.992 - ETA: 8s - loss: 0.0190 - acc: 0.993 - ETA: 7s - loss: 0.0195 - acc: 0.993 - ETA: 7s - loss: 0.0287 - acc: 0.991 - ETA: 7s - loss: 0.0291 - acc: 0.991 - ETA: 7s - loss: 0.0281 - acc: 0.992 - ETA: 8s - loss: 0.0271 - acc: 0.992 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0263 - acc: 0.991 - ETA: 7s - loss: 0.0306 - acc: 0.989 - ETA: 7s - loss: 0.0297 - acc: 0.989 - ETA: 7s - loss: 0.0289 - acc: 0.990 - ETA: 7s - loss: 0.0337 - acc: 0.989 - ETA: 7s - loss: 0.0329 - acc: 0.989 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0302 - acc: 0.990 - ETA: 7s - loss: 0.0293 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.991 - ETA: 7s - loss: 0.0270 - acc: 0.991 - ETA: 6s - loss: 0.0262 - acc: 0.992 - ETA: 6s - loss: 0.0261 - acc: 0.992 - ETA: 6s - loss: 0.0253 - acc: 0.992 - ETA: 6s - loss: 0.0249 - acc: 0.992 - ETA: 6s - loss: 0.0245 - acc: 0.992 - ETA: 6s - loss: 0.0259 - acc: 0.992 - ETA: 6s - loss: 0.0251 - acc: 0.992 - ETA: 6s - loss: 0.0246 - acc: 0.992 - ETA: 6s - loss: 0.0240 - acc: 0.992 - ETA: 6s - loss: 0.0236 - acc: 0.993 - ETA: 6s - loss: 0.0239 - acc: 0.992 - ETA: 6s - loss: 0.0235 - acc: 0.992 - ETA: 5s - loss: 0.0231 - acc: 0.992 - ETA: 5s - loss: 0.0234 - acc: 0.992 - ETA: 5s - loss: 0.0228 - acc: 0.993 - ETA: 5s - loss: 0.0225 - acc: 0.993 - ETA: 5s - loss: 0.0222 - acc: 0.993 - ETA: 5s - loss: 0.0219 - acc: 0.993 - ETA: 5s - loss: 0.0217 - acc: 0.993 - ETA: 5s - loss: 0.0215 - acc: 0.993 - ETA: 5s - loss: 0.0212 - acc: 0.993 - ETA: 5s - loss: 0.0210 - acc: 0.993 - ETA: 5s - loss: 0.0210 - acc: 0.993 - ETA: 5s - loss: 0.0206 - acc: 0.994 - ETA: 5s - loss: 0.0202 - acc: 0.994 - ETA: 5s - loss: 0.0199 - acc: 0.994 - ETA: 5s - loss: 0.0200 - acc: 0.994 - ETA: 5s - loss: 0.0196 - acc: 0.994 - ETA: 4s - loss: 0.0194 - acc: 0.994 - ETA: 4s - loss: 0.0190 - acc: 0.994 - ETA: 4s - loss: 0.0204 - acc: 0.993 - ETA: 4s - loss: 0.0200 - acc: 0.993 - ETA: 4s - loss: 0.0197 - acc: 0.993 - ETA: 4s - loss: 0.0209 - acc: 0.993 - ETA: 4s - loss: 0.0207 - acc: 0.993 - ETA: 4s - loss: 0.0214 - acc: 0.993 - ETA: 4s - loss: 0.0214 - acc: 0.993 - ETA: 4s - loss: 0.0213 - acc: 0.993 - ETA: 4s - loss: 0.0210 - acc: 0.993 - ETA: 3s - loss: 0.0222 - acc: 0.992 - ETA: 3s - loss: 0.0223 - acc: 0.992 - ETA: 3s - loss: 0.0220 - acc: 0.992 - ETA: 3s - loss: 0.0218 - acc: 0.992 - ETA: 3s - loss: 0.0216 - acc: 0.992 - ETA: 3s - loss: 0.0215 - acc: 0.992 - ETA: 3s - loss: 0.0213 - acc: 0.992 - ETA: 3s - loss: 0.0210 - acc: 0.993 - ETA: 3s - loss: 0.0207 - acc: 0.993 - ETA: 3s - loss: 0.0205 - acc: 0.993 - ETA: 3s - loss: 0.0203 - acc: 0.993 - ETA: 3s - loss: 0.0217 - acc: 0.992 - ETA: 3s - loss: 0.0214 - acc: 0.992 - ETA: 3s - loss: 0.0211 - acc: 0.992 - ETA: 2s - loss: 0.0209 - acc: 0.992 - ETA: 2s - loss: 0.0210 - acc: 0.992 - ETA: 2s - loss: 0.0208 - acc: 0.992 - ETA: 2s - loss: 0.0240 - acc: 0.992 - ETA: 2s - loss: 0.0238 - acc: 0.992 - ETA: 2s - loss: 0.0236 - acc: 0.992 - ETA: 2s - loss: 0.0233 - acc: 0.992 - ETA: 2s - loss: 0.0231 - acc: 0.992 - ETA: 2s - loss: 0.0229 - acc: 0.992 - ETA: 2s - loss: 0.0227 - acc: 0.992 - ETA: 2s - loss: 0.0224 - acc: 0.992 - ETA: 2s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0234 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0234 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0232 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0244 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.992 - ETA: 1s - loss: 0.0243 - acc: 0.992 - ETA: 1s - loss: 0.0242 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.992 - ETA: 1s - loss: 0.0239 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.992 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 1s - loss: 0.0235 - acc: 0.992 - ETA: 1s - loss: 0.0235 - acc: 0.992 - ETA: 1s - loss: 0.0233 - acc: 0.992 - ETA: 1s - loss: 0.0234 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.992 - ETA: 1s - loss: 0.0237 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0232 - acc: 0.992 - ETA: 0s - loss: 0.0231 - acc: 0.992 - ETA: 0s - loss: 0.0229 - acc: 0.992 - ETA: 0s - loss: 0.0227 - acc: 0.992 - ETA: 0s - loss: 0.0225 - acc: 0.992 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0222 - acc: 0.993 - ETA: 0s - loss: 0.0239 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.991 - ETA: 0s - loss: 0.0255 - acc: 0.991 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0264 - acc: 0.991 - 10s 3ms/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0379 - val_acc: 0.9870\n",
      "Epoch 28/40\n",
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0198 - acc: 1.000 - ETA: 9s - loss: 0.0122 - acc: 1.000 - ETA: 9s - loss: 0.0202 - acc: 1.000 - ETA: 9s - loss: 0.0169 - acc: 1.000 - ETA: 10s - loss: 0.0137 - acc: 1.00 - ETA: 11s - loss: 0.0125 - acc: 1.00 - ETA: 11s - loss: 0.0108 - acc: 1.00 - ETA: 11s - loss: 0.0096 - acc: 1.00 - ETA: 11s - loss: 0.0097 - acc: 1.00 - ETA: 11s - loss: 0.0094 - acc: 1.00 - ETA: 11s - loss: 0.0098 - acc: 1.00 - ETA: 11s - loss: 0.0097 - acc: 1.00 - ETA: 11s - loss: 0.0123 - acc: 1.00 - ETA: 11s - loss: 0.0119 - acc: 1.00 - ETA: 11s - loss: 0.0136 - acc: 1.00 - ETA: 11s - loss: 0.0128 - acc: 1.00 - ETA: 11s - loss: 0.0121 - acc: 1.00 - ETA: 11s - loss: 0.0115 - acc: 1.00 - ETA: 11s - loss: 0.0112 - acc: 1.00 - ETA: 10s - loss: 0.0111 - acc: 1.00 - ETA: 10s - loss: 0.0256 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 10s - loss: 0.0259 - acc: 0.99 - ETA: 10s - loss: 0.0250 - acc: 0.99 - ETA: 10s - loss: 0.0240 - acc: 0.99 - ETA: 10s - loss: 0.0238 - acc: 0.99 - ETA: 10s - loss: 0.0231 - acc: 0.99 - ETA: 10s - loss: 0.0223 - acc: 0.99 - ETA: 10s - loss: 0.0216 - acc: 0.99 - ETA: 10s - loss: 0.0211 - acc: 0.99 - ETA: 9s - loss: 0.0206 - acc: 0.9962 - ETA: 9s - loss: 0.0211 - acc: 0.996 - ETA: 9s - loss: 0.0207 - acc: 0.996 - ETA: 9s - loss: 0.0203 - acc: 0.996 - ETA: 9s - loss: 0.0198 - acc: 0.996 - ETA: 9s - loss: 0.0211 - acc: 0.995 - ETA: 9s - loss: 0.0206 - acc: 0.995 - ETA: 9s - loss: 0.0203 - acc: 0.995 - ETA: 8s - loss: 0.0194 - acc: 0.995 - ETA: 8s - loss: 0.0190 - acc: 0.995 - ETA: 8s - loss: 0.0191 - acc: 0.995 - ETA: 8s - loss: 0.0192 - acc: 0.995 - ETA: 8s - loss: 0.0189 - acc: 0.996 - ETA: 8s - loss: 0.0186 - acc: 0.996 - ETA: 8s - loss: 0.0187 - acc: 0.996 - ETA: 8s - loss: 0.0186 - acc: 0.996 - ETA: 8s - loss: 0.0183 - acc: 0.996 - ETA: 8s - loss: 0.0181 - acc: 0.996 - ETA: 8s - loss: 0.0181 - acc: 0.996 - ETA: 8s - loss: 0.0179 - acc: 0.996 - ETA: 8s - loss: 0.0192 - acc: 0.996 - ETA: 8s - loss: 0.0193 - acc: 0.996 - ETA: 8s - loss: 0.0191 - acc: 0.996 - ETA: 8s - loss: 0.0194 - acc: 0.996 - ETA: 8s - loss: 0.0191 - acc: 0.996 - ETA: 8s - loss: 0.0192 - acc: 0.996 - ETA: 8s - loss: 0.0191 - acc: 0.996 - ETA: 8s - loss: 0.0189 - acc: 0.997 - ETA: 8s - loss: 0.0190 - acc: 0.997 - ETA: 7s - loss: 0.0291 - acc: 0.995 - ETA: 7s - loss: 0.0287 - acc: 0.995 - ETA: 7s - loss: 0.0283 - acc: 0.995 - ETA: 7s - loss: 0.0280 - acc: 0.995 - ETA: 7s - loss: 0.0279 - acc: 0.995 - ETA: 7s - loss: 0.0275 - acc: 0.995 - ETA: 7s - loss: 0.0272 - acc: 0.995 - ETA: 7s - loss: 0.0269 - acc: 0.995 - ETA: 7s - loss: 0.0268 - acc: 0.995 - ETA: 7s - loss: 0.0281 - acc: 0.995 - ETA: 7s - loss: 0.0277 - acc: 0.995 - ETA: 7s - loss: 0.0274 - acc: 0.995 - ETA: 7s - loss: 0.0271 - acc: 0.995 - ETA: 6s - loss: 0.0475 - acc: 0.992 - ETA: 6s - loss: 0.0480 - acc: 0.991 - ETA: 6s - loss: 0.0475 - acc: 0.991 - ETA: 6s - loss: 0.0470 - acc: 0.991 - ETA: 6s - loss: 0.0466 - acc: 0.991 - ETA: 6s - loss: 0.0460 - acc: 0.992 - ETA: 6s - loss: 0.0455 - acc: 0.992 - ETA: 6s - loss: 0.0451 - acc: 0.992 - ETA: 6s - loss: 0.0473 - acc: 0.991 - ETA: 6s - loss: 0.0469 - acc: 0.991 - ETA: 6s - loss: 0.0463 - acc: 0.991 - ETA: 6s - loss: 0.0460 - acc: 0.991 - ETA: 6s - loss: 0.0451 - acc: 0.991 - ETA: 6s - loss: 0.0446 - acc: 0.991 - ETA: 5s - loss: 0.0442 - acc: 0.991 - ETA: 5s - loss: 0.0445 - acc: 0.991 - ETA: 5s - loss: 0.0458 - acc: 0.989 - ETA: 5s - loss: 0.0452 - acc: 0.990 - ETA: 5s - loss: 0.0446 - acc: 0.990 - ETA: 5s - loss: 0.0471 - acc: 0.989 - ETA: 5s - loss: 0.0479 - acc: 0.988 - ETA: 5s - loss: 0.0471 - acc: 0.989 - ETA: 5s - loss: 0.0463 - acc: 0.989 - ETA: 4s - loss: 0.0457 - acc: 0.989 - ETA: 4s - loss: 0.0450 - acc: 0.989 - ETA: 4s - loss: 0.0445 - acc: 0.989 - ETA: 4s - loss: 0.0439 - acc: 0.989 - ETA: 4s - loss: 0.0432 - acc: 0.990 - ETA: 4s - loss: 0.0427 - acc: 0.990 - ETA: 4s - loss: 0.0430 - acc: 0.989 - ETA: 4s - loss: 0.0426 - acc: 0.990 - ETA: 4s - loss: 0.0423 - acc: 0.990 - ETA: 3s - loss: 0.0417 - acc: 0.990 - ETA: 3s - loss: 0.0416 - acc: 0.990 - ETA: 3s - loss: 0.0411 - acc: 0.990 - ETA: 3s - loss: 0.0412 - acc: 0.990 - ETA: 3s - loss: 0.0409 - acc: 0.990 - ETA: 3s - loss: 0.0403 - acc: 0.990 - ETA: 3s - loss: 0.0398 - acc: 0.990 - ETA: 3s - loss: 0.0409 - acc: 0.990 - ETA: 3s - loss: 0.0414 - acc: 0.990 - ETA: 2s - loss: 0.0413 - acc: 0.990 - ETA: 2s - loss: 0.0408 - acc: 0.990 - ETA: 2s - loss: 0.0403 - acc: 0.990 - ETA: 2s - loss: 0.0402 - acc: 0.990 - ETA: 2s - loss: 0.0398 - acc: 0.990 - ETA: 2s - loss: 0.0393 - acc: 0.990 - ETA: 2s - loss: 0.0388 - acc: 0.990 - ETA: 2s - loss: 0.0384 - acc: 0.991 - ETA: 2s - loss: 0.0379 - acc: 0.991 - ETA: 2s - loss: 0.0376 - acc: 0.991 - ETA: 1s - loss: 0.0373 - acc: 0.991 - ETA: 1s - loss: 0.0370 - acc: 0.991 - ETA: 1s - loss: 0.0366 - acc: 0.991 - ETA: 1s - loss: 0.0362 - acc: 0.991 - ETA: 1s - loss: 0.0359 - acc: 0.991 - ETA: 1s - loss: 0.0358 - acc: 0.991 - ETA: 1s - loss: 0.0355 - acc: 0.991 - ETA: 1s - loss: 0.0351 - acc: 0.992 - ETA: 1s - loss: 0.0350 - acc: 0.992 - ETA: 1s - loss: 0.0346 - acc: 0.992 - ETA: 0s - loss: 0.0343 - acc: 0.992 - ETA: 0s - loss: 0.0341 - acc: 0.992 - ETA: 0s - loss: 0.0338 - acc: 0.992 - ETA: 0s - loss: 0.0335 - acc: 0.992 - ETA: 0s - loss: 0.0332 - acc: 0.992 - ETA: 0s - loss: 0.0329 - acc: 0.992 - ETA: 0s - loss: 0.0328 - acc: 0.992 - ETA: 0s - loss: 0.0325 - acc: 0.992 - ETA: 0s - loss: 0.0323 - acc: 0.992 - ETA: 0s - loss: 0.0322 - acc: 0.992 - ETA: 0s - loss: 0.0320 - acc: 0.992 - ETA: 0s - loss: 0.0321 - acc: 0.992 - 10s 3ms/step - loss: 0.0322 - acc: 0.9924 - val_loss: 0.0866 - val_acc: 0.9784\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0068 - acc: 1.000 - ETA: 7s - loss: 0.0225 - acc: 1.000 - ETA: 9s - loss: 0.0184 - acc: 1.000 - ETA: 9s - loss: 0.0159 - acc: 1.000 - ETA: 8s - loss: 0.0263 - acc: 0.991 - ETA: 8s - loss: 0.0236 - acc: 0.993 - ETA: 8s - loss: 0.0199 - acc: 0.994 - ETA: 8s - loss: 0.0173 - acc: 0.995 - ETA: 8s - loss: 0.0152 - acc: 0.995 - ETA: 7s - loss: 0.0137 - acc: 0.996 - ETA: 7s - loss: 0.0125 - acc: 0.996 - ETA: 7s - loss: 0.0157 - acc: 0.994 - ETA: 7s - loss: 0.0144 - acc: 0.994 - ETA: 7s - loss: 0.0134 - acc: 0.995 - ETA: 7s - loss: 0.0125 - acc: 0.995 - ETA: 7s - loss: 0.0193 - acc: 0.989 - ETA: 7s - loss: 0.0186 - acc: 0.989 - ETA: 6s - loss: 0.0179 - acc: 0.990 - ETA: 6s - loss: 0.0170 - acc: 0.991 - ETA: 6s - loss: 0.0183 - acc: 0.989 - ETA: 6s - loss: 0.0198 - acc: 0.988 - ETA: 6s - loss: 0.0408 - acc: 0.987 - ETA: 6s - loss: 0.0394 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0367 - acc: 0.989 - ETA: 6s - loss: 0.0353 - acc: 0.989 - ETA: 6s - loss: 0.0340 - acc: 0.990 - ETA: 6s - loss: 0.0331 - acc: 0.990 - ETA: 6s - loss: 0.0322 - acc: 0.990 - ETA: 5s - loss: 0.0313 - acc: 0.991 - ETA: 5s - loss: 0.0304 - acc: 0.991 - ETA: 5s - loss: 0.0311 - acc: 0.990 - ETA: 5s - loss: 0.0301 - acc: 0.991 - ETA: 5s - loss: 0.0299 - acc: 0.991 - ETA: 5s - loss: 0.0293 - acc: 0.991 - ETA: 5s - loss: 0.0289 - acc: 0.991 - ETA: 5s - loss: 0.0284 - acc: 0.992 - ETA: 5s - loss: 0.0289 - acc: 0.991 - ETA: 5s - loss: 0.0298 - acc: 0.990 - ETA: 5s - loss: 0.0298 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0285 - acc: 0.991 - ETA: 4s - loss: 0.0283 - acc: 0.991 - ETA: 4s - loss: 0.0277 - acc: 0.992 - ETA: 4s - loss: 0.0278 - acc: 0.992 - ETA: 4s - loss: 0.0282 - acc: 0.992 - ETA: 4s - loss: 0.0280 - acc: 0.992 - ETA: 4s - loss: 0.0280 - acc: 0.992 - ETA: 4s - loss: 0.0275 - acc: 0.992 - ETA: 4s - loss: 0.0271 - acc: 0.993 - ETA: 4s - loss: 0.0308 - acc: 0.992 - ETA: 4s - loss: 0.0303 - acc: 0.992 - ETA: 4s - loss: 0.0300 - acc: 0.992 - ETA: 3s - loss: 0.0299 - acc: 0.991 - ETA: 3s - loss: 0.0317 - acc: 0.990 - ETA: 3s - loss: 0.0312 - acc: 0.991 - ETA: 3s - loss: 0.0308 - acc: 0.991 - ETA: 3s - loss: 0.0305 - acc: 0.991 - ETA: 3s - loss: 0.0311 - acc: 0.990 - ETA: 3s - loss: 0.0306 - acc: 0.991 - ETA: 3s - loss: 0.0305 - acc: 0.991 - ETA: 3s - loss: 0.0302 - acc: 0.991 - ETA: 3s - loss: 0.0302 - acc: 0.991 - ETA: 3s - loss: 0.0298 - acc: 0.991 - ETA: 3s - loss: 0.0296 - acc: 0.991 - ETA: 2s - loss: 0.0293 - acc: 0.991 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0295 - acc: 0.991 - ETA: 2s - loss: 0.0293 - acc: 0.991 - ETA: 2s - loss: 0.0306 - acc: 0.990 - ETA: 2s - loss: 0.0301 - acc: 0.990 - ETA: 2s - loss: 0.0299 - acc: 0.990 - ETA: 2s - loss: 0.0296 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.991 - ETA: 2s - loss: 0.0288 - acc: 0.991 - ETA: 2s - loss: 0.0285 - acc: 0.991 - ETA: 2s - loss: 0.0282 - acc: 0.991 - ETA: 2s - loss: 0.0286 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0279 - acc: 0.991 - ETA: 1s - loss: 0.0276 - acc: 0.991 - ETA: 1s - loss: 0.0274 - acc: 0.991 - ETA: 1s - loss: 0.0273 - acc: 0.991 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0271 - acc: 0.991 - ETA: 1s - loss: 0.0270 - acc: 0.991 - ETA: 1s - loss: 0.0268 - acc: 0.991 - ETA: 1s - loss: 0.0265 - acc: 0.991 - ETA: 1s - loss: 0.0264 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.991 - ETA: 1s - loss: 0.0261 - acc: 0.991 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0251 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0247 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0244 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0241 - acc: 0.992 - ETA: 0s - loss: 0.0240 - acc: 0.992 - 9s 3ms/step - loss: 0.0239 - acc: 0.9927 - val_loss: 0.0842 - val_acc: 0.9719\n",
      "Epoch 30/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 2.6558e-04 - acc: 1.000 - ETA: 7s - loss: 0.0859 - acc: 0.9792    - ETA: 7s - loss: 0.0519 - acc: 0.987 - ETA: 7s - loss: 0.0377 - acc: 0.991 - ETA: 7s - loss: 0.0305 - acc: 0.993 - ETA: 7s - loss: 0.0258 - acc: 0.994 - ETA: 7s - loss: 0.0357 - acc: 0.985 - ETA: 7s - loss: 0.0310 - acc: 0.987 - ETA: 7s - loss: 0.0276 - acc: 0.989 - ETA: 7s - loss: 0.0256 - acc: 0.990 - ETA: 7s - loss: 0.0322 - acc: 0.988 - ETA: 7s - loss: 0.0298 - acc: 0.989 - ETA: 7s - loss: 0.0319 - acc: 0.987 - ETA: 7s - loss: 0.0296 - acc: 0.988 - ETA: 7s - loss: 0.0291 - acc: 0.989 - ETA: 6s - loss: 0.0291 - acc: 0.987 - ETA: 6s - loss: 0.0276 - acc: 0.988 - ETA: 6s - loss: 0.0262 - acc: 0.989 - ETA: 6s - loss: 0.0248 - acc: 0.989 - ETA: 6s - loss: 0.0236 - acc: 0.990 - ETA: 6s - loss: 0.0237 - acc: 0.989 - ETA: 6s - loss: 0.0227 - acc: 0.989 - ETA: 6s - loss: 0.0217 - acc: 0.990 - ETA: 6s - loss: 0.0214 - acc: 0.990 - ETA: 6s - loss: 0.0209 - acc: 0.990 - ETA: 6s - loss: 0.0206 - acc: 0.991 - ETA: 6s - loss: 0.0199 - acc: 0.991 - ETA: 6s - loss: 0.0192 - acc: 0.991 - ETA: 5s - loss: 0.0187 - acc: 0.992 - ETA: 5s - loss: 0.0182 - acc: 0.992 - ETA: 5s - loss: 0.0177 - acc: 0.992 - ETA: 5s - loss: 0.0172 - acc: 0.992 - ETA: 5s - loss: 0.0178 - acc: 0.992 - ETA: 5s - loss: 0.0180 - acc: 0.992 - ETA: 5s - loss: 0.0178 - acc: 0.992 - ETA: 5s - loss: 0.0173 - acc: 0.992 - ETA: 5s - loss: 0.0168 - acc: 0.993 - ETA: 5s - loss: 0.0165 - acc: 0.993 - ETA: 5s - loss: 0.0164 - acc: 0.993 - ETA: 5s - loss: 0.0162 - acc: 0.993 - ETA: 5s - loss: 0.0159 - acc: 0.993 - ETA: 4s - loss: 0.0159 - acc: 0.993 - ETA: 4s - loss: 0.0156 - acc: 0.994 - ETA: 4s - loss: 0.0152 - acc: 0.994 - ETA: 4s - loss: 0.0153 - acc: 0.994 - ETA: 4s - loss: 0.0157 - acc: 0.993 - ETA: 4s - loss: 0.0154 - acc: 0.993 - ETA: 4s - loss: 0.0160 - acc: 0.993 - ETA: 4s - loss: 0.0157 - acc: 0.993 - ETA: 4s - loss: 0.0155 - acc: 0.993 - ETA: 4s - loss: 0.0153 - acc: 0.993 - ETA: 4s - loss: 0.0151 - acc: 0.993 - ETA: 4s - loss: 0.0148 - acc: 0.993 - ETA: 4s - loss: 0.0146 - acc: 0.994 - ETA: 3s - loss: 0.0144 - acc: 0.994 - ETA: 3s - loss: 0.0142 - acc: 0.994 - ETA: 3s - loss: 0.0171 - acc: 0.993 - ETA: 3s - loss: 0.0170 - acc: 0.993 - ETA: 3s - loss: 0.0167 - acc: 0.994 - ETA: 3s - loss: 0.0165 - acc: 0.994 - ETA: 3s - loss: 0.0167 - acc: 0.993 - ETA: 3s - loss: 0.0165 - acc: 0.993 - ETA: 3s - loss: 0.0169 - acc: 0.993 - ETA: 3s - loss: 0.0193 - acc: 0.992 - ETA: 3s - loss: 0.0192 - acc: 0.992 - ETA: 3s - loss: 0.0190 - acc: 0.992 - ETA: 2s - loss: 0.0189 - acc: 0.992 - ETA: 2s - loss: 0.0186 - acc: 0.993 - ETA: 2s - loss: 0.0185 - acc: 0.993 - ETA: 2s - loss: 0.0184 - acc: 0.993 - ETA: 2s - loss: 0.0182 - acc: 0.993 - ETA: 2s - loss: 0.0180 - acc: 0.993 - ETA: 2s - loss: 0.0179 - acc: 0.993 - ETA: 2s - loss: 0.0176 - acc: 0.993 - ETA: 2s - loss: 0.0175 - acc: 0.993 - ETA: 2s - loss: 0.0174 - acc: 0.993 - ETA: 2s - loss: 0.0174 - acc: 0.993 - ETA: 2s - loss: 0.0172 - acc: 0.993 - ETA: 2s - loss: 0.0172 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0168 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.994 - ETA: 1s - loss: 0.0166 - acc: 0.994 - ETA: 1s - loss: 0.0164 - acc: 0.994 - ETA: 1s - loss: 0.0162 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0161 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.994 - ETA: 1s - loss: 0.0158 - acc: 0.994 - ETA: 1s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0153 - acc: 0.994 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - 9s 3ms/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0988 - val_acc: 0.9769\n",
      "Epoch 31/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 6.9841e-04 - acc: 1.000 - ETA: 7s - loss: 0.0020 - acc: 1.0000    - ETA: 7s - loss: 0.0048 - acc: 1.000 - ETA: 7s - loss: 0.0069 - acc: 1.000 - ETA: 7s - loss: 0.0099 - acc: 1.000 - ETA: 7s - loss: 0.0092 - acc: 1.000 - ETA: 7s - loss: 0.0082 - acc: 1.000 - ETA: 7s - loss: 0.0091 - acc: 1.000 - ETA: 7s - loss: 0.0088 - acc: 1.000 - ETA: 7s - loss: 0.0132 - acc: 0.996 - ETA: 7s - loss: 0.0123 - acc: 0.997 - ETA: 7s - loss: 0.0113 - acc: 0.997 - ETA: 7s - loss: 0.0115 - acc: 0.997 - ETA: 7s - loss: 0.0113 - acc: 0.997 - ETA: 6s - loss: 0.0106 - acc: 0.997 - ETA: 6s - loss: 0.0100 - acc: 0.998 - ETA: 6s - loss: 0.0094 - acc: 0.998 - ETA: 6s - loss: 0.0092 - acc: 0.998 - ETA: 6s - loss: 0.0117 - acc: 0.996 - ETA: 6s - loss: 0.0113 - acc: 0.996 - ETA: 6s - loss: 0.0109 - acc: 0.997 - ETA: 6s - loss: 0.0105 - acc: 0.997 - ETA: 6s - loss: 0.0101 - acc: 0.997 - ETA: 6s - loss: 0.0100 - acc: 0.997 - ETA: 6s - loss: 0.0104 - acc: 0.997 - ETA: 6s - loss: 0.0101 - acc: 0.997 - ETA: 6s - loss: 0.0101 - acc: 0.997 - ETA: 5s - loss: 0.0101 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.997 - ETA: 5s - loss: 0.0097 - acc: 0.997 - ETA: 5s - loss: 0.0096 - acc: 0.998 - ETA: 5s - loss: 0.0095 - acc: 0.998 - ETA: 5s - loss: 0.0094 - acc: 0.998 - ETA: 5s - loss: 0.0092 - acc: 0.998 - ETA: 5s - loss: 0.0098 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.997 - ETA: 5s - loss: 0.0097 - acc: 0.997 - ETA: 5s - loss: 0.0094 - acc: 0.997 - ETA: 5s - loss: 0.0092 - acc: 0.997 - ETA: 5s - loss: 0.0090 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.996 - ETA: 4s - loss: 0.0120 - acc: 0.997 - ETA: 4s - loss: 0.0118 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.997 - ETA: 4s - loss: 0.0116 - acc: 0.997 - ETA: 4s - loss: 0.0114 - acc: 0.997 - ETA: 4s - loss: 0.0112 - acc: 0.997 - ETA: 4s - loss: 0.0110 - acc: 0.997 - ETA: 4s - loss: 0.0109 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.996 - ETA: 4s - loss: 0.0117 - acc: 0.996 - ETA: 4s - loss: 0.0115 - acc: 0.996 - ETA: 4s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0112 - acc: 0.997 - ETA: 3s - loss: 0.0152 - acc: 0.996 - ETA: 3s - loss: 0.0150 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0164 - acc: 0.995 - ETA: 3s - loss: 0.0167 - acc: 0.995 - ETA: 3s - loss: 0.0168 - acc: 0.995 - ETA: 3s - loss: 0.0167 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0166 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0171 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.994 - ETA: 2s - loss: 0.0177 - acc: 0.994 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0173 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0177 - acc: 0.994 - ETA: 1s - loss: 0.0175 - acc: 0.994 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0173 - acc: 0.994 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0172 - acc: 0.995 - ETA: 1s - loss: 0.0171 - acc: 0.995 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.994 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0176 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - 9s 3ms/step - loss: 0.0188 - acc: 0.9948 - val_loss: 0.0985 - val_acc: 0.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.2111 - acc: 0.937 - ETA: 8s - loss: 0.0969 - acc: 0.958 - ETA: 7s - loss: 0.0602 - acc: 0.975 - ETA: 8s - loss: 0.0506 - acc: 0.979 - ETA: 7s - loss: 0.0424 - acc: 0.984 - ETA: 7s - loss: 0.0350 - acc: 0.987 - ETA: 7s - loss: 0.0293 - acc: 0.989 - ETA: 7s - loss: 0.0272 - acc: 0.990 - ETA: 7s - loss: 0.0236 - acc: 0.991 - ETA: 7s - loss: 0.0218 - acc: 0.992 - ETA: 7s - loss: 0.0199 - acc: 0.993 - ETA: 7s - loss: 0.0220 - acc: 0.991 - ETA: 7s - loss: 0.0202 - acc: 0.991 - ETA: 7s - loss: 0.0189 - acc: 0.992 - ETA: 6s - loss: 0.0214 - acc: 0.990 - ETA: 7s - loss: 0.0207 - acc: 0.991 - ETA: 6s - loss: 0.0197 - acc: 0.991 - ETA: 6s - loss: 0.0186 - acc: 0.992 - ETA: 6s - loss: 0.0176 - acc: 0.992 - ETA: 6s - loss: 0.0168 - acc: 0.993 - ETA: 6s - loss: 0.0163 - acc: 0.993 - ETA: 6s - loss: 0.0158 - acc: 0.993 - ETA: 6s - loss: 0.0151 - acc: 0.994 - ETA: 6s - loss: 0.0145 - acc: 0.994 - ETA: 6s - loss: 0.0139 - acc: 0.994 - ETA: 6s - loss: 0.0136 - acc: 0.994 - ETA: 6s - loss: 0.0132 - acc: 0.995 - ETA: 6s - loss: 0.0133 - acc: 0.995 - ETA: 5s - loss: 0.0129 - acc: 0.995 - ETA: 5s - loss: 0.0125 - acc: 0.995 - ETA: 5s - loss: 0.0126 - acc: 0.995 - ETA: 5s - loss: 0.0122 - acc: 0.995 - ETA: 5s - loss: 0.0238 - acc: 0.992 - ETA: 5s - loss: 0.0234 - acc: 0.993 - ETA: 5s - loss: 0.0229 - acc: 0.993 - ETA: 5s - loss: 0.0223 - acc: 0.993 - ETA: 5s - loss: 0.0221 - acc: 0.993 - ETA: 5s - loss: 0.0215 - acc: 0.993 - ETA: 5s - loss: 0.0383 - acc: 0.992 - ETA: 5s - loss: 0.0386 - acc: 0.991 - ETA: 5s - loss: 0.0377 - acc: 0.991 - ETA: 4s - loss: 0.0370 - acc: 0.992 - ETA: 4s - loss: 0.0361 - acc: 0.992 - ETA: 4s - loss: 0.0354 - acc: 0.992 - ETA: 4s - loss: 0.0347 - acc: 0.992 - ETA: 4s - loss: 0.0340 - acc: 0.992 - ETA: 4s - loss: 0.0334 - acc: 0.993 - ETA: 4s - loss: 0.0327 - acc: 0.993 - ETA: 4s - loss: 0.0323 - acc: 0.993 - ETA: 4s - loss: 0.0318 - acc: 0.993 - ETA: 4s - loss: 0.0330 - acc: 0.992 - ETA: 4s - loss: 0.0324 - acc: 0.993 - ETA: 4s - loss: 0.0323 - acc: 0.993 - ETA: 4s - loss: 0.0317 - acc: 0.993 - ETA: 3s - loss: 0.0311 - acc: 0.993 - ETA: 3s - loss: 0.0308 - acc: 0.993 - ETA: 3s - loss: 0.0306 - acc: 0.993 - ETA: 3s - loss: 0.0301 - acc: 0.993 - ETA: 3s - loss: 0.0301 - acc: 0.993 - ETA: 3s - loss: 0.0303 - acc: 0.992 - ETA: 3s - loss: 0.0298 - acc: 0.992 - ETA: 3s - loss: 0.0295 - acc: 0.993 - ETA: 3s - loss: 0.0294 - acc: 0.993 - ETA: 3s - loss: 0.0291 - acc: 0.993 - ETA: 3s - loss: 0.0286 - acc: 0.993 - ETA: 3s - loss: 0.0282 - acc: 0.993 - ETA: 3s - loss: 0.0280 - acc: 0.993 - ETA: 3s - loss: 0.0276 - acc: 0.993 - ETA: 2s - loss: 0.0274 - acc: 0.993 - ETA: 2s - loss: 0.0272 - acc: 0.993 - ETA: 2s - loss: 0.0270 - acc: 0.994 - ETA: 2s - loss: 0.0267 - acc: 0.994 - ETA: 2s - loss: 0.0264 - acc: 0.994 - ETA: 2s - loss: 0.0284 - acc: 0.993 - ETA: 2s - loss: 0.0281 - acc: 0.993 - ETA: 2s - loss: 0.0399 - acc: 0.992 - ETA: 2s - loss: 0.0394 - acc: 0.992 - ETA: 2s - loss: 0.0389 - acc: 0.992 - ETA: 2s - loss: 0.0385 - acc: 0.992 - ETA: 2s - loss: 0.0436 - acc: 0.991 - ETA: 2s - loss: 0.0452 - acc: 0.990 - ETA: 1s - loss: 0.0447 - acc: 0.990 - ETA: 1s - loss: 0.0465 - acc: 0.990 - ETA: 1s - loss: 0.0461 - acc: 0.990 - ETA: 1s - loss: 0.0472 - acc: 0.990 - ETA: 1s - loss: 0.0468 - acc: 0.990 - ETA: 1s - loss: 0.0469 - acc: 0.989 - ETA: 1s - loss: 0.0468 - acc: 0.989 - ETA: 1s - loss: 0.0463 - acc: 0.989 - ETA: 1s - loss: 0.0459 - acc: 0.989 - ETA: 1s - loss: 0.0454 - acc: 0.990 - ETA: 1s - loss: 0.0463 - acc: 0.989 - ETA: 1s - loss: 0.0462 - acc: 0.989 - ETA: 0s - loss: 0.0481 - acc: 0.989 - ETA: 0s - loss: 0.0479 - acc: 0.989 - ETA: 0s - loss: 0.0474 - acc: 0.989 - ETA: 0s - loss: 0.0470 - acc: 0.989 - ETA: 0s - loss: 0.0469 - acc: 0.989 - ETA: 0s - loss: 0.0465 - acc: 0.989 - ETA: 0s - loss: 0.0463 - acc: 0.989 - ETA: 0s - loss: 0.0462 - acc: 0.989 - ETA: 0s - loss: 0.0478 - acc: 0.988 - ETA: 0s - loss: 0.0501 - acc: 0.988 - ETA: 0s - loss: 0.0499 - acc: 0.988 - ETA: 0s - loss: 0.0506 - acc: 0.987 - ETA: 0s - loss: 0.0511 - acc: 0.987 - ETA: 0s - loss: 0.0519 - acc: 0.987 - 9s 3ms/step - loss: 0.0518 - acc: 0.9875 - val_loss: 0.9428 - val_acc: 0.8176\n",
      "Epoch 33/40\n",
      "3285/3285 [==============================] - ETA: 9s - loss: 0.0022 - acc: 1.000 - ETA: 8s - loss: 0.0108 - acc: 1.000 - ETA: 7s - loss: 0.0160 - acc: 1.000 - ETA: 7s - loss: 0.0590 - acc: 0.982 - ETA: 7s - loss: 0.0927 - acc: 0.979 - ETA: 7s - loss: 0.0798 - acc: 0.983 - ETA: 7s - loss: 0.0701 - acc: 0.985 - ETA: 7s - loss: 0.0954 - acc: 0.983 - ETA: 7s - loss: 0.0877 - acc: 0.985 - ETA: 7s - loss: 0.0791 - acc: 0.986 - ETA: 7s - loss: 0.0718 - acc: 0.988 - ETA: 7s - loss: 0.0681 - acc: 0.989 - ETA: 7s - loss: 0.0655 - acc: 0.989 - ETA: 7s - loss: 0.0605 - acc: 0.990 - ETA: 7s - loss: 0.0568 - acc: 0.991 - ETA: 7s - loss: 0.0532 - acc: 0.991 - ETA: 6s - loss: 0.0615 - acc: 0.990 - ETA: 6s - loss: 0.0580 - acc: 0.990 - ETA: 6s - loss: 0.0564 - acc: 0.989 - ETA: 6s - loss: 0.0555 - acc: 0.990 - ETA: 6s - loss: 0.0528 - acc: 0.990 - ETA: 6s - loss: 0.0503 - acc: 0.991 - ETA: 6s - loss: 0.0482 - acc: 0.991 - ETA: 6s - loss: 0.0500 - acc: 0.990 - ETA: 6s - loss: 0.0482 - acc: 0.990 - ETA: 6s - loss: 0.0463 - acc: 0.991 - ETA: 6s - loss: 0.0447 - acc: 0.991 - ETA: 6s - loss: 0.0431 - acc: 0.991 - ETA: 6s - loss: 0.0416 - acc: 0.992 - ETA: 5s - loss: 0.0412 - acc: 0.991 - ETA: 5s - loss: 0.0430 - acc: 0.990 - ETA: 5s - loss: 0.0424 - acc: 0.990 - ETA: 5s - loss: 0.0440 - acc: 0.990 - ETA: 5s - loss: 0.0427 - acc: 0.990 - ETA: 5s - loss: 0.0415 - acc: 0.990 - ETA: 5s - loss: 0.0542 - acc: 0.987 - ETA: 5s - loss: 0.0564 - acc: 0.985 - ETA: 5s - loss: 0.0556 - acc: 0.986 - ETA: 5s - loss: 0.0553 - acc: 0.986 - ETA: 5s - loss: 0.0539 - acc: 0.986 - ETA: 5s - loss: 0.0533 - acc: 0.986 - ETA: 5s - loss: 0.0520 - acc: 0.987 - ETA: 5s - loss: 0.0517 - acc: 0.986 - ETA: 5s - loss: 0.0506 - acc: 0.987 - ETA: 4s - loss: 0.0495 - acc: 0.987 - ETA: 4s - loss: 0.0485 - acc: 0.987 - ETA: 4s - loss: 0.0480 - acc: 0.987 - ETA: 4s - loss: 0.0470 - acc: 0.988 - ETA: 4s - loss: 0.0535 - acc: 0.985 - ETA: 4s - loss: 0.0533 - acc: 0.985 - ETA: 4s - loss: 0.0541 - acc: 0.985 - ETA: 4s - loss: 0.0537 - acc: 0.985 - ETA: 4s - loss: 0.0527 - acc: 0.985 - ETA: 4s - loss: 0.0518 - acc: 0.985 - ETA: 4s - loss: 0.0508 - acc: 0.986 - ETA: 4s - loss: 0.0501 - acc: 0.986 - ETA: 4s - loss: 0.0498 - acc: 0.986 - ETA: 3s - loss: 0.0489 - acc: 0.986 - ETA: 3s - loss: 0.0481 - acc: 0.987 - ETA: 3s - loss: 0.0478 - acc: 0.987 - ETA: 3s - loss: 0.0479 - acc: 0.986 - ETA: 3s - loss: 0.0472 - acc: 0.986 - ETA: 3s - loss: 0.0470 - acc: 0.986 - ETA: 3s - loss: 0.0463 - acc: 0.986 - ETA: 3s - loss: 0.0465 - acc: 0.986 - ETA: 3s - loss: 0.0498 - acc: 0.986 - ETA: 3s - loss: 0.0492 - acc: 0.986 - ETA: 3s - loss: 0.0487 - acc: 0.986 - ETA: 3s - loss: 0.0529 - acc: 0.986 - ETA: 2s - loss: 0.0524 - acc: 0.986 - ETA: 2s - loss: 0.0517 - acc: 0.986 - ETA: 2s - loss: 0.0511 - acc: 0.986 - ETA: 2s - loss: 0.0507 - acc: 0.987 - ETA: 2s - loss: 0.0503 - acc: 0.987 - ETA: 2s - loss: 0.0500 - acc: 0.986 - ETA: 2s - loss: 0.0493 - acc: 0.987 - ETA: 2s - loss: 0.0488 - acc: 0.987 - ETA: 2s - loss: 0.0482 - acc: 0.987 - ETA: 2s - loss: 0.0478 - acc: 0.987 - ETA: 2s - loss: 0.0472 - acc: 0.987 - ETA: 2s - loss: 0.0468 - acc: 0.987 - ETA: 1s - loss: 0.0464 - acc: 0.988 - ETA: 1s - loss: 0.0458 - acc: 0.988 - ETA: 1s - loss: 0.0454 - acc: 0.988 - ETA: 1s - loss: 0.0460 - acc: 0.987 - ETA: 1s - loss: 0.0480 - acc: 0.987 - ETA: 1s - loss: 0.0477 - acc: 0.987 - ETA: 1s - loss: 0.0472 - acc: 0.987 - ETA: 1s - loss: 0.0467 - acc: 0.987 - ETA: 1s - loss: 0.0462 - acc: 0.988 - ETA: 1s - loss: 0.0466 - acc: 0.987 - ETA: 1s - loss: 0.0465 - acc: 0.987 - ETA: 1s - loss: 0.0463 - acc: 0.987 - ETA: 1s - loss: 0.0473 - acc: 0.987 - ETA: 1s - loss: 0.0468 - acc: 0.987 - ETA: 1s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0475 - acc: 0.987 - ETA: 0s - loss: 0.0472 - acc: 0.987 - ETA: 0s - loss: 0.0469 - acc: 0.987 - ETA: 0s - loss: 0.0465 - acc: 0.987 - ETA: 0s - loss: 0.0466 - acc: 0.987 - ETA: 0s - loss: 0.0462 - acc: 0.987 - ETA: 0s - loss: 0.0457 - acc: 0.987 - ETA: 0s - loss: 0.0458 - acc: 0.987 - ETA: 0s - loss: 0.0474 - acc: 0.987 - ETA: 0s - loss: 0.0488 - acc: 0.986 - ETA: 0s - loss: 0.0484 - acc: 0.986 - ETA: 0s - loss: 0.0487 - acc: 0.986 - ETA: 0s - loss: 0.0483 - acc: 0.986 - 9s 3ms/step - loss: 0.0482 - acc: 0.9866 - val_loss: 0.0709 - val_acc: 0.9733\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.1277 - acc: 0.937 - ETA: 7s - loss: 0.0596 - acc: 0.979 - ETA: 7s - loss: 0.0370 - acc: 0.987 - ETA: 7s - loss: 0.0267 - acc: 0.991 - ETA: 7s - loss: 0.0263 - acc: 0.993 - ETA: 7s - loss: 0.0302 - acc: 0.988 - ETA: 7s - loss: 0.0259 - acc: 0.990 - ETA: 7s - loss: 0.0292 - acc: 0.991 - ETA: 7s - loss: 0.0274 - acc: 0.992 - ETA: 7s - loss: 0.0274 - acc: 0.990 - ETA: 7s - loss: 0.0948 - acc: 0.982 - ETA: 7s - loss: 0.0866 - acc: 0.983 - ETA: 7s - loss: 0.0833 - acc: 0.985 - ETA: 7s - loss: 0.0784 - acc: 0.986 - ETA: 6s - loss: 0.0766 - acc: 0.984 - ETA: 6s - loss: 0.0724 - acc: 0.985 - ETA: 6s - loss: 0.0688 - acc: 0.986 - ETA: 6s - loss: 0.0650 - acc: 0.987 - ETA: 6s - loss: 0.0619 - acc: 0.988 - ETA: 6s - loss: 0.0615 - acc: 0.987 - ETA: 6s - loss: 0.0591 - acc: 0.987 - ETA: 6s - loss: 0.0568 - acc: 0.988 - ETA: 6s - loss: 0.0543 - acc: 0.988 - ETA: 6s - loss: 0.0523 - acc: 0.989 - ETA: 6s - loss: 0.0511 - acc: 0.989 - ETA: 6s - loss: 0.0492 - acc: 0.990 - ETA: 6s - loss: 0.0476 - acc: 0.990 - ETA: 6s - loss: 0.0470 - acc: 0.990 - ETA: 5s - loss: 0.0457 - acc: 0.991 - ETA: 5s - loss: 0.0514 - acc: 0.989 - ETA: 5s - loss: 0.0498 - acc: 0.989 - ETA: 5s - loss: 0.0489 - acc: 0.989 - ETA: 5s - loss: 0.0477 - acc: 0.990 - ETA: 5s - loss: 0.0498 - acc: 0.988 - ETA: 5s - loss: 0.0485 - acc: 0.989 - ETA: 5s - loss: 0.0472 - acc: 0.989 - ETA: 5s - loss: 0.0466 - acc: 0.989 - ETA: 5s - loss: 0.0469 - acc: 0.988 - ETA: 5s - loss: 0.0462 - acc: 0.988 - ETA: 5s - loss: 0.0456 - acc: 0.989 - ETA: 5s - loss: 0.0570 - acc: 0.983 - ETA: 5s - loss: 0.0577 - acc: 0.983 - ETA: 5s - loss: 0.0563 - acc: 0.983 - ETA: 4s - loss: 0.0553 - acc: 0.984 - ETA: 4s - loss: 0.0563 - acc: 0.983 - ETA: 4s - loss: 0.0553 - acc: 0.983 - ETA: 4s - loss: 0.0552 - acc: 0.983 - ETA: 4s - loss: 0.0541 - acc: 0.983 - ETA: 4s - loss: 0.0539 - acc: 0.983 - ETA: 4s - loss: 0.0532 - acc: 0.984 - ETA: 4s - loss: 0.0522 - acc: 0.984 - ETA: 4s - loss: 0.0514 - acc: 0.984 - ETA: 4s - loss: 0.0504 - acc: 0.985 - ETA: 4s - loss: 0.0495 - acc: 0.985 - ETA: 4s - loss: 0.0486 - acc: 0.985 - ETA: 3s - loss: 0.0483 - acc: 0.986 - ETA: 3s - loss: 0.0474 - acc: 0.986 - ETA: 3s - loss: 0.0469 - acc: 0.986 - ETA: 3s - loss: 0.0462 - acc: 0.986 - ETA: 3s - loss: 0.0455 - acc: 0.987 - ETA: 3s - loss: 0.0464 - acc: 0.986 - ETA: 3s - loss: 0.0458 - acc: 0.986 - ETA: 3s - loss: 0.0454 - acc: 0.987 - ETA: 3s - loss: 0.0447 - acc: 0.987 - ETA: 3s - loss: 0.0440 - acc: 0.987 - ETA: 3s - loss: 0.0433 - acc: 0.987 - ETA: 3s - loss: 0.0433 - acc: 0.987 - ETA: 3s - loss: 0.0426 - acc: 0.987 - ETA: 2s - loss: 0.0420 - acc: 0.987 - ETA: 2s - loss: 0.0414 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.988 - ETA: 2s - loss: 0.0405 - acc: 0.988 - ETA: 2s - loss: 0.0402 - acc: 0.988 - ETA: 2s - loss: 0.0397 - acc: 0.988 - ETA: 2s - loss: 0.0392 - acc: 0.988 - ETA: 2s - loss: 0.0412 - acc: 0.988 - ETA: 2s - loss: 0.0414 - acc: 0.987 - ETA: 2s - loss: 0.0411 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0403 - acc: 0.987 - ETA: 1s - loss: 0.0398 - acc: 0.988 - ETA: 1s - loss: 0.0394 - acc: 0.988 - ETA: 1s - loss: 0.0389 - acc: 0.988 - ETA: 1s - loss: 0.0384 - acc: 0.988 - ETA: 1s - loss: 0.0380 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 1s - loss: 0.0372 - acc: 0.988 - ETA: 1s - loss: 0.0368 - acc: 0.989 - ETA: 1s - loss: 0.0364 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.989 - ETA: 1s - loss: 0.0362 - acc: 0.989 - ETA: 1s - loss: 0.0365 - acc: 0.988 - ETA: 1s - loss: 0.0361 - acc: 0.988 - ETA: 0s - loss: 0.0358 - acc: 0.989 - ETA: 0s - loss: 0.0354 - acc: 0.989 - ETA: 0s - loss: 0.0354 - acc: 0.988 - ETA: 0s - loss: 0.0358 - acc: 0.988 - ETA: 0s - loss: 0.0357 - acc: 0.988 - ETA: 0s - loss: 0.0353 - acc: 0.988 - ETA: 0s - loss: 0.0350 - acc: 0.989 - ETA: 0s - loss: 0.0347 - acc: 0.989 - ETA: 0s - loss: 0.0344 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0344 - acc: 0.989 - ETA: 0s - loss: 0.0352 - acc: 0.989 - 9s 3ms/step - loss: 0.0369 - acc: 0.9884 - val_loss: 0.0635 - val_acc: 0.9820\n",
      "Epoch 35/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 0.2854 - acc: 0.937 - ETA: 7s - loss: 0.0963 - acc: 0.979 - ETA: 7s - loss: 0.0594 - acc: 0.987 - ETA: 7s - loss: 0.0494 - acc: 0.991 - ETA: 7s - loss: 0.0422 - acc: 0.993 - ETA: 7s - loss: 0.0353 - acc: 0.994 - ETA: 7s - loss: 0.0303 - acc: 0.995 - ETA: 7s - loss: 0.0282 - acc: 0.995 - ETA: 7s - loss: 0.0265 - acc: 0.996 - ETA: 7s - loss: 0.0250 - acc: 0.996 - ETA: 7s - loss: 0.0241 - acc: 0.996 - ETA: 7s - loss: 0.0232 - acc: 0.996 - ETA: 7s - loss: 0.0212 - acc: 0.997 - ETA: 7s - loss: 0.0216 - acc: 0.997 - ETA: 7s - loss: 0.0200 - acc: 0.997 - ETA: 7s - loss: 0.0187 - acc: 0.997 - ETA: 7s - loss: 0.0178 - acc: 0.997 - ETA: 7s - loss: 0.0171 - acc: 0.998 - ETA: 7s - loss: 0.0191 - acc: 0.998 - ETA: 7s - loss: 0.0182 - acc: 0.998 - ETA: 6s - loss: 0.0174 - acc: 0.998 - ETA: 6s - loss: 0.0166 - acc: 0.998 - ETA: 6s - loss: 0.0160 - acc: 0.998 - ETA: 6s - loss: 0.0153 - acc: 0.998 - ETA: 6s - loss: 0.0153 - acc: 0.998 - ETA: 6s - loss: 0.0150 - acc: 0.998 - ETA: 6s - loss: 0.0179 - acc: 0.997 - ETA: 6s - loss: 0.0178 - acc: 0.997 - ETA: 6s - loss: 0.0184 - acc: 0.997 - ETA: 6s - loss: 0.0197 - acc: 0.996 - ETA: 6s - loss: 0.0190 - acc: 0.996 - ETA: 6s - loss: 0.0190 - acc: 0.996 - ETA: 5s - loss: 0.0186 - acc: 0.996 - ETA: 5s - loss: 0.0185 - acc: 0.997 - ETA: 5s - loss: 0.0179 - acc: 0.997 - ETA: 5s - loss: 0.0174 - acc: 0.997 - ETA: 5s - loss: 0.0171 - acc: 0.997 - ETA: 5s - loss: 0.0166 - acc: 0.997 - ETA: 5s - loss: 0.0168 - acc: 0.997 - ETA: 5s - loss: 0.0164 - acc: 0.997 - ETA: 5s - loss: 0.0162 - acc: 0.997 - ETA: 5s - loss: 0.0160 - acc: 0.997 - ETA: 5s - loss: 0.0156 - acc: 0.997 - ETA: 5s - loss: 0.0155 - acc: 0.997 - ETA: 4s - loss: 0.0153 - acc: 0.997 - ETA: 4s - loss: 0.0149 - acc: 0.997 - ETA: 4s - loss: 0.0147 - acc: 0.997 - ETA: 4s - loss: 0.0146 - acc: 0.997 - ETA: 4s - loss: 0.0146 - acc: 0.998 - ETA: 4s - loss: 0.0144 - acc: 0.998 - ETA: 4s - loss: 0.0143 - acc: 0.998 - ETA: 4s - loss: 0.0140 - acc: 0.998 - ETA: 4s - loss: 0.0138 - acc: 0.998 - ETA: 4s - loss: 0.0136 - acc: 0.998 - ETA: 4s - loss: 0.0139 - acc: 0.998 - ETA: 4s - loss: 0.0138 - acc: 0.998 - ETA: 4s - loss: 0.0137 - acc: 0.998 - ETA: 3s - loss: 0.0136 - acc: 0.998 - ETA: 3s - loss: 0.0135 - acc: 0.998 - ETA: 3s - loss: 0.0134 - acc: 0.998 - ETA: 3s - loss: 0.0133 - acc: 0.998 - ETA: 3s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0130 - acc: 0.998 - ETA: 3s - loss: 0.0129 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.998 - ETA: 3s - loss: 0.0127 - acc: 0.998 - ETA: 3s - loss: 0.0127 - acc: 0.998 - ETA: 3s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0124 - acc: 0.998 - ETA: 3s - loss: 0.0123 - acc: 0.998 - ETA: 3s - loss: 0.0121 - acc: 0.998 - ETA: 3s - loss: 0.0121 - acc: 0.998 - ETA: 3s - loss: 0.0120 - acc: 0.998 - ETA: 3s - loss: 0.0119 - acc: 0.998 - ETA: 3s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0116 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 2s - loss: 0.0125 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.998 - ETA: 2s - loss: 0.0120 - acc: 0.998 - ETA: 2s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0117 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0139 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0169 - acc: 0.996 - ETA: 0s - loss: 0.0169 - acc: 0.996 - ETA: 0s - loss: 0.0202 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - 9s 3ms/step - loss: 0.0201 - acc: 0.9954 - val_loss: 0.1224 - val_acc: 0.9755\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0462 - acc: 1.000 - ETA: 8s - loss: 0.0163 - acc: 1.000 - ETA: 8s - loss: 0.0100 - acc: 1.000 - ETA: 8s - loss: 0.0094 - acc: 1.000 - ETA: 7s - loss: 0.0076 - acc: 1.000 - ETA: 7s - loss: 0.0075 - acc: 1.000 - ETA: 7s - loss: 0.1166 - acc: 0.985 - ETA: 7s - loss: 0.1017 - acc: 0.987 - ETA: 7s - loss: 0.0900 - acc: 0.989 - ETA: 7s - loss: 0.0817 - acc: 0.990 - ETA: 7s - loss: 0.0741 - acc: 0.991 - ETA: 7s - loss: 0.0940 - acc: 0.986 - ETA: 7s - loss: 0.1045 - acc: 0.985 - ETA: 7s - loss: 0.0971 - acc: 0.986 - ETA: 7s - loss: 0.0909 - acc: 0.987 - ETA: 6s - loss: 0.0859 - acc: 0.987 - ETA: 6s - loss: 0.0807 - acc: 0.988 - ETA: 6s - loss: 0.0844 - acc: 0.987 - ETA: 6s - loss: 0.0801 - acc: 0.988 - ETA: 6s - loss: 0.0772 - acc: 0.988 - ETA: 6s - loss: 0.0827 - acc: 0.984 - ETA: 6s - loss: 0.0926 - acc: 0.982 - ETA: 6s - loss: 0.0906 - acc: 0.983 - ETA: 6s - loss: 0.0885 - acc: 0.983 - ETA: 6s - loss: 0.0877 - acc: 0.983 - ETA: 6s - loss: 0.0859 - acc: 0.984 - ETA: 6s - loss: 0.0842 - acc: 0.984 - ETA: 6s - loss: 0.0826 - acc: 0.984 - ETA: 6s - loss: 0.0830 - acc: 0.983 - ETA: 6s - loss: 0.0814 - acc: 0.984 - ETA: 6s - loss: 0.0799 - acc: 0.984 - ETA: 6s - loss: 0.0785 - acc: 0.984 - ETA: 6s - loss: 0.0757 - acc: 0.985 - ETA: 6s - loss: 0.0733 - acc: 0.985 - ETA: 6s - loss: 0.0721 - acc: 0.986 - ETA: 6s - loss: 0.0697 - acc: 0.986 - ETA: 6s - loss: 0.0679 - acc: 0.986 - ETA: 6s - loss: 0.0676 - acc: 0.986 - ETA: 5s - loss: 0.0657 - acc: 0.986 - ETA: 5s - loss: 0.0639 - acc: 0.987 - ETA: 5s - loss: 0.0621 - acc: 0.987 - ETA: 5s - loss: 0.0604 - acc: 0.987 - ETA: 5s - loss: 0.0589 - acc: 0.988 - ETA: 5s - loss: 0.0577 - acc: 0.988 - ETA: 5s - loss: 0.0563 - acc: 0.988 - ETA: 5s - loss: 0.0551 - acc: 0.989 - ETA: 5s - loss: 0.0540 - acc: 0.989 - ETA: 5s - loss: 0.0527 - acc: 0.989 - ETA: 4s - loss: 0.0522 - acc: 0.989 - ETA: 4s - loss: 0.0576 - acc: 0.987 - ETA: 4s - loss: 0.0579 - acc: 0.987 - ETA: 4s - loss: 0.0568 - acc: 0.987 - ETA: 4s - loss: 0.0556 - acc: 0.988 - ETA: 4s - loss: 0.0549 - acc: 0.988 - ETA: 4s - loss: 0.0539 - acc: 0.988 - ETA: 4s - loss: 0.0529 - acc: 0.988 - ETA: 4s - loss: 0.0520 - acc: 0.989 - ETA: 4s - loss: 0.0510 - acc: 0.989 - ETA: 4s - loss: 0.0501 - acc: 0.989 - ETA: 4s - loss: 0.0492 - acc: 0.989 - ETA: 3s - loss: 0.0484 - acc: 0.989 - ETA: 3s - loss: 0.0477 - acc: 0.990 - ETA: 3s - loss: 0.0470 - acc: 0.990 - ETA: 3s - loss: 0.0463 - acc: 0.990 - ETA: 3s - loss: 0.0456 - acc: 0.990 - ETA: 3s - loss: 0.0449 - acc: 0.990 - ETA: 3s - loss: 0.0442 - acc: 0.990 - ETA: 3s - loss: 0.0437 - acc: 0.990 - ETA: 3s - loss: 0.0433 - acc: 0.991 - ETA: 3s - loss: 0.0427 - acc: 0.991 - ETA: 3s - loss: 0.0420 - acc: 0.991 - ETA: 3s - loss: 0.0414 - acc: 0.991 - ETA: 2s - loss: 0.0409 - acc: 0.991 - ETA: 2s - loss: 0.0405 - acc: 0.991 - ETA: 2s - loss: 0.0400 - acc: 0.991 - ETA: 2s - loss: 0.0395 - acc: 0.992 - ETA: 2s - loss: 0.0390 - acc: 0.992 - ETA: 2s - loss: 0.0388 - acc: 0.992 - ETA: 2s - loss: 0.0383 - acc: 0.992 - ETA: 2s - loss: 0.0378 - acc: 0.992 - ETA: 2s - loss: 0.0373 - acc: 0.992 - ETA: 2s - loss: 0.0371 - acc: 0.992 - ETA: 2s - loss: 0.0366 - acc: 0.992 - ETA: 2s - loss: 0.0362 - acc: 0.992 - ETA: 1s - loss: 0.0358 - acc: 0.992 - ETA: 1s - loss: 0.0356 - acc: 0.992 - ETA: 1s - loss: 0.0353 - acc: 0.993 - ETA: 1s - loss: 0.0354 - acc: 0.992 - ETA: 1s - loss: 0.0350 - acc: 0.992 - ETA: 1s - loss: 0.0349 - acc: 0.992 - ETA: 1s - loss: 0.0347 - acc: 0.992 - ETA: 1s - loss: 0.0345 - acc: 0.993 - ETA: 1s - loss: 0.0341 - acc: 0.993 - ETA: 1s - loss: 0.0340 - acc: 0.993 - ETA: 1s - loss: 0.0339 - acc: 0.993 - ETA: 1s - loss: 0.0338 - acc: 0.993 - ETA: 1s - loss: 0.0334 - acc: 0.993 - ETA: 1s - loss: 0.0331 - acc: 0.993 - ETA: 0s - loss: 0.0327 - acc: 0.993 - ETA: 0s - loss: 0.0324 - acc: 0.993 - ETA: 0s - loss: 0.0321 - acc: 0.993 - ETA: 0s - loss: 0.0318 - acc: 0.993 - ETA: 0s - loss: 0.0316 - acc: 0.993 - ETA: 0s - loss: 0.0313 - acc: 0.993 - ETA: 0s - loss: 0.0310 - acc: 0.993 - ETA: 0s - loss: 0.0307 - acc: 0.993 - ETA: 0s - loss: 0.0304 - acc: 0.994 - ETA: 0s - loss: 0.0302 - acc: 0.994 - ETA: 0s - loss: 0.0300 - acc: 0.994 - ETA: 0s - loss: 0.0298 - acc: 0.994 - ETA: 0s - loss: 0.0296 - acc: 0.994 - 9s 3ms/step - loss: 0.0324 - acc: 0.9939 - val_loss: 0.0714 - val_acc: 0.9827\n",
      "Epoch 37/40\n",
      "3285/3285 [==============================] - ETA: 7s - loss: 5.0994e-04 - acc: 1.000 - ETA: 9s - loss: 0.0019 - acc: 1.0000    - ETA: 8s - loss: 0.0089 - acc: 1.000 - ETA: 8s - loss: 0.0066 - acc: 1.000 - ETA: 8s - loss: 0.0053 - acc: 1.000 - ETA: 7s - loss: 0.0083 - acc: 1.000 - ETA: 7s - loss: 0.0084 - acc: 1.000 - ETA: 7s - loss: 0.0078 - acc: 1.000 - ETA: 7s - loss: 0.0072 - acc: 1.000 - ETA: 7s - loss: 0.0065 - acc: 1.000 - ETA: 7s - loss: 0.0070 - acc: 1.000 - ETA: 7s - loss: 0.0068 - acc: 1.000 - ETA: 7s - loss: 0.0063 - acc: 1.000 - ETA: 7s - loss: 0.0058 - acc: 1.000 - ETA: 7s - loss: 0.0075 - acc: 0.997 - ETA: 7s - loss: 0.0072 - acc: 0.997 - ETA: 6s - loss: 0.0072 - acc: 0.998 - ETA: 6s - loss: 0.0069 - acc: 0.998 - ETA: 6s - loss: 0.0069 - acc: 0.998 - ETA: 6s - loss: 0.0070 - acc: 0.998 - ETA: 6s - loss: 0.0072 - acc: 0.998 - ETA: 6s - loss: 0.0070 - acc: 0.998 - ETA: 6s - loss: 0.0092 - acc: 0.997 - ETA: 6s - loss: 0.0089 - acc: 0.997 - ETA: 6s - loss: 0.0086 - acc: 0.997 - ETA: 6s - loss: 0.0083 - acc: 0.997 - ETA: 6s - loss: 0.0080 - acc: 0.997 - ETA: 6s - loss: 0.0078 - acc: 0.997 - ETA: 6s - loss: 0.0076 - acc: 0.997 - ETA: 6s - loss: 0.0073 - acc: 0.997 - ETA: 5s - loss: 0.0074 - acc: 0.997 - ETA: 5s - loss: 0.0072 - acc: 0.997 - ETA: 5s - loss: 0.0093 - acc: 0.997 - ETA: 5s - loss: 0.0091 - acc: 0.997 - ETA: 5s - loss: 0.0092 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.996 - ETA: 5s - loss: 0.0096 - acc: 0.996 - ETA: 5s - loss: 0.0094 - acc: 0.996 - ETA: 5s - loss: 0.0093 - acc: 0.996 - ETA: 5s - loss: 0.0093 - acc: 0.996 - ETA: 5s - loss: 0.0098 - acc: 0.996 - ETA: 5s - loss: 0.0102 - acc: 0.996 - ETA: 5s - loss: 0.0101 - acc: 0.996 - ETA: 4s - loss: 0.0098 - acc: 0.996 - ETA: 4s - loss: 0.0098 - acc: 0.996 - ETA: 4s - loss: 0.0112 - acc: 0.994 - ETA: 4s - loss: 0.0115 - acc: 0.995 - ETA: 4s - loss: 0.0113 - acc: 0.995 - ETA: 4s - loss: 0.0111 - acc: 0.995 - ETA: 4s - loss: 0.0111 - acc: 0.995 - ETA: 4s - loss: 0.0116 - acc: 0.995 - ETA: 4s - loss: 0.0114 - acc: 0.995 - ETA: 4s - loss: 0.0114 - acc: 0.995 - ETA: 4s - loss: 0.0118 - acc: 0.995 - ETA: 4s - loss: 0.0150 - acc: 0.994 - ETA: 4s - loss: 0.0147 - acc: 0.994 - ETA: 3s - loss: 0.0145 - acc: 0.994 - ETA: 3s - loss: 0.0144 - acc: 0.994 - ETA: 3s - loss: 0.0142 - acc: 0.995 - ETA: 3s - loss: 0.0140 - acc: 0.995 - ETA: 3s - loss: 0.0137 - acc: 0.995 - ETA: 3s - loss: 0.0135 - acc: 0.995 - ETA: 3s - loss: 0.0138 - acc: 0.994 - ETA: 3s - loss: 0.0136 - acc: 0.994 - ETA: 3s - loss: 0.0136 - acc: 0.995 - ETA: 3s - loss: 0.0150 - acc: 0.994 - ETA: 3s - loss: 0.0149 - acc: 0.994 - ETA: 3s - loss: 0.0149 - acc: 0.994 - ETA: 3s - loss: 0.0148 - acc: 0.994 - ETA: 3s - loss: 0.0149 - acc: 0.994 - ETA: 3s - loss: 0.0148 - acc: 0.994 - ETA: 2s - loss: 0.0147 - acc: 0.994 - ETA: 2s - loss: 0.0145 - acc: 0.994 - ETA: 2s - loss: 0.0143 - acc: 0.995 - ETA: 2s - loss: 0.0141 - acc: 0.995 - ETA: 2s - loss: 0.0140 - acc: 0.995 - ETA: 2s - loss: 0.0143 - acc: 0.995 - ETA: 2s - loss: 0.0141 - acc: 0.995 - ETA: 2s - loss: 0.0139 - acc: 0.995 - ETA: 2s - loss: 0.0139 - acc: 0.995 - ETA: 2s - loss: 0.0137 - acc: 0.995 - ETA: 2s - loss: 0.0147 - acc: 0.995 - ETA: 2s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0147 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0138 - acc: 0.995 - ETA: 0s - loss: 0.0137 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.994 - ETA: 0s - loss: 0.0149 - acc: 0.994 - ETA: 0s - loss: 0.0148 - acc: 0.994 - ETA: 0s - loss: 0.0148 - acc: 0.994 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0144 - acc: 0.995 - 9s 3ms/step - loss: 0.0192 - acc: 0.9948 - val_loss: 0.0765 - val_acc: 0.9784\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 8s - loss: 0.0209 - acc: 1.000 - ETA: 7s - loss: 0.0197 - acc: 1.000 - ETA: 7s - loss: 0.0141 - acc: 1.000 - ETA: 7s - loss: 0.0107 - acc: 1.000 - ETA: 7s - loss: 0.0143 - acc: 0.993 - ETA: 7s - loss: 0.0117 - acc: 0.994 - ETA: 7s - loss: 0.0101 - acc: 0.995 - ETA: 7s - loss: 0.0088 - acc: 0.995 - ETA: 7s - loss: 0.0107 - acc: 0.996 - ETA: 7s - loss: 0.0111 - acc: 0.996 - ETA: 7s - loss: 0.0108 - acc: 0.997 - ETA: 7s - loss: 0.0120 - acc: 0.997 - ETA: 7s - loss: 0.0125 - acc: 0.997 - ETA: 7s - loss: 0.0121 - acc: 0.997 - ETA: 7s - loss: 0.0221 - acc: 0.995 - ETA: 7s - loss: 0.0209 - acc: 0.995 - ETA: 6s - loss: 0.0201 - acc: 0.996 - ETA: 6s - loss: 0.0190 - acc: 0.996 - ETA: 6s - loss: 0.0222 - acc: 0.994 - ETA: 6s - loss: 0.0213 - acc: 0.995 - ETA: 6s - loss: 0.0202 - acc: 0.995 - ETA: 6s - loss: 0.0198 - acc: 0.995 - ETA: 6s - loss: 0.0189 - acc: 0.995 - ETA: 6s - loss: 0.0185 - acc: 0.995 - ETA: 6s - loss: 0.0177 - acc: 0.996 - ETA: 6s - loss: 0.0170 - acc: 0.996 - ETA: 6s - loss: 0.0166 - acc: 0.996 - ETA: 6s - loss: 0.0164 - acc: 0.996 - ETA: 6s - loss: 0.0158 - acc: 0.996 - ETA: 5s - loss: 0.0153 - acc: 0.996 - ETA: 5s - loss: 0.0148 - acc: 0.996 - ETA: 5s - loss: 0.0143 - acc: 0.996 - ETA: 5s - loss: 0.0139 - acc: 0.997 - ETA: 5s - loss: 0.0138 - acc: 0.997 - ETA: 5s - loss: 0.0135 - acc: 0.997 - ETA: 5s - loss: 0.0134 - acc: 0.997 - ETA: 5s - loss: 0.0130 - acc: 0.997 - ETA: 5s - loss: 0.0127 - acc: 0.997 - ETA: 5s - loss: 0.0126 - acc: 0.997 - ETA: 5s - loss: 0.0123 - acc: 0.997 - ETA: 5s - loss: 0.0125 - acc: 0.997 - ETA: 4s - loss: 0.0123 - acc: 0.997 - ETA: 4s - loss: 0.0122 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.997 - ETA: 4s - loss: 0.0167 - acc: 0.997 - ETA: 4s - loss: 0.0166 - acc: 0.997 - ETA: 4s - loss: 0.0164 - acc: 0.997 - ETA: 4s - loss: 0.0162 - acc: 0.997 - ETA: 4s - loss: 0.0165 - acc: 0.997 - ETA: 4s - loss: 0.0172 - acc: 0.996 - ETA: 4s - loss: 0.0182 - acc: 0.996 - ETA: 4s - loss: 0.0187 - acc: 0.995 - ETA: 4s - loss: 0.0183 - acc: 0.995 - ETA: 4s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0178 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.996 - ETA: 3s - loss: 0.0173 - acc: 0.996 - ETA: 3s - loss: 0.0172 - acc: 0.996 - ETA: 3s - loss: 0.0170 - acc: 0.996 - ETA: 3s - loss: 0.0168 - acc: 0.996 - ETA: 3s - loss: 0.0166 - acc: 0.996 - ETA: 3s - loss: 0.0167 - acc: 0.996 - ETA: 3s - loss: 0.0170 - acc: 0.995 - ETA: 3s - loss: 0.0169 - acc: 0.996 - ETA: 3s - loss: 0.0167 - acc: 0.996 - ETA: 3s - loss: 0.0165 - acc: 0.996 - ETA: 3s - loss: 0.0165 - acc: 0.996 - ETA: 2s - loss: 0.0163 - acc: 0.996 - ETA: 2s - loss: 0.0161 - acc: 0.996 - ETA: 2s - loss: 0.0163 - acc: 0.996 - ETA: 2s - loss: 0.0160 - acc: 0.996 - ETA: 2s - loss: 0.0162 - acc: 0.996 - ETA: 2s - loss: 0.0162 - acc: 0.996 - ETA: 2s - loss: 0.0160 - acc: 0.996 - ETA: 2s - loss: 0.0159 - acc: 0.996 - ETA: 2s - loss: 0.0157 - acc: 0.996 - ETA: 2s - loss: 0.0157 - acc: 0.996 - ETA: 2s - loss: 0.0160 - acc: 0.996 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 1s - loss: 0.0176 - acc: 0.996 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0191 - acc: 0.995 - ETA: 1s - loss: 0.0189 - acc: 0.995 - ETA: 1s - loss: 0.0218 - acc: 0.994 - ETA: 1s - loss: 0.0216 - acc: 0.994 - ETA: 1s - loss: 0.0214 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0211 - acc: 0.995 - ETA: 1s - loss: 0.0211 - acc: 0.995 - ETA: 0s - loss: 0.0210 - acc: 0.995 - ETA: 0s - loss: 0.0208 - acc: 0.995 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.995 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.995 - ETA: 0s - loss: 0.0213 - acc: 0.995 - ETA: 0s - loss: 0.0211 - acc: 0.995 - ETA: 0s - loss: 0.0209 - acc: 0.995 - 9s 3ms/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.1072 - val_acc: 0.9805\n",
      "Epoch 39/40\n",
      "3285/3285 [==============================] - ETA: 9s - loss: 0.0758 - acc: 0.937 - ETA: 8s - loss: 0.0285 - acc: 0.979 - ETA: 8s - loss: 0.0184 - acc: 0.987 - ETA: 7s - loss: 0.0155 - acc: 0.991 - ETA: 7s - loss: 0.0805 - acc: 0.979 - ETA: 7s - loss: 0.0669 - acc: 0.983 - ETA: 7s - loss: 0.0959 - acc: 0.976 - ETA: 7s - loss: 0.0886 - acc: 0.979 - ETA: 7s - loss: 0.0995 - acc: 0.977 - ETA: 7s - loss: 0.1022 - acc: 0.977 - ETA: 7s - loss: 0.1168 - acc: 0.967 - ETA: 7s - loss: 0.1103 - acc: 0.967 - ETA: 7s - loss: 0.1074 - acc: 0.967 - ETA: 7s - loss: 0.0996 - acc: 0.969 - ETA: 7s - loss: 0.0930 - acc: 0.972 - ETA: 6s - loss: 0.1004 - acc: 0.969 - ETA: 6s - loss: 0.0944 - acc: 0.971 - ETA: 6s - loss: 0.0902 - acc: 0.973 - ETA: 6s - loss: 0.0859 - acc: 0.974 - ETA: 6s - loss: 0.1030 - acc: 0.967 - ETA: 6s - loss: 0.1124 - acc: 0.961 - ETA: 6s - loss: 0.1073 - acc: 0.963 - ETA: 6s - loss: 0.1045 - acc: 0.963 - ETA: 6s - loss: 0.1003 - acc: 0.965 - ETA: 6s - loss: 0.0984 - acc: 0.965 - ETA: 6s - loss: 0.0953 - acc: 0.966 - ETA: 6s - loss: 0.0935 - acc: 0.967 - ETA: 6s - loss: 0.0901 - acc: 0.968 - ETA: 6s - loss: 0.0886 - acc: 0.969 - ETA: 6s - loss: 0.0904 - acc: 0.968 - ETA: 6s - loss: 0.0889 - acc: 0.969 - ETA: 6s - loss: 0.0874 - acc: 0.969 - ETA: 6s - loss: 0.0860 - acc: 0.970 - ETA: 6s - loss: 0.0846 - acc: 0.970 - ETA: 6s - loss: 0.0839 - acc: 0.970 - ETA: 6s - loss: 0.0817 - acc: 0.971 - ETA: 5s - loss: 0.0798 - acc: 0.972 - ETA: 5s - loss: 0.0776 - acc: 0.973 - ETA: 5s - loss: 0.0758 - acc: 0.974 - ETA: 5s - loss: 0.0737 - acc: 0.974 - ETA: 5s - loss: 0.0730 - acc: 0.974 - ETA: 5s - loss: 0.0711 - acc: 0.975 - ETA: 5s - loss: 0.0698 - acc: 0.976 - ETA: 5s - loss: 0.0681 - acc: 0.976 - ETA: 5s - loss: 0.0664 - acc: 0.977 - ETA: 5s - loss: 0.0657 - acc: 0.977 - ETA: 5s - loss: 0.0649 - acc: 0.977 - ETA: 5s - loss: 0.0635 - acc: 0.978 - ETA: 4s - loss: 0.0621 - acc: 0.978 - ETA: 4s - loss: 0.0609 - acc: 0.979 - ETA: 4s - loss: 0.0597 - acc: 0.979 - ETA: 4s - loss: 0.0591 - acc: 0.979 - ETA: 4s - loss: 0.0586 - acc: 0.980 - ETA: 4s - loss: 0.0581 - acc: 0.980 - ETA: 4s - loss: 0.0570 - acc: 0.980 - ETA: 4s - loss: 0.0578 - acc: 0.980 - ETA: 4s - loss: 0.0575 - acc: 0.980 - ETA: 4s - loss: 0.0569 - acc: 0.980 - ETA: 4s - loss: 0.0565 - acc: 0.980 - ETA: 4s - loss: 0.0554 - acc: 0.981 - ETA: 4s - loss: 0.0544 - acc: 0.981 - ETA: 4s - loss: 0.0534 - acc: 0.981 - ETA: 4s - loss: 0.0527 - acc: 0.982 - ETA: 4s - loss: 0.0519 - acc: 0.982 - ETA: 3s - loss: 0.0511 - acc: 0.982 - ETA: 3s - loss: 0.0506 - acc: 0.983 - ETA: 3s - loss: 0.0502 - acc: 0.982 - ETA: 3s - loss: 0.0496 - acc: 0.983 - ETA: 3s - loss: 0.0488 - acc: 0.983 - ETA: 3s - loss: 0.0481 - acc: 0.983 - ETA: 3s - loss: 0.0477 - acc: 0.984 - ETA: 3s - loss: 0.0475 - acc: 0.983 - ETA: 3s - loss: 0.0468 - acc: 0.984 - ETA: 3s - loss: 0.0461 - acc: 0.984 - ETA: 3s - loss: 0.0455 - acc: 0.984 - ETA: 2s - loss: 0.0458 - acc: 0.984 - ETA: 2s - loss: 0.0452 - acc: 0.984 - ETA: 2s - loss: 0.0445 - acc: 0.984 - ETA: 2s - loss: 0.0441 - acc: 0.984 - ETA: 2s - loss: 0.0438 - acc: 0.985 - ETA: 2s - loss: 0.0433 - acc: 0.985 - ETA: 2s - loss: 0.0427 - acc: 0.985 - ETA: 2s - loss: 0.0421 - acc: 0.985 - ETA: 2s - loss: 0.0416 - acc: 0.985 - ETA: 2s - loss: 0.0415 - acc: 0.986 - ETA: 2s - loss: 0.0409 - acc: 0.986 - ETA: 2s - loss: 0.0404 - acc: 0.986 - ETA: 2s - loss: 0.0400 - acc: 0.986 - ETA: 1s - loss: 0.0396 - acc: 0.986 - ETA: 1s - loss: 0.0391 - acc: 0.986 - ETA: 1s - loss: 0.0387 - acc: 0.987 - ETA: 1s - loss: 0.0385 - acc: 0.987 - ETA: 1s - loss: 0.0388 - acc: 0.986 - ETA: 1s - loss: 0.0384 - acc: 0.987 - ETA: 1s - loss: 0.0379 - acc: 0.987 - ETA: 1s - loss: 0.0376 - acc: 0.987 - ETA: 1s - loss: 0.0377 - acc: 0.987 - ETA: 1s - loss: 0.0373 - acc: 0.987 - ETA: 1s - loss: 0.0371 - acc: 0.987 - ETA: 1s - loss: 0.0368 - acc: 0.987 - ETA: 0s - loss: 0.0430 - acc: 0.987 - ETA: 0s - loss: 0.0426 - acc: 0.987 - ETA: 0s - loss: 0.0422 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.987 - ETA: 0s - loss: 0.0415 - acc: 0.987 - ETA: 0s - loss: 0.0411 - acc: 0.987 - ETA: 0s - loss: 0.0407 - acc: 0.987 - ETA: 0s - loss: 0.0403 - acc: 0.987 - ETA: 0s - loss: 0.0400 - acc: 0.988 - ETA: 0s - loss: 0.0397 - acc: 0.988 - ETA: 0s - loss: 0.0393 - acc: 0.988 - ETA: 0s - loss: 0.0390 - acc: 0.988 - 9s 3ms/step - loss: 0.0390 - acc: 0.9884 - val_loss: 0.0848 - val_acc: 0.9820\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - ETA: 7s - loss: 0.0063 - acc: 1.000 - ETA: 8s - loss: 0.0038 - acc: 1.000 - ETA: 8s - loss: 0.0364 - acc: 0.987 - ETA: 7s - loss: 0.0302 - acc: 0.991 - ETA: 7s - loss: 0.0251 - acc: 0.993 - ETA: 7s - loss: 0.0207 - acc: 0.994 - ETA: 7s - loss: 0.0180 - acc: 0.995 - ETA: 8s - loss: 0.0167 - acc: 0.995 - ETA: 7s - loss: 0.0180 - acc: 0.992 - ETA: 7s - loss: 0.0221 - acc: 0.989 - ETA: 7s - loss: 0.0203 - acc: 0.990 - ETA: 7s - loss: 0.0185 - acc: 0.991 - ETA: 7s - loss: 0.0171 - acc: 0.992 - ETA: 7s - loss: 0.0162 - acc: 0.992 - ETA: 7s - loss: 0.0165 - acc: 0.993 - ETA: 7s - loss: 0.0159 - acc: 0.993 - ETA: 7s - loss: 0.0159 - acc: 0.994 - ETA: 7s - loss: 0.0158 - acc: 0.994 - ETA: 6s - loss: 0.0149 - acc: 0.994 - ETA: 6s - loss: 0.0142 - acc: 0.995 - ETA: 6s - loss: 0.0136 - acc: 0.995 - ETA: 6s - loss: 0.0131 - acc: 0.995 - ETA: 6s - loss: 0.0184 - acc: 0.994 - ETA: 6s - loss: 0.0181 - acc: 0.994 - ETA: 6s - loss: 0.0174 - acc: 0.994 - ETA: 6s - loss: 0.0168 - acc: 0.995 - ETA: 6s - loss: 0.0165 - acc: 0.995 - ETA: 6s - loss: 0.0160 - acc: 0.995 - ETA: 6s - loss: 0.0157 - acc: 0.995 - ETA: 6s - loss: 0.0160 - acc: 0.995 - ETA: 6s - loss: 0.0157 - acc: 0.995 - ETA: 5s - loss: 0.0153 - acc: 0.995 - ETA: 5s - loss: 0.0149 - acc: 0.996 - ETA: 5s - loss: 0.0152 - acc: 0.996 - ETA: 5s - loss: 0.0147 - acc: 0.996 - ETA: 5s - loss: 0.0143 - acc: 0.996 - ETA: 5s - loss: 0.0141 - acc: 0.996 - ETA: 5s - loss: 0.0137 - acc: 0.996 - ETA: 5s - loss: 0.0154 - acc: 0.995 - ETA: 5s - loss: 0.0153 - acc: 0.995 - ETA: 5s - loss: 0.0150 - acc: 0.996 - ETA: 5s - loss: 0.0149 - acc: 0.996 - ETA: 5s - loss: 0.0151 - acc: 0.996 - ETA: 4s - loss: 0.0157 - acc: 0.995 - ETA: 4s - loss: 0.0155 - acc: 0.995 - ETA: 4s - loss: 0.0152 - acc: 0.995 - ETA: 4s - loss: 0.0162 - acc: 0.995 - ETA: 4s - loss: 0.0159 - acc: 0.995 - ETA: 4s - loss: 0.0156 - acc: 0.995 - ETA: 4s - loss: 0.0153 - acc: 0.995 - ETA: 4s - loss: 0.0159 - acc: 0.994 - ETA: 4s - loss: 0.0158 - acc: 0.995 - ETA: 4s - loss: 0.0157 - acc: 0.995 - ETA: 4s - loss: 0.0154 - acc: 0.995 - ETA: 4s - loss: 0.0151 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.995 - ETA: 3s - loss: 0.0145 - acc: 0.995 - ETA: 3s - loss: 0.0145 - acc: 0.995 - ETA: 3s - loss: 0.0153 - acc: 0.995 - ETA: 3s - loss: 0.0152 - acc: 0.995 - ETA: 3s - loss: 0.0155 - acc: 0.995 - ETA: 3s - loss: 0.0153 - acc: 0.995 - ETA: 3s - loss: 0.0151 - acc: 0.995 - ETA: 3s - loss: 0.0150 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0147 - acc: 0.995 - ETA: 3s - loss: 0.0145 - acc: 0.995 - ETA: 3s - loss: 0.0144 - acc: 0.995 - ETA: 2s - loss: 0.0144 - acc: 0.995 - ETA: 2s - loss: 0.0143 - acc: 0.995 - ETA: 2s - loss: 0.0142 - acc: 0.995 - ETA: 2s - loss: 0.0142 - acc: 0.995 - ETA: 2s - loss: 0.0141 - acc: 0.995 - ETA: 2s - loss: 0.0140 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0138 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0138 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0135 - acc: 0.996 - ETA: 2s - loss: 0.0135 - acc: 0.996 - ETA: 2s - loss: 0.0134 - acc: 0.996 - ETA: 2s - loss: 0.0134 - acc: 0.996 - ETA: 2s - loss: 0.0142 - acc: 0.995 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0169 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0170 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0169 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0168 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.995 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0156 - acc: 0.995 - ETA: 0s - loss: 0.0155 - acc: 0.995 - ETA: 0s - loss: 0.0154 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - 9s 3ms/step - loss: 0.0143 - acc: 0.9960 - val_loss: 0.0851 - val_acc: 0.9798\n",
      "1387/1387 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 366us/step\n"
     ]
    }
   ],
   "source": [
    "model_dyn = Sequential()\n",
    "model_dyn.add(Conv1D(filters=64, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model_dyn.add(Conv1D(filters=32, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model_dyn.add(Dropout(0.5))\n",
    "model_dyn.add(MaxPooling1D(pool_size=1,strides=1))\n",
    "\n",
    "model_dyn.add(Flatten())\n",
    "model_dyn.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_dyn.add(BatchNormalization()) \n",
    "model_dyn.add(Dropout(0.5))\n",
    "\n",
    "model_dyn.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_dyn.add(BatchNormalization()) \n",
    "model_dyn.add(Dropout(0.5))\n",
    "\n",
    "model_dyn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_dyn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dyn.fit(X_train_d, Y_train_d, epochs=40, batch_size=16,validation_data=(X_val_d, Y_val_d),verbose=1)\n",
    "\n",
    "#Evaluate the model_dyn \n",
    "score = model_dyn.evaluate(X_val_d, Y_val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[492   3   1]\n",
      " [  0 419   1]\n",
      " [  3  20 448]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix_cnn(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    #return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "    return metrics.confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix_cnn(Y_val_d, model_dyn.predict(X_val_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAIxCAYAAABaRiKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xm8FXX9+PHXG1CpXACXxAsGmqmgKEiaZmpppeKSaaXmQlZ+W8zUtEytzLJyb7P6WalZmoppIVqolS3mBm6ouKBiLK5oalkg1/fvj5mL517uBnLvueec19PHeXhm5jMz7zlzL+d935/PzERmIkmSVMv6VTsASZKk18uERpIk1TwTGkmSVPNMaCRJUs0zoZEkSTXPhEaSJNU8ExpJklTzTGgkSVLNM6GRJEk1b0C1A5AkSb0r1hqYLHq153bw0itTM3PXntvB0kxoJElqNItehW3W6bnt3zBvrZ7bePvscpIkSTXPCo0kSY0ootoRrFBWaCRJUs2zQiNJUqMJ6q6kYUIjSVIjsstJkiSpb7FCI0lSI6qvAo0VGkmSVPus0EiS1HDCMTSSJEl9jRUaSZIaTR1etl1nhyNJkhqRFRpJkhpRnY2hMaGRJKkR1Vc+Y5eTJEmqfVZoJElqNAH0q68SjRUaSZJU86zQSJLUiOqrQGOFRpIk1T4rNJIkNaI6u2zbCo0kSap5VmgkSWpE9VWgMaGRJKnheNm2JElS32OFRpKkRlRfBRorNJIkqfZZoZEkqeGEl21LkiT1NVZoJElqNF7lJEmS1PdYoZEkqRHVV4HGCo0kSap9VmgkSWpEdXaVkwmNJEmNqL7yGbucJElS7bNCI0lSo/GybUmSpL7HCo0kSY2ovgo0VmgkSVLts0IjSVIjqrPLtq3QSJKkmmeFRpKkRlRnJQ0TGkmSGk2EXU6SVCkiTo6IX5Xv14+If0dE/xW8j9kRscuK3GY39vnpiHiqPJ41X8d2/h0RG6zI2KolIu6LiJ2qHYfUHhMaqY8rv8yfiog3Vcz7RETcWMWw2pWZ/8zMVTOzudqxvB4RsRJwNvC+8ngWLO+2yvUfXXHRrXgRcWFEfLOrdpk5OjNv7IWQ1BuiB19VYEIj1YYBwOdf70ai4O99194MDATuq3YgfUFEODxBfZ7/sEm14Qzg2IgY1N7CiNguIm6PiBfK/29XsezGiDg1Im4CXgY2KOd9MyL+UXaJXB0Ra0bExRHxYrmNERXb+F5EzCmXTY+Id3UQx4iIyIgYEBHblttuef0vImaX7fpFxPER8UhELIiIyyNiSMV2Do6Ix8tlJ3b2wUTEGyLirLL9CxHx94h4Q7lsr7Kb5F/lMW9asd7siDg2Iu4p17ssIgZGxNuAB8tm/4qIP1UeV5vP9RPl+7dGxF/K7TwbEZdVtMuIeGv5fo2IuCgininjPaklwYyIiWXsZ0bE8xHxWETs1slxz46I48r4/xMRP4+IN0fE7yPipYi4ISIGV7SfFBFPljH+NSJGl/MPBz4KfLHlZ6Fi+1+KiHuA/5TndEnXX0RcGxFnVWz/sog4v7NzpT6mZRxNT7yqwIRGqg3TgBuBY9suKBOBa4DvA2tSdJVcE63HfRwMHA6sBjxeztu/nN8EbAjcDFwADAFmAl+rWP92YMty2SXApIgY2FnAmXlz2d2yKjAYuAX4dbn4SOADwI7AesDzwLnl8YwCflzGtl55TMM62dWZwFbAdmV8XwReLROTXwNHAWsD1wJXR8TKFet+GNgVGAmMASZm5kPA6HL5oMx8T2fHWfoGcF15nMOAH3TQ7gfAGsAG5bEfAnysYvk2FMnUWsDpwM8jOv122Bd4L/A2YE/g98AJ5fr9KD7nFr8HNgLWAe4ALgbIzPPK96eX52vPinUOACZQfA6L2+z7MODgiHhPRHwUeDsroIooLS8TGql2fBX4XESs3Wb+BODhzPxlZi7OzF8DD1B8wbW4MDPvK5e/Us67IDMfycwXKL7sHsnMG8ovrknA2JaVM/NXmbmgXP8sYBVg42WI/fvAf4CWasv/ASdm5tzMXAicDOxXVkD2A6Zk5l/LZV8BXm1vo2V14zDg85k5LzObM/Mf5XofAa7JzOvLYz4TeANF4rMkrsycn5nPAVdTJG3L4xXgLcB6mfm/zPx7O7H2L2P6cma+lJmzgbMoErcWj2fmT8sxSL8AhlJ0f3XkB5n5VGbOA/4G3JqZd5bHfxWtz+H55X5bPu8tImKNLo7r+5k5JzP/23ZBZj4JfKqM83vAIZn5UhfbU1/SrwdfVWBCI9WIzLwXmAIc32bRerxWdWnxOEXlpcWcdjb5VMX7/7YzvWrLRER8ISJmlt0V/6KoMqzVnbgj4v+AnYADM7MlMXkLcFXZFfQviopQM8WX93qV8Wbmf4COBuWuRTHW5ZF2lrX6XMp9z6H15/JkxfuXqTjmZfRFiqGQt5VdXId1EOvKtD5Xbc/Tkngy8+XybWcxdescRkT/iPhO2cX3IjC7IqbOtPdzU2kK0B94sL0kTupNJjRSbfka8ElafwnOp0gQKq0PzKuYzuXdYTle5ksU3TODM3MQ8ALduJahXPcbwN5lJajFHGC3zBxU8RpYVhqeAIZXbOONFN1O7XkW+B9Fl1lbrT6XsutmOK0/l+76T/n/N1bMW7flTWY+mZmfzMz1KKpPP2oZN9Mm1pZKTou256mnHAjsDexCkYyOKOe3nMOOfj66+rk5lSIZHRoRB7zOGNWbAsfQSKqezJwFXEbrsRHXAm+LiAPLgZsfAUZR/PW8IqwGLAaeAQZExFeB1btaKSKGl7EeUo5LqfQT4NSIeEvZdu2I2LtcdgWwR0RsX453OYUO/q0qqy7nA2dHxHplJWLbiFgFuByYEBE7R3EZ9heAhcA/lunoi/08Q5F4HFTu4zAqkqiI+FBEtIzzeZ4iEWhus43mMqZTI2K18tiPAX61rPEsh9Uojn0BRVL2rTbLn6IY19NtEbEDxfifQ8rXDyKiqfO11Kd42bakKjsFWHJPmvIeKXtQfGEvoOj+2CMzn11B+5tKMcbmIYoukv/RdVcEwM4UVYwr4rUrnVoug/4eMBm4LiJeohgwvE15PPcBn6UYfPwERYIwt5P9HAvMoBi4/BxwGtAvMx8EDqIYiPssxZiiPTNzUTePu61PAsdRfMajaZ0YvR24NSL+XR7X5zPzsXa28TmKas+jwN/LY+yNK4Muojh384D7KT7vSj8HRpVdgL/tamMRsXq5zSPKsUt/L7dxQReDmKUeE5nLXYmWJEk1KNZ5Q/Lh9npqV5Bz75uemeN7bgdLs0IjSZJqnnd/lCSpEdVZ76AVGkmSVPOs0EiS1GiqeDVST7FCI0mSap4VGq1QsXK/ZKA/VrVu7EabVTsErSB1NkyiIT0++588++yzK/hMBj15hX01rp/2m0cr1sAB8I51qh2FXqebrv1btUPQCuJtYWrfO7fZvke2W28JjV1OkiSp5lmhkSSpAdVb8c4KjSRJqnlWaCRJajAB9OvBEk1z101WOCs0kiSp5lmhkSSp0UT9XQFnhUaSJNU8KzSSJDWgeqvQmNBIktRwevZOwdVgl5MkSap5VmgkSWpAdVagsUIjSZJqnxUaSZIaTFB/g4Kt0EiSpJpnhUaSpEbjjfUkSZL6His0kiQ1oMAKjSRJUp9ihUaSpAZUb2NoTGgkSWpAdZbP2OUkSZJqnxUaSZIaTBD0q7MSjRUaSZJU86zQSJLUgOptULAVGkmSVPNMaCRJajTlow966tXl7iN2jYgHI2JWRBzfzvL1I+LPEXFnRNwTEbt3tU0TGkmS1Gsioj9wLrAbMAo4ICJGtWl2EnB5Zo4F9gd+1NV2HUMjSVIDquIQmq2BWZn5aBFHXArsDdxf0SaB1cv3awDzu9qoCY0kSQ0mqOqg4CZgTsX0XGCbNm1OBq6LiM8BbwJ26WqjdjlJkqQVba2ImFbxOrxiWXuZVLaZPgC4MDOHAbsDv4yITnMWKzSSJDWgHq7QPJuZ4ztYNhcYXjE9jKW7lD4O7AqQmTdHxEBgLeDpjnZohUaSJPWm24GNImJkRKxMMeh3cps2/wR2BoiITYGBwDOdbdQKjSRJDad7l1f3hMxcHBFHAFOB/sD5mXlfRJwCTMvMycAXgJ9GxNEU3VETM7Ntt1QrJjSSJKlXZea1wLVt5n214v39wDuXZZsmNJIkNZrw0QeSJEl9jhUaSZIaUJ0VaExoJElqNFW+sV6PsMtJkiTVPCs0kiQ1ICs0kiRJfYwVGkmSGlA/KzSSJEl9ixUaSZIaTdTfZdtWaCRJUs2zQiNJUoOJKj6csqeY0EiS1ICC+kpo7HKSKrx//E488PO/8PAFf+dLH/nsUsvXX6eJG067lLt/cj1/PmMSTWsNXbLsOx8/gRnn3cCM827gwzvu2Zthq43rpl7PFqPHstkmYzjz9LOWWr5w4UIOPvAQNttkDDtstxOPz34cgAULFrDrLrux9qA3c/SRx/R22Grjuj9cx5hRWzJ6480547Qzl1q+cOFCDjrgEEZvvDnv2nbHJecR4IzvnMHojTdnzKgtuX7q9b0ZtqrEhEYq9evXj3OP+Ca7nXgwoz75bg7YaW82XX+jVm3OPPwrXHTDFWzxqfdyysXn8O3Djgdg963fw7iNNmPLT72fbY7ck+M+9ClWe+Oq1TiMhtfc3MzRRx7Db6++kjvumcakSycx8/6ZrdpceP4vGDRoEPc+cA+f+/xnOemErwAwcOBAvnryV/jWaadWI3RVaG5u5qgjj+F3U67izhnTmXRZ++dx8OBB3PfgDD531BGc+OXiPM68fyaTLr+CO+6ZxuRrfsvnP3c0zc3N1TiMPi0ieuxVDSY0Umnrjbdk1vzZPPbkP3ll8Stc+pffsfd272vVZtT6G/HHO28C4M93/YO9ty2Wj3rL2/jLPbfQ/GozL//vv9z96Ex2Hb9Tbx+CgGm3TWPDDTdg5AYjWXnlldnvI/sx5eprWrW55uprOOjgjwKwz777cOOfbiQzedOb3sR222/HwIEDqxG6Ktze5jx+6MP7MWXylFZtpkyewkfL8/jBivM4ZfIUPvTh/VhllVUYMXIEG264AbffNq0KR6HeZEIjlZrWGsqcZ55YMj33mSdpWnNoqzZ3PzqTfbffHYB93rkbq79pNYasNoi7H72f3d7+bt6wykDWXH0w795iW4avvV6vxq/C/PnzaRo2bMl0U1MT8+fNX7rN8KLNgAEDWH2NNViwYEGvxqnOzZ8/n2HDK87jsCbmzX+iwzbFeVydBQsWMG/+E0utO39+658BWaHRChQR50TEURXTUyPiZxXTZ0XEMeX7oyPifxGxRsXynSKi9Z8sxfwbI2J8+X5ERDwcEe+vbB8REyPi1YgYU7HevRExony/akT8OCIeiYg7I2J6RHxyxX8KfUd7v4KZ2Wr62PO+wY5j3sEdP/oDO455B3OfeYLFzc1cP/2vXHvbn/jHd3/Hr084l5tn3sFiS9xV0facwdLPrOlOG1VX987j0utFRLsLPL/1z4Smuv4BbAcQEf2AtYDRFcu3A24q3x8A3A7s092NR8QwYCrwhcyc2k6TucCJHaz+M+B5YKPMHAvsCgzp7r5r0dxnn2D42q9VZIatvS7zn3uyVZsnnnuKfU/5JOM+sysnXnAaAC++/BIA3/r1Dxj76ffzvuMPJAgenvdY7wWvJZqampg3d+6S6Xnz5jF0vaFLt5lTtFm8eDEvvvACQ4bU9Y93zWlqamLunIrzOHce6w1dt02b9Za0Kc7jiwwZMqTV/JZ1hw5t/TOg4sZ6PfWqBhOa6rqJMqGhSGTuBV6KiMERsQqwKXBnRGwIrAqcRJHYdMe6wHXASZk5uYM2U4DREbFx5cxyf1uX674KkJnPZOZp3T+02nP7g3ezUdNIRqw7nJUGrMT+O+7N5JtbXx2x5uqDl/yl9+X9j+D8qZcBxYDiIasNAmDzkZsyZoNNuG76X3r3AATAVm/filmzHmH2Y7NZtGgRV1x2BRP22L1Vm9332J1f/fJiAK76zVXs+O4d/Qu+jxnf5jxOuvwKJuw5oVWbCXtO4OLyPF5ZcR4n7DmBSZdfwcKFC5n92GxmzXqEt289vhqHoV7kfWiqKDPnR8TiiFifIrG5GWgCtgVeAO7JzEURcQDwa+BvwMYRsU5mPt3F5i+iSEgmddLmVeB04ATg0Ir5o4G7W5KZRtH8ajNH/PArTP3WxfTv14/zp17G/Y8/xNcPOZZpD93N1bdcz05bbMe3DzuezOSvM27lsz8sClwr9V+Jv519JQAvvvxvDvrOkTS/apdTNQwYMICzv3cWe034AM3NzRwy8WBGjR7FKSd/g3FbjWOPPScw8bBD+fjET7DZJmMYPHgwF1184ZL1N3nrKF568SUWLVrE1ZOncPW1v2PTUZtW7Xga1YABAzjne2ex5+5709zczKETDynO49e+wbjxr53Hww79BKM33pzBgwfzy0t+AcCo0aPYd799Gbv5VgwYMIDvfv9s+vfvX+Uj6luKSkp9JfHRXj+lek9EXAxcDewGnE2R0GxHkdCsmZnHR8S9wD6Z+XBEnA08kpnnRsROwLGZuUebbd4IPA0MB3bOzJfL+UvaR8REYDxwFHAfRZfS1cAewBjgY5m5T7neicCHgHUyc6mRrhFxOHA4AAP7b8W71m3bRDXm5WsfrHYIWkHq7UurEb1zm+2ZPu2OFXoiBw5fI4cf9Y4VuclWZh173fTM7NWymF1O1dcyjmZzii6nWygqNNsBN5WDdjcCro+I2cD+dK/b6XTgVmBSRHRYicvMxcBZwJcqZt8PbFGO6yEzT83MLYHVO9jGeZk5PjPHs5I/UpKk3ue3T/XdRFEVeS4zmzPzOWAQRVJzM0XycnJmjihf6wFNEfGWbmz7aOBF4OfR+Z9pFwK7AGsDZOYsYBrwzYjoDxARA2n/QiBJUg3ysm2taDMorm66pc28FzLzWYqKzFVt1rmqnA+wc0TMrXht29Ioi/7EQ4GhFBWbdmXmIuD7wDoVsz8BrAnMiojpwA20ruJIktRnOCi4yjKzmTZdOZk5seL9yHbWqXzIzBva2exOFW0XAZW3u72xnH8hRWWmpd33KZKalukXgf/rxiFIkmpQvQ2vskIjSZJqnhUaSZIaUL1dAWeFRpIk1TwrNJIkNZh6vLGeFRpJklTzrNBIktSA6q1CY0IjSVIDqrN8xi4nSZJU+6zQSJLUcKr3iIKeYoVGkiTVPCs0kiQ1ICs0kiRJfYwVGkmSGow31pMkSeqDrNBIktSA6qxAY0IjSVIjsstJkiSpj7FCI0lSI7JCI0mS1LdYoZEkqeH46ANJkqQ+xwqNJEmNJupuCI0VGkmSVPus0EiS1GCC+rsPjQmNJEkNqN4SGrucJElSzbNCI0lSA7JCI0mS1MdYoZEkqQHVWYHGCo0kSap9VmgkSWo04aMPJEmS+hwrNJIkNRhvrCdJkupCvSU0djlJkqSaZ4VGkqQGZIVGkiSpj7FCI0lSowlvrCdJktTnWKGRJKkBOYZGkiSpj7FCI0lSgwl89IEkSVKfY4VGkqQGVG8VGhMaSZIaUJ3lM3Y5SZKk2meFRpKkRhP11+VkhUaSJNU8KzSSJDUiKzSSJEl9ixUaSZIaUL2NoTGh0Qo17m2bcdPv/17tMPQ6vWHXt1U7BK0g//3DQ9UOQeoVJjSSJDWYAPrVV4HGhEaSpMbjs5wkSZL6HCs0kiQ1moB+VmgkSZL6Fis0kiQ1mKD+Ltu2QiNJknpVROwaEQ9GxKyIOL6DNh+OiPsj4r6IuKSrbVqhkSSpAVWrohER/YFzgfcCc4HbI2JyZt5f0WYj4MvAOzPz+YhYp6vtWqGRJEm9aWtgVmY+mpmLgEuBvdu0+SRwbmY+D5CZT3e1USs0kiQ1oCpe5dQEzKmYngts06bN2wAi4iagP3ByZv6hs42a0EiS1GB6YVDwWhExrWL6vMw8r2L3bWWb6QHARsBOwDDgbxGxWWb+q6MdmtBIkqQV7dnMHN/BsrnA8IrpYcD8dtrckpmvAI9FxIMUCc7tHe3QMTSSJDWcoF/03KsLtwMbRcTIiFgZ2B+Y3KbNb4F3A0TEWhRdUI92tlETGkmS1GsyczFwBDAVmAlcnpn3RcQpEbFX2WwqsCAi7gf+DByXmQs6265dTpIkNZqo7o31MvNa4No2875a8T6BY8pXt1ihkSRJNc8KjSRJDSaov4pGvR2PJElqQFZoJElqQFW8sV6PMKGRJKkB+bRtSZKkPsYKjSRJDSaovy4nKzSSJKnmWaGRJKkB1Vd9xgqNJEmqA1ZoJElqON16iGRNsUIjSZJqnhUaSZIaTET9XeVkQiNJUgPyxnqSJEl9jBUaSZIaUL11OVmhkSRJNc8KjSRJDSaovxvrdZjQRMTqna2YmS+u+HAkSZKWXWcVmvuApHUS1zKdwPo9GJckSepB9TaGpsOEJjOH92YgkiRJy6tbY2giYn9gg8z8VkQMA96cmdN7NjRJktQzGvDRBxHxQ+DdwMHlrJeBn/RkUJIkScuiOxWa7TJzXETcCZCZz0XEyj0clyRJ6iER9Xen4O4kNK9ERD+KgcBExJrAqz0alSRJ6lEN1+UEnAv8Blg7Ir4O/B04rUejkiRJWgZdVmgy86KImA7sUs76UGbe27NhSZKknlRf9Znu3ym4P/AKRbeTj0uQJEl9SneucjoR+DWwHjAMuCQivtzTgUmSpJ4RFGNoeupVDd2p0BwEbJWZLwNExKnAdODbPRmYJElSd3UnoXm8TbsBwKM9E44kSeoN9XaVU2cPpzyHYszMy8B9ETG1nH4fxZVOkiRJfUJnFZqWK5nuA66pmH9Lz4UjSZJ6XjTOjfUy8+e9GYgkSeodQf1dstydq5w2jIhLI+KeiHio5dUbwUm97bo/XMeYUVsyeuPNOeO0M5davnDhQg464BBGb7w579p2Rx6f/fiSZWd85wxGb7w5Y0ZtyfVTr+/NsFXh5184k6cuv4sZ593QYZvvfeYUHr7w79z9/65n7Fs3WzL/kPfux0MX/o2HLvwbh7x3v94IV53w91HLojsJ2oXABRQJ3W7A5cClPRiTVBXNzc0cdeQx/G7KVdw5YzqTLpvEzPtntmpz4fm/YPDgQdz34Aw+d9QRnPjlrwAw8/6ZTLr8Cu64ZxqTr/ktn//c0TQ3N1fjMBrehddNYtcTDupw+W5bv4eNmkay0cTtOfy7X+LHRxYXbA5ebRBfO/hotvncnmx9xB587eCjGbTqGr0Vttrw97GHlc9y6qlXNXQnoXljZk4FyMxHMvMkiqdvS3Xl9tumseGGGzByg5GsvPLKfOjD+zFl8pRWbaZMnsJHD/4oAB/cdx9u/NONZCZTJk/hQx/ej1VWWYURI0ew4YYbcPtt06pwFPrbjFt57qV/dbh8723fx0U3XAHArTPvYNCqq7PukHV4//gduX7633j+pX/xr3+/wPXT/8aub9+pl6JWW/4+all1J6FZGEW69UhEfCoi9gTW6eG4pF43f/58hg0ftmS6aVgT8+Y/0WGbAQMGsPoaq7NgwQLmzX9iqXXnz5/fO4FrmTSttS5znn7t3Mx99gma1lqXpjXXZc4zbeavuW41QhT+PvaGRryx3tHAqsCRwKnAGsBhPRmUVA2ZudS8tqXTdpoUbbqxrvqG9s5LZrY/n3ZOuHqFv49aVl1WaDLz1sx8KTP/mZkHZ+ZemXlTbwTXIiLOiYijKqanRsTPKqbPiohjyvdHR8T/ImKNiuU7RUTrWmUx/8aIGF++HxERD0fE+yvbR8TEiHg1IsZUrHdvRIwo368aET+OiEci4s6ImB4Rn+zkWJaKJSIujIj9KmJ6MCLujoibImLjcv4e5fbvjoj7I+L/IuLEiLirfDVXvD+yYtt3R8Svu7m/2yNiy4p2h0XEjHJA+L0RsXdHx1UPmpqamDtn7pLpeXPnsd7Qddu0WW9Jm8WLF/PiCy8yZMiQVvNb1h06dGjvBK5lMveZJxi+znpLpoetNZT5C55i7rNPMHztpeerOvx97Fn1+OiDDhOaiLgqIq7s6NWbQQL/ALYr4+oHrAWMrli+HdCSZB0A3A7s092NR8QwYCrwhZbxQm3MBU7sYPWfAc8DG2XmWGBXYEh3992Bj2bmFsAvgDMiYiXgPGDPcv5Y4MbMPDUzt8zMLYH/trzPzO+Xx7UpxTneISLe1I39/Qg4o1x3WHnM22fmGOAdwD2v87j6tPFv34pZsx5h9mOzWbRoEZMuv4IJe05o1WbCnhO4+JcXA3Dlb65ix3fvSEQwYc8JTLr8ChYuXMjsx2Yza9YjvH3r8dU4DHVh8s3XccguxRVM22w6jhf+8xJPPvc0U6f9hfdttQODVl2DQauuwfu22oGp0/5S5Wgbl7+PWladdTn9sNei6NpNwDnl+9EUN/0bGhGDKe5kvClwZ0RsSNE9dhxwAsUVWl1ZF7gIOCkzJ3fQZgpFUrBxZj7YMrPc39bAgZn5KkBmPgOctmyH16G/AkcBq1GcqwXlPhYCD3ayXosDgV9SfD57UTxktDM3U3x2UIyTegn4d7nPf7e8r1cDBgzgnO+dxZ67701zczOHTjyEUaNHccrXvsG48ePYY88JTDzsUA479BOM3nhzBg8ezC8v+QUAo0aPYt/99mXs5lsxYMAAvvv9s+nfv3+Vj6gxXXLCD9lpzLastcYQ5lxyO1+76CxWGlD8U/f/pvyKa2/7E7tv8x5m/eLvvLzwf3zszGMAeP6lf/GNi7/H7T8s7iN6ysXf5flOBherZ/n72PPqrRsu2uun7IsiYjawA8Wl4wE0UXwBvwB8OzN3iIiTymWnUjxvauvMfDoidgKOzcw92mzzRmAMRTLzo4r5S9pHxERgPHAbsHNmHhoR9wJ7lOt+LDOXpRq0VCwRcSEwJTOvKGM6NjOnRcRxwPjM/EjZxbYX8EeKBOvXLUlUuY1/Z+aqbfb1EPBeYGPgiMzcq4v9HQWsk5knRER/4FqKZOiPwJWZeXUHx3Q4cDjA8PWHb/XQow909+NQH/WGXd9W7RC0gvz3D942rNa9c5vtmT7tjhWafay7ydA85OeHrshNtnLG9qdNz8xeLYvV0o0Cb6LoWtqOIpG5uWL6H2Wb/YFLyy//7ncoAAAgAElEQVT6K4EPdWO7NwAHR8Qbu2h3CfCOiBjZUYOKMS2dDafvKIOsnH9xRNwFvBM4FiAzPwHsTJFYHQuc31mwEfF24JnMfJwiIRlXVrTac3FEzAW+BPyg3F8zRffZfsBDwDkRcXK7gWeel5njM3P82muv1VlYkiT1iFpKaFrG0WxO0eV0C7BtOe+mctDuRsD1ZTVnf4rxNF05HbgVmBQRnT0KYjFwFsWXfov7gS3KcT20jGkBVu9kfwuAtonFEODZiumPlmNhPpCZcypimJGZ51BUXfbt4rgOADYpP4tHypg6WuejwEiKpO3civ1lZt6Wmd+m+Dy72qckqUZEA95YD4CIWKUnA+mGmyi6eZ7LzObMfA4YRJHU3EzxBX5yZo4oX+sBTRHxlm5s+2jgReDn0fmZuBDYBVgbIDNnAdOAb5ZdNETEQIpur448DKxXDtiljG8L4K6OViivpNqpYtaWwOMdNG8ZOP0hYEzL5wHsTScJXma+ApxEUYXaNCLWi4hx3d2nJEnV1J1nOW0dETMovoiJiC0i4gc9HtnSZlBc3XRLm3kvZOazFBWEq9qsc1U5H2DniJhb8dq2pVEWA4kOBYZSVGzalZmLgO/T+saCnwDWBGZFxHSKLqwvtbN6yzYWAgcBF5TdSlcAn8jMFzo88iJB+mJ5efVdwNeBiZ203wGYl5nzKub9FRgVER1eu5iZ/6WoQh0LrAScGREPlPv8CPD5TvYpSaoREfV32XaXg4Ij4haKL7PflpclExH3ZuZmna6ohrTV+HF5061/r3YYep0cFFw/HBRc+3piUPDQTYfmxPM/tiI32cp3tvt2rw8K7s6dgvtl5uNtemJ8ypckSTUsOh0dUXu6k9DMiYitgSzHiXyO4qoXdSIiNqe4B0ylhZm5TTXikSSpnnUnofk0xbiR9YGnKMaIfLong6oHmTmDYiCtJEl9Tr3dWK/LhCYzn+a1gbWSJKnGBdUbvNtTukxoIuKntHMzuMw8vEcikiRJWkbd6XK6oeL9QIqHPs7poK0kSaoBUVP31u1ad7qcLqucjohfAtf3WESSJEnLqDsVmrZGAt25+64kSeqjGnEMzfO8NoamH/AccHxPBiVJkrQsOk1oyucabQG03EL/1ezq1sKSJKnPq7fLtjsdEVQmL1eVD4NsNpmRJEl9UXeGON/W5qnLkiSphkUP/1cNHXY5RcSAzFwMbA98MiIeAf5D8eTnzEyTHEmSalE01qDg24BxwAd6KRZJkqTl0llCEwCZ+UgvxSJJknpJvQ0K7iyhWTsijuloYWae3QPxSJIkLbPOEpr+wKpQpdE9kiSpRwTQr4EeffBEZp7Sa5FIkiQtpy7H0EiSpHoTdTeGprN60869FoUkSdLr0GGFJjOf681AJElS72mkCo0kSVJN6PJp25Ikqf70q7OhsiY0kiQ1mMAuJ0mSpD7HCo0kSY2mDh9OaYVGkiTVPCs0kiQ1nCDqbFCwFRpJklTzrNBIktRgAugX9VXTqK+jkSRJDckKjSRJDaje7kNjQiNJUgNyULAkSVIfY4VGkqSGE95YT5Ikqa8xoZEkqcEELbfW65n/utx/xK4R8WBEzIqI4ztpt19EZESM72qbJjSSJKnXRER/4FxgN2AUcEBEjGqn3WrAkcCt3dmuCY0kSQ2oX0SPvbqwNTArMx/NzEXApcDe7bT7BnA68L9uHc+yHLwkSdLr1ATMqZieW85bIiLGAsMzc0p3N+pVTpIkNZqA6NlHH6wVEdMqps/LzPNe2/tSckloRWDnABOXZYcmNJIkNZwef9r2s5nZ0UDeucDwiulhwPyK6dWAzYAby7sZrwtMjoi9MrMySWrFLidJktSbbgc2ioiREbEysD8wuWVhZr6QmWtl5ojMHAHcAnSazIAVGkmSGk7xtO3q3FgvMxdHxBHAVKA/cH5m3hcRpwDTMnNy51tonwmNJEnqVZl5LXBtm3lf7aDtTt3ZpgmNJEkNqN6etu0YGkmSVPOs0EiS1ID69exVTr3OCo0kSap5VmgkSWowQf2NoTGh0QqVCZnZdUP1aS9cM6PaIWgFecOHR1c7BL1ej8zvus0yi56+U3Cvq6+jkSRJDckKjSRJDchBwZIkSX2MFRpJkhpMRP0NCrZCI0mSap4VGkmSGlA4hkaSJKlvsUIjSVLDCcfQSJIk9TVWaCRJakD1dh8aExpJkhpM8Syn+uqkqa+jkSRJDckKjSRJDSe8bFuSJKmvsUIjSVID8rJtSZKkPsYKjSRJDcgxNJIkSX2MFRpJkhpQvY2hMaGRJKnBBPV3p2C7nCRJUs2zQiNJUqMJn7YtSZLU51ihkSSpAUWd1TTq62gkSVJDskIjSVIDcgyNJElSH2OFRpKkBhPU36MPTGgkSWo4QT+7nCRJkvoWKzSSJDWgeutyskIjSZJqnhUaSZIakJdtS5Ik9TFWaCRJajDFZdv1VdOor6ORJEkNyQqNJEkNJ+puDI0JjSRJDaifl21LkiT1LVZoJElqNOFl25IkSX2OFRpJkhpMPT5t2wqNJEmqeVZoJElqQI6hkSRJ6mOs0EiS1HCi7h59YEIjSVID6meXkyRJUt9ihUaSpAbjZdtSnbtu6vVsMXosm20yhjNPP2up5QsXLuTgAw9hs03GsMN2O/H47McBWLBgAbvushtrD3ozRx95TG+HrTZumHoDW222NVtuuhVnn/HdpZYvXLiQiR89jC033Yr3bL8Lj8/+Z6vlc/45l/WGDOf7Z/+gt0JWO96/5Q488P0/8vAP/8yX9vnUUsvXX7uJG772K+4++/f8+eu/pmnIukuWLb58FneeeQ13nnkNvzv+p70ZtqrEhEYqNTc3c/SRx/Dbq6/kjnumMenSScy8f2arNhee/wsGDRrEvQ/cw+c+/1lOOuErAAwcOJCvnvwVvnXaqdUIXRWam5v5wue/yBWTL+e2u2/mN5f9hgdmPtCqzUUX/IpBgwZx18zpfObIT/O1E09utfzLx53ALu/fuRejVlv9+vXj3E+ewm6nTmTUUe/jgO33YtNhb23V5sxDTuCiv1zJFsfsximTvs+3D/rikmX/XfQ/xh47gbHHTmDv73yyt8OvCRHRY69qMKGRStNum8aGG27AyA1GsvLKK7PfR/ZjytXXtGpzzdXXcNDBHwVgn3334cY/3Uhm8qY3vYnttt+OgQMHViN0VZh++3Q22HAkIzcYwcorr8wHP/xBrrn6963aXHv1tRx48P4AfOCDe/OXP/+VzARgyu+uYcTIEWw6apPeDl0Vtn7rFsx68nEee2oOryx+hUv/fjV7v/29rdqMGv5W/njPPwD48703s/fbd6lGqOojTGik0vz582kaNmzJdFNTE/PnzV+6zfCizYABA1h9jTVYsGBBr8apzs2f/wRNw5uWTDc1rccT855o1eaJ+U/QNKxoM2DAAFZffXWeW/Ac//nPf/juWd/j+JO+iKqraci6zHn2tfM297knaVpz3VZt7p49k3233RWAfbZ5P6u/cTWGrDoIgIErr8Ltp/2Om799JXtv3ToREhQjaHruv2pwULBUavkLvVLb0ml32qi6Xs95/NYp3+EzR36aVVddtcfiU/e093vV9rwd+4tv8cNPfJ2JO+3HX2fextwFT7D41WYA1v+/d/LE808z8s3D+dPJlzDj8Qd59Kl/LrVN1Y8eq9BExDkRcVTF9NSI+FnF9FkRcUz5/uiI+F9ErFGxfKeImNLOdm+MiPHl+xER8XBEvL+yfURMjIhXI2JMxXr3RsSI8v2qEfHjiHgkIu6MiOkR0WEna7mf/5ZtZ0bEbRFxaJs2H4iIeyLigYiYEREfKOdvERF3VbQ7ICJejoiVyunNI+KeimObVtF2fETcWL5/Y0RcXG773oj4e0S8JSLuKl9PRsS8iumVy/X2iYiMiE0qtjsiIu6t+JxfKI/tgYg4s6LdmyNiSkTcHRH3R8S1HX1G9aCpqYl5c+cumZ43bx5D1xu6dJs5RZvFixfz4gsvMGTIkF6NU51ralqPeXPmLZmeN28+667X+i/79ZrWY97cos3ixYt58cUXGTxkMNNvn87XTjiZzd+2BT/+wU846/RzOO9HDiithrkLnmD4Wq/9/g0bsi7zn3uqVZsnnn+afc/4NOOO24MTLyn+6Xrx5ZeWLAN47Kk53HjfLYwdObqXIq8djqHpvn8A2wFERD9gLaDyJ2o74Kby/QHA7cA+3d14RAwDpgJfyMyp7TSZC5zYweo/A54HNsrMscCuQFffSo9k5tjM3BTYHzg6Ij5WxrIFcCawd2ZuAuwFnFkmVDOAt0TEauV2tgMeAMZWTN9UsZ91ImK3dvb/eeCpzNw8MzcDPg48mZlbZuaWwE+Ac1qmM3NRud4BwN/LmDvyt/JzGAvsERHvLOefAlyfmVtk5ijg+C4+o5q21du3YtasR5j92GwWLVrEFZddwYQ9dm/VZvc9dudXv7wYgKt+cxU7vntHKzR9zLjx43hk1qPMfuxxFi1axJWXX8nue+zaqs3ue+zGJb+8FIDfXvk7dtjpXUQEf/jTtcx46G5mPHQ3n/7cp/jCF4/m8M84oLQabp91DxsNHcGIdYax0oCV2H/7PZk87YZWbdZcbfCS378vf/AznP+nSQAMetPqrDxg5SVt3rnJVtw/9+HePQD1up7scroJOKd8Pxq4FxgaEYOBl4FNgTsjYkNgVeA44ATgwm5se13gIuCkzJzcQZspwA4RsXFmPtgys9zf1sCBmfkqQGY+A5zW3QPLzEfL6tJZwAXAscC3MvOxcvljEfFt4LjMPDgibge2AW4AtgLOpUhkbiv/X/lbegZwEtB6FCMMBR6viOFBuhARqwLvBN4NTAZO7uK4/ltWk1oGIAwFrqtYfk9X+6xlAwYM4OzvncVeEz5Ac3Mzh0w8mFGjR3HKyd9g3Fbj2GPPCUw87FA+PvETbLbJGAYPHsxFF1+4ZP1N3jqKl158iUWLFnH15Clcfe3v2HTUplU7nkY1YMAAzvzu6Xxwj/1obm7moIkfZdNRm3Lq17/F2HFj2X3P3Tj4Ywdx+Mc+xZabbsXgIYM5/5c/63rD6lXNrzZzxM++xtSvXET/fv04/0+TuH/Ow3x9/6OZNmsGV0+7gZ1Gv4NvH3QcmfDX+2/jsz/9KgCbDnsr/+//TuXVTPpF8J2rfsLMubOqfER9SwD96mwYbbTXl7zCNh4xG9gB2I3i82sCbgZeAL6dmTtExEnlslOBR4GtM/PpiNgJODYz92izzRuBMRTJzI8q5i9pHxETgfEUCcPOmXlo2cWyR7nuxzJzWapBI4ApZWWkZd4g4InMfENE3FFu8+6K5VsAF2TmuIg4GXiVIgGaChxaHv+HI+Jh4P1lknQjRXJ0OvAN4CXgzMzcKSK2pEguHgH+CPwiMx+u2N/JwL8zs7LL6CDg3Zn58Yj4B3BEZt5ReTxtPrfBFMnVhMx8MiLeD1wG3FnOvyAzW4+SLfZzOHA4wPD1h2/14CMz2zZRjXnl1UVdN1JNWOOAcdUOQa/Xn+aTzy9coaXgTbfcOC+47rwVuclWtn3zTtMzc3yP7aAdPZ2e3URRgdiOIpG5uWL6H2Wb/YFLy2rJlcCHurHdG4CDI+KNXbS7BHhHRIzsqEFEnFiOOVnqi7oL0eZ928ywcl7L57A1cHtmPgK8NSLWBlbNzEfbrPtNiirNEpl5F7ABRQVnCHB7RHT15/8BwKXl+0vL6fa8qxzH8yRFovNkuc+p5T5/CmxCUVFbu+3KmXleZo7PzPFrrbVWFyFJkrTi9XRC0zKOZnOKLqdbgG3LeTeVY0w2Aq4vqzn70/GXbqXTgVuBSRHRYbdZZi6mqIp8qWL2/cAW5bgeMvPUcgzK6st2aIwFWkoR91FUhCqNK/cFxXG/HdieIqmDYozP/ryW2FXG/SdgIPCONvP/nZlXZuZngF8Bu7ddt0VErAm8B/hZ+dkeB3wk2h/w8bfMHENxnj5dVoNa9vlcZl6SmQdTjHPaoaN9SpJqRf1dtt0bFZo9gOcyszkznwMGUSQ1N1MkLydn5ojytR7QFBFv6ca2jwZeBH7ewZd0iwuBXYC1ATJzFjAN+GZE9AeIiIHQ/TNQdtmcCbTcF/1M4Mvx2lVUIyjGA51V7vMlYA4wkdcSmpuBo2gnoSmdCiy5GUZEvLPsEqK8gmkUFWNq2rEfcFFmvqX8bIcDj1EkVe3KzIeAb1MmgBHxnpYqWDmoeUPA6x4lSX1OTyc0MyiubrqlzbwXMvNZigrFVW3WuYrXrsjZOSLmVry2bWmUxeCfQykGrp7eUQDl1T7fB9apmP0JYE1gVkRMp+jC+lI7q1fasLy0eSZwOfCDzLyg3Mdd5fpXR8QDwNXAF8v5LW4CVsnMOeX0zRTdOe0mNJl5LfBM5f6Bv0TEDIoxLdOA33QS7wEs/dn+Bjiwi+P8CcVg6pEUA5inld1RNwM/y8zbu1hfklQD6u2y7R4dFKzGM26rcXnTrX+rdhh6nRwUXD8cFFwHemRQ8Cb5i+t77h5L26yzQ68PCvZOwZIkNaBqjXXpKSY0FSJic+CXbWYvzMxtqhGPJEnqHhOaCpk5A9iyy4aSJNWwwAqNJEmqB3X22Jb6uu+xJElqSFZoJElqONW7AV5PsUIjSZJqnhUaSZIaULVugNdTrNBIkqSaZ4VGkqQG5BgaSZKkPsYKjSRJDajeKjQmNJIkNZjAQcGSJEl9jhUaSZIajjfWkyRJel0iYteIeDAiZkXE8e0sPyYi7o+IeyLijxHxlq62aUIjSVIDih78r9P9RvQHzgV2A0YBB0TEqDbN7gTGZ+YY4Arg9K6Ox4RGkiT1pq2BWZn5aGYuAi4F9q5skJl/zsyXy8lbgGFdbdQxNJIkNZqo6lVOTcCcium5wDadtP848PuuNmpCI0mSVrS1ImJaxfR5mXle+b69TCrb20hEHASMB3bsaocmNJIkNaAevsrp2cwc38GyucDwiulhwPy2jSJiF+BEYMfMXNjVDk1oJElqMFW+sd7twEYRMRKYB+wPHFjZICLGAv8P2DUzn+7ORh0ULEmSek1mLgaOAKYCM4HLM/O+iDglIvYqm50BrApMioi7ImJyV9u1QiNJUsOp7o31MvNa4No2875a8X6XZd2mFRpJklTzrNBIktSAfPSBJElSH2OFRpKkBlTFq5x6hBUaSZJU86zQSJLUgOptDI0JjSRJDSaov4TGLidJklTzrNBIktRwwkHBkiRJfY0VGkmSGpIVGkmSpD7FCo0kSY0mvLGeJElSn2OFRpKkBuR9aCRJkvoYKzSSJDWgeqvQmNBIktRgwhvrSZIk9T1WaCRJakD11uVkhUaSJNU8KzSSJDUgKzSSJEl9jBUaSZIakFc5SZIk9TFWaCRJakD1NobGhEaSpAZTjzfWM6HRCnXnHXc++8aVVn282nH0sLWAZ6sdhF43z2N9aITz+JZqB1ALTGi0QmXm2tWOoadFxLTMHF/tOPT6eB7rg+dx+dVbl5ODgiVJUs2zQiNJUkOyQiM1uvOqHYBWCM9jffA8CrBCIy2zzPQf0DrgeawPnsflV1/1GSs0kiSpDlihkSSpAXkfGkmSVAfqK6Gxy0mSVJMiYs2I2Ccitqp2LKo+ExqpAxHx8Yg4rmJ6XkS8GBEvRcSnqxmblk1E7BkRb6mY/mpE3B0RkyNiZDVjU/dFxJSI2Kx8PxS4FzgM+GVEHFXV4GpQ9OCrGkxopI59Cji/YvrpzFwdWBs4oDohaTmdCjwDEBF7AAdRfBFOBn5Sxbi0bEZm5r3l+48B12fmnsA2FOdTDcyERupYv8xcUDE9CSAz/we8oTohaTllZr5cvv8g8PPMnJ6ZP6NIUFUbXql4vzNwLUBmvgS8WpWIalZP1meqU6NxULDUsTUqJzLzWwAR0Q9YsyoRaXlFRKwKvEzxRfijimUDqxOSlsOciPgcMBcYB/wBICLeAKxUzcBUfVZopI5dFxHfbGf+KcB1vR2MXpfvAncB04CZmTkNICLGAk9UMzAtk48Do4GJwEcy81/l/HcAF1QrqFoUUVy23VOvarBCI3XsOOBnETELuLuctwXFl+InqhaVlllmnh8RU4F1eO1cAjxJ8eWoGpCZT1OMbWs7/88R8WgVQlIfYkIjdSAz/wMcEBEbUPxVCHB/Zj5SxbC0nDJzHjCvzezVgWOBT/Z+RFoeEbEt0AT8NTOfjogxwPHAu4DhVQ1OVWWXk9SBiFg/ItYHFlP8VX838ErFfNWIiBgTEddFxL0R8c2IeHNE/Ab4I3B/teNT90TEGRRXHu4LXBMRXwOuB24FNqpmbLUoevC/arBCI3XsGiBpPWQ/Ka6KWQfoX42gtFx+CvwYuBnYFbgDuAT4aHnVmmrDBGBsZv4vIgYD84ExmflwleNSH2BCI3UgMzevnI6IEcCXgF2Ab1UhJC2/VTLzwvL9gxFxLHB8ZjZXMSYtu/+2JKCZ+XxEPGgys/yqVUnpKSY0UhciYiPgRIqbd50FHJmZr3S+lvqYgeUVTS3/gv8bGBPl5RiZeUfVItOy2DAiJldMj6iczsy9qhCT+ggTGqkD5S3WT6QYEHw68HH/oq9ZTwJndzCdwHt6PSItj73bTJ9VlSjUJ5nQSB27G5hDMZZma2DryvsrZOaRVYpLyygzd6p2DHr9MvMv1Y5BfZcJjdSxj1P89a4aFxEf7Gx5Zl7ZW7Fo+UXEDNr/nQyKx1uM6eWQalq1boDXU0xopA5UDCJV7duzk2UJmNDUhj2qHYD6LhMaqQMRcTWdVGgcgFg7MvNjHS2LiDf3Zixafpn5eHvzI+KdwIHAZ3s3IvUlJjRSx86sdgDqGRGxBsXN2Q4ENqW486xqSERsSXH+Pgw8hlW2hmdCI3Vs5cy8vr0FEXEa4ADFGlI+kXkvii/BccBqwAeAv1YzLnVfRLwN2B84AFgAXAZEZr67qoHVpOrd0ben+OgDqWPnRsSEyhkR0S8iLqR4SKVqRERcDDwEvA/4ITACeD4zb8zMV6sZm5bJA8DOwJ6ZuX1m/gDwVgrLLXrw1ftMaKSOvQ84q+UKmfIv/MnAynQ+yFR9z2bA88BM4IHyfkJewVZ79qW4h9CfI+KnEbEz1fr2VJ9jl5PUgcycHRG7AFMjYh3gYODWzDymyqFpGWXmFhGxCUV30w0R8TSwWkSsm5lPVjk8dVNmXgVcFRFvouguPBp4c0T8GLgqM6+raoA1pHp1lJ5jhUbqQESMo3gI5ReBUylusveriBhXLlMNycwHMvOrmbkxxRfhRcBtEfGPKoemboqIAQCZ+Z/MvDgz9wCGAXcBx1c1OFVdZFp1ldoTEX/uZHFmprfLrxERcURm/rCd+QHs4B1oa0NE3JGZ/jGxAozdasv888039Nj2B6+y9vTMHN9jO2iHXU5SBzq7ciIi3tGbseh1O4xiMHArWfxFZzJTO+qtl0QrkAmNtHwuB9avdhBSg1k7Ijocw5aZZ3e0TO2pr/zQhEZaPvX1L0H9GxMRL7Yzv+UZQKv3dkBaLv2BVfH3T+0woZGWj4PPasuMzBxb7SD0uj2RmadUO4h6UW9ZoQmN1IFOnuUUwJq9HI6k+vsOrrL6+jhNaKSOdfYsJ5/zVFsmVTsArRB7R8RKmfkKQERsDOwOPJ6ZPsupwXkfGqkDmfmX9l7Ao8DW1Y5Py+SZiNgIiku1I+KCiHgxIu7xnkI15VcUj60gIt76/9u792C7yvqM498naEiMCShRtAYMCUEawiVBlMtUHaRprFzEsUPwrlFL2hEBFRFjYzvipUGngpc2UkepU1TQTGMZRKgaDCSRGAJB0IDKLeCohCAgVIhP/1jrmN1jzsneB89599r7+WTWnL3XWtnr2TmTc377Xe8FWAPMAP5e0kcL5mogIY3eVkIKmog2SJoqabGka4DvAXsXjhSdeRdwR/34VOAQYD/gLOBThTJF555h+7b68ZuAS2y/E3gFcHy5WNENUtBEDEHSZElvlPQt4AfA/sAM2zNtv6dwvOjMEwO3Kah+8V1s+37bVwOTCuaKzrT2aTsWuArA9u+ALDLa59KHJmJov6QqZJYAq21b0smFM8XI/F7Sc6kWqHw51VIWAyaWiRQjcJOk84EtVB8wvg0gac+iqaIrpIUmYmjnAhOAzwHvlzSzcJ4YuX8A1lPddlpp+0cAkl5K1ScqmuHtwK+p+tHMt/3bev9s0lG/I9XilKP3p8h7ylpOEcOTNIOq38VCYBawlGpl381Fg0VH6oUNJ9t+oGXfJKqfgw+XSxYx9uYdPter1g63XN2TM2X8M7KWU0S3kHQGsBrYaPs84DxJB1MVN1cAabFpiHqE0zJgf0mbgPfY3mL7kcLRogP1grFDfQq37ZePZZ7myzw0Ef1iGnABcKCkm4DrgGuB822fWzRZdOoLwMXANcCJwIXAq4smipHYWWf8I4Gzqfq8RQd6q5xJQRMxpIGRTJLGAy8EjqZatfnzkrbZnl0yX3Rksu3P14+XSdpQNE2MiO0fDjyu+z99ENgdOM32FcWCRVdIQROxaxOBKcAe9XYvsKlooujUBElz2fGhdGLrc9spcBpC0l9RFTKPAefZHr2OID2u1AR4oyUFTcQQJC0HDgIeAtZR3XL6ZGun0miMXwCfHOK5qeY0iS4n6XrgWVT9odbU+/4w03MK0/6WgiZiaPtSNWffRjXvxT3AtqKJYkRsv6x0hviTeAR4GHhNvbVKYdoR0Wu9aFLQRAzB9gJVbbIHUfWfeTcwR9JWYI3tpUUDRtskDe4AbKr5TDbafqhApBiBFKYxnBQ0EcNwNVHTzZK2AQ/W2/FUi1OmoGmOE3ay75nAIZIW2f7OWAeKzkm6kWoqheuAa23fUTZRs/VW+0wKmoghSTqdqmXmGOBxqiHba6iGAKdTcIPYfsvO9kt6PvA14MVjmyhG6HVU/yf/ElhaT4x43cBme13JcFFWCpqIoU0HLgPOtH1f4SwxCmzfKemppXNEe7VI748AAA0XSURBVGzfDNwMLAeQNJVqBu8zqJY+2K1cuibqrTaaFDQRQ7B9VukMMbokvQD439I5oj2SdgPmsqPldCZVh/2LqEc9RZuUYdsREY0j6Zv88ZT5zwSeC7x+7BPFCP0GuBX4DHCO7Z8XzhNdJAVNRPSDwSsxG7gfuM327wrkiZF5G3BU/fUt9bw0a6hGHW4pmiw6ImkB8Cmq24QX2f7YoOO7Uy1XcjjV/9VTdtUJPAVNRPQ826vaOU/SGttHjXaeGBnblwCXAEh6GtVow2OAj0oab/v5JfNFe+pbh5+h6tx9D3C9pJW2b2k5bRHwgO39JS0EPg6cMtzrpqCJiNhhQukAMbx6ZNOL2dGP5gjgbqpRiNGmalq9Yn1oXgTcbvtnAJK+ApwEtBY0JwEfqh9fBnxakuqpNHYqBU1ExA5D/rCM8iTdQDWD93qqodqfANbafrhosAba8MMbrpz4lElTR/ESEyStb3m+3Pby+vHzqIrQAffwx1Mn/OEc209IehDYi2pCzJ1KQRMREU3xJmDTcJ/Soz22FxS8/M6ahgZ/T9s55/8ZN+I4ERG9p7fGsfYY2zcBB0n6kqT1kq6vHx9SOlt05B5gn5bn04B7hzpH0lOAPYCtw71oCpqIiB3eUDpADE3SScAKYBXwVqrRTquAr9fHohmuB2ZJ2k/SeKrJEVcOOmclVYscVAuRfmdXLXNKy11E9DpJi4Bn2l5WP98CTKZqkTnb9udK5ov21Gs5nTR4+K6k6cB/2T60QKwYAUl/DfwL1bDtL9g+T9I/Aettr5Q0AfgPqokUtwILBzoRD/maKWgiotfV85UssH1//fwG23PrH5rftv2SsgmjHZJusT2702PRH3LLKSL6wbiBYqZ2KYDtx4CJZSLFCDwuad/BO+tFRp8okCe6SEY5RUQ/2KP1ie2PAEgaRzUUNJphKXC1pI8AP6Qa9XIEcA7wvpLBorzccoqInifps8BW20sG7f8wMNX2aWWSRackHQq8GziIqg/Uj4Dzbd9YNFgUl4ImInpePbvsRVSf5gd+8R1KNUHb2zIxW0TzpaCJiL4haQbVJ3uAW2z/tGSe6JykNwGnAwfWu24FLrB9cblU0Q3ShyYiel5LR9In2NFC84f9tu8qkSs6I+mNwBnAWcAGqltO84BlkkhR09/SQhMRPU/SJqoOpK0zARt4FvBs27sVCRYdkbSWaj6SOwbtnw58xfaRBWJFl0gLTUT0PNsHtz6vfwG+DzgO+EiBSDEyUwYXMwC275A0pUCe6CKZhyYi+oakWZK+CFxBNex3tu0Ly6aKDjw6wmPRB3LLKSJ6nqQ5wAeoOgT/M3CJ7e1lU0WnJP0WuH1nh4AZtieNcaToIiloIqLnSdoO3A1cDvxRIWP79DEPFR2rZwQeku07xypLdJ/0oYmIfrCIqhNwNFi7BYukNbaPGu080V3SQhMRET1lYPHR0jlibKWFJiJ6nqRvMkwLje0TxzBOjL58Uu9DKWgioh+cXzpARIyuFDQR0Q/G275qZwckfRxYNcZ5YnRp16dEr8k8NBHRDz4j6ZWtOySNq+ekObRMpBhFbygdIMZeCpqI6AfzgU9IejWApInASmA8cELJYNE+SYskvbfl+RZJv5H0kKTFA/tt31wmYZSUUU4R0RckTQOuBC6k+gS/zvZZZVNFJyRdDyywfX/9/AbbcyVNAL5t+yVlE0ZJ6UMTET1P0rz64dnAxcBVwJcH9tveUCpbdGTcQDFTuxTA9mN1q1v0sbTQRETPk/TdYQ7b9rFjFiZGTNLttvffyf5xwO22ZxSIFV0iBU1E9DVJR9peWzpH7JqkzwJbbS8ZtP/DwFTbp5VJFt0gBU1E9DVJd9net3SO2DVJk4CLgCOAG+vdhwLrgbfZfrhUtigvBU1E9DVJd9vep3SOaJ+kGVQrpwPcYvunJfNEd0hBExF9LS00zSFp2O+T7bvGKkt0n4xyioieN8xaTgL2GuM4MXKXU30fW2cCNvAs4NnAbiVCRXdIC01E9DxJLx3uuO0sfdBAkqYD7wOOAy6wfWHRQFFUCpqI6FuS9gEW2l5WOku0T9Is4APAi4FPAF+y/XjZVFFalj6IiL4iaaqkxZKuAb4H7F04UrRJ0hxJlwBfB64G5ti+KMVMQFpoIqIPSJoMnAy8FjgAWAGcYnta0WDREUnbgbup+tJsH3zc9uljHiq6RjoFR0Q/+CXwA2AJsNq2JZ1cOFN0bhE779wdkRaaiOh9ks4EFgKTgP8EvgpclanyI3pHCpqI6Bv1hGynUhU3s4ClwArbm4sGi7YMM/weANsnjmGc6DIpaCKi50k6A1gNbLT9RL3vYKri5hTbM0vmi/Zk+H0MJ31oIqIfTAMuAA6UdBNwHXAtcL7tc4smi06Mt33Vzg5I+jiQgqaPpYUmIvqGpPHAC4GjgaPqbZvt2UWDRVskbQbOtH15y75xwBeA59heUCxcFJcWmojoJxOBKcAe9XYvsKlooujEfOBbkna3/Q1JE4FLgd8AJ5SNFqWlhSYiep6k5VSrMz8ErAPWAmttP1A0WHRM0jTgSuBC4A3AOttnlU0V3SAzBUdEP9gX2B34BbAFuAfYVjRRdEzSPKpFKM8GzqOaZO/LkubVx6KPpYUmIvqCJFG10hxdb3OArcAa20tLZov2SPruMIdt+9gxCxNdJwVNRPSV+pbFMVRFzfHAXrb3LJsqnixJR9peWzpHlJOCJiJ6nqTTqQqYY4DHqYZsr6m/brL9+4Lx4k9A0l229y2dI8rJKKeI6AfTgcuohvzeVzhLjA6VDhBlpYUmIiIaLy00kRaaiIhohGHWchKw1xjHiS6TFpqIiGiErOUUw0lBExERjSZpH2Ch7WWls0Q5mVgvIiIaR9JUSYslXQN8D9i7cKQoLH1oIiKiESRNBk4GXgscAKwAZtieVjRYdIXccoqIiEaQ9CjwA2AJsNq2Jf3M9ozC0aIL5JZTREQ0xbnABOBzwPslzSycJ7pIWmgiIqJRJM0ATgUWArOApcAK25uLBouiUtBEREQjSDoDWA1stP1Eve9gquLmFNtpseljKWgiIqIRJJ1PtSbXgcBNwHXU63LZ3loyW5SXgiYiIhpF0njghVTFzVH1ts327KLBoqgM246IiKaZCEwB9qi3e4FNRRNFcWmhiYiIRpC0HDgIeAhYB6wF1tp+oGiw6AoZth0REU2xL7A78AtgC3APsK1oougaaaGJiIjGkCSqVpqj620OsJWqY/DSktmirBQ0ERHROJKmAcdQFTXHA3vZ3rNsqigpBU1ERDSCpNOpCphjgMeph2zXXzfZ/n3BeFFYRjlFRERTTAcuA860fV/hLNFl0kITERERjZdRThEREdF4KWgiIiKi8VLQRMSYkLRd0kZJN0u6VNLTnsRrvUzSf9ePT5R0zjDn7inp70ZwjQ9Jek+7+wed80VJr+ngWtMl3dxpxojYIQVNRIyVR20fZnsO8DvgtNaDqnT8M8n2StsfG+aUPYGOC5qIaJYUNBFRwveB/euWiVslfRbYAOwjab6kNZI21C05TweQtEDSjyWtBl498EKS3izp0/XjvSWtkHRjvR0NfAyYWbcOLavPe6+k6yXdJOkfW17rA5J+Iulq4AW7ehOS3l6/zo2Svj6o1ek4Sd+XtFnS8fX5u0la1nLtv32y/5ARUUlBExFjStJTgFewYzHBFwAX254LPAIsAY6zPQ9YD5wlaQLweeAE4C+A5wzx8hcAq2wfCswDfgScA/y0bh16r6T5wCzgRcBhwOGSXiLpcGAhMJeqYDqijbfzDdtH1Ne7FVjUcmw68FLglcC/1u9hEfCg7SPq13+7pP3auE5E7ELmoYmIsTJR0sb68feBfwf+DLjT9tp6/5HAbODaaoZ7xlNNnHYg8HPbtwFI+jLwjp1c41jgjQC2twMPSnrGoHPm19sN9fOnUxU4k4EVtn9bX2NlG+9pjqQPU93WejpwZcuxr9UTvd0m6Wf1e5gPHNLSv2aP+tqb27hWRAwjBU1EjJVHbR/WuqMuWh5p3QVcZfvUQecdBvypJs0S8FHb/zboGmeM4BpfBF5l+0ZJbwZe1nJs8Gu5vvY7bbcWPkia3uF1I2KQ3HKKiG6yFjhG0v4Akp4m6QDgx8B+kmbW5506xN//H2Bx/Xd3kzQFeIiq9WXAlcBbW/rmPE/Ss4FrgJMlTZQ0mer21q5MBu6T9FTgdYOO/Y2kcXXmGcBP6msvrs9H0gGSJrVxnYjYhbTQRETXsP2ruqXjEkm717uX2N4s6R3A5ZJ+DaymWmV5sHcByyUtArYDi22vkXRtPSz6irofzZ8Da+oWooeB19veIOmrwEbgTqrbYrvyQWBdff4m/n/h9BNgFbA3cJrtxyRdRNW3ZkO9avSvgFe1968TEcPJ0gcRERHReLnlFBEREY2XgiYiIiIaLwVNRERENF4KmoiIiGi8FDQRERHReCloIiIiovFS0ERERETjpaCJiIiIxvs/tkzvhTW6PnIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "cm = confusion_matrix_cnn(Y_val_d, model_dyn.predict(X_val_d))\n",
    "plot_confusion_matrix(cm, classes=['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS'], \n",
    "                      normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Greens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model_dyn.save('final_model_dynamic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Data directory\n",
    "    DATADIR = 'UCI_HAR_Dataset'\n",
    "    # Raw data signals\n",
    "    # Signals are from Accelerometer and Gyroscope\n",
    "    # The signals are in x,y,z directions\n",
    "    # Sensor signals are filtered to have only body acceleration\n",
    "    # excluding the acceleration due to gravity\n",
    "    # Triaxial acceleration from the accelerometer is total acceleration\n",
    "    SIGNALS = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\"\n",
    "        ]\n",
    "    # Utility function to read the data from csv file\n",
    "    def _read_csv(filename):\n",
    "        return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Utility function to load the load\n",
    "    def load_signals(subset):\n",
    "        signals_data = []\n",
    "\n",
    "        for signal in SIGNALS:\n",
    "            filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "            signals_data.append( _read_csv(filename).as_matrix()) \n",
    "\n",
    "        # Transpose is used to change the dimensionality of the output,\n",
    "        # aggregating the signals by combination of sample/timestep.\n",
    "        # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "        return np.transpose(signals_data, (1, 2, 0))\n",
    "    \n",
    "    def load_y(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        return y\n",
    "    \n",
    "    X_train, X_val = load_signals('train'), load_signals('test')\n",
    "    Y_train, Y_val = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, Y_train, X_val,  Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raftaar Singh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test Y (2947,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val,  Y_val = data()\n",
    "print('shape of test Y',Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading keras models and picle files for scaling data \n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "model_2class = load_model('final_model_2class.h5')\n",
    "model_dynamic = load_model('final_model_dynamic.h5')\n",
    "model_static = load_model('final_model_static.h5')\n",
    "scale_2class = pickle.load(open('Scale_2class.p','rb'))\n",
    "scale_static = pickle.load(open('Scale_static.p','rb'))\n",
    "scale_dynamic = pickle.load(open('Scale_dynamic.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scaling the data\n",
    "def transform_data(X,scale):\n",
    "    X_temp = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "    X_temp = scale.transform(X_temp)\n",
    "    return X_temp.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting output activity\n",
    "def predict_activity(X):\n",
    "    ##predicting whether dynamic or static\n",
    "    predict_2class = model_2class.predict(transform_data(X,scale_2class))\n",
    "    Y_pred_2class =  np.argmax(predict_2class, axis=1)\n",
    "    #static data filter\n",
    "    X_static = X[Y_pred_2class==1]\n",
    "    #dynamic data filter\n",
    "    X_dynamic = X[Y_pred_2class==0]\n",
    "    #predicting static activities\n",
    "    predict_static = model_static.predict(transform_data(X_static,scale_static))\n",
    "    predict_static = np.argmax(predict_static,axis=1)\n",
    "    #adding 4 because need to get inal prediction lable as output\n",
    "    predict_static = predict_static + 4\n",
    "    #predicting dynamic activites\n",
    "    predict_dynamic = model_dynamic.predict(transform_data(X_dynamic,scale_dynamic))\n",
    "    predict_dynamic = np.argmax(predict_dynamic,axis=1)\n",
    "    #adding 1 because need to get inal prediction lable as output\n",
    "    predict_dynamic = predict_dynamic + 1\n",
    "    ##appending final output to one list in the same sequence of input data\n",
    "    i,j = 0,0 \n",
    "    final_pred = []\n",
    "    for mask in Y_pred_2class:\n",
    "        if mask == 1:\n",
    "            final_pred.append(predict_static[i])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            final_pred.append(predict_dynamic[j])\n",
    "            j = j + 1 \n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predicting \n",
    "final_pred_val = predict_activity(X_val)\n",
    "final_pred_train = predict_activity(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train data 0.963411316648531\n",
      "Accuracy of validation data 0.9518154054971157\n"
     ]
    }
   ],
   "source": [
    "##accuracy of train and test\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of train data',accuracy_score(Y_train,final_pred_train))\n",
    "print('Accuracy of validation data',accuracy_score(Y_val,final_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[492,   1,   3,   0,   0,   0],\n",
       "       [  3, 448,  20,   0,   0,   0],\n",
       "       [  0,   1, 419,   0,   0,   0],\n",
       "       [  0,   2,   0, 437,  52,   0],\n",
       "       [  1,   0,   0,  59, 472,   0],\n",
       "       [  0,   0,   0,   0,   0, 537]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion metric\n",
    "cm = metrics.confusion_matrix(Y_val, final_pred_val,labels=range(1,7))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "labels=['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n",
    "plot_confusion_matrix(cm, classes=labels, \n",
    "                      normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Greens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, require optimal parameters are........\n",
    "\n",
    "- Using 2 layer Conv1D layer with large dropout rate and followed by 2 layer dense network, able to get desired accuracy\n",
    "\n",
    "So using reference:\n",
    "Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening\n",
    "https://www.mdpi.com/1424-8220/18/4/1055/pdf\n",
    "\n",
    "- Divide and Conquer approch with CNN is giving good result with final test accuracy of 0.9518154054971157 and train accuracy 0.96341131664853.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
